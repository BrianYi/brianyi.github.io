[{"title":"[机器学习-CS229-homework]ps2_solution","url":"%2Fposts%2F9c2a9562%2F","content":"# 下载\n作业下载: [CS 229, Public Course Problem Set #2: Kernels, SVMs, and Theory](problemset2.pdf)\n答案下载: [CS 229, Public Course Problem Set #2 Solutions: Kernels, SVMs, and Theory](ps2_solution.pdf)\n数据集下载: [PS2-data.zip](PS2-data.zip)\n\n# Kernel ridge regression\nIn contrast to ordinary least squares which has a cost function\n$$\\begin{aligned}\n    J(\\theta)=\\frac{1}{2} \\sum_{i=1}^{m}\\left(\\theta^{T} x^{(i)}-y^{(i)}\\right)^{2}\n\\end{aligned}$$\nwe can also add a term that penalizes large weights in θ. In ridge regression, our least squares cost is regularized by adding a term $\\lambda\\Vert\\theta\\Vert^2$, where $\\theta\\gt 0$ is a fixed (known) constant (regularization will be discussed at greater length in an upcoming course lecutre). The ridge regression cost function is then\n$$\\begin{aligned}\n    J(\\theta)=\\frac{1}{2} \\sum_{i=1}^{m}\\left(\\theta^{T} x^{(i)}-y^{(i)}\\right)^{2}+\\frac{\\lambda}{2}\\|\\theta\\|^{2}\n\\end{aligned}$$\n(a) Use the vector notation described in class to find a closed-form expreesion for the value of $\\theta$ which minimizes the ridge regression cost function.\n**Answer:** Using the design matrix notation, we can rewrite $J(\\theta)$ as\n$$\\begin{aligned}\n    J(\\theta)=\\frac{1}{2}(X \\theta-\\vec{y})^{T}(X \\theta-\\vec{y})+\\frac{\\lambda}{2} \\theta^{T} \\theta\n\\end{aligned}$$\nThen the gradient is\n$$\\begin{aligned}\n    \\nabla_{\\theta} J(\\theta)=X^{T} X \\theta-X^{T} \\vec{y}+\\lambda \\theta\n\\end{aligned}$$\nSetting the gradient to 0 gives us\n$$\\begin{aligned}\n    \\begin{aligned} 0 &=X^{T} X \\theta-X^{T} \\vec{y}+\\lambda \\theta \\\\ \\theta &=\\left(X^{T} X+\\lambda I\\right)^{-1} X^{T} \\vec{y} \\end{aligned}\n\\end{aligned}$$\n(b) Suppose that we want to use kernels to implicitly represent our feature vectors in a high-dimensional (possibly infinite imensional) space. Using a feature mapping φ, the ridge regression cost function becomes\n$$\\begin{aligned}\n    J(\\theta)=\\frac{1}{2} \\sum_{i=1}^{m}\\left(\\theta^{T} \\phi\\left(x^{(i)}\\right)-y^{(i)}\\right)^{2}+\\frac{\\lambda}{2}\\|\\theta\\|^{2}\n\\end{aligned}$$\nMaking a prediction on a new input $x_{new}$ would now be done by computing $\\theta^T\\phi(x_{new})$. Show how we can use the “kernel trick” to obtain a closed form for the prediction on the new input without ever explicitly computing $\\phi(x_{new})$.You may assume that the parameter vector $\\theta$ can be expressed as a linear combination of the input feature vectors; i.e., $\\theta=\\sum_{i=1}^{m} \\alpha_{i} \\phi\\left(x^{(i)}\\right)$ for some set of parameters $\\alpha_i$.\n[Hint: You may find the following identity useful:\n$$\\begin{aligned}\n    (\\lambda I+B A)^{-1} B=B(\\lambda I+A B)^{-1}\n\\end{aligned}$$\nIf you want, you can try to prove this as well, though this is not required for the problem.]\n**Answer:** Let $\\Phi$ be the design matrix associated with the feature vectors $\\phi(x^{(i)})$. Then from parts (a) and (b),\n> supplement:\n> $\\Phi=(\\phi(x^{(1)}),\\phi(x^{(2)}),...,\\phi(x^{(m)}))^T,\\Phi\\Phi^T=K$\n> $$\\begin{aligned}\n>    \\Phi\\Phi^T&=\\begin{bmatrix}\n>        (\\phi(x^{(1)}))^T\\\\\n>        (\\phi(x^{(2)}))^T\\\\\n>        \\vdots\\\\\n>        (\\phi(x^{(m)}))^T\\\\\n>    \\end{bmatrix}\\cdot\\begin{bmatrix}\n>        \\phi(x^{(1)})&\\phi(x^{(2)})&\\cdots&\\phi(x^{(m)})\n>    \\end{bmatrix}\\\\\n>    &=\\begin{bmatrix}\n>        \\lang\\phi(x^{(1)}),\\phi(x^{(1)})\\rang&\\lang\\phi(x^{(1)}),\\phi(x^{(2)})\\rang&\\cdots&\\lang\\phi(x^{(1)}),\\phi(x^{(m)})\\rang\\\\\n>        \\vdots&\\vdots&\\cdots&\\vdots\\\\\n>        \\lang\\phi(x^{(m)}),\\phi(x^{(1)})\\rang&\\lang\\phi(x^{(m)}),\\phi(x^{(2)})\\rang&\\cdots&\\lang\\phi(x^{(m)}),\\phi(x^{(m)})\\rang\n>    \\end{bmatrix}\\\\\n>    &=K\n>\\end{aligned}$$\n\n$$\\begin{aligned}\n    \\begin{aligned} \\theta &=\\left(\\Phi^{T} \\Phi+\\lambda I\\right)^{-1} \\Phi^{T} \\vec{y} \\\\ &=\\Phi^{T}\\left(\\Phi \\Phi^{T}+\\lambda I\\right)^{-1} \\vec{y} \\\\ &=\\Phi^{T}(K+\\lambda I)^{-1} \\vec{y} \\end{aligned}\n\\end{aligned}$$\nwhere $K$ is the kernel matrix for the training set (since $(\\Phi\\Phi^T)_{i,j}=\\phi(x^{(i)})^T\\phi(x^{(j)})=\\lang \\phi(x^{(i)}),\\phi(x^{(j)})\\rang=K_{ij}$ .)\nTo predict a new value ynew, we can compute\n> supplement:\n> $\\Phi=(\\phi(x^{(1)}),\\phi(x^{(2)}),...,\\phi(x^{(m)}))^T$\n> $$\\begin{aligned}\n>    \\Phi\\phi(x_{new})&=\\begin{bmatrix}\n>        (\\phi(x^{(1)}))^T\\\\\n>        (\\phi(x^{(2)}))^T\\\\\n>        \\vdots\\\\\n>        (\\phi(x^{(m)}))^T\\\\\n> \\end{bmatrix}\\phi(x_{new})=\\begin{bmatrix}\n>     \\lang \\phi(x^{(1)}),\\phi(x_{new})\\rang\\\\\n>     \\lang \\phi(x^{(2)}),\\phi(x_{new})\\rang\\\\\n>     \\vdots\\\\\n>     \\lang \\phi(x^{(m)}),\\phi(x_{new})\\rang\\\\\n> \\end{bmatrix}\\\\\n> &=\\begin{bmatrix}\n>     K(x^{(1)},x_{new})\\\\\n>     K(x^{(2)},x_{new})\\\\\n>     \\vdots\\\\\n>     K(x^{(m)},x_{new})\\\\\n> \\end{bmatrix}\\\\\n> 1^T\\Phi\\phi(x_{new})&=\\sum_{i=1}^mK(x^{(i)},x_{new})\\\\\n> ((K+\\lambda I)^{-1}\\vec{y})^T(\\Phi\\phi(x_{new}))&=\\sum_{i=1}^m((K+\\lambda I)^{-1}\\vec{y})_i(\\Phi\\phi(x_{new}))_i\\\\\n> &=\\sum_i^m\\alpha_iK(x^{(i)},x_{new})\n> \\end{aligned}$$\n\n\n$$\\begin{aligned}\n    \\begin{aligned} \\vec{y}_{\\text { new }} &=\\theta^{T} \\phi\\left(x_{\\text { new }}\\right) \\\\ &=\\vec{y}^{T}(K+\\lambda I)^{-1} \\Phi \\phi\\left(x_{\\text { new }}\\right) \\\\ &=\\sum_{i=1}^{m} \\alpha_{i} K\\left(x^{(i)}, x_{\\text { new }}\\right) \\end{aligned}\n\\end{aligned}$$\nwhere $\\alpha=(K+\\lambda I)^{-1} \\vec{y}$. All these terms can be efficiently computing using the kernel function.\nTo prove the identity from the hint, we left-multiply by $\\lambda(I + BA)$ and right-multiply by $\\lambda(I + AB)$ on both sides. That is,\n$$\\begin{aligned}\n    \\begin{aligned}(\\lambda I+B A)^{-1} B &=B(\\lambda I+A B)^{-1} \\\\ B &=(\\lambda I+B A) B(\\lambda I+A B)^{-1} \\\\ B(\\lambda I+A B) &=(\\lambda I+B A) B \\\\ \\lambda B+B A B &=\\lambda B+B A B \\end{aligned}\n\\end{aligned}$$\nThis last line clearly holds, proving the identity.\n# $\\ell_{2}$norm soft margin SVMs\nIn class, we saw that if our data is not linearly separable, then we need to modify our\nsupport vector machine algorithm by introducing an error margin that must be minimized.\nSpecifically, the formulation we have looked at is known as the $\\ell_1$ norm soft margin SVM. In this problem we will consider an alternative method, known as the $\\ell_2$ norm soft margin SVM. This new algorithm is given by the following optimization problem (notice that the slack penalties are now squared):\n$$\\begin{aligned}\n    \\begin{array}{cl}{\\min _{w, b, \\xi}} & {\\frac{1}{2}\\|w\\|^{2}+\\frac{C}{2} \\sum_{i=1}^{m} \\xi_{i}^{2}} \\\\ {\\mathrm{s.t.}} & {y^{(i)}\\left(w^{T} x^{(i)}+b\\right) \\geq 1-\\xi_{i}, \\quad i=1, \\ldots, m}\\end{array}\n\\end{aligned}$$\n(a) Notice that we have dropped the $\\xi_i\\ge{0}$ constraint in the $\\ell_2$ problem. Show that these non-negativity constraints can be removed. That is, show that the optimal value of the objective will be the same whether or not these constraints are present.\n**Answer:** Consider a potential solution to the above problem with some $\\xi\\lt{0}$. Then\nthe constraint $y^{(i)}\\left(w^{T} x^{(i)}+b\\right) \\geq 1-\\xi_{i}$ would also be satisfied for $\\xi_i=0$, and the\nobjective function would be lower, proving that this could not be an optimal solution.\n(b) What is the Lagrangian of the $\\ell_2$ soft margin SVM optimization problem?\n**Answer:** \n$$\\begin{aligned}\n    \\mathcal{L}(w, b, \\xi, \\alpha)=\\frac{1}{2} w^{T} w+\\frac{C}{2} \\sum_{i=1}^{m} \\xi_{i}^{2}-\\sum_{j=1}^{m} \\alpha_{i}\\left[y^{(i)}\\left(w^{T} x^{(i)}+b\\right)-1+\\xi_{i}\\right]\n\\end{aligned}$$\nwhere $\\alpha_i\\ge{0}$ for $i=1,\\cdots,m$.\n(c) Minimize the Lagrangian with respect to w, b, and ξ by taking the following gradients:$\\nabla_{w} \\mathcal{L}, \\frac{\\partial \\mathcal{L}}{\\partial b}$, and $\\nabla_{\\xi} \\mathcal{L}$, and then setting them equal to 0. Here, $\\xi=\\left[\\xi_{1}, \\xi_{2}, \\ldots, \\xi_{m}\\right]^{T}$.\n**Answer:** Taking the gradient with respect to $w$, we get\n$$\\begin{aligned}\n    0=\\nabla_{w} \\mathcal{L}=w-\\sum_{i=1}^{m} \\alpha_{i} y^{(i)} x^{(i)}\n\\end{aligned}$$\nwhich gives us\n$$\\begin{aligned}\n    w=\\sum_{i=1}^{m} \\alpha_{i} y^{(i)} x^{(i)}\n\\end{aligned}$$\nTaking the derivative with respect to $b$, we get\n$$\\begin{aligned}\n    0=\\frac{\\partial \\mathcal{L}}{\\partial b}=-\\sum_{i=1}^{m} \\alpha_{i} y^{(i)}\n\\end{aligned}$$\ngiving us\n$$\\begin{aligned}\n    0=\\sum_{i=1}^{m} \\alpha_{i} y^{(i)}\n\\end{aligned}$$\nFinally, taking the gradient with respect to $\\xi$, we have\n$$\\begin{aligned}\n    0=\\nabla_{\\xi} \\mathcal{L}=C \\xi-\\alpha\n\\end{aligned}$$\nwhere $\\alpha=\\left[\\alpha_{1}, \\alpha_{2}, \\ldots, \\alpha_{m}\\right]^{T}$. Thus, for each $i=1,\\cdots,m$, we get\n$$\\begin{aligned}\n    0=C \\xi_{i}-\\alpha_{i} \\quad \\Rightarrow \\quad C \\xi_{i}=\\alpha_{i}\n\\end{aligned}$$\n(d) What is the dual of the $\\ell_2$ soft margin SVM optimization problem?\n**Answer:** The objective function for the dual is\n$$\\begin{aligned}\nW(\\alpha) =&\\min _{w, b, \\xi} \\mathcal{L}(w, b, \\xi, \\alpha) \\\\ =&\\frac{1}{2} \\sum_{i=1}^{m} \\sum_{j=1}^{m}\\left(\\alpha_{i} y^{(i)} x^{(i)}\\right)^{T}\\left(\\alpha_{j} y^{(j)} x^{(j)}\\right)+\\frac{1}{2} \\sum_{i=1}^{m} \\frac{\\alpha_{i}}{\\xi_{i}} \\xi_{i}^{2}\\\\&-\\sum_{i=1}^{m} \\alpha_{i}\\left[y^{(i)}\\left(\\left(\\sum_{j=1}^{m} \\alpha_{j} y^{(j)} x^{(j)}\\right)^{T} x^{(i)}+b\\right)-1+\\xi_{i}\\right]\\\\=&-\\frac{1}{2} \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\alpha_{i} \\alpha_{j} y^{(i)} y^{(j)}\\left(x^{(i)}\\right)^{T} x^{(j)}+\\frac{1}{2} \\sum_{i=1}^{m} \\alpha_{i} \\xi_{i}\\\\&-\\left(\\sum_{i=1}^{m} \\alpha_{i} y^{(i)}\\right) b+\\sum_{i=1}^{m} \\alpha_{i}-\\sum_{i=1}^{m} \\alpha_{i} \\xi_{i} \\\\ =&\\sum_{i=1}^{m} \\alpha_{i}-\\frac{1}{2} \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\alpha_{i} \\alpha_{j} y^{(i)} y^{(j)}\\left(x^{(i)}\\right)^{T} x^{(j)}-\\frac{1}{2} \\sum_{i=1}^{m}\\alpha_{i}\\xi_i \\\\ =&\\sum_{i=1}^{m} \\alpha_{i}-\\frac{1}{2} \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\alpha_{i} \\alpha_{j} y^{(i)} y^{(j)}\\left(x^{(i)}\\right)^{T} x^{(j)}-\\frac{1}{2} \\sum_{i=1}^{m} \\frac{\\alpha_{i}^{2}}{C} \n\\end{aligned}$$\nThen the dual formulation of our problem is\n$$\\begin{aligned}\n    \\begin{array}{cl}{\\displaystyle\\max _{\\alpha}} & {\\sum_{i=1}^{m} \\alpha_{i}-\\frac{1}{2} \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\alpha_{i} \\alpha_{j} y^{(i)} y^{(j)}\\left(x^{(i)}\\right)^{T} x^{(j)}-\\frac{1}{2} \\sum_{i=1}^{m} \\frac{\\alpha_{i}^{2}}{C}} \\\\ {\\text { s.t. }} & {\\alpha_{i} \\geq 0, \\quad i=1, \\ldots, m} \\\\ {} & {\\sum_{i=1}^{m} \\alpha_{i} y^{(i)}=0}\\end{array}\n\\end{aligned}$$\n# SVM with Gaussian kernel\nConsider the task of training a support vector machine using the Gaussian kernel $K(x, z) =\n\\exp(−\\frac{\\Vert x-z\\Vert^2}{\\tau^2})$. We will show that as long as there are no two identical points in the training set, we can always find a value for the bandwidth parameter τ such that the SVM achieves zero training error.\n(a). Recall from class that the decision function learned by the support vector machine\ncan be written as\n$$\\begin{aligned}\n    f(x)=\\sum_{i=1}^{m} \\alpha_{i} y^{(i)} K\\left(x^{(i)}, x\\right)+b\n\\end{aligned}$$\nAssume that the training data $\\left\\{\\left(x^{(1)}, y^{(1)}\\right), \\ldots,\\left(x^{(m)}, y^{(m)}\\right)\\right\\}$ consists of points which\nare separated by at least a distance of $\\epsilon$; that is, $\\Vert x^{(j)}-x^{(i)}\\vert\\ge{\\epsilon}$ for any $i\\neq{j}$. Find values for the set of parameters $\\{\\alpha_1,\\cdots,\\alpha_m,b\\}$ and Gaussian kernel width $\\tau$ such that $x^{(i)}$ is correctly classified, for all $i=1,\\cdots,m.$ [Hint: Let $\\alpha_i=1$ for all $i$ and $b=0$. Now notice that for $y\\in\\{-1,+1\\}$ the prediction on $x^{(i)}$ will be correct if $\\vert f(x^{(i)})-y^{(i)}\\vert\\lt 1$, so find a value of $\\tau$ that satisfies this inequality for all $i$.]\n**Answer:** First we set $\\alpha_i=1$ for all $i=1,\\cdots,m$ and $b=0$. Then, for a training example $\\{x^{(i)},y^{(i)}\\}$, we get\n$$\\begin{aligned} \\left| f \\left( x ^ { ( i ) } \\right) - y ^ { ( i ) } \\right| &= \\left| \\sum _ { j = 1 } ^ { m } y ^ { ( j ) } K \\left( x ^ { ( j ) } , x ^ { ( i ) } \\right) - y ^ { ( i ) } \\right| \\\\ &= \\left| \\sum _ { j = 1 } ^ { m } y ^ { ( j ) } \\exp \\left( - \\left\\| x ^ { ( j ) } - x ^ { ( i ) } \\right\\| ^ { 2 } / \\tau ^ { 2 } \\right) - y ^ { ( i ) } \\right| \\\\&= \\left| y ^ { ( i ) } + \\sum _ { j \\neq i } y ^ { ( j ) } \\exp \\left( \\left\\| x ^ { ( j ) } - x ^ { ( i ) } \\right\\| ^ { 2 } / \\tau ^ { 2 } \\right) - y ^ { ( i ) } \\right|\\\\&= \\left| \\sum _ { j \\neq i } y ^ { ( j ) } \\exp \\left( - \\left\\| x ^ { ( j ) } - x ^ { ( i ) } \\right\\| ^ { 2 } / \\tau ^ { 2 } \\right) \\right| \\\\ &\\leq \\sum _ { j \\neq i } \\left| y ^ { ( j ) } \\exp \\left( - \\left\\| x ^ { ( j ) } - x ^ { ( i ) } \\right\\| ^ { 2 } / \\tau ^ { 2 } \\right) \\right|  \\\\ &= \\sum _ { j \\neq i } \\left| y ^ { ( j ) } \\right| \\cdot \\exp \\left( \\left\\| x ^ { ( j ) } - x ^ { ( i ) } \\right\\| ^ { 2 } / \\tau ^ { 2 } \\right) | \\\\ &= \\sum _ { j \\neq i } \\exp \\left( - \\left\\| x ^ { ( j ) } - x ^ { ( i ) } \\right\\| ^ { 2 } / \\tau ^ { 2 } \\right) \\\\ &\\leq \\sum _ { j \\neq i } \\exp \\left( - \\epsilon ^ { 2 } / \\tau ^ { 2 } \\right)\n\\end{aligned}$$\nThe first inequality comes from repeated application of the triangle inequality $| a + b | \\leq |a|+|b|$, and the second inequality (1) from the assumption that $\\Vert x^{(j)}-x^{(i)}\\Vert\\ge \\epsilon$ for all $i\\neq j$. Thus we need to choose a $\\gamma$ such that\n$$\\begin{aligned}\n    ( m - 1 ) \\exp \\left( - \\epsilon ^ { 2 } / \\tau ^ { 2 } \\right) < 1\n\\end{aligned}$$\nor\n$$\\begin{aligned}\n    \\tau < \\frac { \\epsilon } { \\log ( m - 1 ) }\n\\end{aligned}$$\nBy choosing, for example, $\\tau = \\epsilon / \\log m$ we are done.\n(b). Suppose we run a SVM with slack variables using the parameter $\\tau$ you found in part (a). Will the resulting classifier necessarily obtain zero training error? Why or why not? A short explanation (without proof) will suffice.\n**Answer:** The classifier will obtain zero training error. The SVM without slack variables will always return zero training error if it is able to find a solution, so all that remains to be shown is that there exists at least one feasible point.\nConsider the constraint $y ^ { ( i ) } \\left( w ^ { T } x ^ { ( i ) } + b \\right)$ for some $i$, and let $b=0$. Then\n$$\\begin{aligned}\n    y ^ { ( i ) } \\left( w ^ { T } x ^ { ( i ) } + b \\right) = y ^ { ( i ) } \\cdot f \\left( x ^ { ( i ) } \\right) > 0\n\\end{aligned}$$\nsince $f(x^{(i)})$ and $y^{(i)}$ have the same sign, and shown above. Therefore, as we choose all the $\\alpha_i$'s large enough, $y ^ { ( i ) } \\left( w ^ { T } x ^ { ( i ) } + b \\right) > 1$, so the optimization problem is feasible.\n(c).Suppose we run the SMO algorithm to train an SVM with slack variables, under the conditions stated above, using the value of $\\tau$ you picked in the previous part, and using some arbitrary value of C (which you do not know beforehand). Will this necessarily result in a classifier that achieve zero training error? Why or why not? Again, a short explanation is sufficient.\n**Answer:** The resulting classifier will not necessarily obtain zero training error. The C parameter controls the relative weights of the $(C\\sum_{i=1}^m\\xi_i)$ and $(\\frac{1}{2}\\Vert w\\Vert^2)$ terms of the SVM training objective. If the C parameter is sufficiently small, then the former component will\nhave relatively little contribution to the objective. In this case, a weight vector which has a very small norm but does not achieve zero training error may achieve a lower objective value than one which achieves zero training error. For example, you can consider the extreme case where $C=0$, and the objective is just the norm of $w$. In this case, $w = 0$ is the solution to the optimization problem regardless of the choise of $\\tau$, this this may not obtain zero training error.\n# 参考","tags":["homework"],"categories":["CS229"]},{"title":"[paper]Semi-Supervised Learning Literature Survey","url":"%2Fposts%2F6ff0c14%2F","content":"\n# FAQ\nQ: 什么是半监督学习?\nA: 无标记数据较易获得.半监督学习主要是通过使用大量标记和无标记数据来构建更好的分类器,解决这个问题.半监督分类的表亲，半监督聚类和回归，在第11.3和11.4节中作了简要讨论。\nQ: 我们真的能从未标注的数据中学到什么吗？听起来很神奇。\nA: 是的，在某些假设下我们可以。它不是魔术，而是问题结构与模型假设的良好匹配。\n即使您(或您的领域专家)没有花那么多时间在训练数据的标签上，您也需要花费合理的时间为半监督学习设计好的模型/特征/核/相似函数的工作量。在我看来，这种努力比为监督学习弥补缺乏标签的训练数据更关键.\nQ：目前有多少种半监督学习方法？\nA: 许多。一些常用的方法包括：具有生成混合模型的EM、自训练、协同训练、transductive support vector machines和基于图的方法。有关更多方法，请参见以下部分.\n# 参考","tags":["paper"],"categories":["paper"]},{"title":"[paper]Semi-Supervised Learning by Augmented Distribution Alignment","url":"%2Fposts%2F20152674%2F","content":"# Keywords\n1. adversarial training strategy\n2. sample interpolation stratrgy\n3. kernel density estimations\n4. two-moon distribution\n\n# Abstract\nQ1:我们发现，由于标记样本的数量有限，在半监督学习中存在着本质的抽样偏差，这往往导致标记数据和未标记数据之间的经验分布不匹配。\nA1:一方面，我们采用了一种对抗训练策略，以最大限度地减少受领域适应性作品启发的标记和未标记数据之间的分布距离。\n另一方面，针对标记数据的小样本问题，提出了一种简单的插值策略来生成伪训练样本。\n# Introduction\n我们的方法旨在解决经验分布不匹配的问题，方法是在潜在空间(右上)调整样本的分布，并用标记和未标记数据之间的插值增强训练样本(右下角)。\n虽然已经提出了许多利用未标记数据来提高模型性能的策略，但在文献中很少讨论SSL(Semi-Supervised Learning)中的基本抽样偏差问题。也就是说，由于标记数据的抽样规模有限，标记数据的经验分布往往偏离真实样本分布。\nContributions:\n1. 我们提供了一个新的经验分布不匹配的观点来理解半监督学习。SSL场景中普遍存在的经验分布失配问题，但现有的半监督学习工作还没有揭示出来。\n2. 我们提出了一种增强分布一致性的方法，以显式的解决SSL的经验分布不匹配问题。\n3. 我们的方法可以很容易地实现到现有的神经网络的SSL，无需花费太多功夫。\n4. 尽管简单，但我们提出的方法在用于SSL任务的基准SVHN和CIFAR 10数据集上实现了新的最先进的分类性能。\n# Related Work\n## Semi-supervised learning\n相关方法包括,label propagation, graph regularization, co-training, etc.\n# 参考","tags":["paper"],"categories":["paper"]},{"title":"[机器学习-CS229-homework]ps1_solution","url":"%2Fposts%2F771d2e61%2F","content":"# 下载\n作业下载: [CS 229, Public Course Problem Set #1: Supervised Learning](problemset1.pdf)\n答案下载: [CS 229, Public Course Problem Set #1 Solutions: Supervised Learning](ps1_solution.pdf)\n数据集下载: [PS1-data.zip](PS1-data.zip)\n\n# 解析\n## Newton’s method for computing least squares\n看答案解析\n## 局部加权逻辑回归(Locally-weighted logistic regression,LWLR)\n(a)\n```python\nimport numpy as np\n\ndef LoadData(fileName):\n    n = len(open(fileName).readline().strip().split())\n    fr = open(fileName)\n    dataSet = []\n    for oneRow in fr.readlines():\n        rawData = oneRow.strip().split()\n        dataSet = dataSet + [[float(var) for var in rawData]]\n    fr.close()\n    return dataSet\n\ndef lwrl(X_train, y_train, x, tau):\n    X_train = np.array(X_train)\n    y_train = np.array(y_train)\n    m = X_train.shape[0]\n    n = X_train.shape[1]\n    theta = np.zeros(n)\n    lam = 1e-4\n\n    w = np.exp(-np.sum((X_train - x)**2, axis=1) / (2.0 * tau))\n\n    deltaL = np.ones(n)\n    while np.linalg.norm(deltaL) > 1e-6:\n        h = 1.0 / (1 + np.exp(-X_train @ theta))\n        deltaL = X_train.T@(w*(y_train-h))-lam*theta\n        H = -X_train.T@np.diag(w*h*(1-h))@X_train-lam*np.eye(n)\n        theta=theta-np.linalg.inv(H)@deltaL\n\n    return float(x@theta>0.0)\n\ndef plot_lwlr(X_train, y_train, tau, resolution):\n    X_train = np.array(X_train)\n    m = X_train.shape[0]\n    y_pred = []\n    for x_test in X_train:\n        y_p = lwrl(X_train, y_train, x_test, tau)\n        y_pred = y_pred + [y_p]\n    acc = np.sum(y_pred == y_train)/(1.0*m)\n    return acc\n\nX_train, y_train = LoadData('q2/data/x.dat'), LoadData('q2/data/y.dat')\ny_train=np.array(y_train).flatten()\nacc = plot_lwlr(X_train, y_train, 0.01, 0)\nprint(acc)\n```\n(b) \n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\ndef LoadData(fileName):\n    n = len(open(fileName).readline().strip().split())\n    fr = open(fileName)\n    dataSet = []\n    for oneRow in fr.readlines():\n        rawData = oneRow.strip().split()\n        dataSet = dataSet + [[float(var) for var in rawData]]\n    fr.close()\n    return dataSet\n\ndef lwrl(X_train, y_train, x, tau):\n    X_train = np.array(X_train)\n    y_train = np.array(y_train)\n    m = X_train.shape[0]\n    n = X_train.shape[1]\n    theta = np.zeros(n)\n    lam = 1e-4\n\n    w = np.exp(-np.sum((X_train - x)**2, axis=1) / (2.0 * tau))\n\n    deltaL = np.ones(n)\n    while np.linalg.norm(deltaL) > 1e-6:\n        h = 1.0 / (1 + np.exp(-X_train @ theta))\n        deltaL = X_train.T@(w*(y_train-h))-lam*theta\n        H = -X_train.T@np.diag(w*h*(1-h))@X_train-lam*np.eye(n)\n        theta=theta-np.linalg.inv(H)@deltaL\n\n    return float(x@theta>0.0)\n\ndef plot_lwlr(X_train, y_train, X_test, tau, resolution):\n    X_train = np.array(X_train)\n    m = X_train.shape[0]\n    y_pred = []\n    for x_test in X_test:\n        y_p = lwrl(X_train, y_train, x_test, tau)\n        y_pred = y_pred + [y_p]\n    return np.array(y_pred)\n\ndef plot_subplot(X_train, y_train, tau, subplt=111):\n    plt.subplot(subplt)\n    X1, X2 = np.meshgrid(np.arange(start=X_train[:, 0].min() - 0.1, stop=X_train[:, 0].max() + 0.1, step=0.05),\n                         np.arange(start=X_train[:, 1].min() - 0.1, stop=X_train[:, 1].max() + 0.1, step=0.05))\n    y = plot_lwlr(X_train, y_train, np.array([X1.ravel(), X2.ravel()]).T, tau, 0).reshape(X1.shape)\n    plt.contourf(X1, X2, y, alpha=1, cmap=ListedColormap(('blue', 'red')))\n    plt.xlim(X1.min(), X1.max())\n    plt.ylim(X2.min(), X2.max())\n    for i in range(X_train.shape[0]):\n        if y_train[i] == 0.0:\n            plt.scatter(X_train[i, 0], X_train[i, 1], marker='o', color='black', facecolors='none')\n        else:\n            plt.scatter(X_train[i, 0], X_train[i, 1], marker='x', color='black')\n    plt.title('tau='+str(tau))\n\n\nX_train, y_train = LoadData('q2/data/x.dat'), LoadData('q2/data/y.dat')\nX_train, y_train = np.array(X_train), np.array(y_train)\ny_train=np.array(y_train).flatten()\nplt.figure(figsize=(15,10))\nplot_subplot(X_train, y_train, 0.01, 231)\nplot_subplot(X_train, y_train, 0.05, 232)\nplot_subplot(X_train, y_train, 0.1, 233)\nplot_subplot(X_train, y_train, 0.5, 234)\nplot_subplot(X_train, y_train, 1, 235)\nplot_subplot(X_train, y_train, 5, 236)\nplt.show()\n```\n效果如下:\n{% asset_img 2019072614155826.png %}\n## Multivariate least squares\n$$\\begin{aligned}\n    \\left\\{\\left(x^{(i)}, y^{(i)}\\right), i=1, \\ldots, m\\right\\}, x^{(i)} \\in \\mathbb{R}^{n}, y^{(i)} \\in \\mathbb{R}^{p}, \\Theta \\in \\mathbb{R}^{n \\times p}\n\\end{aligned}$$\n(a). 要求推出下式的矩阵向量形式\n$$\\begin{aligned}\n    J(\\Theta)=\\frac{1}{2} \\sum_{i=1}^{m} \\sum_{j=1}^{p}\\left(\\left(\\Theta^{T} x^{(i)}\\right)_{j}-y_{j}^{(i)}\\right)^{2}\n\\end{aligned}$$\n答案:\n$$\\begin{aligned}\n    \\begin{aligned} J(\\Theta) &=\\frac{1}{2} \\operatorname{tr}\\left((X \\Theta-Y)^{T}(X \\Theta-Y)\\right) \\\\ &=\\frac{1}{2} \\sum_{i}(X \\Theta-Y)^{T}(X \\Theta-Y) )_{i i} \\\\ &=\\frac{1}{2} \\sum_{i} \\sum_{j}(X \\Theta-Y)_{i j}^{2} \\\\ &=\\frac{1}{2} \\sum_{i=1}^{m} \\sum_{j=1}^{p}\\left(\\left(\\Theta^{T} x^{(i)}\\right)_{j}-y_{j}^{(i)}\\right)^{2} \\end{aligned}\n\\end{aligned}$$\n(b). 看参考答案\n(c). \n# 参考","tags":["homework"],"categories":["CS229"]},{"title":"[leetcode]96.不同的二叉搜索树","url":"%2Fposts%2Fff2b3e38%2F","content":"{% asset_img 2019072511112925.png %}\n\n# 解题思路\n`思路1:卡特兰数` \n$n$个结点可以构成的二叉树个数为$Catalan$数,设卡特兰数为$C_n$\n$$\\begin{aligned}\n    C_{n}=\\left(\\begin{array}{c}{2 n} \\\\ {n}\\end{array}\\right)-\\left(\\begin{array}{c}{2 n} \\\\ {n+1}\\end{array}\\right)=\\frac{1}{n+1}\\left(\\begin{array}{c}{2 n} \\\\ {n}\\end{array}\\right)\n\\end{aligned}$$\n那么\n$$\\begin{aligned}\n    C_{0}=1,\\quad C_{n+1}=\\frac{2(2 n+1)}{n+2} C_{n}\n\\end{aligned}$$\n递推有点麻烦\n$$\\begin{aligned}\n    \\begin{aligned} \\text { Catalan }_{n+1} &=\\frac{1}{n+2} C_{2 n+2}^{n+1} \\\\ &=\\frac{1}{n+2} * \\frac{(2 n+2) !}{(n+1) ! *(n+1) !} \\\\ &=\\frac{1}{n+2} * \\frac{(2 n) ! *(2 n+1) *(2 n+2)}{n ! * n ! *(n+1)^{2}} \\\\ &=\\frac{1}{n+2} * \\frac{(2 n+1) *(2 n+2)}{(n+1)} * \\frac{1}{n+1} * \\frac{(2 n) !}{n ! * n !} \\\\ &=\\frac{2(2 n+1) *(2 n+2)}{n+1} * C_{2 n}^{n} \\\\ &=\\frac{2(2n+1)}{n+2} \\text { Catalan }_{n} \\end{aligned}\n\\end{aligned}$$\n递推过程源自[卡特兰数 (Catalan Number) 公式的推导](https://www.cnblogs.com/zyt1253679098/p/9190217.html)\n```cpp\nusing ull = unsigned long long;\nint numTrees(int n) {\n    ull c=1;\n    for(int i=0;i<n;++i)\n        c=2*(2*i+1)*c/(i+2);\n    return c;\n}\n```\n时间复杂度: $O(n)$\n空间复杂度: $O(1)$\n# 参考\n1. [LeetCode不同的二叉搜索树](https://leetcode-cn.com/problems/two-sum/solution/bu-tong-de-er-cha-sou-suo-shu-by-leetcode/)\n2. [卡特兰数 (Catalan Number) 公式的推导](https://www.cnblogs.com/zyt1253679098/p/9190217.html)","tags":["卡特兰数"],"categories":["OJ"]},{"title":"[leetcode]337.打家劫舍 III","url":"%2Fposts%2F440808e7%2F","content":"{% asset_img 2019072508394724.png %}\n\n# 解题思路 \n`思路1:`单独抽出一个3个结点构成的二叉树,找出模式为:`包含头结点的子树最大值,不包含头结点的子树最大值.`\n```cpp\nint rob(TreeNode* root) {\n    auto res = doRob(root);\n    return max(res[0],res[1]);\n}\nvector<int> doRob(TreeNode* root)\n{\n    vector<int> res(2,0); // res[0]不包含根节点的最大值,res[1]包含根节点的最大值\n    if (!root) return vector<int>(2,0);\n    auto resL = doRob(root->left);\n    auto resR = doRob(root->right);\n    // 不包含根节点，最大值为两个子树的最大值之和\n    res[0] = max(resL[0],resL[1])+max(resR[0],resR[1]);\n    // 包含根节点，最大值为两个子树不包含根节点的最大值加上根节点的值\n    res[1] = resL[0]+resR[0]+root->val;\n    return res;\n}\n```\n\n# 参考\n1. [horanol题解](https://leetcode-cn.com/problems/two-sum/solution/java-2ms-by-horanol/)\n","tags":["二叉树"],"categories":["OJ"]},{"title":"[paper-notes]MMD计算的核技巧公式推导","url":"%2Fposts%2Ffe7fde9c%2F","content":"\n\n# MMD计算的核技巧公式推导\n了解公式:\n1. $||\\mathbf{A}||^2=\\rm{tr}(\\mathbf{AA}^T)$\n2. $\\rm{tr}(\\mathbf{AB})=\\rm{tr}(\\mathbf{BA})$\n\n## JDA paper\n\n$\\mathbf{X}=\\left[\\mathbf{x}_{1}, \\dots, \\mathbf{x}_{n}\\right] \\in \\mathbb{R}^{m \\times n},\\mathbf{A} \\in \\mathbb{R}^{m \\times k}$\n\n$$\\begin{aligned}\n    &\\left\\|\\frac{1}{n_{s}} \\sum_{i=1}^{n_{s}} \\mathbf{A}^{\\mathrm{T}} \\mathbf{x}_{i}-\\frac{1}{n_{t}} \\sum_{j=n_{s}+1}^{n_{s}+n_{t}} \\mathbf{A}^{\\mathrm{T}} \\mathbf{x}_{j}\\right\\|^{2}\\\\\n    =&\\left\\|\\frac{1}{n_{s}} \\mathbf{A}^{\\mathrm{T}}\\left[\\begin{array}{cccc}{\\mathbf{x}_{1}} & {\\mathbf{x}_{2}} & {\\cdots} & {\\mathbf{x}_{n_{s}}}\\end{array}\\right]_{1 \\times n_{s}}\\left[\\begin{array}{c}{1} \\\\ {1} \\\\ {\\vdots} \\\\ {1}\\end{array}\\right]_{n_{s} \\times 1}-\\frac{1}{n_{t}} \\mathbf{A}^{\\mathrm{T}}\\left[\\begin{array}{cccc}{\\mathbf{x}_{1}} & {\\mathbf{x}_{2}} & {\\cdots} & {\\mathbf{x}_{n_{t}}}\\end{array}\\right]_{1 \\times n_{t}}\\left[\\begin{array}{c}{1} \\\\ {1} \\\\ {\\vdots} \\\\ {1}\\end{array}\\right]_{n_{t} \\times 1}\\right\\|^{2}\\\\\n    =&\\rm{tr}\\left[\\left(\\frac{1}{n_s}\\mathbf{A^TX_s1}-\\frac{1}{n_t}\\mathbf{A^TX_t1})(\\frac{1}{n_s}\\mathbf{A^TX_s1}-\\frac{1}{n_t}\\mathbf{A^TX_t1}\\right)^T\\right]\\\\\n    =&\\operatorname{tr}\\left(\\frac{1}{n_{s}^{2}} \\mathbf{A}^{\\mathrm{T}} \\mathbf{X}_{s} \\mathbf{1}\\left(\\mathbf{A}^{\\mathrm{T}} \\mathbf{X}_{s} \\mathbf{1}\\right)^{\\mathrm{T}}+\\frac{1}{n_{t}^{2}} \\mathbf{A}^{\\mathrm{T}} \\mathbf{X}_{t} \\mathbf{1}\\left(\\mathbf{A}^{\\mathrm{T}} \\mathbf{X}_{t} \\mathbf{1}\\right)^{\\mathrm{T}}-\\frac{1}{n_{s} n_{t}} \\mathbf{A}^{\\mathrm{T}} \\mathbf{X}_{s} \\mathbf{1}\\left(\\mathbf{A}^{\\mathrm{T}} \\mathbf{X}_{t} \\mathbf{1}\\right)^{\\mathrm{T}}-\\frac{1}{n_{s} n_{t}} \\mathbf{A}^{\\mathrm{T}} \\mathbf{X}_{t} \\mathbf{1}\\left(\\mathbf{A}^{\\mathrm{T}} \\mathbf{X}_{s} \\mathbf{1}\\right)^{\\mathrm{T}}\\right)\\\\\n    =&\\operatorname{tr}\\left(\\frac{1}{n_{s}^{2}} \\mathbf{A}^{\\mathrm{T}} \\mathbf{X}_{s} \\mathbf{1} \\mathbf{1}^{\\mathrm{T}} \\mathbf{X}_{s}^{\\mathrm{T}} \\mathbf{A}+\\frac{1}{n_{t}^{2}} \\mathbf{A}^{\\mathrm{T}} \\mathbf{X}_{t} \\mathbf{1} \\mathbf{1}^{\\mathrm{T}} \\mathbf{X}_{t}^{\\mathrm{T}} \\mathbf{A}-\\frac{1}{n_{s} n_{t}} \\mathbf{A}^{\\mathrm{T}} \\mathbf{X}_{s} \\mathbf{1} \\mathbf{1}^{\\mathrm{T}} \\mathbf{X}_{t}^{\\mathrm{T}}-\\frac{1}{n_{s} n_{t}} \\mathbf{A}^{\\mathrm{T}} \\mathbf{X}_{t} \\mathbf{1} \\mathbf{1}^{\\mathrm{T}} \\mathbf{X}_{s}^{\\mathrm{T}}\\right)\\\\\n    =&\\rm{tr}\\left[\\mathbf{A}^{\\mathrm{T}}\\left(\\frac{1}{n_{s}^{2}} \\mathbf{X}_{s}\\mathbf{11^T} \\mathbf{\\mathbf { X }}_{s}^{\\mathrm{T}} +\\frac{1}{n_{t}^{2}} \\mathbf{X}_{t}\\mathbf{11^T} \\mathbf{X}_{t}^{\\mathrm{T}}-\\frac{1}{n_{s} n_{t}} \\mathbf{X}_{s}\\mathbf{11^T} \\mathbf{X}_{t}^{\\mathrm{T}} -\\frac{1}{n_{s} n_{t}} \\mathbf{X}_{t}\\mathbf{11^T}\\mathbf{X}_{s}^{\\mathrm{T}} \\right) \\mathbf{A}\\right]\\\\\n    =&\\operatorname{tr}\\left(\\mathbf{A}^{\\mathrm{T}}\\left[\\mathbf{X}_{s} \\quad \\mathbf{X}_{t}\\right]\\left[\\begin{array}{cc}{\\frac{1}{n_{\\mathrm{s}}^{2}} \\mathbf{1} \\mathbf{1}^{\\mathrm{T}}} & {\\frac{-1}{n_{s} n_{t}} \\mathbf{1 1}^{\\mathrm{T}}} \\\\ {\\frac{-1}{n_{s} n_{t}} \\mathbf{1 1}^{\\mathrm{T}}} & {\\frac{1}{n_{t}^{2}} \\mathbf{1 1}^{\\mathrm{T}}}\\end{array}\\right]\\left[\\begin{array}{c}{\\mathbf{X}^T_{s}} \\\\ {\\mathbf{X}^T_{t}}\\end{array}\\right] \\mathbf{A}\\right)\\\\\n    =&\\operatorname{tr}\\left(\\mathbf{A}^{\\mathrm{T}} \\mathbf{X} \\mathbf{M} \\mathbf{X}^{\\mathrm{T}} \\mathbf{A}\\right)\n\\end{aligned}$$\n## TCA paper\n$$\\begin{aligned}\n    &\\left\\|\\frac{1}{n_{s}} \\sum_{i=1}^{n_{s}} \\phi\\left(\\mathbf{x}_{i}\\right)-\\frac{1}{n_{t}} \\sum_{j=1}^{n_{t}} \\phi\\left(\\mathbf{x}_{j}\\right)\\right\\|^{2}\\\\\n    =&\\operatorname{tr}\\left(\\left[\\phi\\left(\\mathbf{x}_{s}\\right) \\quad \\phi\\left(\\mathbf{x}_{t}\\right)\\right]\\left[\\begin{array}{cc}{\\frac{1}{n_{s}^{2}} 11^{\\mathrm{T}}} & {\\frac{-1}{n_{s} n_{t}} 11^{\\mathrm{T}}} \\\\ {\\frac{-1}{n_{s} n_{t}} 11^{\\mathrm{T}}} & {\\frac{1}{n_{t}^{2}} 11^{\\mathrm{T}}}\\end{array}\\right]\\left[\\begin{array}{c}{\\phi^T\\left(\\mathbf{x}_{s}\\right)} \\\\ {\\phi^T\\left(\\mathbf{x}_{t}\\right)}\\end{array}\\right]\\right)\\\\\n    =&\\operatorname{tr}\\left(\\left[\\begin{array}{c}{\\phi^T\\left(\\mathbf{x}_{s}\\right)} \\\\ {\\phi^T\\left(\\mathbf{x}_{t}\\right)}\\end{array}\\right]\\left[\\phi\\left(\\mathbf{x}_{s}\\right) \\quad \\phi\\left(\\mathbf{x}_{t}\\right)\\right]\\left[\\begin{array}{cc}{\\frac{1}{n_{s}^{2}} 11^{\\mathrm{T}}} & {\\frac{-1}{n_{s} n_{t}} 11^{\\mathrm{T}}} \\\\ {\\frac{-1}{n_{s} n_{t}} 11^{\\mathrm{T}}} & {\\frac{1}{n_{t}^{2}} 11^{\\mathrm{T}}}\\end{array}\\right]\\right)\\\\\n    =&\\operatorname{tr}\\left(\\begin{bmatrix}{<\\phi\\left(\\mathbf{x}_{s}\\right), \\phi\\left(\\mathbf{x}_{s}\\right)><\\phi\\left(\\mathbf{x}_{s}\\right), \\phi\\left(\\mathbf{x}_{t}\\right)>} \\\\ {<\\phi\\left(\\mathbf{x}_{t}\\right), \\phi\\left(\\mathbf{x}_{s}\\right)><\\phi\\left(\\mathbf{x}_{t}\\right), \\phi\\left(\\mathbf{x}_{t}\\right)>}\\end{bmatrix}\\mathbf{M}\\right)\\\\\n    =&\\operatorname{tr}\\left(\\left[\\begin{array}{cc}{K_{s, s}} & {K_{s, t}} \\\\ {K_{t, s}} & {K_{t, t}}\\end{array}\\right] \\mathbf{M}\\right)\n\\end{aligned}$$\n$$\\begin{aligned}\n    (M)_{i j}=\\left\\{\\begin{array}{ll}{\\frac{1}{n_{s} n_{s}},} & {\\mathbf{x}_{i}, \\mathbf{x}_{j} \\in \\mathcal{D}_{s}} \\\\ {\\frac{1}{n_{t} n_{t}},} & {\\mathbf{x}_{i}, \\mathbf{x}_{j} \\in \\mathcal{D}_{t}} \\\\ {\\frac{-1}{n_{s} n_{t}},} & {\\text { otherwise }}\\end{array}\\right.\n\\end{aligned}$$\n# 参考\n1. [The Derivation of TCA](The_Derivation_of_TCA.pdf)","tags":["paper"],"categories":["paper"]},{"title":"[leetcode]1036.逃离大迷宫","url":"%2Fposts%2F1091fd73%2F","content":"{% asset_img 2019072410565923.png %}\n\n### 解题思路 \n本题如果想用$0\\le blocked.length\\le 200$这个条件,则围成个三角形可以围住最多的点,即$\\frac{200\\times 200}{2}-\\frac{200}{2}=19900$个点(三角形面积+斜边超出部分的面积),从起点$S$走了超出$19900$步没有被围住,推出结论:认为是可以抵达终点$T$是不对的,因为障碍点可能围住了终点$T$,所以也得需要判断从终点$T$是否也可到达起点$S$,只有$(S->T)\\&\\&(T->S)$都满足可达,才能得出可达的结论.\n```cpp\nusing ll = long long;\nconst ll MAXN = 1e6; // 最大边长\nconst ll LIMIT = 19900;\nbool isEscapePossible(vector<vector<int>>& blocked, vector<int>& source, vector<int>& target) {\n    unordered_set<ll> ub; // 存放障碍物位置\n    for(auto it:blocked)\n        ub.insert(it[0]*MAXN+it[1]);\n    auto check = [&](vector<int>&S, vector<int>&T)->bool\n    {\n        unordered_set<ll> step; // 存放已经走过的点\n        int tot=0;\n        queue<ll> Q; // 存放将要走的点\n        Q.push(S[0]*MAXN+S[1]);\n        step.insert(Q.back());\n        int dx[]={-1,+1,0,0},dy[]={0,0,-1,+1};\n        while(!Q.empty())\n        {\n            int x=Q.front()/MAXN;\n            int y=Q.front()%MAXN;\n            Q.pop();\n            if (tot++>LIMIT||(x==T[0]&&y==T[1])) return true;\n            for(int d=0;d<4;++d)\n            {\n                int nx=x+dx[d];\n                int ny=y+dy[d];\n                if (0<=nx&&nx<MAXN&&0<=ny&&ny<MAXN&&!ub.count(nx*MAXN+ny)&&!step.count(nx*MAXN+ny))\n                {\n                    Q.push(nx*MAXN+ny);\n                    // 记住,step一定要放在这里,因为只要放进Q队列中的点,都认为是已经走过的点\n                    step.insert(Q.back()); \n                }\n            }\n        }\n        return false;\n    };\n    return check(source,target)&&check(target,source);\n}\n```\n### 参考","tags":["bfs"],"categories":["OJ"]},{"title":"[leetcode]213.打家劫舍 II","url":"%2Fposts%2Fa2712817%2F","content":"{% asset_img 2019072409221122.png %}\n\n### 解题思路\n基于第一题思路[[leetcode]198.打家劫舍](ea2ff6ee)\n`思路1:dp` \n```cpp\nint dp[9999];\nint alternative_sum(vector<int> nums)\n{\n    int len = nums.size();\n    if(len == 0)\n        return 0;\n    dp[0] = 0;\n    dp[1] = nums[0];\n    for(int i = 2; i <= len; i++) {\n        dp[i] = max(dp[i-1], dp[i-2] + nums[i-1]);\n    }\n    return dp[len];   \n}\nint rob(vector<int>& nums) {\n    if (nums.empty()) return 0;\n    if (nums.size()==1) return nums[0];\n    return max(alternative_sum(vector<int>(nums.begin(),nums.end()-1)),alternative_sum(vector<int>(nums.begin()+1,nums.end())));\n}\n```\n`思路2:交替法`\n```cpp\nint alternative_sum(vector<int> nums)\n{\n    int maxOdd=0,maxEven=0;\n    for(int i=0;i<nums.size();++i)\n    {\n        if (i%2)\n        {\n            maxOdd+=nums[i];\n            maxOdd=max(maxOdd,maxEven);\n        }\n        else\n        {\n            maxEven+=nums[i];\n            maxEven=max(maxEven,maxOdd);\n        }\n    }\n    return max(maxOdd,maxEven);\n}\nint rob(vector<int>& nums) {\n    if (nums.empty()) return 0;\n    if (nums.size()==1) return nums[0];\n    return max(alternative_sum(vector<int>(nums.begin(),nums.end()-1)),alternative_sum(vector<int>(nums.begin()+1,nums.end())));\n}\n```\n### 参考","categories":["OJ"]},{"title":"[leetcode]198.打家劫舍","url":"%2Fposts%2Fea2ff6ee%2F","content":"{% asset_img 2019072408234321.png %}\n\n### 解题思路 \n`思路1:DP`\n```cpp\n// dp[i]保存到第i(i从1开始)个数时,能够偷到的最高金额\nint dp[9999];\nint rob(vector<int>& nums) {\n    int len = nums.size();\n    if(len == 0)\n        return 0;\n    dp[0] = 0;\n    dp[1] = nums[0];\n    for(int i = 2; i <= len; i++) {\n        dp[i] = max(dp[i-1], dp[i-2] + nums[i-1]);\n    }\n    return dp[len];\n}\n```\n`思路2:`\n交替法进行,交替保留最大,并更新\n```cpp\nint rob(vector<int>& nums) {\n    int n=nums.size(),sumEven=0,sumOdd=0;\n    for(int i=0;i<n;++i)\n    {\n        if (i%2)\n        {\n            sumOdd+=nums[i];\n            sumOdd=max(sumOdd,sumEven);\n        }\n        else\n        {\n            sumEven+=nums[i];\n            sumEven=max(sumEven,sumOdd);\n        }\n    }\n    return max(sumEven,sumOdd);\n}\n```\n### 参考","tags":["交替法"],"categories":["OJ"]},{"title":"[leetcode]257.二叉树的所有路径","url":"%2Fposts%2Fe2f5a73d%2F","content":"{% asset_img 2019072314032020.png %}\n\n### 解题思路 \n```cpp\nvector<string> res;\nvoid bfs_backtrace(TreeNode* root, string tmp)\n{\n    if (!root) return;\n    tmp+=to_string(root->val);\n    if (!root->left&&!root->right)\n    {\n        res.push_back(tmp);\n        return;\n    }\n    if (root->left)\n        bfs_backtrace(root->left,tmp+\"->\");\n    if (root->right)\n        bfs_backtrace(root->right,tmp+\"->\");\n}\nvector<string> binaryTreePaths(TreeNode* root) {\n    bfs_backtrace(root,\"\");\n    return res;\n}\n```\n### 参考","tags":["bfs"],"categories":["OJ"]},{"title":"[leetcode]1128.等价多米诺骨牌对的数量","url":"%2Fposts%2F28517038%2F","content":"{% asset_img 2019072310574519.png %}\n\n### 解题思路 \n实际为一个组合问题:假如有7对相同元素(变换顺序后),则$\\frac{7\\times{6}}{2}=21$个,也可以这样算$1+2+\\cdots+6=21$个.\n```cpp\nint numEquivDominoPairs(vector<vector<int>>& dominoes) {\n    map<pair<int,int>,int> freq;\n    int count=0;\n    for(auto it:dominoes)\n        count+=freq[make_pair(min(it[0],it[1]),max(it[0],it[1]))]++;\n    return count;\n}\n```\n### 参考","tags":["leetcode"],"categories":["OJ"]},{"title":"[leetcode]33.搜索旋转排序数组","url":"%2Fposts%2Fcc7b1f98%2F","content":"{% asset_img 2019072310382918.png %}\n\n### 解题思路 \n\n```cpp\nint search(vector<int>& nums, int target) {\n    int l=0,r=nums.size()-1,m=0;\n    while(l<=r)\n    {\n        m=(l+r)>>1;\n        if (nums[m]==target) return m;\n        // 右半部分有序\n        if (nums[m]<nums[r])\n        {\n            if (nums[m]<target&&target<=nums[r]) // 继续右半部分\n                l=m+1;\n            else // 否则选择左半部分\n                r=m-1;\n        }\n        // 左半部分有序\n        else\n        {\n            if (nums[m]>target&&target>=nums[l]) // 继续左半部分\n                r=m-1;\n            else // 否则选择右半部分\n                l=m+1;                \n        }\n    }\n    return -1;\n}\n```\n### 参考","tags":["二分查找"],"categories":["OJ"]},{"title":"[leetcode]1131.绝对值表达式的最大值","url":"%2Fposts%2F15e9476f%2F","content":"{% asset_img 2019072309361617.png %}\n\n### 解题思路 \n```cpp\nconst int INF=1e9;\nint maxAbsValExpr(vector<int>& arr1, vector<int>& arr2) {\n    // |a1-b1|+|a2-b2|+|i-j|=|a1-b1|+|a2-b2|+(i-j)\n    // 1.(a1-b1)+(a2-b2)+(i-j) = (+a1+a2+i)+(-b1-b2-j)\n    // 2.(a1-b1)+(b2-a2)+(i-j) = (+a1-a2+i)+(-b1+b2-j)\n    // 3.(b1-a1)+(a2-b2)+(i-j) = (-a1+a2+i)+(+b1-b2-j)\n    // 4.(b1-a1)+(b2-a2)+(i-j) = (-a1-a2+i)+(+b1+b2-j)\n    int ans=-INF,ret1=-INF,ret2=-INF,ret3=-INF,ret4=-INF;\n    for(int i=0;i<arr1.size();++i)\n    {\n        // 先求后面,再求和\n        ret1=max(ret1,-arr1[i]-arr2[i]-i);\n        ret2=max(ret2,-arr1[i]+arr2[i]-i);\n        ret3=max(ret3,+arr1[i]-arr2[i]-i);\n        ret4=max(ret4,+arr1[i]+arr2[i]-i);\n        ans=max(ans,ret1+arr1[i]+arr2[i]+i);\n        ans=max(ans,ret2+arr1[i]-arr2[i]+i);\n        ans=max(ans,ret3-arr1[i]+arr2[i]+i);\n        ans=max(ans,ret4-arr1[i]-arr2[i]+i);\n    }\n    return ans;\n}\n```\n\n### 参考","tags":["leetcode"],"categories":["OJ"]},{"title":"[机器学习-CS229]Hoeffding不等式","url":"%2Fposts%2F8c8c27f9%2F","content":"# CS229 课程讲义中文翻译\nCS229 Supplementary notes\n\n|原作者|翻译|\n|---|---|\n|John Duchi|[XiaoDong_Wang](https://github.com/Dongzhixiao) |\n\n\n|相关链接|\n|---|\n|[Github 地址](https://github。com/Kivy-CN/Stanford-CS-229-CN)|\n|[知乎专栏](https://zhuanlan。zhihu。com/MachineLearn)|\n|[斯坦福大学 CS229 课程网站](http://cs229。stanford。edu/)|\n|[网易公开课中文字幕视频](http://open。163。com/movie/2008/1/M/C/M6SGF6VB4_M6SGHFBMC。html)|\n\n\n### Hoeffding不等式\n\n#### 1. 基本概率边界\n\n概率论、统计学和机器学习的一个基本问题是：给定一个期望为$E[Z]$的随机变量$Z$，$Z$接近其期望的可能性有多大？更准确地说，它有多接近的概率是多少？考虑到这一点，本节内容提供了一些计算型如下面公式边界的方法：\n\n$$\n\\mathbb{P}(Z \\geq \\mathbb{E}[Z]+t) \\text { and } \\mathbb{P}(Z \\leq \\mathbb{E}[Z]-t)\\qquad\\qquad(1)\n$$\n\n其中$t\\ge 0$。\n\n我们的第一个边界可能是所有概率不等式中最基本的，它被称为马尔可夫不等式。考虑到它的基本性质，其证明基本上只有一行就不足为奇了。\n\n**命题1**（马尔可夫不等式）令$z\\ge 0$是一个非负随机变量。则对于所有$t\\ge 0$有：\n\n$$\n\\mathbb{P}(Z \\geq t) \\leq \\frac{\\mathbb{E}[Z]}{t}\n$$\n\n**证明** 我们注意到$\\mathbb{P}(Z \\geq t)=\\mathbb{E}[1\\{Z \\geq | t\\}]$，以及如果$z\\ge t$，则一定可得$Z / t \\geq 1 \\geq 1\\{Z \\geq t\\}$，然而如果$z < t$，则我们仍然有$Z / t \\geq 0=1\\{Z \\geq t\\}$。因此：\n\n$$\n\\mathbb{P}(Z \\geq t)=\\mathbb{E}[1\\{Z \\geq t\\}] \\leq \\mathbb{E}\\left[\\frac{Z}{t}\\right]=\\frac{\\mathbb{E}[Z]}{t}\n$$\n\n跟希望的一样。\n\n本质上，$(1)$式概率上的所有其他边界都是马尔可夫不等式的变化。第一个变量用二阶矩表示随机变量的方差，而不是简单的均值，称为Chebyshev不等式。\n\n**命题2** （切比雪夫不等式）。设$Z$为$Var(Z) < 1$的任意随机变量。则：\n\n$$\n\\mathbb{P}(Z \\geq \\mathbb{E}[Z]+t \\text { or } Z \\leq \\mathbb{E}[Z]-t) \\leq \\frac{\\operatorname{Var}(Z)}{t^{2}}\n$$\n\n对于$t\\ge 0$。\n\n**证明** 这个结果是马尔可夫不等式的直接结果。我们注意到如果$Z \\geq \\mathbb{E}[Z]+t$，则我们一定能得到$(Z-\\mathbb{E}[Z])^{2} \\geq t^{2}$，并且类似的如果$Z \\leq \\mathbb{E}[Z]-t$，我们有$(Z-\\mathbb{E}[Z])^{2} \\geq t^{2}$。因此：\n\n$$\n\\begin{aligned}\n\\mathbb{P}(Z \\geq \\mathbb{E}[Z]+t \\text { or } Z \\leq \\mathbb{E}[Z]-t) \n&=\\mathbb{P}\\left((Z-\\mathbb{E}[Z])^{2} \\geq t^{2}\\right) \\\\ \n& \\stackrel{(i)}{ \\leq} \\frac{\\mathbb{E}\\left[(Z-\\mathbb{E}[Z])^{2}\\right]}{t^{2}}=\\frac{\\operatorname{Var}(Z)}{t^{2}} \n\\end{aligned}\n$$\n\n其中步骤$(i)$是马尔可夫不等式。\n\n切比雪夫不等式的一个很好的结果是有限方差随机变量的均值收敛于它们的均值。让我们举个例子。假设$Z_i$是独立同分布的，并满足$\\mathbb{E}\\left[Z_{i}\\right]=0$。则$\\mathbb{E}\\left[Z_{i}\\right]=0$，如果我们定义$\\overline{Z}=\\frac{1}{n} \\sum_{i=1}^{n} Z_{i}$，则：\n\n$$\n\\operatorname{Var}(\\overline{Z})=\\mathbb{E}\\left[\\left(\\frac{1}{n} \\sum_{i=1}^{n} Z_{i}\\right)^{2}\\right]=\\frac{1}{n^{2}} \\sum_{i, j \\leq n} \\mathbb{E}\\left[Z_{i} Z_{j}\\right]=\\frac{1}{n^{2}} \\sum_{i=1}^{n} \\mathbb{E}\\left[Z_{i}^{2}\\right]=\\frac{\\operatorname{Var}\\left(Z_{1}\\right)}{n}\n$$\n\n特别的，对于任意$t\\ge 0$，我们有：\n\n$$\n\\mathbb{P}\\left(\\left|\\frac{1}{n} \\sum_{i=1}^{n} Z_{i}\\right| \\geq t\\right) \\leq \\frac{\\operatorname{Var}\\left(Z_{1}\\right)}{n t^{2}}\n$$\n\n因此对于任意$t > 0$有$\\mathbb{P}(|\\overline{Z}| \\geq t) \\rightarrow 0$。\n\n#### 2. 矩母函数(Moment generating functions)\n\n通常，我们希望对随机变量$Z$超出其期望的概率有更精确的（甚至是指数）边界。考虑到这一点，我们需要一个比有限方差更强的条件，对于有限方差，矩母函数是自然的候选条件。（方便的是，我们将看到它们也能很好地处理和。）回忆一下，对于随机变量$Z$, $Z$的矩母函数是下面这个函数：\n\n$$\nM_{Z}(\\lambda) :=\\mathbb{E}[\\exp (\\lambda Z)]\\qquad\\qquad(2)\n$$\n\n其中对于一些$\\lambda$来说是无限的。\n\n##### 2.1 切尔诺夫边界\n\n切尔诺夫边界利用矩母函数的基本方法给出指数偏差界限。\n\n**命题3** （切尔诺夫边界）。设$Z$为任意随机变量。然后对任何$t\\ge 0$有：\n\n$$\n\\mathbb{P}(Z \\geq \\mathbb{E}[Z]+t) \\leq \\min _{\\lambda \\geq 0} \\mathbb{E}\\left[e^{\\lambda(Z-\\mathbb{E}[Z])}\\right] e^{-\\lambda t}=\\min _{\\lambda \\geq 0} M_{Z-\\mathbb{E}[Z]}(\\lambda) e^{-\\lambda t}\n$$\n\n以及：\n\n$$\n\\mathbb{P}(Z \\leq \\mathbb{E}[Z]-t) \\leq \\min _{\\lambda \\geq 0} \\mathbb{E}\\left[e^{\\lambda(\\mathbb{E}[Z]-Z)}\\right] e^{-\\lambda t}=\\min _{\\lambda \\geq 0} M_{\\mathbb{E}[Z]-Z}(\\lambda) e^{-\\lambda t}\n$$\n\n**证明** 我们只证明了第一个不等式，因为第二个不等式是完全等价的。我们使用马尔可夫不等式。对于任意$\\lambda > 0$，当且仅当$e^{\\lambda Z} \\geq e^{\\lambda \\mathbb{E}[Z]+\\lambda t}$或$e^{\\lambda(Z-\\mathbb{E}[Z])} \\geq e^{\\lambda t}$时我们有$Z \\geq \\mathbb{E}[Z]+t$。因此可得：\n\n$$\n\\mathbb{P}(Z-\\mathbb{E}[Z] \\geq t)=\\mathbb{P}\\left(e^{\\lambda(Z-\\mathbb{E}[Z])} \\geq e^{\\lambda t}\\right) \\stackrel{(i)}{ \\leq} \\mathbb{E}\\left[e^{\\lambda(Z-\\mathbb{E}[Z])}\\right] e^{-\\lambda t}\n$$\n\n其中不等式$(i)$来自马尔科夫不等式。既然我们选择的$\\lambda > 0$无关紧要，因此我们可以通过最小化边界的右边来得到最好的一个。（要注意的是，这个界限在$\\lambda = 0$处是成立的。）\n\n重要的结果是切诺夫边界很好地处理了求和，这是矩母函数的结果。假设$Z_i$是独立的。则我们可得：\n\n$$\nM_{Z_{1}+\\cdots+Z_{n}}(\\lambda)=\\prod_{i=1}^{n} M_{Z_{i}}(\\lambda)\n$$\n\n这是因为：\n\n$$\n\\mathbb{E}\\left[\\exp \\left(\\lambda \\sum_{i=1}^{n} Z_{i}\\right)\\right]=\\mathbb{E}\\left[\\prod_{i=1}^{n} \\exp \\left(\\lambda Z_{i}\\right)\\right]=\\prod_{i=1}^{n} \\mathbb{E}\\left[\\exp \\left(\\lambda Z_{i}\\right)\\right]\n$$\n\n由于$Z_i$是独立的。这意味着当我们计算一个独立同分布变量和的切尔诺夫边界界时，我们只需要计算其中一个变量的矩母函数。事实上，假设$Z_i$是独立同分布的，并且（为了简单起见）均值为零。则可得：\n\n$$\n\\begin{aligned} \n\\mathbb{P}\\left(\\sum_{i=1}^{n} Z_{i} \\geq t\\right) \n& \\leq \\frac{\\prod_{i=1}^{n} \\mathbb{E}\\left[\\exp \\left(\\lambda Z_{i}\\right)\\right]}{e^{\\lambda t}} \\\\ \n&=\\left(\\mathbb{E}\\left[e^{\\lambda Z_{1}}\\right]\\right)^{n} e^{-\\lambda t} \n\\end{aligned}\n$$\n\n根据切尔诺夫边界。\n\n##### 2.2 矩母函数例子\n\n现在我们给出几个矩母函数的例子，这些例子能让我们得到一些很好的偏差不等式。后面我们给出的所有例子都可以用如下的非常方便的边界形式：\n\n$$\nM_{Z}(\\lambda)=\\mathbb{E}\\left[e^{\\lambda Z}\\right] \\leq \\exp \\left(\\frac{C^{2} \\lambda^{2}}{2}\\right) \\text { for all } \\lambda \\in \\mathbb{R}\n$$\n\n对于一些$C\\in \\mathbb{R}$（这取决于$Z$的分布）；这个形式非常适合应用切尔诺夫边界。\n\n我们从经典正态分布开始，其中$Z \\sim \\mathcal{N}\\left(0, \\sigma^{2}\\right)$。则我们可得：\n\n$$\n\\mathbb{E}[\\exp (\\lambda Z)]=\\exp \\left(\\frac{\\lambda^{2} \\sigma^{2}}{2}\\right)\n$$\n\n我们省略了计算过程。（如果你好奇的话，应该自己把它算出来！）\n\n第二个例子使用的是Rademacher随机变量，或者随机符号变量。令$S=1$有概率$\\frac 12$，$S=-1$有概率$\\frac 12$，则我们可以声明：\n\n$$\n\\mathbb{E}\\left[e^{\\lambda S}\\right] \\leq \\exp \\left(\\frac{\\lambda^{2}}{2}\\right) \\quad \\text { for all } \\lambda \\in \\mathbb{R}\\qquad\\qquad(3)\n$$\n\n要理解不等式$(3)$，我们来用指数函数的泰勒展开式，即$e^{x}=\\sum_{k=0}^{\\infty} \\frac{x^{k}}{k !}$。注意当$k$是奇数的时候$\\mathbb{E}\\left[S^{k}\\right]=0$。当$k$是偶数的时候$\\mathbb{E}\\left[S^{k}\\right]=1$。则我们可得：\n\n$$\n\\begin{aligned} \n\\mathbb{E}\\left[e^{\\lambda S}\\right] &=\\sum_{k=0}^{\\infty} \\frac{\\lambda^{k} \\mathbb{E}\\left[S^{k}\\right]}{k !} \\\\ \n&=\\sum_{k=0,2,4, \\ldots} \\frac{\\lambda^{k}}{k !}=\\sum_{k=0}^{\\infty} \\frac{\\lambda^{2 k}}{(2 k) !} \n\\end{aligned}\n$$\n\n最后，对于所有的$k=0,1,2, \\ldots$我们使用$(2 k) ! \\geq 2^{k} \\cdot k !$。因此：\n\n$$\n\\mathbb{E}\\left[e^{\\lambda S}\\right] \\leq \\sum_{k=0}^{\\infty} \\frac{\\left(\\lambda^{2}\\right)^{k}}{2^{k} \\cdot k !}=\\sum_{k=0}^{\\infty}\\left(\\frac{\\lambda^{2}}{2}\\right)^{k} \\frac{1}{k !}=\\exp \\left(\\frac{\\lambda^{2}}{2}\\right)\n$$\n\n让我们把不等式$(3)$代入到一个切尔诺夫函数中，看看独立同分布随机符号的和有多大。\n\n我们知道如果$Z=\\sum_{i=1}^{n} S_{i}$，其中$S_{i} \\in\\{ \\pm 1\\}$是随机符号，则$\\mathbb{E}[Z]=0$。根据切尔诺夫边界，很明显可得：\n\n$$\n\\mathbb{P}(Z \\geq t) \\leq \\mathbb{E}\\left[e^{\\lambda Z}\\right] e^{-\\lambda t}=\\mathbb{E}\\left[e^{\\lambda S_{1}}\\right]^{n} e^{-\\lambda t} \\leq \\exp \\left(\\frac{n \\lambda^{2}}{2}\\right) e^{-\\lambda t}\n$$\n\n应用切诺夫边界定理，我们可以在$\\lambda \\geq 0$时将其最小化，等价于下式：\n\n$$\n\\min _{\\lambda \\geq 0}\\left\\{\\frac{n \\lambda^{2}}{2}-\\lambda t\\right\\}\n$$\n\n幸运的是，这是一个很容易最小化的函数，对该函数求导并将其设为零，我们可得$n \\lambda-t=0$或$\\lambda=t / n$，这样给出了：\n\n$$\n\\mathbb{P}(Z \\geq t) \\leq \\exp \\left(-\\frac{t^{2}}{2 n}\\right)\n$$\n\n特别的，取$t=\\sqrt{2 n \\log \\frac{1}{\\delta}}$，我们可得：\n\n$$\n\\mathbb{P}\\left(\\sum_{i=1}^{n} S_{i} \\geq \\sqrt{2 n \\log \\frac{1}{\\delta}}\\right) \\leq \\delta\n$$\n\n因此有很高的概率得出$Z=\\sum_{i=1}^{n} S_{i}=O(\\sqrt{n})$——$n$个独立随机符号的和基本上不会大于$O(\\sqrt{n})$。\n\n#### 3. Hoeffding引理和Hoeffding不等式\n\nHoeffding不等式是一种强大的技术，它可能是学习理论中最重要的不等式，用于确定有界随机变量和过大或过小的概率的边界。我们先给出该不等式，然后我们将证明它是基于我们之前的矩母函数计算的一个弱化版本。\n\n**定理 4（Hoeffding不等式）** 令$Z_{1}, \\ldots, Z_{n}$是独立边界随机变量，并且对于所有$i$有$Z_{i} \\in[a, b]$，其中$-\\infty< a \\leq b < \\infty$。则：\n\n$$\n\\mathbb{P}\\left(\\frac{1}{n} \\sum_{i=1}^{n}\\left(Z_{i}-\\mathbb{E}\\left[Z_{i}\\right]\\right) \\geq t\\right) \\leq \\exp \\left(-\\frac{2 n t^{2}}{(b-a)^{2}}\\right)\n$$\n\n以及：\n\n$$\n\\mathbb{P}\\left(\\frac{1}{n} \\sum_{i=1}^{n}\\left(Z_{i}-\\mathbb{E}\\left[Z_{i}\\right]\\right) \\leq-t\\right) \\leq \\exp \\left(-\\frac{2 n t^{2}}{(b-a)^{2}}\\right)\n$$\n\n对于所有$t\\ge 0$都成立。\n\n我们用$(1)$切诺夫边界和$(2)$一个经典引理Hoeffding引理的组合来证明定理$4$，我们现在陈述这个引理。\n\n**引理 5（Hoeffding引理）** 令$Z$是边界随机变量，有$z\\in [a,b]$。则：\n\n$$\n\\mathbb{E}[\\exp (\\lambda(Z-\\mathbb{E}[Z]))] \\leq \\exp \\left(\\frac{\\lambda^{2}(b-a)^{2}}{8}\\right) \\quad \\text { for all } \\lambda \\in \\mathbb{R}\n$$\n\n**证明** 我们证明了这个引理的一个稍微弱一点的版本，它的因子是$2$而不是$8$，我们使用随机符号矩来生成边界和一个不相等性，称为Jensen不等式（我们将在后面的[EM算法](https://kivy-cn.github.io/Stanford-CS-229-CN/#/Markdown/cs229-notes8)推导中看到这个非常重要的不等式）。Jensen不等式表述如下：如果$f : \\mathbb{R} \\rightarrow \\mathbb{R}$是一个凸函数，即$f$是碗型函数，则：\n\n$$\nf(\\mathbb{E}[Z]) \\leq \\mathbb{E}[f(Z)]\n$$\n\n\n\n我们在概率论中使用了一种称为对称化的聪明技术来给出我们的结果（可能并不希望知道这一点，但它是概率论、机器学习和统计中非常常见的技术，所以学习它很好）。首先，让$Z'$是具有相同分布的随机变量$Z$的一个独立副本，使得$Z^{\\prime} \\in[a, b]$和$\\mathbb{E}\\left[Z^{\\prime}\\right]=\\mathbb{E}[Z]$，但$Z$和$Z'$是独立的。则：\n\n$$\n\\mathbb{E}_{Z}\\left[\\exp \\left(\\lambda\\left(Z-\\mathbb{E}_{Z}[Z]\\right)\\right)\\right]=\\mathbb{E}_{Z}\\left[\\exp \\left(\\lambda\\left(Z-\\mathbb{E}_{Z^{\\prime}}\\left[Z^{\\prime}\\right]\\right)\\right)\\right] \\stackrel{(i)}{ \\leq} \\mathbb{E}_{Z}\\left[\\mathbb{E}_{Z^{\\prime}} \\exp \\left(\\lambda\\left(Z-Z^{\\prime}\\right)\\right)\\right]\n$$\n\n其中$\\mathbb{E}_{Z}$和$\\mathbb{E}_{Z'}$代表$Z$和$Z'$的期望。之后，在步骤$(i)$中将Jensen不等式应用到函数$f(x)=e^{-x}$上。现在我们有：\n\n$$\n\\mathbb{E}[\\exp (\\lambda(Z-\\mathbb{E}[Z]))] \\leq \\mathbb{E}\\left[\\exp \\left(\\lambda\\left(Z-Z^{\\prime}\\right)\\right]\\right.\n$$\n\n现在，我们注意到一个奇怪的事实：差异$Z-Z'$关于零对称，因此如果$S \\in\\{-1,1\\}$是一个随机符号变量，则$S\\left(Z-Z^{\\prime}\\right)$完全和$Z-Z^{\\prime}$同分布。因此我们可得：\n\n$$\n\\begin{aligned} \n\\mathbb{E}_{Z, Z^{\\prime}}\\left[\\exp \\left(\\lambda\\left(Z-Z^{\\prime}\\right)\\right)\\right] &=\\mathbb{E}_{Z, Z^{\\prime}, S}\\left[\\exp \\left(\\lambda S\\left(Z-Z^{\\prime}\\right)\\right)\\right] \\\\ \n&=\\mathbb{E}_{Z, Z^{\\prime}}\\left[\\mathbb{E}_{S}\\left[\\exp \\left(\\lambda S\\left(Z-Z^{\\prime}\\right)\\right) | Z, Z^{\\prime}\\right]\\right] \n\\end{aligned}\n$$\n\n现在我们用不等式$(3)$对随机符号的矩母函数求导可以得到：\n\n$$\n\\mathbb{E}_{S}\\left[\\exp \\left(\\lambda S\\left(Z-Z^{\\prime}\\right)\\right) | Z, Z^{\\prime}\\right] \\leq \\exp \\left(\\frac{\\lambda^{2}\\left(Z-Z^{\\prime}\\right)^{2}}{2}\\right)\n$$\n\n当然，假设我们有$\\left|Z-Z^{\\prime}\\right| \\leq(b-a),$ so $\\left(Z-Z^{\\prime}\\right)^{2} \\leq(b-a)^{2}$，这就得到了：\n\n$$\n\\mathbb{E}_{Z, Z^{\\prime}}\\left[\\exp \\left(\\lambda\\left(Z-Z^{\\prime}\\right)\\right)\\right] \\leq \\exp \\left(\\frac{\\lambda^{2}(b-a)^{2}}{2}\\right)\n$$\n\n这就是结果（除了因子$2$而不是$8$）。\n\n现在我们用Hoeffding引理来证明定理$4$，只给出了上尾（即，$\\frac{1}{n} \\sum_{i=1}^{n}\\left(Z_{i}-\\mathbb{E}\\left[Z_{i}\\right]\\right) \\geq t$的概率）因为下面的尾巴也有类似的证明。我们使用切诺夫边界可以得到：\n\n$$\n\\begin{aligned} \n\\mathbb{P}\\left(\\frac{1}{n} \\sum_{i=1}^{n}\\left(Z_{i}-\\mathbb{E}\\left[Z_{i}\\right]\\right) \\geq t\\right) &=\\mathbb{P}\\left(\\sum_{i=1}^{n}\\left(Z_{i}-\\mathbb{E}\\left[Z_{i}\\right]\\right) \\geq n t\\right) \\\\ & \\leq \\mathbb{E}\\left[\\exp \\left(\\lambda \\sum_{i=1}^{n}\\left(Z_{i}-\\mathbb{E}\\left[Z_{i}\\right]\\right)\\right)\\right] e^{-\\lambda n t} \\\\\n&=\\left(\\prod_{i=1}^{n} \\mathbb{E}\\left[e^{\\lambda\\left(Z_{i}-\\mathbb{E}\\left[Z_{2}\\right)\\right.}\\right]\\right) e^{-\\lambda n t} \\begin{array}{l}{(i)} \\\\ { \\leq}\\end{array}\\left(\\prod_{i=1}^{n} e^{\\frac{\\lambda^{2}(b-\\alpha)^{2}}{8}}\\right) e^{-\\lambda n t}\n\\end{aligned}\n$$\n\n不等式$(i)$在Hoeffding引理（引理$5$）中。稍微重写一下，然后在$\\lambda\\ge 0$上最小化，我们可得：\n\n$$\n\\mathbb{P}\\left(\\frac{1}{n} \\sum_{i=1}^{n}\\left(Z_{i}-\\mathbb{E}\\left[Z_{i}\\right]\\right) \\geq t\\right) \\leq \\min _{\\lambda \\geq 0} \\exp \\left(\\frac{n \\lambda^{2}(b-a)^{2}}{8}-\\lambda n t\\right)=\\exp \\left(-\\frac{2 n t^{2}}{(b-a)^{2}}\\right)\n$$\n\n跟希望的结果一样。\n\n### 参考","tags":["CS229"],"categories":["CS229"]},{"title":"[机器学习-CS229]表示函数","url":"%2Fposts%2F44688108%2F","content":"# CS229 课程讲义中文翻译\nCS229 Supplementary notes\n\n|原作者|翻译|\n|---|---|\n|John Duchi|[XiaoDong_Wang](https://github.com/Dongzhixiao) |\n\n\n|相关链接|\n|---|\n|[Github 地址](https://github。com/Kivy-CN/Stanford-CS-229-CN)|\n|[知乎专栏](https://zhuanlan。zhihu。com/MachineLearn)|\n|[斯坦福大学 CS229 课程网站](http://cs229。stanford。edu/)|\n|[网易公开课中文字幕视频](http://open。163。com/movie/2008/1/M/C/M6SGF6VB4_M6SGHFBMC。html)|\n\n\n### 表示函数\n\n#### 1. 广义损失函数\n\n基于我们对监督学习的理解可知学习步骤：\n\n- $(1)$选择问题的表示形式，\n- $(2)$选择损失函数，\n- $(3)$最小化损失函数。\n\n让我们考虑一个稍微通用一点的监督学习公式。在我们已经考虑过的监督学习设置中，我们输入数据$x \\in \\mathbb{R}^{n}$和目标$y$来自空间$\\mathcal{Y}$。在线性回归中，相应的$y \\in \\mathbb{R}$，即$\\mathcal{Y}=\\mathbb{R}$。在logistic回归等二元分类问题中，我们有$y \\in \\mathcal{Y}=\\{-1,1\\}$，对于多标签分类问题，我们对于分类数为$k$的问题有$y \\in \\mathcal{Y}=\\{1,2, \\ldots, k\\}$。\n\n对于这些问题，我们对于某些向量$\\theta$基于$\\theta^Tx$做了预测，我们构建了一个损失函数$\\mathrm{L} : \\mathbb{R} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$，其中$\\mathrm{L}\\left(\\theta^{T} x, y\\right)$用于测量我们预测$\\theta^Tx$时的损失，对于logistic回归，我们使用logistic损失函数：\n\n$$\n\\mathrm{L}(z, y)=\\log \\left(1+e^{-y z}\\right) \\text { or } \\mathrm{L}\\left(\\theta^{T} x, y\\right)=\\log \\left(1+e^{-y \\theta^{T} x}\\right)\n$$\n\n对于线性回归，我们使用平方误差损失函数：\n\n$$\n\\mathrm{L}(z, y)=\\frac{1}{2}(z-y)^{2} \\quad \\text { or } \\quad \\mathrm{L}\\left(\\theta^{T} x, y\\right)=\\frac{1}{2}\\left(\\theta^{T} x-y\\right)^{2}\n$$\n\n对于多类分类，我们有一个小的变体，其中我们对于$\\theta_{i} \\in \\mathbb{R}^{n}$来说令$\\Theta=\\left[\\theta_{1} \\cdots \\theta_{k}\\right]$，并且使用损失函数$\\mathrm{L} : \\mathbb{R}^{k} \\times\\{1, \\ldots, k\\} \\rightarrow \\mathbb{R}$：\n\n$$\n\\mathrm{L}(z, y)=\\log \\left(\\sum_{i=1}^{k} \\exp \\left(z_{i}-z_{y}\\right)\\right) \\operatorname{or} \\mathrm{L}\\left(\\Theta^{T} x, y\\right)=\\log \\left(\\sum_{i=1}^{k} \\exp \\left(x^{T}\\left(\\theta_{i}-\\theta_{y}\\right)\\right)\\right)\n$$\n\n这个想法是我们想要对于所有$i \\neq k$得到$\\theta_{y}^{T} x>\\theta_{i}^{T}$。给定训练集合对$\\left\\{x^{(i)}, y^{(i)}\\right\\}$，通过最小化下面式子的经验风险来选择$\\theta$：\n\n$$\nJ(\\theta)=\\frac{1}{m} \\sum_{i=1}^{m} L\\left(\\theta^{T} x^{(i)}, y^{(i)}\\right)\\qquad\\qquad(1)\n$$\n\n#### 2 表示定理\n\n让我们考虑一个稍微不同方法来选择$\\theta$使得等式$(1)$中的风险最小化。在许多情况下——出于我们将在以后的课程中学习更多的原因——将正则化添加到风险$J$中是很有用的。我们添加正则化的原因很多：通常它使问题$(1)$容易计算出数值解，它还可以让我们使得等式$(1)$中的风险最小化而选择的$\\theta$够推广到未知数据。通常，正则化被认为是形式$r(\\theta)=\\|\\theta\\|$ 或者 $r(\\theta)=\\|\\theta\\|^{2}$其中$\\|\\cdot\\|$是$\\mathbb{R}^{n}$中的范数。最常用的正则化是$l_2$-正则化，公式如下：\n\n$$\nr(\\theta)=\\frac{\\lambda}{2}\\|\\theta\\|_{2}^{2}\n$$\n\n其中$\\|\\theta\\|_{2}=\\sqrt{\\theta^{T} \\theta}$被称作向量$\\theta$的欧几里得范数或长度。基于此可以得到正规化风险：\n\n$$\nJ_{\\lambda}(\\theta)=\\frac{1}{m} \\sum_{i=1}^{m} \\mathrm{L}\\left(\\theta^{T} x^{(i)}, y^{(i)}\\right)+\\frac{\\lambda}{2}\\|\\theta\\|_{2}^{2}\\qquad\\qquad(2)\n$$\n\n让我们考虑使得等式$(2)$中的风险最小化而选择的$\\theta$的结构。正如我们通常做的那样，我们假设对于每一个固定目标值$y \\in \\mathcal{Y}$，函数$\\mathrm{L}(z, y)$是关于$z$的凸函数。（这是线性回归、二元和多级逻辑回归的情况，以及我们将考虑的其他一些损失。）结果表明，在这些假设下，我们总是可以把问题$(2)$的解写成输入变量$x^{(i)}$的线性组合。更准确地说，我们有以下定理，称为表示定理。\n\n**定理2.1** 假设在正规化风险$(2)$的定义中有$\\lambda\\ge 0$。然后，可以得到令正则化化风险$(2)$最小化的式子：\n\n$$\n\\theta=\\sum_{i=1}^{m} \\alpha_{i} x^{(i)}\n$$\n\n其中$\\alpha_i$为一些实值权重。\n\n**证明** 为了直观，我们给出了在把$\\mathrm{L}(x,y)$看做关于$z$的可微函数，并且$\\lambda>0$的情况下结果的证明。详情见附录A，我们给出了定理的一个更一般的表述以及一个严格的证明。\n\n令$\\mathrm{L}^{\\prime}(z, y)=\\frac{\\partial}{\\partial z} L(z, y)$代表损失函数关于$z$的导致。则根据链式法则，我们得到了梯度恒等式：\n\n$$\n\\nabla_{\\theta} \\mathrm{L}\\left(\\theta^{T} x, y\\right)=\\mathrm{L}^{\\prime}\\left(\\theta^{T} x, y\\right) x \\text { and } \\nabla_{\\theta} \\frac{1}{2}\\|\\theta\\|_{2}^{2}=\\theta\n$$\n\n其中$\\nabla_{\\theta}$代表关于$\\theta$的梯度。由于风险在所有固定点（包括最小值点）的梯度必须为$0$，我们可以这样写：\n\n$$\n\\nabla J_{\\lambda}(\\theta)=\\frac{1}{m} \\sum_{i=1}^{m} \\mathrm{L}^{\\prime}\\left(\\theta^{T} x^{(i)}, y^{(i)}\\right) x^{(i)}+\\lambda \\theta=\\overrightarrow{0}\n$$\n\n特别的，令$w_{i}=\\mathrm{L}^{\\prime}\\left(\\theta^{T} x^{(i)}, y^{(i)}\\right)$，因为$\\mathrm{L}^{\\prime}\\left(\\theta^{T} x^{(i)}, y^{(i)}\\right)$是一个标量（依赖于$\\theta$，但是无论$\\theta$是多少，$w_i$始终是一个实数），所以我们有：\n\n$$\n\\theta=-\\frac{1}{\\lambda} \\sum_{i=1}^{n} w_{i} x^{(i)}\n$$\n\n设$\\alpha_{i}=-\\frac{w_{i}}{\\lambda}$以得到结果。\n\n#### 3 非线性特征与核\n\n基于表示定理$2.1$我们看到，我们可以写出向量$\\theta$的作为数据$\\left\\{x^{(i)}\\right\\}_{i=1}^{m}$的线性组合。重要的是，这意味着我们总能做出预测：\n\n$$\n\\theta^{T} x=x^{T} \\theta=\\sum_{i=1}^{m} \\alpha_{i} x^{T} x^{(i)}\n$$\n\n也就是说，在任何学习算法中，我们都可以将$\\theta^{T} x$替换成$\\sum_{i=1}^{m} \\alpha_{i} x^{(i)^{T}}x$，然后直接通过$\\alpha \\in \\mathbb{R}^{m}$使其最小化。\n\n让我们从更普遍的角度来考虑这个问题。在我们讨论线性回归时，我们遇到一个问题，输入$x$是房子的居住面积，我们考虑使用特征$x$，$x^2$和$x^3$（比方说）来进行回归，得到一个三次函数。为了区分这两组变量，我们将“原始”输入值称为问题的输入**属性**（在本例中，$x$是居住面积）。当它被映射到一些新的量集，然后传递给学习算法时，我们将这些新的量称为输入**特征**。（不幸的是，不同的作者使用不同的术语来描述这两件事，但是我们将在本节的笔记中始终如一地使用这个术语。）我们还将让$\\phi$表示特征映射，映射属性的功能。例如，在我们的例子中，我们有：\n\n$$\n\\phi(x)=\\left[ \\begin{array}{c}{x} \\\\ {x^{2}} \\\\ {x^{3}}\\end{array}\\right]\n$$\n\n与其使用原始输入属性$x$应用学习算法，不如使用一些特征$\\phi(x)$来学习。要做到这一点，我们只需要回顾一下之前的算法，并将其中的$x$替换为$\\phi(x)$。 \n\n因为算法可以完全用内积$\\langle x, z\\rangle$来表示，这意味着我们可以将这些内积替换为$\\langle\\phi(x), \\phi(z)\\rangle$。特别是给定一个特征映射$\\phi$，我们可以将相应**核**定义为：\n\n$$\nK(x, z)=\\phi(x)^{T} \\phi(z)\n$$\n\n然后，在我们之前的算法中，只要有$\\langle x, z\\rangle$，我们就可以用$K(x, z)$替换它，并且现在我们的算法可以使用特征$\\phi$来学习。让我们更仔细地写出来。我们通过表示定理（定理2.1）看到我们可以对于一些权重$\\alpha_i$写出$\\theta=\\sum_{i=1}^{m} \\alpha_{i} \\phi\\left(x^{(i)}\\right)$。然后我们可以写出（正则化）风险：\n\n$$\n\\begin{aligned} \nJ_{\\lambda}(\\theta) \n&=J_{\\lambda}(\\alpha) \\\\ \n&=\\frac{1}{m} \\sum_{i=1}^{m} L\\left(\\phi\\left(x^{(i)}\\right)^{T} \\sum_{j=1}^{m} \\alpha_{j} \\phi\\left(x^{(j)}\\right), y^{(i)}\\right)+\\frac{\\lambda}{2}\\left\\|\\sum_{i=1}^{m} \\alpha_{i} \\phi\\left(x^{(i)}\\right)\\right\\|_{2}^{2} \\\\ \n&=\\frac{1}{m} \\sum_{i=1}^{m} L\\left(\\sum_{j=1}^{m} \\alpha_{j} \\phi\\left(x^{(i)}\\right)^{T} \\phi\\left(x^{(j)}\\right), y^{(i)}\\right)+\\frac{\\lambda}{2} \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\alpha_{i} \\alpha_{j} \\phi\\left(x^{(i)}\\right)^{T} \\phi\\left(x^{(j)}\\right) \\\\ \n&=\\frac{1}{m} \\sum_{i=1}^{m} \\mathrm{L}\\left(\\sum_{j=1}^{m} \\alpha_{j} K\\left(x^{(i)}, x^{(j)}\\right)+\\frac{\\lambda}{2} \\sum_{i, j} \\alpha_{i} \\alpha_{i} K\\left(x^{(i)}, x^{(j)}\\right)\\right.\n\\end{aligned}\n$$\n\n也就是说，我们可以把整个损失函数写成核矩阵的最小值：\n\n$$\nK=\\left[K\\left(x^{(i)}, x^{(j)}\\right)\\right]_{i, j=1}^{m} \\in \\mathbb{R}^{m \\times m}\n$$\n\n现在，给定$\\phi$，我们可以很容易地通过$\\phi(x)$和$\\phi(z)$和内积计算$K(x,z)$。但更有趣的是，通常$K(x,z)$可能非常廉价的计算，即使$\\phi(x)$本身可能是非常难计算的（可能因为这是一个极高维的向量）。在这样的设置中，通过在我们的算法中一个有效的方法来计算$K(x,z)$，我们可以学习的高维特征空间空间由$\\phi$给出，但没有明确的找到或表达向量$\\phi(x)$。例如，一些核（对应于无限维的向量$\\phi$）包括：\n\n$$\nK(x, z)=\\exp \\left(-\\frac{1}{2 \\tau^{2}}\\|x-z\\|_{2}^{2}\\right)\n$$\n\n称为高斯或径向基函数(RBF)核，适用于任何维数的数据，或最小核（适用于$x\\in R$）由下式得：\n\n$$\nK(x, z)=\\min \\{x, z\\}\n$$\n\n有关这些内核机器的更多信息，请参见[支持向量机(SVMs)的课堂笔记](https://kivy-cn.github.io/Stanford-CS-229-CN/#/Markdown/cs229-notes3)。\n\n#### 4 核机器学习的随机梯度下降算法\n\n如果我们定义$K \\in \\mathbb{R}^{m \\times m}$为核矩阵，简而言之，定义向量：\n\n$$\nK^{(i)}=\\left[ \\begin{array}{c}{K\\left(x^{(i)}, x^{(1)}\\right)} \\\\ {K\\left(x^{(i)}, x^{(2)}\\right)} \\\\ {\\vdots} \\\\ {K\\left(x^{(i)}, x^{(m)}\\right)}\\end{array}\\right]\n$$\n\n其中$K=\\left[K^{(1)} K^{(2)} \\cdots K^{(m)}\\right]$，然后，我们可以将正则化风险写成如下简单的形式：\n\n$$\nJ_{\\lambda}(\\alpha)=\\frac{1}{m} \\sum_{i=1}^{m} \\mathrm{L}\\left(K^{(i)^{T}} \\alpha, y^{(i)}\\right)+\\frac{\\lambda}{2} \\alpha^{T} K \\alpha\n$$\n\n现在，让我们考虑对上述风险$J_{\\lambda}$取一个随机梯度。也就是说，我们希望构造一个（易于计算的）期望为$\\nabla J_{\\lambda}(\\alpha)$的随机向量，其没有太多的方差。为此,我们首先计算$J(\\alpha)$的梯度。我们通过下式来计算单个损失项的梯度：\n\n$$\n\\nabla_{\\alpha} \\mathrm{L}\\left(K^{(i)^{T}} \\alpha, y^{(i)}\\right)=\\mathrm{L}^{\\prime}\\left(K^{(i)^{T}} \\alpha, y^{(i)}\\right) K^{(i)}\n$$\n\n而：\n\n$$\n\\nabla_{\\alpha}\\left[\\frac{\\lambda}{2} \\alpha^{T} K \\alpha\\right]=\\lambda K \\alpha=\\lambda \\sum_{i=1}^{m} K^{(i)} \\alpha_{i}\n$$\n\n因此我们可得：\n\n$$\n\\nabla_{\\alpha} J_{\\lambda}(\\alpha)=\\frac{1}{m} \\sum_{i=1}^{m} \\mathrm{L}^{\\prime}\\left(K^{(i)^{T}} \\alpha, y^{(i)}\\right) K^{(i)}+\\lambda \\sum_{i=1}^{m} K^{(i)} \\alpha_{i}\n$$\n\n因此，如果我们选择一个随机索引 ，我们有下式：\n\n$$\n\\mathrm{L}^{\\prime}\\left(K^{(i)^{T}} \\alpha, y^{(i)}\\right) K^{(i)}+m \\lambda K^{(i)} \\alpha_{i}\n$$\n\n上式是关于$J_{\\lambda}(\\alpha)$的随机梯度。这给我们一个核监督学习问题的随机梯度算法，如图$1$所示。关于算法$1$，有一点需要注意：因为我们为了保持梯度的无偏性而在$\\lambda K^{(i)} \\alpha_{i}$项上乘了$m$，所以参数$\\lambda>0$不能太大，否则算法就会有点不稳定。此外，通常选择的步长是$\\eta_{t}=1 / \\sqrt{t}$，或者是它的常数倍。\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229noterff1.png)\n\n#### 5 支持向量机\n\n现在我们讨论支持向量机(SVM)的一种方法，它适用于标签为$y \\in\\{-1,1\\}$的二分类问题。并给出了损失函数$\\mathrm{L}$的一种特殊选择，特别是在支持向量机中，我们使用了基于边缘的损失函数：\n\n$$\n\\mathrm{L}(z, y)=[1-y z]_{+}=\\max \\{0,1-y z\\}\\qquad\\qquad(3)\n$$\n\n因此，在某种意义上，支持向量机只是我们前面描述的一般理论结果的一个特例。特别的，我们有经验正则化风险：\n\n$$\nJ_{\\lambda}(\\alpha)=\\frac{1}{m} \\sum_{i=1}^{m}\\left[1-y^{(i)} K^{(i)^{T}} \\alpha\\right]_{+}+\\frac{\\lambda}{2} \\alpha^{T} K \\alpha\n$$\n\n其中矩阵$K=\\left[K^{(1)} \\cdots K^{(m)}\\right]$通过$K_{i j}=K\\left(x^{(i)}, x^{(j)}\\right)$来定义。\n\n在课堂笔记中，你们可以看到另一种推导支持向量机的方法以及我们为什么称呼其为支持向量机的描述。\n\n#### 6 一个例子\n\n在本节中，我们考虑一个特殊的例子——核，称为高斯或径向基函数(RBF)核。这个核由下式给出：\n\n$$\nK(x, z)=\\exp \\left(-\\frac{1}{2 \\tau^{2}}\\|x-z\\|_{2}^{2}\\right)\\qquad\\qquad(4)\n$$\n\n其中$\\tau>0$是一个控制内核带宽的参数。直观地，当$\\tau$非常小时，除非$x \\approx z$我们将得到$K(x, z) \\approx 0$。即$x$和$z$非常接近，在这种情况下我们有$K(x, z) \\approx 1$。然而，当$\\tau$非常大时，则我们有一个更平滑的核函数$K$。这个核的功能函数$\\phi$是在无限维$^1$的空间中。即便如此，通过考虑一个新的例子$x$所做的分类，我们可以对内核有一些直观的认识：我们预测：\n\n<blockquote><details><summary>上一小段上标1的说明（详情请点击本行）</summary>\n\n如果你看过特征函数或者傅里叶变换，那么你可能会认出RBF核是均值为零，方差为$\\tau^{2}$的高斯分布的傅里叶变换。也就是在$\\mathbb{R}^{n}$中令$W\\sim \\mathrm{N}\\left(0, \\tau^{2} I_{n \\times n}\\right)$，使得$W$的概率密函数为$p(w)=\\frac{1}{\\left(2 \\pi \\tau^{2}\\right)^{n / 2}} \\exp \\left(-\\frac{\\|w\\|_{2}^{2}}{2 \\tau^{2}}\\right)$。令$i=\\sqrt{-1}$为虚数单位，则对于任意向量$v$我们可得：\n\n$$\n\\begin{aligned} \n\\mathbb{E}\\left[\\exp \\left(i v^{T} W\\right)\\right]\n=\\int \\exp \\left(i v^{T} w\\right) p(w) d w \n&=\\int \\frac{1}{\\left(2 \\pi \\tau^{2}\\right)^{n / 2}} \\exp \\left(i v^{T} w-\\frac{1}{2 \\tau^{2}}\\|w\\|_{2}^{2}\\right) d w \\\\ \n&=\\exp \\left(-\\frac{1}{2 \\tau^{2}}\\|v\\|_{2}^{2}\\right) \n\\end{aligned}\n$$\n\n因此，如果我们定义“向量”（实际上是函数）$\\phi(x, w)=e^{i x^{T} w}$并令$a^*$是$a \\in \\mathbb{C}$的共轭复数，则我们可得：\n\n$$\n\\mathbb{E}\\left[\\phi(x, W) \\phi(z, W)^{*}\\right]=\\mathbb{E}\\left[e^{i x^{T} W} e^{-i x^{T} W}\\right]=\\mathbb{E}\\left[\\exp \\left(i W^{T}(x-z)\\right)\\right]=\\exp \\left(-\\frac{1}{2 \\tau^{2}}\\|x-z\\|_{2}^{2}\\right)\n$$\n\n特别地，我们看到$K(x, z)$是一个函数空间的内积这个函数空间可以对$p(w)$积分。\n\n</details></blockquote>\n\n$$\n\\sum_{i=1}^{m} K\\left(x^{(i)}, x\\right) \\alpha_{i}=\\sum_{i=1}^{m} \\exp \\left(-\\frac{1}{2 \\tau^{2}}\\left\\|x^{(i)}-x\\right\\|_{2}^{2}\\right) \\alpha_{i}\n$$\n\n所以这就变成了一个权重，取决于$x$离每个$x^{(i)}$有多近，权重的贡献$\\alpha_i$乘以$x$到$x^{(i)}$的相似度，由核函数决定。\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229noterff2.png)\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229noterff3.png)\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229noterff4.png)\n\n在图$2$、$3$和$4$中，我们通过最小化下式显示了训练$6$个不同内核分类器的结果：\n\n$$\nJ_{\\lambda}(\\alpha)=\\sum_{i=1}^{m}\\left[1-y^{(i)} K^{(i)^{T}} \\alpha\\right]_{+}+\\frac{\\lambda}{2} \\alpha^{T} K \\alpha\n$$\n\n其中$m=200,\\lambda=1/m$，核公式$(4)$中的$\\tau$取不同的值。我们绘制了训练数据（正例为蓝色的x，负例为红色的o）以及最终分类器的决策面。也就是说，我们画的线由下式：\n\n$$\n\\left\\{x \\in \\mathbb{R}^{2} : \\sum_{i=1}^{m} K\\left(x, x^{(i)}\\right) \\alpha_{i}=0\\right\\}\n$$\n\n定义给出了学习分类器进行预测$\\sum_{i=1}^{m} K\\left(x, x^{(i)}\\right) \\alpha_{i}>0$和$\\sum_{i=1}^{m} K\\left(x, x^{(i)}\\right) \\alpha_{i}<0$的区域。从图中我们看到，对于数值较大的$\\tau$的一个非常简单的分类器：它几乎是线性的，而对于$τ=.1$，分类器有实质性的变化，是高度非线性的。为了便于参考，在图$5$中，我们根据训练数据绘制了最优分类器；在训练数据无限大的情况下，最优分类器能最大限度地减小误分类误差。\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229noterff5.png)\n\n#### A 一个更一般的表示定理\n\n在这一节中，我们给出了一个更一般版本的表示定理以及一个严格的证明。令$r : \\mathbb{R} \\rightarrow \\mathbb{R}$为任何非降函数的自变量，并考虑正则化风险：\n\n$$\nJ_{r}(\\theta)=\\frac{1}{m} \\sum_{i=1}^{m} \\mathrm{L}\\left(x^{(i)^{T}} \\theta, y^{(i)}\\right)+r\\left(\\|\\theta\\|_{2}\\right)\\qquad\\qquad(5)\n$$\n\n通常，我们取$r(t)=\\frac{\\lambda}{2} t^{2}$，与通常的选择$l_2$-正则化相对应。但是下一个定理表明这对于表示定理来说是不必要的。事实上，我们可以对所有的$t$取$r(t) = 0$，这个定理仍然成立。\n\n**定理 A.1** （向量空间$R^n$中的表示定理）。令$\\theta \\in \\mathbb{R}^{n}$为任意向量。则存在$\\alpha \\in \\mathbb{R}^{m}$和$\\theta^{(\\alpha)}=\\sum_{i=1}^{m} \\alpha_{i} x^{(i)}$使得：\n\n$$\nJ_{r}\\left(\\theta^{(\\alpha)}\\right) \\leq J_{r}(\\theta)\n$$\n\n特别的，没有普遍的损失函数总是假设我们可以最小化$J(\\theta)$来写出优化问题，其中最小化$J(\\theta)$时仅仅考虑$\\theta$在数据的张成的空间中的情况。\n\n**证明** 我们的证明依赖于线性代数中的一些性质，它允许我们证明简洁，但是如果你觉得太过简洁，请随意提问。\n\n向量$\\left\\{x^{(i)}\\right\\}_{i=1}^{m}$在向量空间$\\mathbb{R}^{n}$中。因此有$\\mathbb{R}^{n}$中的子空间$V$使得：\n\n$$\nV=\\left\\{\\sum_{i=1}^{m} \\beta_{i} x^{(i)} : \\beta_{i} \\in \\mathbb{R}\\right\\}\n$$\n\n那么$V$对于向量$v_{i} \\in \\mathbb{R}^{n}$有一个标准正交基$\\left\\{v_{1}, \\dots, v_{n_{0}}\\right\\}$，其中标准正交基的长度（维度）是$n_{0} \\leq n$。因此我们可以写出$V=\\left\\{\\sum_{i=1}^{n_{0}} b_{i} v_{i}:b_{i} \\in \\mathbb{R} \\right\\}$，回忆一下正交性是是指向量$v_i$满足$\\left\\|v_{i}\\right\\|_{2}=1$和对于任意$i\\neq j$有$v_{i}^{T} v_{j}=0$。还有一个正交子空间$V^{\\perp}=\\left\\{u \\in \\mathbb{R}^{n} : u^{T} v=0\\quad for\\quad all\\quad v \\in V\\right\\}$，其有一个维度是$n_{\\perp}=n-n_{0} \\geq 0$的正交基，我们可以写作$\\left\\{u_{1}, \\ldots, u_{n_{\\perp}}\\right\\} \\subset \\mathbb{R}^{n}$，其对于所有$i,j$都满足$u_{i}^{T} v_{j}=0$。\n\n因为$\\theta \\in \\mathbb{R}^{n}$，我们可以把它唯一地写成：\n\n$$\n\\theta=\\sum_{i=1}^{n_{0}} \\nu_{i} v_{i}+\\sum_{i=1}^{n_{\\perp}} \\mu_{i} u_{i}, \\quad \\text { where } \\nu_{i} \\in \\mathbb{R} \\text { and } \\mu_{i} \\in \\mathbb{R}\n$$\n\n其中$\\mu, \\nu$的值是唯一的。现在通过$\\left\\{x^{(i)}\\right\\}_{i=1}^{m}$张成的空间$V$的定义可知，存在$\\alpha \\in \\mathbb{R}^{m}$使得：\n\n$$\n\\sum_{i=1}^{n_{0}} \\nu_{i} v_{i}=\\sum_{i=1}^{m} \\alpha_{i} x^{(i)}\n$$\n\n因此我们有：\n\n$$\n\\theta=\\sum_{i=1}^{m} \\alpha_{i} x^{(i)}+\\sum_{i=1}^{n_{\\perp}} \\mu_{i} u_{i}\n$$\n\n定义$\\theta^{(\\alpha)}=\\sum_{i=1}^{m} \\alpha_{i} x^{(i)}$。现在对于任意数据点$x^{(j)}$，我们有：\n\n$$\nu_{i}^{T} x^{(j)}=0 \\text { for all } i=1, \\ldots, n_{\\perp}\n$$\n\n使得$u_{i}^{T} \\theta^{(\\alpha)}=0$。因此我们可得：\n\n$$\n\\|\\theta\\|_{2}^{2}=\\left\\|\\theta^{(\\alpha)}+\\sum_{i=1}^{n_{\\perp}} \\mu_{i} u_{i}\\right\\|_{2}^{2}=\\left\\|\\theta^{(\\alpha)}\\right\\|_{2}^{2}+\\underbrace{2 \\sum_{i=1}^{n_{\\perp}} \\mu_{i} u_{i}^{T} \\theta^{(\\alpha)}}_{=0}+\\left\\|\\sum_{i=1}^{n_{\\perp}} \\mu_{i} u_{i}\\right\\|_{2}^{2} \\geq\\left\\|\\theta^{(\\alpha)}\\right\\|_{2}^{2}\\quad(6a)\n$$\n\n同时我们可得：\n\n$$\n\\theta^{(\\alpha)^{T}} x^{(i)}=\\theta^{T} x^{(i)}\\qquad\\qquad(6b)\n$$\n\n对于所有点$x^{(i)}$都成立。\n\n即，通过使用$\\|\\theta\\|_{2} \\geq\\left\\|\\theta^{(\\alpha)}\\right\\|_{2}$以及等式$(6b)$，我们可得：\n\n$$\n\\begin{aligned}\nJ_{r}(\\theta)=\\frac{1}{m} \\sum_{i=1}^{m} \\mathrm{L}\\left(\\theta^{T} x^{(i)}, y^{(i)}\\right)+r\\left(\\|\\theta\\|_{2}\\right) &\\stackrel{(6 \\mathrm{b})}{=} \\frac{1}{m} \\sum_{i=1}^{m} \\mathrm{L}\\left(\\theta^{(\\alpha)^{T}} x^{(i)}, y^{(i)}\\right)+r\\left(\\|\\theta\\|_{2}\\right)\\\\\n&\\stackrel{(6 \\mathrm{a})}{ \\geq} \\frac{1}{m} \\sum_{i=1}^{m} \\mathrm{L}\\left(\\theta^{(\\alpha)^{T}} x^{(i)}, y^{(i)}\\right)+r\\left(\\left\\|\\theta^{(\\alpha)}\\right\\|_{2}\\right)\\\\\n&=J_{r}\\left(\\theta^{(\\alpha)}\\right)\n\\end{aligned}\n$$\n\n这是期望的结果。\n\n### 参考","tags":["CS229"],"categories":["CS229"]},{"title":"[机器学习-CS229]提升","url":"%2Fposts%2Fa3c2b0dc%2F","content":"# CS229 课程讲义中文翻译\nCS229 Supplementary notes\n\n|原作者|翻译|\n|---|---|\n|John Duchi|[XiaoDong_Wang](https://github.com/Dongzhixiao) |\n\n\n|相关链接|\n|---|\n|[Github 地址](https://github。com/Kivy-CN/Stanford-CS-229-CN)|\n|[知乎专栏](https://zhuanlan。zhihu。com/MachineLearn)|\n|[斯坦福大学 CS229 课程网站](http://cs229。stanford。edu/)|\n|[网易公开课中文字幕视频](http://open。163。com/movie/2008/1/M/C/M6SGF6VB4_M6SGHFBMC。html)|\n\n\n### 提升(Boosting)\n\n#### 1. 提升\n\n到目前为止，我们已经了解了如何在已经选择了数据表示的情况下解决分类（和其他）问题。我们现在讨论一个称为boost的过程，它最初是由Rob Schapire发现的，后来由Schapire和Yoav Freund进一步开发，它自动选择特性表示。我们采用了一种基于优化的视角，这与Freund和Schapire最初的解释和论证有些不同，但这有助于我们的方法的理解：\n- $(1)$选择模型表达，\n- $(2)$选择损失，\n- $(3)$最小化损失函数。\n\n在阐明这个问题之前，我们先对我们要做的事情有一点直觉。粗略地说，提升的思想是用一个弱学习算法——任何一个只要比随机选择好一点的分类学习算法——然后把它转换成一个比随机选择好很多的强分类器。为了对此建立一些直觉，考虑一个的数字识别案例，在这个案例中，我们希望区分$0$和$1$，我们接收到的图像必须进行分类。那么一个自然的弱学习器可能会取图像的中间像素，如果它是有颜色的，就把图像称为$1$，如果它是空白的，就把图像称为$0$。这个分类器可能远远不够完美，但它可能比随机分类器要好。提升过程通过收集这些弱分类器，然后对它们的贡献进行加权，从而形成一个比任何单个分类器精度都高得多的分类器。\n\n考虑到这一点，让我们来阐明这个问题。我们对提升的理解是在无限维空间中采用坐标下降法，虽然听起来很复杂，但其实该方法并不像听上去那么难。首先，假设我们有原始标签为$y \\in\\{-1,1\\}$的输入示例$x \\in \\mathbb{R}^{n}$，该示例在二分类中很常见。我们还假设我们有无穷多个特征函数$\\phi_{j} : \\mathbb{R}^{n} \\rightarrow\\{-1,1\\}$以及无穷向量$\\theta=\\left[ \\begin{array}{lll}{\\theta_{1}} & {\\theta_{2}} & {\\cdots}\\end{array}\\right]^{T}$，然而我们总假设只有有限数量的非零元素。我们使用的分类器如下：\n\n$$\nh_{\\theta}(x)=\\operatorname{sign}\\left(\\sum_{j=1}^{\\infty} \\theta_{j} \\phi_{j}(x)\\right)\n$$\n\n这里让我们随便使用下符号，并定义$\\theta^{T} \\phi(x)=\\sum_{j=1}^{\\infty} \\theta_{j} \\phi_{j}(x)$。\n\n在提升中，我们通常称特征$\\phi_{j}$为弱假设。给定一个训练集$\\left(x^{(1)}, y^{(1)}\\right), \\ldots,\\left(x^{(m)}, y^{(m)}\\right)$，我们称一个向量$p=\\left(p^{(1)}, \\ldots, p^{(m)}\\right)$为样本中的一个分布，如果对于所有$i$以及下式成立时有$p^{(i)} \\geq 0$。\n\n$$\n\\sum_{i=1}^{m} p^{(i)}=1\n$$\n\n然后我们称一个弱学习器具有边界$\\gamma>0$，如果对于任意在第$m$个训练样本的分布$p$存在一个弱假设$\\phi_j$，使得：\n\n$$\n\\sum_{i=1}^{m} p^{(i)} 1\\left\\{y^{(i)} \\neq \\phi_{j}\\left(x^{(i)}\\right)\\right\\} \\leq \\frac{1}{2}-\\gamma \\qquad\\qquad(1)\n$$\n\n`译者注：实际该式左边就是第j个弱学习器在测试集上分类错误的期望`也就是说，我们假设有一些分类器比对数据集的随机猜测稍微好一些。弱学习算法的存在只是一个假设，但令人惊讶的是，我们可以将任何弱学习算法转换成一个具有完美精度的算法。\n\n在更一般的情况下，我们假设我们采用了一个弱学习器算法，该算法以训练示例上的一个分布（权重）$p$作为输入，并返回一个性能略好于随机选择的分类器。我们将展示了在给定弱学习算法的条件下，boost如何返回在训练数据上具有完美精度的分类器。（诚然，我们希望分类器能够很好地推广到测试集数据中，但目前我们忽略了这个问题。)\n\n##### 1.1 提升算法\n\n粗略地说，boost首先在数据集中为每个训练示例分配相等的权重。然后，它接收到一个弱假设，该弱假设在当前训练实例的权重下分类性能良好，并将其合并到当前的分类模型中。然后对训练实例进行权重调整，在分类出错的例子上分配更高的权重，使弱学习算法集中于对这些例子进行分类的处理，而没有分类错误的例子上分配更低的权重。这种在训练数据上重复调整权重，使其虽然加上了一个学习能力较差的学习器，但是该学习器在当前分类器表现不佳的示例上做得很好，这样总体来说就产生了性能较好的分类器。\n\n该算法对分类问题的指数损失进行坐标下降法，目标为：\n\n$$\nJ(\\theta)=\\frac{1}{m} \\sum_{i=1}^{m} \\exp \\left(-y^{(i)} \\theta^{T} \\phi\\left(x^{(i)}\\right)\\right)\n$$\n\n我们首先展示如何使用坐标下降法更新计算得到损失函数$J(\\theta)$的准确形式。坐标下降迭代法如下：\n\n(i) 选择一个坐标$j \\in \\mathbb{N}$\n\n(ii) 更新$\\theta_j$为：\n\n$$\n\\theta_{j}=\\underset{\\theta_{j}}{\\operatorname{argmin}} J(\\theta)\n$$\n\n对于所有$k\\neq j$不相同时留下$\\theta_k$。\n\n我们迭代上面的过程直到收敛。\n\n在提升的情况下，由于自然指数函数分析很方便，坐标更新的推导并不困难。现在我们展示如何进行更新。假设我们想要更新坐标$k$。定义：\n\n$$\nw^{(i)}=\\exp \\left(-y^{(i)} \\sum_{j \\neq k} \\theta_{j} \\phi_{j}\\left(x^{(i)}\\right)\\right)\n$$\n\n为权重，并注意优化坐标$k$达到相应的最小化：\n\n$$\n\\sum_{i=1}^{m} w^{(i)} \\exp \\left(-y^{(i)} \\phi_{k}\\left(x^{(i)}\\right) \\alpha\\right)\n$$\n\n在$\\alpha=\\theta_k$时。现在定义：\n\n$$\nW^{+} :=\\sum_{i : y^{(i)} \\phi_{k}\\left(x^{(i)}\\right)=1} w^{(i)} \\text { and } W^{-} :=\\sum_{i : y^{(i)}\\phi_{k}\\left(x^{(i)}\\right)=-1} w^{(i)}\n$$\n\n为了实例权值之和使得$\\phi_k$分别分类正确和错误。找出$\\theta_k$与对下面的式子进行的选择相同：\n\n$$\n\\alpha=\\underset{\\alpha}{\\arg \\min }\\left\\{W^{+} e^{-\\alpha}+W^{-} e^{\\alpha}\\right\\}=\\frac{1}{2} \\log \\frac{W^{+}}{W^{-}}\n$$\n\n为了得到最终的等式，我们求导并将其设为零。我们得到$-W^{+} e^{-\\alpha}+W^{-} e^{\\alpha}=0$。即$W^{-} e^{2 \\alpha}=W^{+}$或$\\alpha=\\frac{1}{2} \\log \\frac{W^{+}}{W^{-}}$。\n\n剩下的就是选择特定的坐标来执行坐标下降。我们假设已经使用了如图$1$所示的弱学习器算法，在第$t$次迭代时使用具有分布$p$的训练集作为输入，并返回一个具有边缘分布$(1)$的弱假设$\\phi_t$。我们提出总提升算法如图$2$所示。它在迭代$t=1,2,3, \\ldots$中进行计算。我们通过弱学习算法在$t$时刻返回的$\\left\\{\\phi_{1}, \\dots, \\phi_{t}\\right\\}$来表示假设集。\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229notebf1.png)\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229notebf2.png)\n\n#### 2 提升的收敛性\n\n我们现在认为，提升程序达到$0$训练误差，我们也提供了收敛速度为零。为此，我们提出了一个保证取得提升的引理。\n\n**引理 2.1** 令：\n\n$$\nJ\\left(\\theta^{(t)}\\right)=\\frac{1}{m} \\sum_{i=1}^{m} \\exp \\left(-y^{(i)} \\sum_{\\tau=1}^{t} \\theta_{\\tau} \\phi_{\\tau}\\left(x^{(i)}\\right)\\right)\n$$\n\n则：\n\n$$\nJ\\left(\\theta^{(t)}\\right) \\leq \\sqrt{1-4 \\gamma^{2}} J\\left(\\theta^{(t-1)}\\right)\n$$\n\n由于引理的证明是有点复杂的，而不是本节笔记的中心——尽管知道该算法会收敛是很重要的！——我们把证明推迟到附录A.1。让我们描述一下它如何保证增强过程收敛到一个训练误差为零的分类器。\n\n我们在$\\theta^{(0)}=\\overrightarrow{0}$处初始化过程，以便初始经验损失为$J\\left(\\theta^{(0)}\\right)=1$。现在,我们注意到对于任何$\\theta$，误分类误差满足：\n\n$$\n1\\left\\{\\operatorname{sign}\\left(\\theta^{T} \\phi(x)\\right) \\neq y\\right\\}=1\\left\\{y \\theta^{T} \\phi(x) \\leq 0\\right\\} \\leq \\exp \\left(-y \\theta^{T} \\phi(x)\\right)\n$$\n\n因为对于所有$z\\ge 0$有$e^z\\ge 1$。由此可知，误分类错误率具有上界：\n\n$$\n\\frac{1}{m} \\sum_{i=1}^{m} 1\\left\\{\\operatorname{sign}\\left(\\theta^{T} \\phi\\left(x^{(i)}\\right)\\right) \\neq y^{(i)}\\right\\} \\leq J(\\theta)\n$$\n\n因此如果$J(\\theta)<\\frac{1}{m}$，则向量$\\theta$在训练样本中没有错误。经过$t$次提升迭代，得到经验风险满足：\n\n$$\nJ\\left(\\theta^{(t)}\\right) \\leq\\left(1-4 \\gamma^{2}\\right)^{\\frac{t}{2}} J\\left(\\theta^{(0)}\\right)=\\left(1-4 \\gamma^{2}\\right)^{\\frac{t}{2}}\n$$\n\n为了找出多少次迭代才能保证$J\\left(\\theta^{(t)}\\right)<\\frac{1}{m}$。我们对$J\\left(\\theta^{(t)}\\right)<1 / m$取对数：\n\n$$\n\\frac{t}{2} \\log \\left(1-4 \\gamma^{2}\\right)<\\log \\frac{1}{m}, \\quad \\text { or } \\quad t>\\frac{2 \\log m}{-\\log \\left(1-4 \\gamma^{2}\\right)}\n$$\n\n用一阶泰勒展开，得到$\\log \\left(1-4 \\gamma^{2}\\right) \\leq-4 \\gamma^{2}$，我们看到，如果我们使用提升的轮数——我们使用弱分类器的数量——满足下面的条件：\n\n$$\nt>\\frac{\\log m}{2 \\gamma^{2}} \\geq \\frac{2 \\log m}{-\\log \\left(1-4 \\gamma^{2}\\right)}\n$$\n\n则$J\\left(\\theta^{(t)}\\right)<\\frac{1}{m}$。\n\n#### 3 实现弱学习器(Implementing weak-learners)\n\n增强算法的一个主要优点是，它们可以自动从原始数据中为我们生成特性。此外，由于弱假设总是返回$\\{1,1\\}$中的值，所以在使用学习算法时不需要将特征标准化，使其具有相似的尺度，这在实践中会产生很大的差异。此外。虽然这不是理论上易于理解，许多类型的弱学习器程序中引入非线性智能分类器，可以产生比到目前为止我们已经看到的更多的表达模型的简单线性模型形式$\\theta^Tx$。\n\n##### 3.1 决策树桩(Decision stumps)\n\n有很多策略对应学习能力差的弱学习器，这里我们只关注其中一种，即决策树桩。为了具体说明这个方法，让我们假设输入变量$x \\in \\mathbb{R}^{n}$是实值的。决策树桩是一个函数$f$，它由阈值$s$和索引$j\\in\\{1,2,\\dots, n\\}$确定，然后返回：\n\n$$\n\\phi_{j, s}(x)=\\operatorname{sign}\\left(x_{j}-s\\right)=\\left\\{\\begin{array}{ll}{1} & {\\text { if } x_{j} \\geq s} \\\\ {-1} & {\\text { otherwise }}\\end{array}\\right. \\qquad\\qquad(2)\n$$\n\n正如我们现在所描述的，这些分类器非常简单，我们甚至可以有效地将它们用于加权数据集。\n\n事实上，一个决策树桩弱学习器的计算过程如下。我们从一个分布开始——权重$p^{(1)}, \\ldots, p^{(m)}$组成的集合中所有元素之和为$1$——在训练集上。我们希望选择公式$(2)$中的一个决策残差来最小化训练集上的误差，也就是说，我们希望找到一个阈值$s \\in \\mathbb{R}$和索引$j$，使得：\n\n$$\n\\widehat{\\operatorname{Err}}\\left(\\phi_{j, s}, p\\right)=\\sum_{i=1}^{m} p^{(i)} 1\\left\\{\\phi_{j, s}\\left(x^{(i)}\\right) \\neq y^{(i)}\\right\\}=\\sum_{i=1}^{m} p^{(i)} 1\\left\\{y^{(i)}\\left(x_{j}^{(i)}-s\\right) \\leq 0\\right\\}\\qquad\\qquad(3)\n$$\n\n达到最小。简单来说，这可能是一个低效的计算，但是一个更智能的过程允许我们在大约$O(nmlogm)$时间内解决这个问题。为每一个特征$j=1,2, \\dots, n$，我们对原始输入特性进行排序：\n\n$$\nx_{j}^{\\left(i_{1}\\right)} \\geq x_{j}^{\\left(i_{2}\\right)} \\geq \\cdots \\geq x_{j}^{\\left(i_{m}\\right)}\n$$\n\n由于决策残差的误差仅能改变$s$的值，所以决策残差只能改变$x_{j}^{(i)}$的值 一点巧妙的推导使我们能够计算：\n\n$$\n\\sum_{i=1}^{m} p^{(i)} 1\\left\\{y^{(i)}\\left(x_{j}^{(i)}-s\\right) \\leq 0\\right\\}=\\sum_{k=1}^{m} p^{\\left(i_{k}\\right)} 1\\left\\{y^{\\left(i_{k}\\right)}\\left(x_{j}^{\\left(i_{k}\\right)}-s\\right) \\leq 0\\right\\}\n$$\n\n以排序的顺序递增地修改和，这是有效的，在我们已经对值排序之后，这将花费$O(m)$的时间\n（这里我们不详细描述算法，留给感兴趣的读者来推导。）因此，对每个$n$个输入特性执行这种计算需要总时间$O(nmlogm)$ 我们可以选择指标$j$和阈值$s$来给出误差公式$(3)$的最佳决策残差。\n\n需要注意的一个非常重要的问题是，通过翻转重新排序的决策树桩的符号$\\phi_{j, s}$，我们实现误差$1-\\widehat{\\operatorname{Err}}\\left(\\phi_{j, s}, p\\right)$，即下式的误差：\n\n$$\n\\widehat{\\operatorname{Err}}\\left(-\\phi_{j, s}, p\\right)=1-\\widehat{\\operatorname{Err}}\\left(\\phi_{j, s}, p\\right)\n$$\n\n（你应该让自己相信这是真的。）因此，跟踪在所有阈值上$1-\\widehat{\\operatorname{Err}}\\left(\\phi_{j, s}, p\\right)$的最小值也是很重要的，因为这个可能比$\\widehat{\\operatorname{Err}}\\left(\\phi_{j, s}, p\\right)$小。这样可以给出一个更好的弱学习器。对我们的弱学习者使用这个过程（图1）给出了基本的，但非常有用的提升分类器。\n\n##### 3.2 例子\n\n现在，我们给出一个示例，展示对简单数据集进行提升的行为。特别地，我们考虑一个数据点为$x \\in \\mathbb{R}^{2}$的问题，其中最优分类器是：\n\n$$\ny=\\left\\{\\begin{array}{ll}{1} & {\\text { if } x_{1}<.6 \\text { and } x_{2}<.6} \\\\ {-1} & {\\text { otherwise }}\\end{array}\\right.\\qquad\\qquad(4)\n$$\n\n这是一个简单的非线性决策规则，但标准线性分类器（如logistic回归）是不可能学习的。在图$3$中，我们展示了logistic回归学习的最佳决策线，其中正例为圆圈，负例为X。很明显，logistic回归对数据的拟合不是特别好。\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229notebf3.png)\n\n然而，对于简单的非线性分类问题$(4)$，使用提升决策树桩可以获得更好的拟合效果。图$4$显示了经过不同次数的提升迭代之后我们学习到的增强分类器，使用的训练集大小为$m = 150$。从图中可以看出，第一个决策残差是将特征$x_1$的阈值设为$s \\approx .23$，即对于$s \\approx .23$有$\\phi(x)=\\operatorname{sign}\\left(x_{1}-s\\right)$。\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229notebf4.png)\n\n##### 3.3 其他策略\n\n在基本的增强决策树桩思想上有大量的变化。首先，我们不要求输入特性$x_j$是实值的。有时可能是分类的值，也就是说对于一些$k$，有$x_{j} \\in\\{1,2, \\ldots, k\\}$。在这种情况下，自然决策障碍就是下面的形式：\n\n$$\n\\phi_{j}(x)=\\left\\{\\begin{array}{ll}{1} & {\\text { if } x_{j}=l} \\\\ {-1} & {\\text { otherwise }}\\end{array}\\right.\n$$\n\n如果$x_{j} \\in C$，对于在标签下某些集合$C \\subset\\{1, \\ldots, k\\}$，与变量设定为$\\phi_{j}(x)=1$一样。\n\n另一种自然的变种是提升决策树，在这种树中，我们考虑的不是针对弱学习者的单一层次决策，而是特征的组合或决策树。谷歌可以帮助您找到关于这类问题的示例和信息。\n\n#### A 附录\n\n##### A.1 引理2.1的证明\n\n现在我们返回来证明进展引理。我们通过直接展示$t$时刻权重与$t-1$时刻权重之间的关系来证明这个结果。我们尤其注意到这一点：\n\n$$\nJ\\left(\\theta^{(t)}\\right)=\\min _{\\alpha}\\left\\{W_{t}^{+} e^{-\\alpha}+W_{t}^{-} e^{\\alpha}\\right\\}=2 \\sqrt{W_{t}^{+} W_{t}^{-}}\n$$\n\n以及：\n\n$$\nJ\\left(\\theta^{(t-1)}\\right)=\\frac{1}{m} \\sum_{i=1}^{m} \\exp \\left(-y^{(i)} \\sum_{\\tau=1}^{t-1} \\theta_{\\tau} \\phi_{\\tau}\\left(x^{(i)}\\right)\\right)=W_{t}^{+}+W_{t}^{-}\n$$\n\n我们通过弱学习假设知道：\n\n$$\n\\sum_{i=1}^{m} p^{(i)} 1\\left\\{y^{(i)} \\neq \\phi_{t}\\left(x^{(i)}\\right)\\right\\} \\leq \\frac{1}{2}-\\gamma, \\quad \\text { or } \\frac{1}{W_{t}^{+}+W_{t}^{-}}\\underbrace{\\sum_{i : y^{(i)} \\phi_{t}\\left(x^{(i)}\\right)=-1} w^{(i)}}_{=W_{t}^{-}}\\le\\frac 12-\\gamma\n$$\n\n重写这个表达式注意到右边的和就是$W_{t}^{-}$，我们有：\n\n$$\nW_{t}^{-} \\leq\\left(\\frac{1}{2}-\\gamma\\right)\\left(W_{t}^{+}+W_{t}^{-}\\right), \\quad \\text { or } \\quad W_{t}^{+} \\geq \\frac{1+2 \\gamma}{1-2 \\gamma} W_{t}^{-}\n$$\n\n通过在最小化定义$J\\left(\\theta^{(t)}\\right)$中使用$\\alpha=\\frac{1}{2} \\log \\frac{1+2 \\gamma}{1-2 \\gamma}$，我们可以得到下式：\n\n$$\n\\begin{aligned}\n J\\left(\\theta^{(t)}\\right) \n & \\leq W_{t}^{+} \\sqrt{\\frac{1-2 \\gamma}{1+2 \\gamma}}+W_{t}^{-} \\sqrt{\\frac{1+2 \\gamma}{1-2 \\gamma}} \\\\ \n &=W_{t}^{+} \\sqrt{\\frac{1-2 \\gamma}{1+2 \\gamma}}+W_{t}^{-}(1-2 \\gamma+2 \\gamma) \\sqrt{\\frac{1+2 \\gamma}{1-2 \\gamma}} \\\\ \n & \\leq W_{t}^{+} \\sqrt{\\frac{1-2 \\gamma}{1+2 \\gamma}}+W_{t}^{-}(1-2 \\gamma) \\sqrt{\\frac{1+2 \\gamma}{1-2 \\gamma}}+2 \\gamma \\frac{1-2 \\gamma}{1-2 \\gamma} \\sqrt{\\frac{1+2 \\gamma}{1-2 \\gamma}} W_{t}^{+} \\\\ \n &=W_{t}^{+}\\left[\\sqrt{\\frac{1-2 \\gamma}{1+2 \\gamma}}+2 \\gamma \\sqrt{\\frac{1-2 \\gamma}{1+2 \\gamma}}\\right]+W_{t}^{-} \\sqrt{1-4 \\gamma^{2}} \n \\end{aligned}\n$$\n\n其中我们使用了$W_{t}^{-} \\leq \\frac{1-2 \\gamma}{1+2 \\gamma} W_{t}^{+}$。做一些代数运算，我们看到最后的表达式等于：\n\n$$\n\\sqrt{1-4 \\gamma^{2}}\\left(W_{t}^{+}+W_{t}^{-}\\right)\n$$\n\n即$J\\left(\\theta^{(t)}\\right) \\leq \\sqrt{1-4 \\gamma^{2}} J\\left(\\theta^{(t-1)}\\right)$。 \n\n### 参考","tags":["CS229"],"categories":["CS229"]},{"title":"[机器学习-CS229]损失函数","url":"%2Fposts%2F8701aa3b%2F","content":"# CS229 课程讲义中文翻译\nCS229 Supplementary notes\n\n|原作者|翻译|\n|---|---|\n|John Duchi|[XiaoDong_Wang](https://github.com/Dongzhixiao) |\n\n\n|相关链接|\n|---|\n|[Github 地址](https://github。com/Kivy-CN/Stanford-CS-229-CN)|\n|[知乎专栏](https://zhuanlan。zhihu。com/MachineLearn)|\n|[斯坦福大学 CS229 课程网站](http://cs229。stanford。edu/)|\n|[网易公开课中文字幕视频](http://open。163。com/movie/2008/1/M/C/M6SGF6VB4_M6SGHFBMC。html)|\n\n\n### 损失函数\n\n#### 1. 二分类\n\n在**二分类问题**中，目标$y$只能取两个值。在本节笔记中，我们将展示如何建模这个问题，通过令$y \\in\\{-1,+1\\}$，这里如果这个例子是正类的一个元素，我们说$y$是$1$，如果这个例子是负类的一个元素，我们说$y = - 1$。和往常一样，我们假设我们的输入特征$x \\in \\mathbb{R}^{n}$。\n\n和我们处理监督学习问题的标准方法一样，我们首先为我们的假设类选择一个表示（我们试图学习的内容），然后选择一个损失函数，我们将使其最小化。在二分类问题中，通常使用假设类的形式为$h_{\\theta}(x)=\\theta^{T} x$比较方便，当出现一个新例子$x$，我们把它归类为正例或负例取决于$\\theta^{T} x$的符号，也就是说，我们预测如下的式子：\n\n$$\n\\operatorname{sign}\\left(h_{\\theta}(x)\\right)=\\operatorname{sign}\\left(\\theta^{T} x\\right) \\text { where } \\operatorname{sign}(t)=\\left\\{\\begin{array}{ll}{1} & {\\text { if } t>0} \\\\ {0} & {\\text { if } t=0} \\\\ {-1} & {\\text { if } t<0}\\end{array}\\right.\n$$\n\n那么在一个二元分类问题中，参数为$\\theta$的假设函数$h_\\theta$分类一个特定的例子$(x,y)$是否正确可以通过下式来判断：\n\n$$\n\\operatorname{sign}\\left(\\theta^{T} x\\right)=y \\quad \\text { or equivalently } \\quad y \\theta^{T} x>0 \\qquad\\qquad(1)\n$$\n\n式$(1)$中$y \\theta^{T} x$的值在二分类问题中是一个非常重要的量。重要到我们称下式的值\n\n$$\ny \\theta^{T} x\n$$\n\n为例子$(x,y)$的边界(margin)。尽管不总是这样，但是人们通常会将$h_{\\theta}(x)=x^{T} \\theta$的值解释为对参数向量$\\theta$为点$x$分配标签的置信度的度量。标签的判别标准是：如果$x^{T} \\theta$非常负（或非常正），则我们更坚信标签$y$是负例（正例）。\n\n既然选择了数据的表示形式，就必须选择损失函数。直观上，我们想要选择一些这样的损失函数，即对于我们的训练数据$\\left\\{\\left(x^{(i)}, y^{(i)}\\right)\\right\\}_{i=1}^{m}$来说，参数$\\theta$的选择让边界$y^{(i)} \\theta^{T} x^{(i)}$对于每一个训练样本都非常大。让我们修正一个假设的例子$(x,y)$，令$z=y x^{T} \\theta$代表边界，并且设$\\varphi : \\mathbb{R} \\rightarrow \\mathbb{R}$为损失函数——也就是说，例子$(x, y)$的损失在边界$z=y x^{T} \\theta$上是$\\varphi(z)=\\varphi\\left(y x^{T} \\theta\\right)$。对于任何特定的损失函数，我们最小化的经验风险是：\n\n$$\nJ(\\theta)=\\frac{1}{m} \\sum_{i=1}^{m} \\varphi\\left(y^{(i)} \\theta^{T} x^{(i)}\\right)\\qquad\\qquad(2)\n$$\n\n考虑我们想要的行为：我们希望$y^{(i)} \\theta^{T} x^{(i)}$对于每个训练样本$i=1, \\ldots, m$都是正的，我们应该惩罚那些$\\theta$，它们使得训练数据中经常出现$y^{(i)} \\theta^{T} x^{(i)}<0$。因此,一个直观的选择是，如果$z > 0$（边界是正的）我们的损失$\\varphi(z)$小，而如果$z < 0$（边界是负的）则$\\varphi(z)$大。也许最自然的这种损失是$0-1$损失，由下式得出：\n\n$$\n\\varphi_{\\mathrm{zo}}(z)=\\left\\{\\begin{array}{ll}{1} & {\\text { if } z \\leq 0} \\\\ {0} & {\\text { if } z>0}\\end{array}\\right.\n$$\n\n在这种情况下，损失$J(\\theta)$是简单的参数$\\theta$在训练集上使得结果错误（误分类）的平均数量。不幸的是，损失函数$\\phi_{zo}$是不连续、非凸的（为什么会出现这种情况的解释有点超出本课程的范围了），甚至更令人烦恼的是，使其最小化是一个$NP$难问题。因此，我们更愿意选择具有图$1$中所示形状的损失。也就是说，我们基本上总是使用满足下面条件的损失:\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229notelff1.png)\n\n$$\n\\varphi(z) \\rightarrow 0 \\text { as } z \\rightarrow \\infty, \\quad \\text { while } \\varphi(z) \\rightarrow \\infty \\text { as } z \\rightarrow-\\infty\n$$\n\n下面来一些不同的例子，这里有三个损失函数，我们将会在这门课或现在或以后上看到，这些损失函数都是在机器学习中很常用的。\n\n(i) logistic损失函数：\n\n$$\n\\varphi_{\\text { logistic }}(z)=\\log \\left(1+e^{-z}\\right)\n$$\n\n(ii) 铰链损失函数(hinge loss)：\n\n$$\n\\varphi_{\\text { hinge }}(z)=[1-z]_{+}=\\max \\{1-z, 0\\}\n$$\n\n(iii) 指数损失函数：\n\n$$\n\\varphi_{\\exp }(z)=e^{-z}\n$$\n\n在图$2$中，我们画出了这些损失相对于边界$z=y x^{T} \\theta$的图像，注意随着边界增长，每个损失函数都趋于零，并且每个损失函数往往随着边界负向增长而趋近$+\\infin$。不同的损失函数将产生不同的机器学习过程；特别要指出，logistic损失$\\varphi_{\\text { logistic }}$对应于逻辑回归问题，铰链损失$\\varphi_{\\text { hinge }}$产生所谓的支持向量机，以及指数损失产生的经典的提升版本，这两个我们以后将更深入地探讨。\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229notelff2.png)\n\n#### 2. 逻辑回归\n\n有了这一背景知识，我们现在对Andrew Ng课堂笔记中的逻辑回归给出了一个补充的观点。当我们使用二分标签$y \\in\\{-1,1\\}$时，可以更简洁地编写逻辑回归算法。特别地，我们使用Logistic损失：\n\n$$\n\\varphi_{\\text { logistic }}\\left(y x^{T} \\theta\\right)=\\log \\left(1+\\exp \\left(-y x^{T} \\theta\\right)\\right)\n$$\n\n和逻辑回归算法对应于选择$\\theta$来最小化下面的式子：\n\n$$\nJ(\\theta)=\\frac{1}{m} \\sum_{i=1}^{m} \\varphi_{\\text {logistic}}\\left(y^{(i)} \\theta^{T} x^{(i)}\\right)=\\frac{1}{m} \\sum_{i=1}^{m} \\log \\left(1+\\exp \\left(-y^{(i)} \\theta^{T} x^{(i)}\\right)\\right)\n$$\n\n粗略地，我们希望选择$\\theta$最小化平均logistic损失，即产生的一个$\\theta$对于大多数（甚至全部）训练样本，都可以使得$y^{(i)} \\theta^{T} x^{(i)}>0$。\n\n##### 2.1 概率解释\n\n与线性回归（最小二乘）类似，逻辑回归也可以用概率解释。为此，我们定义sigmoid函数（也常称为逻辑函数）：\n\n$$\ng(z)=\\frac{1}{1+e^{-z}}\n$$\n\n如图$3$所示。特别的，sigmoid函数满足：\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229notelff3.png)\n\n$$\ng(z)+g(-z)=\\frac{1}{1+e^{-z}}+\\frac{1}{1+e^{z}}=\\frac{e^{z}}{1+e^{z}}+\\frac{1}{1+e^{z}}=1\n$$\n\n因此我们可以用它来定义一个概率模型进行二分类。特别的，对于$y \\in\\{-1,1\\}$，我们将分类的逻辑模型定义为：\n\n$$\np(Y=y | x ; \\theta)=g\\left(y x^{T} \\theta\\right)=\\frac{1}{1+e^{-y x^{T} \\theta}}\n$$\n\n对于积分，我们看到如果边界$y x^{T} \\theta$很大——比如超过$5$等——则$p(Y=y | x ; \\theta)=g\\left(y x^{T} \\theta\\right) \\approx 1$，也就是说，我们给标签为$y$的事件分配了接近$1$的概率。相反地，如果$y x^{T} \\theta$很小，则$p(Y=y | x ; \\theta) \\approx 0$。\n\n通过将假设类重新定义为：\n\n$$\nh_{\\theta}(x)=g\\left(\\theta^{T} x\\right)=\\frac{1}{1+e^{-\\theta^{T} x}}\n$$\n\n然后我们得到训练数据的似然函数是：\n\n$$\nL(\\theta)=\\prod_{i=1}^{m} p\\left(Y=y^{(i)} | x^{(i)} ; \\theta\\right)=\\prod_{i=1}^{m} h_{\\theta}\\left(y^{(i)} x^{(i)}\\right)\n$$\n\n对数似然函数的精确解是：\n\n$$\n\\ell(\\theta)=\\sum_{i=1}^{m} \\log h_{\\theta}\\left(y^{(i)} x^{(i)}\\right)=-\\sum_{i=1}^{m} \\log \\left(1+e^{-y^{(i)} \\theta^{T} x^{(i)}}\\right)=-m J(\\theta)\n$$\n\n其中$J(\\theta)$是来自等式$(3)$的准确的逻辑回归损失。也就是说，逻辑模型中的最大似然$(4)$是和平均逻辑损失同样最小化了，我们又一次得到了逻辑回归。\n\n##### 2.2 梯度下降法\n\n逻辑回归的最后一部分是拟合实际模型。通常情况下，我们考虑基于梯度下降的过程来执行这种最小化。考虑到这一点，我们现在展示如何对逻辑损失求导。对于函数$\\varphi_{\\text { logistic }}(z)=\\log \\left(1+e^{-z}\\right)$我们有一维导数：\n\n$$\n\\frac{d}{d z} \\varphi_{\\text { logistic }}(z)=\\varphi_{\\text { logistic }}^{\\prime}(z)=\\frac{1}{1+e^{-z}} \\cdot \\frac{d}{d z} e^{-z}=-\\frac{e^{-z}}{1+e^{-z}}=-\\frac{1}{1+e^{z}}=-g(-z)\n$$\n\n其中$g$是sigmoid函数。然后我们应用链式法则来找出单独的训练样本$(x, y)$的结果，我们有\n\n$$\n\\frac{\\partial}{\\partial \\theta_{k}} \\varphi_{\\text {logistic}}\\left(y x^{T} \\theta\\right)=-g\\left(-y x^{T} \\theta\\right) \\frac{\\partial}{\\partial \\theta_{k}}\\left(y x^{T} \\theta\\right)=-g\\left(-y x^{T} \\theta\\right) y x_{k}\n$$\n\n因此，随机梯度下降最小化$J(\\theta)$算法对于$t=1,2, \\ldots$，$\\alpha_t$为在时间$t$时刻的步长，执行以下迭代：\n\n1. 随机均匀的从$i \\in\\{1, \\ldots, m\\}$中选择一个样本\n2. 执行梯度更新：\n\n$$\n\\begin{aligned} \n\\theta^{(t+1)} &=\\theta^{(t)}-\\alpha_{t} \\cdot \\nabla_{\\theta} \\varphi_{\\text { logistic }}\\left(y^{(i)} x^{(i)^{T}} \\theta^{(t)}\\right) \\\\ \n&=\\theta^{(t)}+\\alpha_{t} g\\left(-y^{(i)} x^{(i)^{T}} \\theta^{(t)}\\right) y^{(i)} x^{(i)}=\\theta^{(t)}+\\alpha_{t} h_{\\theta^{(t)}}\\left(-y^{(i)} x^{(i)}\\right) y^{(i)} x^{(i)} \n\\end{aligned}\n$$\n\n这个更新是很直观的：如果当前的假设$h_{\\theta^{(t)}}$为不正确的标签$-y^{(i)}$分配接近$1$的概率，则我们通过将$\\theta$朝着$y^{(i)} x^{(i)}$方向移动来尽量减少损失。相反，如果当前的假设$h_{\\theta^{(t)}}$为不正确的标签$-y^{(i)}$分配接近$0$的概率，则更新实际上什么也不做。 \n\n### 参考","tags":["CS229"],"categories":["CS229"]},{"title":"[机器学习-CS229]高斯过程","url":"%2Fposts%2Fd89cd700%2F","content":"# CS229 课程讲义中文翻译\nCS229 Section notes\n\n|原作者|翻译|\n|---|---|\n|Chuong B. Do|[XiaoDong_Wang](https://github.com/Dongzhixiao) |\n\n\n|相关链接|\n|---|\n|[Github 地址](https://github。com/Kivy-CN/Stanford-CS-229-CN)|\n|[知乎专栏](https://zhuanlan。zhihu。com/MachineLearn)|\n|[斯坦福大学 CS229 课程网站](http://cs229。stanford。edu/)|\n|[网易公开课中文字幕视频](http://open。163。com/movie/2008/1/M/C/M6SGF6VB4_M6SGHFBMC。html)|\n\n\n### 高斯过程\n\n#### 介绍\n\n我们在本课程上半部分讨论的许多经典机器学习算法都符合以下模式：给定一组从未知分布中采样的独立同分布的示例训练样本集：\n\n1. 求解一个凸优化问题，以确定数据单一的“最佳拟合”模型，并\n2. 使用这个估计模型对未来的测试输入点做出“最佳猜测”的预测。\n\n在本节的笔记中，我们将讨论一种不同的学习算法，称为**贝叶斯方法。** 与经典的学习算法不同，贝叶斯算法并不试图识别数据的“最佳匹配”模型（或者类似地，对新的测试输入做出“最佳猜测”的预测）。相反，其计算模型上的后验分布（或者类似地，计算新的输出的测试数据的后验预测分布）。这些分布提供了一种有用的方法来量化模型估计中的不确定性，并利用我们对这种不确定性的知识来对新的测试点做出更可靠的预测。\n\n我们来关注下**回归**问题，即：目标是学习从某个$n$维向量的输入空间$\\mathcal{X} = R^n$到实值目标的输出空间$\\mathcal{Y} = R$的映射。特别地，我们将讨论一个基于核的完全贝叶斯回归算法，称为高斯过程回归。本节的笔记中涉及的内容主要包括我们之前在课堂上讨论过的许多不同主题（即线性回归$^1$的概率解释、贝叶斯方法$^2$、核方法$^3$和多元高斯$^4$的性质）。\n\n>1 参见“[监督学习，判别算法](https://kivy-cn.github.io/Stanford-CS-229-CN/#/Markdown/cs229-notes1)”课程讲义。\n\n>2 参见“[正则化和模型选择](https://kivy-cn.github.io/Stanford-CS-229-CN/#/Markdown/cs229-notes5)”课程讲义。\n\n>3 参见“[支持向量机](https://kivy-cn.github.io/Stanford-CS-229-CN/#/Markdown/cs229-notes3)”课程讲义。\n\n>4 参见“[因子分析](https://kivy-cn.github.io/Stanford-CS-229-CN/#/Markdown/cs229-notes9)”课程讲义。\n\n本节的笔记后续内容的组织如下。在第1小节中，我们简要回顾了多元高斯分布及其性质。在第2小节中，我们简要回顾了贝叶斯方法在概率线性回归中的应用。第3小节给出了高斯过程的中心思想，第4小节给出了完整的高斯过程回归模型。\n\n#### 1. 多元高斯分布\n\n我们称一个概率密度函数是一个均值为$\\mu\\in R^n$，协方差矩阵为$\\Sigma\\in S_{++}^n$的一个**多元正态分布（或高斯分布）(multivariate normal (or Gaussian) distribution)，** 其随机变量是向量值$x\\in R^n$，该概率密度函数可以通过下式表达：\n\n$$\np(x;\\mu,\\Sigma)=\\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)\\right)\\qquad\\qquad(1)\n$$\n\n我们可以写作$x\\sim\\mathcal{N}(\\mu,\\Sigma)$。这里，回想一下线性代数的笔记中$S_{++}^n$指的是对称正定$n\\times n$矩阵$^5$的空间。\n\n>5 实际上，在某些情况下，我们需要处理的多元高斯分布的$\\Sigma$是正半定而非正定（即，$\\Sigma$不满秩）。在这种情况下，$\\Sigma^{-1}$不存在，所以$(1)$式中给出的高斯概率密度函数的定义并不适用。例子可以参阅“[因子分析](https://kivy-cn.github.io/Stanford-CS-229-CN/#/Markdown/cs229-notes9)”课程讲义。\n\n一般来说，高斯随机变量在机器学习和统计中非常有用，主要有两个原因。首先，它们在统计算法中建模“噪声”时非常常见。通常，噪声可以被认为是影响测量过程的大量独立的小随机扰动的累积；根据中心极限定理，独立随机变量的和趋于高斯分布。其次，高斯随机变量对于许多分析操作都很方便，因为许多涉及高斯分布的积分实际上都有简单的闭式解。在本小节的其余部分，我们将回顾多元高斯分布的一些有用性质。\n\n给定随机向量$x \\in R^{n}$服从多元高斯分布$x\\sim\\mathcal{N}(\\mu,\\Sigma)$。假设$x$中的变量被分成两个集合$x_{A}=\\left[x_{1} \\cdots x_{r}\\right]^{T} \\in R^{r}$和$x_{B}=\\left[x_{r+1} \\cdots x_{n}\\right]^{T} \\in R^{n-r}$（对于$\\mu$和$\\Sigma$也进行同样的拆分），则有：\n\n$$\nx=\\left[ \\begin{array}{c}{x_{A}} \\\\ {x_{B}}\\end{array}\\right] \\qquad \n\\mu=\\left[ \\begin{array}{c}{\\mu_{A}} \\\\ {\\mu_{B}}\\end{array}\\right] \\qquad \n\\Sigma=\\left[ \\begin{array}{cc}{\\sum_{A A}} & {\\sum_{A B}} \\\\ {\\Sigma_{B A}} & {\\Sigma_{B B}}\\end{array}\\right]\n$$\n\n因为$\\Sigma=E\\left[(x-\\mu)(x-\\mu)^{T}\\right]=\\Sigma^{T}$，所以上式中有$\\Sigma_{A B}=\\Sigma_{B A}^{T}$。下列性质适用：\n\n1. **规范性。** 概率密度函数的归一化，即：\n\n$$\n\\int_{x} p(x ; \\mu, \\Sigma) dx = 1\n$$\n\n这个特性乍一看似乎微不足道，但实际上对于计算各种积分非常有用，即使是那些看起来与概率分布完全无关的积分（参见附录A.1）！\n\n2. **边缘性。** 边缘概率密度函数：\n\n$$\n\\begin{aligned} p\\left(x_{A}\\right) &=\\int_{x_{B}} p\\left(x_{A} , x_{B} ; \\mu, \\Sigma\\right) d x_{B} \\\\ \np\\left(x_{B}\\right) &=\\int_{x_{A}} p\\left(x_{A}, x_{B} ; \\mu,\\Sigma\\right) d x_{A} \\end{aligned}\n$$\n\n是高斯分布：\n\n$$\n\\begin{aligned} x_{A} & \\sim \\mathcal{N}\\left(\\mu_{A}, \\Sigma_{A A}\\right) \\\\ \nx_{B} & \\sim \\mathcal{N}\\left(\\mu_{B}, \\Sigma_{B B}\\right) \\end{aligned}\n$$\n\n3. **条件性。** 条件概率密度函数：\n\n$$\n\\begin{aligned} p\\left(x_{A} | x_{B}\\right) &=\\frac{p\\left(x_{A}, x_{B} ; \\mu, \\Sigma\\right)}{\\int_{x_{A}} p\\left(x_{A}, x_{B} ; \\mu, \\Sigma\\right) d x_{A}} \\\\ p\\left(x_{B} | x_{A}\\right) &=\\frac{p\\left(x_{A}, x_{B} ; \\mu, \\Sigma\\right)}{\\int_{x_{B}} p\\left(x_{A}, x_{B} ; \\mu, \\Sigma\\right) d x_{B}} \\end{aligned}\n$$\n\n是高斯分布：\n\n$$\nx_{A} | x_{B} \\sim \\mathcal{N}\\left(\\mu_{A}+\\Sigma_{A B} \\Sigma_{B B}^{-1}\\left(x_{B}-\\mu_{B}\\right), \\Sigma_{A A}-\\Sigma_{A B} \\Sigma_{B B}^{-1} \\Sigma_{B A}\\right) \\\\\nx_{B} | x_{A} \\sim \\mathcal{N}\\left(\\mu_{B}+\\Sigma_{B A} \\Sigma_{A A}^{-1}\\left(x_{A}-\\mu_{A}\\right), \\Sigma_{B B}-\\Sigma_{B A} \\Sigma_{A A}^{-1} \\Sigma_{A B}\\right)\n$$\n\n附录A.2给出了这一性质的证明。（参见附录A.3的更简单的派生版本。）\n\n4. **求和性。** （相同维数的）独立高斯随机变量$y \\sim \\mathcal{N}(\\mu, \\Sigma)$和$z \\sim \\mathcal{N}\\left(\\mu^{\\prime}, \\Sigma^{\\prime}\\right)$之和同样是高斯分布：\n\n$$\ny+z \\sim \\mathcal{N}\\left(\\mu+\\mu^{\\prime}, \\Sigma+\\Sigma^{\\prime}\\right)\n$$\n\n#### 2. 贝叶斯线性回归\n\n设$S=\\left\\{\\left(x^{(i)}, y^{(i)}\\right)\\right\\}_{i=1}^{m}$是一组来自未知分布的满足独立同分布的训练集。线性回归的标准概率解释的公式说明了这一点：\n\n$$\ny^{(i)}=\\theta^{T} x^{(i)}+\\varepsilon^{(i)}, \\quad i=1, \\dots, m\n$$\n\n其中$\\varepsilon^{(i)}$是独立同分布的“噪声”变量并且服从分布$\\mathcal{N}(0,\\Sigma^2)$，由此可见$y^{(i)}-\\theta^{T} x^{(i)} \\sim \\mathcal{N}\\left(0, \\sigma^{2}\\right)$，或等价表示为：\n\n$$\nP\\left(y^{(i)} | x^{(i)}, \\theta\\right)=\\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left(-\\frac{\\left(y^{(i)}-\\theta^{T} x^{(i)}\\right)^{2}}{2 \\sigma^{2}}\\right)\n$$\n\n为了方便标记，我们定义了：\n\n$$\nX=\\left[ \\begin{array}{c}{-\\left(x^{(1)}\\right)^{T}-} \\\\ {-\\left(x^{(2)}\\right)^{T}-} \\\\ {\\vdots} \\\\ {-\\left(x^{(m)}\\right)^{T}-}\\end{array}\\right] \\in \\mathbf{R}^{m \\times n} \\qquad \n\\vec{y}=\\left[ \\begin{array}{c}{y^{(1)}} \\\\ {y^{(2)}} \\\\ {\\vdots} \\\\ {y^{(m)}}\\end{array}\\right] \\in \\mathbf{R}^{m} \\qquad \n\\overrightarrow{\\varepsilon}=\\left[ \\begin{array}{c}{\\varepsilon^{(1)}} \\\\ {\\varepsilon^{(2)}} \\\\ {\\vdots} \\\\ {\\varepsilon^{(m)}}\\end{array}\\right] \\in \\mathbf{R}^{m}\n$$\n\n在贝叶斯线性回归中，我们假设参数的**先验分布**也是给定的；例如，一个典型的选择是$\\theta \\sim \\mathcal{N}\\left(0, \\tau^{2} I\\right)$。使用贝叶斯规则可以得到**后验参数：**\n\n$$\np(\\theta | S)=\\frac{p(\\theta) p(S | \\theta)}{\\int_{\\theta^{\\prime}} p\\left(\\theta^{\\prime}\\right) p\\left(S | \\theta^{\\prime}\\right) d \\theta^{\\prime}}=\\frac{p(\\theta) \\prod_{i=1}^{m} p\\left(y^{(i)} | x^{(i)}, \\theta\\right)}{\\int_{\\theta^{\\prime}} p\\left(\\theta^{\\prime}\\right) \\prod_{i=1}^{m} p\\left(y^{(i)} | x^{(i)}, \\theta^{\\prime}\\right) d \\theta^{\\prime}}\\qquad\\qquad(2)\n$$\n\n假设测试点上的噪声模型与我们的训练点上的噪声模型相同，那么贝叶斯线性回归在一个新的测试点$x_*$上的“输出”不只是一个猜测$y_*$，而可能是输出的整个概率分布，称为**后验预测分布：**\n\n$$\np\\left(y_{*} | x_{*}, S\\right)=\\int_{\\theta} p\\left(y_{*} | x_{*}, \\theta\\right) p(\\theta | S) d \\theta \\qquad\\qquad(3)\n$$\n\n对于许多类型的模型，$(2)$和$(3)$中的积分是很难计算的，因此，我们经常使用近似的方法，例如MAP估计（参见[正则化和模型选择](https://kivy-cn.github.io/Stanford-CS-229-CN/#/Markdown/cs229-notes5)的课程讲义）。\n\n然而，在贝叶斯线性回归的情况下，积分实际上是可处理的！特别是对于贝叶斯线性回归，（在做了大量工作之后！）我们可以证明：\n\n$$\n\\theta | S \\sim \\mathcal{N}\\left(\\frac{1}{\\sigma^{2}} A^{-1} X^{T} \\vec{y}, A^{-1}\\right) \\\\ \ny_{*} | x_{*}, S \\sim \\mathcal{N}\\left(\\frac{1}{\\sigma^{2}} x_{*}^{T} A^{-1} X^{T} \\vec{y}, x_{*}^{T} A^{-1} x_{*}+\\sigma^{2}\\right)\n$$\n\n其中$A=\\frac{1}{\\sigma^{2}} X^{T} X+\\frac{1}{\\tau^{2}} I$。这些公式的推导有点复杂。$^6$但是从这些方程中，我们至少可以大致了解贝叶斯方法的含义：对于测试输入$x_*$，测试输出$y_*$的后验分布是高斯分布——这个分布反映了在我们预测$y_{*}=\\theta^{T} x_{*}+\\varepsilon_{*}$时，由$\\epsilon_*$的随机性以及我们选择参数$\\theta$的不确定而导致预测结果的不确定性。相反，古典概率线性回归模型直接从训练数据估计参数$\\theta$，但没有提供估计这些参数的可靠性（参见图1）。\n\n>6 有关完整的推导，可以参考[1]`注：参考资料[1]见文章最下方`。或者参考附录，其中给出了一些基于平方补全技巧的参数，请自己推导这个公式！\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229notegpf1.png)\n\n图1：一维线性回归问题的贝叶斯线性回归$y^{(i)}=\\theta x^{(i)}+\\epsilon^{(i)}$，其中噪音独立同分布的服从$\\epsilon^{(i)}\\sim \\mathcal{N}(0,1)$。绿色区域表示模型预测的$95\\%$置信区间。注意，绿色区域的（垂直）宽度在末端最大，但在中部最窄。这个区域反映了参数$\\theta$估计的不确定性。与之相反，经典线性回归模型会显示一个等宽的置信区域，在输出中只反映噪声服从$\\mathcal{N}(0,\\sigma^2)$。\n\n#### 3. 高斯过程\n\n如第$1$节所述，多元高斯分布由于其良好的分析性质，对于实值变量的有限集合建模是有用的。高斯过程是多元高斯函数的推广，适用于无穷大小的实值变量集合。特别地，这个扩展将允许我们把高斯过程看作不仅仅是随机向量上的分布，而实际上是**随机函数**上的分布。\n\n>7 令$\\mathcal{H}$是一类$\\mathcal{X}\\rightarrow\\mathcal{Y}$的函数映射。一个来自$\\mathcal{H}$的随机函数$f(\\cdot)$代表根据$\\mathcal{H}$的概率分布随机从$\\mathcal{H}$中选择一个函数。一个潜在的困惑是：你可能倾向于认为随机函数的输出在某种程度上是随机的；事实并非如此。一个随机函数$f(\\cdot)$，一旦有概率的从$\\mathcal{H}$中选择，则表示从输入$\\mathcal{X}$到输出$\\mathcal{Y}$的确定性映射。\n\n##### 3.1 有限域函数上的概率分布\n\n要了解如何对函数上的概率分布进行参数化，请考虑下面的简单示例。设$\\mathcal{X}=\\left\\{x_{1}, \\dots, x_{m}\\right\\}$为任何有限元素集。现在，考虑集合$\\mathcal{H}$，该集合代表所有可能的从$\\mathcal{X}$到$R$的函数映射。例如，可以给出如下的函数$f_0(\\cdot)\\in\\mathcal{H}$的例子：\n\n$$\nf_{0}\\left(x_{1}\\right)=5, \\quad f_{0}\\left(x_{2}\\right)=2.3, \\quad f_{0}\\left(x_{2}\\right)=-7, \\quad \\ldots, \\quad f_{0}\\left(x_{m-1}\\right)=-\\pi, \\quad f_{0}\\left(x_{m}\\right)=8\n$$\n\n因为任意函数$f(\\cdot) \\in \\mathcal{H}$的定义域仅有$m$个元素，所以我们可以简介的使用$m$维向量$\\vec{f}=\\left[f\\left(x_{1}\\right) \\quad f\\left(x_{2}\\right) \\quad \\cdots \\quad f\\left(x_{m}\\right)\\right]^{T}$表达$f(\\cdot)$。为了指定函数$f(\\cdot) \\in \\mathcal{H}$上的概率分布，我们必须把一些“概率密度”与$\\mathcal{H}$中的每个函数联系起来。一种自然的方法是利用函数$f(\\cdot) \\in \\mathcal{H}$和他们的向量表示$\\vec{f}$之间的一一对应关系。特别是，如果我们指定$\\vec{f} \\sim \\mathcal{N}\\left(\\overrightarrow{\\mu}, \\sigma^{2} I\\right)$，则意味着函数$f(\\cdot)$上的概率分布，其中函数$f(\\cdot)$的概率密度函数可以通过下面的式子给出：\n\n$$\np(h)=\\prod_{i=1}^{m} \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left(-\\frac{1}{2 \\sigma^{2}}\\left(f\\left(x_{i}\\right)-\\mu_{i}\\right)^{2}\\right)\n$$\n\n在上面的例子中，我们证明了有限域函数上的概率分布可以用函数输出$f\\left(x_{1}\\right), \\ldots, f\\left(x_{m}\\right)$的有限数量的输入点$x_{1}, \\dots, x_{m}$上的有限维多元高斯分布来表示。当定义域的大小可能是无穷大时，我们如何指定函数上的概率分布？为此，我们转向一种更奇特的概率分布类型，称为高斯过程。\n\n##### 3.2 无限域函数上的概率分布\n\n随机过程是随机变量的集合$\\{f(x) : x \\in \\mathcal{X}\\}$，其来自某个集合$\\mathcal{X}$的元素索引，称为索引集。$^8$ **高斯过程**是一个随机过程，任何有限子集合的随机变量都有一个多元高斯分布。\n\n>8 通常，当$\\mathcal{X} = R$时，可以将标识符$x\\in \\mathcal{X}$解释为表示时间，因此变量$f(x)$表示随时间的随机量的时间演化。然而，在高斯过程回归模型中，将标识符集作为回归问题的输入空间。\n\n特别是一组随机变量集合$\\{f(x) : x \\in \\mathcal{X}\\}$被称为来自于一个具有**平均函数**$m(\\cdot)$和**协方差函数**$k(\\cdot, \\cdot)$的高斯过程，满足对于任意元素是$x_{1}, \\ldots, x_{m} \\in \\mathcal{X}$有限集合，相关的有限随机变量集$f\\left(x_{1}\\right), \\ldots, f\\left(x_{m}\\right)$具有如下分布：\n\n$$\n\\left[ \\begin{array}{c}{f\\left(x_{1}\\right)} \\\\ {\\vdots} \\\\ {f\\left(x_{m}\\right)}\\end{array}\\right]\\sim\n\\mathcal{N}\\left(\\left[ \\begin{array}{c}{m\\left(x_{1}\\right)} \\\\ {\\vdots} \\\\ {m\\left(x_{m}\\right)}\\end{array}\\right], \\left[ \\begin{array}{ccc}{k\\left(x_{1}, x_{1}\\right)} & {\\cdots} & {k\\left(x_{1}, x_{m}\\right)} \\\\ {\\vdots} & {\\ddots} & {\\vdots} \\\\ {k\\left(x_{m}, x_{1}\\right)} & {\\cdots} & {k\\left(x_{m}, x_{m}\\right)}\\end{array}\\right]\\right)\n$$\n\n我们用下面的符号来表示：\n\n$$\nf(\\cdot) \\sim \\mathcal{G P}(m(\\cdot), k(\\cdot, \\cdot))\n$$\n\n注意，均值函数和协方差函数的名称很恰当，因为上述性质意味着：\n\n$$\n\\begin{aligned} m(x) &=E[x] \\\\ k\\left(x, x^{\\prime}\\right) &=E\\left[(x-m(x))\\left(x^{\\prime}-m\\left(x^{\\prime}\\right)\\right)\\right.\\end{aligned}\n$$\n\n对于任意$x,x'\\in\\mathcal{X}$。\n\n直观地说，我们可以把从高斯过程中得到的函数$f(\\cdot)$看作是由高维多元高斯函数得到的高维向量。这里，高斯函数的每个维数对应于标识符集合$\\mathcal{X}$中的一个元素$x$，随机向量的对应分量表示$f(x)$的值。利用多元高斯函数的边缘性，我们可以得到任意有限子集合所对应的多元高斯函数的边缘概率密度函数。\n\n什么样的函数$m(\\cdot)$和$k(\\cdot,\\cdot)$才能产生有效的高斯过程呢？一般情况下，任何实值函数$m(\\cdot)$都是可以接受的，但是对于$k(\\cdot,\\cdot)$，对于任何一组元素$x_{1}, \\ldots, x_{m} \\in \\mathcal{X}$都必须是可以接受的，结果矩阵如下：\n\n$$\nK=\\left[ \\begin{array}{ccc}{k\\left(x_{1}, x_{1}\\right)} & {\\cdots} & {k\\left(x_{1}, x_{m}\\right)} \\\\ {\\vdots} & {\\ddots} & {\\vdots} \\\\ {k\\left(x_{m}, x_{1}\\right)} & {\\cdots} & {k\\left(x_{m}, x_{m}\\right)}\\end{array}\\right]\n$$\n\n是一个有效的协方差矩阵，对应于某个多元高斯分布。概率论中的一个标准结果表明，如果$K$是正半定的，这是正确的。听起来是不是很熟悉？\n\n基于任意输入点计算协方差矩阵的正半定条件，实际上与核的Mercer条件相同！函数$k(\\cdot,\\cdot)$是一个有效的核，前提是对于任意一组输入点$x_{1}, \\ldots, x_{m} \\in \\mathcal{X}$，因此，任何有效的核函数都可以用作协方差函数，这就是基于核的概率分布。\n\n##### 3.3 平方指数核\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229notegpf2.png)\n\n图2：样本来自于一个零均值高斯过程，以$k_{S E}(\\cdot, \\cdot)$为先验协方差函数。使用(a) $\\tau=0.5,$ (b) $\\tau=2,$ and $(\\mathrm{c}) \\tau=10$。注意,随着带宽参数$\\tau$的增加，然后点比以前更远会有较高的相关性，因此采样函数往往整体是流畅的。\n\n为了直观地了解高斯过程是如何工作的，考虑一个简单的零均值高斯过程：\n\n$$\nf(\\cdot) \\sim \\mathcal{G P}(0, k(\\cdot, \\cdot))\n$$\n\n定义一些函数$h:\\mathcal{X}\\rightarrow R$，其中$\\mathcal{X}=R$。这里，我们选择核函数 作为**平方指数**$^9$核函数，定义如下：\n\n>9 在支持向量机的背景下，我们称之为高斯核；为了避免与高斯过程混淆，我们将这个核称为平方指数核，尽管这两个核在形式上是相同的。\n\n$$\nk_{S E}\\left(x, x^{\\prime}\\right)=\\exp \\left(-\\frac{1}{2 \\tau^{2}}\\left\\|x-x^{\\prime}\\right\\|^{2}\\right)\n$$\n\n对于一些$\\tau> 0$。从这个高斯过程中采样的随机函数是什么样的？\n\n在我们的例子中，由于我们使用的是一个零均值高斯过程，我们期望高斯过程中的函数值会趋向于分布在零附近。此外，对于任意一对元素$x, x^{\\prime} \\in \\mathcal{X}$。\n\n- $f(x)$和$f(x')$将趋向于有高协方差$x$和$x'$在输入空间“附近”（即：$\\left\\|x-x^{\\prime}\\right\\|=\\left|x-x^{\\prime}\\right| \\approx 0,$ 因此 $\\exp \\left(-\\frac{1}{2 \\tau^{2}}\\left\\|x-x^{\\prime}\\right\\|^{2}\\right) \\approx 1$）\n- 当$x$和$x'$相距很远时，$f(x)$和$f(x')$的协方差很低（即：$\\left\\|x-x^{\\prime}\\right\\| \\gg 0,$ 因此$\\exp \\left(-\\frac{1}{2 \\tau^{2}}\\left\\|x-x^{\\prime}\\right\\|^{2}\\right) \\approx 0$）\n\n更简单地说，从一个零均值高斯过程中得到的函数具有平方指数核，它将趋向于局部光滑，具有很高的概率；即：附近的函数值高度相关，并且在输入空间中相关性作为距离的函数递减（参见图2）。\n\n#### 4. 高斯过程回归\n\n正如上一节所讨论的，高斯过程为函数上的概率分布提供了一种建模方法。在这里，我们讨论了如何在贝叶斯回归的框架下使用函数上的概率分布。\n\n##### 4.1 高斯过程回归模型\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229notegpf3.png)\n\n图3：高斯过程回归使用一个零均值高斯先验过程，以$k_{S E}(\\cdot, \\cdot)$为协方差函数（其中$\\tau=0.1$），其中噪声等级为$\\sigma=1$以及$(a)m=10,(b) m=20，(c)m=40$训练样本。蓝线表示后验预测分布的均值，绿色阴影区域表示基于模型方差估计的$95%$置信区间。随着训练实例数量的增加，置信区域的大小会缩小，以反映模型估计中不确定性的减少。还请注意，在图像$(a)$中，$95%$置信区间在训练点附近缩小，但在远离训练点的地方要大得多，正如人们所期望的那样。\n\n设$S=\\left\\{\\left(x^{(i)}, y^{(i)}\\right)\\right\\}_{i=1}^{m}$是一组来自未知分布的满足独立同分布的训练集。在高斯过程回归模型中公式说明了这一点：\n\n$$\ny^{(i)}=f\\left(x^{(i)}\\right)+\\varepsilon^{(i)}, \\quad i=1, \\ldots, m\n$$\n\n其中$\\varepsilon^{(i)}$是独立同分布的“噪声”变量并且服从分布$\\mathcal{N}(0,\\Sigma^2)$。就像在贝叶斯线性回归中，我们也假设一个函数$f(\\cdot)$的**先验分布。** 特别地，我们假设一个零均值高斯过程先验：\n\n$$\nf(\\cdot) \\sim \\mathcal{G} \\mathcal{P}(0, k(\\cdot, \\cdot))\n$$\n\n对于一些有效的协方差函数$k(\\cdot, \\cdot)$。\n\n现在，设$T=\\left\\{\\left(x_{*}^{(i)}, y_{*}^{(i)}\\right)\\right\\}_{i=1}^{m_{*}}$是从一些未知分布$S$中取得的独立同分布的测试点集合。$^10$为了方便标记，我们定义：\n\n>10 我们还假设$T$和$S$是相互独立的。\n\n$$\nX=\n\\left[ \\begin{array}{c}{-\\left(x^{(1)}\\right)^{T}-} \\\\ {-\\left(x^{(2)}\\right)^{T}-} \\\\ {\\vdots} \\\\ {-\\left(x^{(m)}\\right)^{T}-}\\end{array}\\right] \\in \\mathbf{R}^{m \\times n} \\quad \n\\vec{f}=\n\\left[ \\begin{array}{c}{f\\left(x^{(1)}\\right)} \\\\ {f\\left(x^{(2)}\\right)} \\\\ {\\vdots} \\\\ {f\\left(x^{(m)}\\right)}\\end{array}\\right], \\quad \n\\overrightarrow{\\varepsilon}=\n\\left[ \\begin{array}{c}{\\varepsilon^{(1)}} \\\\ {\\varepsilon^{(2)}} \\\\ {\\vdots} \\\\ {\\varepsilon^{(m)}}\\end{array}\\right], \\quad \n\\vec{y}=\n\\left[ \\begin{array}{c}{y^{(1)}} \\\\ {y^{(2)}} \\\\ {\\vdots} \\\\ {y^{(m)}}\\end{array}\\right] \\in \\mathbf{R}^{m} \\\\\nX_{*}=\n\\left[ \\begin{array}{c}{-\\left(x_{*}^{(1)}\\right)^{T}-} \\\\ {-\\left(x_{*}^{(2)}\\right)^{T}-} \\\\ {\\vdots} \\\\ {-\\left(x_{*}^{\\left(m_{*}\\right)}\\right)^{T}-}\\end{array}\\right] \\in \\mathbf{R}^{m_{*} \\times n} \\quad \n\\overrightarrow{f_{*}}=\n\\left[ \\begin{array}{c}{f\\left(x_{*}^{(1)}\\right)} \\\\ {f\\left(x_{*}^{(2)}\\right)} \\\\ {\\vdots} \\\\ {f\\left(x_{*}^{\\left(m_{*}\\right)}\\right)}\\end{array}\\right], \\quad\n\\overrightarrow{\\varepsilon}_{*}=\n\\left[ \\begin{array}{c}{\\varepsilon_{*}^{(1)}} \\\\ {\\varepsilon_{*}^{(2)}} \\\\ {\\vdots} \\\\ {\\varepsilon_{*}^{\\left(m_{*}\\right)}}\\end{array}\\right], \\quad \n\\vec{y}_{*}=\n\\left[ \\begin{array}{c}{y_{*}^{(1)}} \\\\ {y_{*}^{(2)}} \\\\ {\\vdots} \\\\ {y_{*}^{\\left(m_{*}\\right)}}\\end{array}\\right] \\in \\mathbf{R}^{m}\n$$\n\n给定训练数据$S$，先验$p(h)$，以及测试输入$X_*$，我们如何计算测试输出的后验预测分布？对于第$2$节中的贝叶斯线性回归，我们使用贝叶斯规则来计算后验参数，然后对于新的测试点$x_*$使用后验参数计算后验预测分布$p\\left(y_{*} | x_{*}, S\\right)$。然而，对于高斯过程回归，结果是存在一个更简单的解决方案！\n\n##### 4.2 预测\n\n回想一下，对于从具有协方差函数$k(\\cdot,\\cdot)$的零均值高斯先验过程中得到的任何函数$f(\\cdot)$，其任意一组输入点上的边缘分布必须是一个联合的多元高斯分布。特别是，这必须适用于训练和测试点，所以我们有下式：\n\n$$\n\\left[ \\begin{array}{c}{\\vec{f}} \\\\ {\\vec{f}_*}\\end{array}\\right] | X, X_{*} \\sim \\mathcal{N}\\left(\\overrightarrow{0}, \\left[ \\begin{array}{cc}{K(X, X)} & {K\\left(X, X_{*}\\right)} \\\\ {K\\left(X_{*}, X\\right)} & {K\\left(X_{*}, X_{*}\\right)}\\end{array}\\right]\\right)\n$$\n\n其中：\n\n$$\n\\vec{f} \\in \\mathbf{R}^{m} \\text { such that } \\vec{f}=\\left[f\\left(x^{(1)}\\right) \\cdots f\\left(x^{(m)}\\right)\\right]^{T}\\\\\n\\vec{f}_{*} \\in \\mathbf{R}^{m} \\cdot \\text { such that } \\vec{f}_{*}=\\left[f\\left(x_{*}^{(1)}\\right) \\cdots f\\left(x_{*}^{(m)}\\right)\\right]^{T} \\\\\nK(X, X) \\in \\mathbf{R}^{m \\times m} \\text { such that }(K(X, X))_{i j}=k\\left(x^{(i)}, x^{(j)}\\right) \\\\\nK\\left(X, X_{*}\\right) \\in \\mathbf{R}^{m \\times m_*} \\text { such that }\\left(K\\left(X, X_{*}\\right)\\right)_{i j}=k\\left(x^{(i)}, x_{*}^{(j)}\\right) \\\\\nK\\left(X_{*}, X\\right) \\in \\mathbf{R}^{m_* \\times m} \\text { such that }\\left(K\\left(X_{*}, X\\right)\\right)_{i j}=k\\left(x_{*}^{(i)}, x^{(j)}\\right) \\\\\nK\\left(X_{*}, X_{*}\\right) \\in \\mathbf{R}^{m_{*} \\times m_{*}} \\text { such that }\\left(K\\left(X_{*}, X_{*}\\right)\\right)_{i j}=k\\left(x_{*}^{(i)}, x_{*}^{(j)}\\right)\n$$\n\n根据我们独立同分布噪声假设，可以得到：\n\n$$\n\\left[ \\begin{array}{c}{\\overrightarrow{\\varepsilon}} \\\\ {\\overrightarrow{\\varepsilon}_{*}}\\end{array}\\right]\\sim\\mathcal{N}\\left(0,\\left[ \\begin{array}{cc}{\\sigma^{2} I} & {\\overrightarrow{0}} \\\\ {\\overrightarrow{0}^{T}} & {\\sigma^{2} I}\\end{array}\\right]\\right)\n$$\n\n独立高斯随机变量的和也是高斯的，所以有：\n\n$$\n\\left[ \\begin{array}{c}{\\vec{y}} \\\\ {\\vec{y}_{*}}\\end{array}\\right] | X, X_{*}=\n\\left[ \\begin{array}{c}{\\vec{f}} \\\\ {\\vec{f}}\\end{array}\\right]+\\left[ \\begin{array}{c}{\\overrightarrow{\\varepsilon}} \\\\ {\\overrightarrow{\\varepsilon}_{*}}\\end{array}\\right] \\sim \n\\mathcal{N}\\left(\\overrightarrow{0}, \\left[ \\begin{array}{cc}{K(X, X)+\\sigma^{2} I} & {K\\left(X, X_{*}\\right)} \\\\ {K\\left(X_{*}, X\\right)} & {K\\left(X_{*}, X_{*}\\right)+\\sigma^{2} I}\\end{array}\\right]\\right)\n$$\n\n现在，用高斯函数的条件设定规则，它遵循下面的式子：\n\n$$\n\\overrightarrow{y_{*}} | \\vec{y}, X, X_{*} \\sim \\mathcal{N}\\left(\\mu^{*}, \\Sigma^{*}\\right)\n$$\n\n其中：\n\n$$\n\\begin{aligned} \\mu^{*} &=K\\left(X_{*}, X\\right)\\left(K(X, X)+\\sigma^{2} I\\right)^{-1} \\vec{y} \\\\\n\\Sigma^{*} &=K\\left(X_{*}, X_{*}\\right)+\\sigma^{2} I-K\\left(X_{*}, X\\right)\\left(K(X, X)+\\sigma^{2} I\\right)^{-1} K\\left(X, X_{*}\\right) \\end{aligned}\n$$\n\n就是这样！值得注意的是，在高斯过程回归模型中进行预测非常简单，尽管高斯过程本身相当复杂！$^{11}$\n\n>11 有趣的是，贝叶斯线性回归，当以正确的方式进行核化时，结果与高斯过程回归完全等价！但贝叶斯线性回归的后验预测分布的推导要复杂得多，对算法进行核化的工作量更大。高斯过程透视图当然要简单得多。\n\n#### 5. 总结\n\n在结束对高斯过程的讨论时，我们指出了高斯过程在回归问题中是一个有吸引力的模型的一些原因，在某些情况下，高斯过程可能优于其他模型（如线性和局部加权线性回归）：\n\n1. 作为贝叶斯方法，高斯过程模型不仅可以量化问题的内在噪声，还可以量化参数估计过程中的误差，从而使预测的不确定性得到量化。此外，贝叶斯方法中的许多模型选择和超参数选择方法都可以立即应用于高斯过程（尽管我们没有在这里讨论这些高级主题）。\n2. 与局部加权线性回归一样，高斯过程回归是非参数的，因此可以对输入点的任意函数进行建模。\n3. 高斯过程回归模型为将核引入回归建模框架提供了一种自然的方法。通过对核的仔细选择，高斯过程回归模型有时可以利用数据中的结构（尽管我们也没有在这里研究这个问题）。\n4. 高斯过程回归模型，尽管在概念上可能有些难以理解，但仍然导致了简单而直接的线性代数实现。\n\n##### 参考资料\n\n>[1] Carl E. Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning. MIT Press, 2006. Online: [http://www.gaussianprocess.org/gpml/](http://www.gaussianprocess.org/gpml/)\n\n##### 附录 A.1\n\n在这个例子中，我们展示了如何使用多元高斯的归一化特性来计算相当吓人的多元积分，而不需要执行任何真正的微积分！假设你想计算下面的多元积分：\n\n$$\nI(A, b, c)=\\int_{x} \\exp \\left(-\\frac{1}{2} x^{T} A x-x^{T} b-c\\right) d x\n$$\n\n尽管可以直接执行多维积分（祝您好运！），但更简单的推理是基于一种称为“配方法”的数学技巧。特别的：\n\n$$\n\\begin{aligned} I(A, b, c) \n&=\\exp (-c) \\cdot \\int_{x} \\exp \\left(-\\frac{1}{2} x^{T} A x-x^{T} A A^{-1} b\\right)d x \\\\ \n&=\\exp (-c) \\cdot \\int_{x} \\exp \\left(-\\frac{1}{2}\\left(x-A^{-1} b\\right)^{T} A\\left(x-A^{-1} b\\right)-b^{T} A^{-1} b\\right) d x \\\\ \n&=\\exp \\left(-c-b^{T} A^{-1} b\\right) \\cdot \\int_{x} \\exp \\left(-\\frac{1}{2}\\left(x-A^{-1} b\\right)^{T} A\\left(x-A^{-1} b\\right)\\right) d x \\end{aligned}\n$$\n\n定义$\\mu=A^{-1} b$ 和 $\\Sigma=A^{-1}$，可以得到$I(A,b,c)$等于：\n\n$$\n\\frac{(2 \\pi)^{m / 2}|\\Sigma|^{1 / 2}}{\\exp \\left(c+b^{T} A^{-1} b\\right)} \\cdot\\left[\\frac{1}{(2 \\pi)^{m / 2}|\\Sigma|^{1 / 2}} \\int_{x} \\exp \\left(-\\frac{1}{2}(x-\\mu)^{T} \\Sigma^{-1}(x-\\mu)\\right) d x\\right]\n$$\n\n然而，括号中的项在形式上与多元高斯函数的积分是相同的！因为我们知道高斯密度可以归一化，所以括号里的项等于$1$。因此：\n\n$$\nI(A, b, c)=\\frac{(2 \\pi)^{m / 2}\\left|A^{-1}\\right|^{1 / 2}}{\\exp \\left(c+b^{T} A^{-1} b\\right)}\n$$\n\n##### 附录 A.2\n\n推导出给定$x_B$下$x_A$的分布形式；另一个结果可以立即根据对称性可以得到。注意到：\n\n$$\n\\begin{aligned}\np\\left(x_{A} | x_{B}\\right)&=\\frac{1}{\\int_{x_{A}} p\\left(x_{A}, x_{B} ; \\mu, \\Sigma\\right) d x_{A}} \\cdot\\left[\\frac{1}{(2 \\pi)^{m / 2}|\\Sigma|^{1 / 2}} \\exp \\left(-\\frac{1}{2}(x-\\mu)^{T} \\Sigma^{-1}(x-\\mu)\\right)\\right] \\\\\n&=\\frac{1}{Z_{1}} \\exp \\left\\{-\\frac{1}{2}\\left(\\left[ \\begin{array}{c}{x_{A}} \\\\ {x_{B}}\\end{array}\\right]-\\left[ \\begin{array}{c}{\\mu_{A}} \\\\ {\\mu_{B}}\\end{array}\\right]\\right)^{T} \\left[ \\begin{array}{cc}{V_{A A}} & {V_{A B}} \\\\ {V_{B A}} & {V_{B B}}\\end{array}\\right]\\left(\\left[ \\begin{array}{c}{x_{A}} \\\\ {x_{B}}\\end{array}\\right]-\\left[ \\begin{array}{c}{\\mu_{A}} \\\\ {\\mu_{B}}\\end{array}\\right]\\right)\\right\\}\n\\end{aligned}\n$$\n\n其中$Z_1$是不依赖于$x_A$的比例常数，且：\n\n$$\n\\Sigma^{-1}=V=\\left[ \\begin{array}{ll}{V_{A A}} & {V_{A B}} \\\\ {V_{B A}} & {V_{B B}}\\end{array}\\right]\n$$\n\n要简化这个表达式，请观察下面的式子：\n\n$$\n\\begin{aligned}\n&\\left(\\left[ \\begin{array}{c}{x_{A}} \\\\ {x_{B}}\\end{array}\\right]-\\left[ \\begin{array}{c}{\\mu_{A}} \\\\ {\\mu_{B}}\\end{array}\\right]\\right)^{T} \\left[ \\begin{array}{cc}{V_{A A}} & {V_{A B}} \\\\ {V_{B A}} & {V_{B B}}\\end{array}\\right]\\left(\\left[ \\begin{array}{c}{x_{A}} \\\\ {x_{B}}\\end{array}\\right]-\\left[ \\begin{array}{c}{\\mu_{A}} \\\\ {\\mu_{B}}\\end{array}\\right]\\right) \\\\\n&\\qquad =\\left(x_{A}-\\mu_{A}\\right)^{T} V_{A A}\\left(x_{A}-\\mu_{A}\\right)+\\left(x_{A}-\\mu_{A}\\right)^{T} V_{A B}\\left(x_{B}-\\mu_{B}\\right) \\\\\n&\\qquad\\qquad +\\left(x_{B}-\\mu_{B}\\right)^{T} V_{B A}\\left(x_{A}-\\mu_{A}\\right)+\\left(x_{B}-\\mu_{B}\\right)^{T} V_{B B}\\left(x_{B}-\\mu_{B}\\right)\n\\end{aligned}\n$$\n\n只保留依赖于$x_A$的项（利用$V_{A B}=V_{B A}^{T}$），我们有：\n\n$$\np\\left(x_{A} | x_{B}\\right)=\\frac{1}{Z_{2}} \\exp \\left(-\\frac{1}{2}\\left[x_{A}^{T} V_{A A} x_{A}-2 x_{A}^{T} V_{A A} \\mu_{A}+2 x_{A}^{T} V_{A B}\\left(x_{B}-\\mu_{B}\\right)\\right]\\right)\n$$\n\n其中$Z_2$是一个同样不依赖于$x_A$新的比例常数。最后，使用“配方”参数（参见附录A.1），我们得到：\n\n$$\np\\left(x_{A} | x_{B}\\right)=\\frac{1}{Z_{3}} \\exp \\left(-\\frac{1}{2}\\left(x_{A}-\\mu^{\\prime}\\right)^{T} V_{A A}\\left(x_{A}-\\mu^{\\prime}\\right)\\right)\n$$\n\n其中$Z_3$是一个新的不依赖于$x_A$的比例常数，并且$\\mu'=\\mu_{A}-V_{A A}^{-1} V_{A B}\\left(x_{B}-\\mu_{B}\\right)$。最后这个表述表明以$x_B$为条件下$x_A$的分布，同样是多元高斯函数的形式。事实上，从归一化性质可以直接得出：\n\n$$\nx_{A} | x_{B} \\sim \\mathcal{N}\\left(\\mu_{A}-V_{A A}^{-1} V_{A B}\\left(x_{B}-\\mu_{B}\\right), V_{A A}^{-1}\\right)\n$$\n\n为了完成证明，我们只需要注意：\n\n$$\n\\left[ \\begin{array}{cc}{V_{A A}} & {V_{A B}} \\\\ {V_{B A}} & {V_{B B}}\\end{array}\\right]=\n\\left[ \\begin{array}{c}{\\left(\\Sigma_{A A}-\\Sigma_{A B} \\Sigma_{B B}^{-1} \\Sigma_{B A}\\right)^{-1}}&-\\left(\\Sigma_{A A}-\\Sigma_{A B} \\Sigma_{B B}^{-1} \\Sigma_{B A}\\right)^{-1} \\Sigma_{A B} \\Sigma_{B B}^{-1} \\\\ {-\\Sigma_{B B}^{-1} \\Sigma_{B A}\\left(\\Sigma_{A A}-\\Sigma_{A B} \\Sigma_{B B}^{-1} \\Sigma_{B A}\\right)^{-1}}&\\left(\\Sigma_{B B}-\\Sigma_{B A} \\Sigma_{A A}^{-1} \\Sigma_{A B}\\right)^{-1}\\end{array} \\right]\n$$\n\n由分块矩阵的逆的标准公式推出。将相关的块替换到前面的表达式中就得到了想要的结果。\n\n##### 附录 A.3\n\n在这一节中，我们提出了多元高斯分布条件分布的另一种（更简单的）推导方法。注意，正如附录A.2所示，我们可以这样写出$p\\left(x_{A} | x_{B}\\right)$的形式：\n\n$$\n\\begin{aligned} \np\\left(x_{A} | x_{B}\\right) \n&=\\frac{1}{\\int_{x_{A}} p\\left(x_{A}, x_{B} ; \\mu, \\Sigma\\right) d x_{A}} \\cdot\\left[\\frac{1}{(2 \\pi)^{m / 2}|\\Sigma|^{1 / 2}} \\exp \\left(-\\frac{1}{2}(x-\\mu)^{T} \\Sigma^{-1}(x-\\mu)\\right)\\right] &(4)\\\\ \n&=\\frac{1}{Z_{1}} \\exp \\left\\{-\\frac{1}{2}\\left(\\left[ \\begin{array}{c}{x_{A}-\\mu_{A}} \\\\ {x_{B}-\\mu_{B}}\\end{array}\\right]\\right)^{T} \\left[ \\begin{array}{cc}{V_{A A}} & {V_{A B}} \\\\ {V_{B A}} & {V_{B B}}\\end{array}\\right] \\left[ \\begin{array}{c}{x_{A}-\\mu_{A}} \\\\ {x_{B}-\\mu_{B}}\\end{array}\\right]\\right\\} &(5)\n\\end{aligned}\n$$\n\n其中$Z_1$是不依赖于$x_A$的比例常数。\n\n这个推导使用了一个附加的假设，即条件分布是一个多元高斯分布；换句话说，我们假设$p\\left(x_{A} | x_{B}\\right) \\sim \\mathcal{N}\\left(\\mu^{*}, \\Sigma^{*}\\right)$有一些参数$\\mu^{*}, \\Sigma^{*}$（或者，你可以把这个推导看作是寻找“配方法”另一种方法）。\n\n这个推导的关键直觉是当$x_{A}=\\mu^{*} \\triangleq x_{A}^{*}$时，$p\\left(x_{A} | x_{B}\\right)$将会最大化。我们计算$\\log p\\left(x_{A} | x_{B}\\right)$关于$x_A$的梯度，并设其为零。利用等式$(5)$，我们可以得到：\n\n$$\n\\begin{aligned}\n&\\nabla_{x_{A}} \\log p(x_A | x_B)|_{x_A=x_A^{*}}  &\\qquad\\qquad\\qquad(6)\\\\ \n&{=-V_{A A}\\left(x_{A}^{*}-\\mu_{A}\\right)-V_{A B}\\left(x_{B}-\\mu_{B}\\right)} &(7)\\\\ \n&{=0}&(8)\n\\end{aligned}\n$$\n\n这意味着：\n\n$$\n\\mu^{*}=x_{A}^{*}=\\mu_{A}-V_{A A}^{-1} V_{A B}\\left(x_{B}-\\mu_{B}\\right)\\qquad\\qquad\\qquad\\qquad (9)\n$$\n\n类似地，我们利用高斯分布$p(\\cdot)$的逆协方差矩阵是$\\log p(\\cdot)$的负海森矩阵。换句话说，高斯分布$p\\left(x_{A} | x_{B}\\right)$的逆协方差矩阵是$\\log p\\left(x_{A} | x_{B}\\right)$的负海森矩阵。利用式$(5)$，我们有：\n\n$$\n\\begin{aligned} \n\\Sigma^{*-1} &=-\\nabla_{x_{A}} \\nabla_{x_{A}}^{T} \\log p\\left(x_{A} | x_{B}\\right)&\\qquad\\qquad\\qquad(10) \\\\ \n&=V_{A A} &(11)\n\\end{aligned}\n$$\n\n因此，我们得到：\n\n$$\n\\Sigma^{*}=V_{A A}^{-1} \\qquad\\qquad\\qquad(11)\n$$\n\n\n### 参考","tags":["CS229"],"categories":["CS229"]},{"title":"[机器学习-CS229]更多关于多元高斯分布","url":"%2Fposts%2Fab9cd239%2F","content":"# CS229 课程讲义中文翻译\nCS229 Section notes\n\n|原作者|翻译|\n|---|---|\n|Chuong B. Do|[XiaoDong_Wang](https://github.com/Dongzhixiao) |\n\n\n|相关链接|\n|---|\n|[Github 地址](https://github。com/Kivy-CN/Stanford-CS-229-CN)|\n|[知乎专栏](https://zhuanlan。zhihu。com/MachineLearn)|\n|[斯坦福大学 CS229 课程网站](http://cs229。stanford。edu/)|\n|[网易公开课中文字幕视频](http://open。163。com/movie/2008/1/M/C/M6SGF6VB4_M6SGHFBMC。html)|\n\n\n### 更多关于多元高斯分布\n\n#### 介绍\n\n到目前为止的课堂上，多元高斯分布已经出现在许多应用中，比如线性回归的概率解释、高斯判别分析、高斯混合聚类，以及最近学习的因子分析。在本节的笔记中，我们试图揭开多元高斯函数在最近学习的因子分析课程中引入的一些奇特的性质。本节笔记的目的是让大家对这些性质的来源有一些直观的了解，这样你就可以在作业（提醒你写作业的线索！）中更加明确地使用这些性质。\n\n#### 1. 定义\n\n我们称一个概率密度函数是一个均值为$\\mu\\in R^n$，协方差矩阵为$\\Sigma\\in S_{++}^n$的$^1$一个**多元正态分布（或高斯分布）(multivariate normal (or Gaussian) distribution)，** 其随机变量是向量值$x\\in R^n$，该概率密度函数可以通过下式表达：\n\n<blockquote><details><summary>上一小段上标1的说明（详情请点击本行）</summary>\n\n1 复习一下线性代数章节中介绍的$S_{++}^n$是一个对称正定的$n\\times n$矩阵空间，定义为：\n\n$$\nS_{++}^n=\\{A\\in R^{n\\times n}:A=A^T\\quad and\\quad x^TAx>0\\quad for\\quad all\\quad x\\in R^n\\quad such\\quad that\\quad x\\neq 0\\}\n$$\n\n</details></blockquote>\n\n$$\np(x;\\mu,\\Sigma)=\\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)\\right)\n$$\n\n我们可以写作$x\\sim\\mathcal{N}(\\mu,\\Sigma)$。\n\n#### 2. 高斯分布的特点\n\n多元高斯在实践中非常方便，因为其如下的特点:\n\n- **特点 #1：** 如果你知道以$x$为随机变量的高斯分布的均值$\\mu$和协方差矩阵$\\Sigma$。则你可以直接写出关于$x$的概率密度函数。\n\n- **特点 #2：** 下列高斯积分具有闭式解(closed-form solutions)：\n\n$$\n\\begin{aligned}\n\\int_{x\\in R^n}p(x;\\mu,\\Sigma)dx &= \\int_{-\\infin}^{\\infin}\\dots\\int_{-\\infin}^{\\infin}p(x;\\mu,\\Sigma)dx_1\\dots dx_2=1 \\\\\n\\int_{x\\in R^n}x_ip(x;\\mu,\\sigma)dx &= \\mu_i \\\\\n\\int_{x\\in R^n}(x_i-\\mu_i)(x_j-\\mu_j)p(x;\\mu,\\sigma)dx &=\\Sigma_{ij}\n\\end{aligned}\n$$\n\n- **特点 #3：** 高斯函数遵循一些封闭性质(closure properties:)：\n    - 独立高斯随机变量的和是高斯分布。\n    - 联合高斯分布的边缘分布是高斯分布。\n    - 联合高斯分布的条件是高斯分布。\n\n乍一看，这些事实中的一些结论，尤其是第$1$和第$2$条，似乎要么是直观上显而易见的，要么至少是可信的。然而，我们可能不太清楚的是为什么这些特点如此有用。在本文档中，我们将提供一些直观解释说明如何在平常操作处理多元高斯随机变量时使用这些特点。\n\n#### 3. 封闭性质\n\n在本节中，我们将详细讨论前面描述的每个封闭属性，我们将使用特点#1和#2来证明属性，或者至少给出一些关于属性正确性的直觉。\n\n下面是我们本节将要介绍的内容的路线图：\n\n||独立高斯分布的和|联合高斯分布的边缘分布|联合高斯分布的条件分布|\n|:-:|:-:|:-:|:-:|\n|为什么是高斯函数的解释|不介绍|介绍|介绍|\n|概率密度函数的结果|介绍|介绍|介绍|\n\n##### 3.1 独立高斯分布的和是高斯分布\n\n本规则的正式表述为：\n\n设有$y\\sim\\mathcal{N}(\\mu,\\Sigma)$和$z\\sim\\mathcal{N}(\\mu',\\Sigma')$为独立高斯分布，其中随机变量$\\mu,\\mu'\\in R^n$且$\\Sigma,\\Sigma'\\in S_{++}^n$。则它们的和也同样是高斯分布：\n\n$$\ny+z\\sim\\mathcal{N}(\\mu+\\mu',\\Sigma+\\Sigma')\n$$\n\n在我们证明上面的结论前，先给出一些直观结果：\n\n1. 首先要指出的是上述规则中独立假设的重要性。为了了解为什么这很重要，假设$y\\sim\\mathcal{N}(\\mu,\\sigma)$是服从于均值$\\mu$方差$\\sigma$的多元高斯分布，并且假设$z=-y$。很明显，$z$也是服从于与多元高斯分布（事实上，$z\\sim\\mathcal{N}(-\\mu,\\sigma)$），但是$y+z$等于零（不是高斯分布）！\n2. 第二件需要指出的事情是许多学生感到困惑的一点：如果我们把两个高斯概率密度函数（多维空间中的“肿块(bumps)”）加在一起，我们会得到一些峰（即“双峰(two-humped)”的概率密度函数）么？在这里，我们要注意到随机变量$y + z$的概率密度函数并不是简单的将两个单独的概率密度函数的随机变量$y$和$z$相加，而是会变成$y$和$z$的卷积的概率密度函数。$^2$ 然而证明“两个高斯概率密度函数的卷积得到一个高斯概率密度函数”超出了这门课的范围。\n\n<blockquote><details><summary>上一小段上标2的说明（详情请点击本行）</summary>\n\n2 例如，如果$y$和$z$是单变量高斯函数（即：$y\\sim\\mathcal{N}(\\mu,\\sigma^2),z\\sim\\mathcal{N}(\\mu,\\sigma'^2)$），则它们的概率密度的卷积由下式给出：\n\n$$\n\\begin{aligned}\np(y+z;\\mu,\\mu',\\sigma,\\sigma'^2) &=\\int_{-\\infin}^{\\infin}p(w;\\mu,\\sigma^2)p(y+z-w;\\mu',\\sigma'^2)dw \\\\\n&= \\int_{-\\infin}^{\\infin}\\frac 1{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac 1{2\\sigma^2}(w-\\mu)^2\\right)\\cdot \\frac 1{\\sqrt{2\\pi}\\sigma'}\\exp\\left(-\\frac 1{2\\sigma'^2}(y+z-w-\\mu')^2\\right)dw\n\\end{aligned}\n$$\n\n</details></blockquote>\n\n转换一下思路，让我们用卷积给出高斯概率密度函数的观察结果，加上特点#1，来算出概率密度函数$p(y+z|\\mu,\\Sigma)$的解析解。如果我们要计算卷积。我们该怎么做呢？回顾特点#1，高斯分布完全由它的均值向量和协方差矩阵指定。如果我们能确定这些值是什么，那么我们就能计算出其解析解了。\n\n这很简单！对应期望而言，我们有：\n\n$$\nE[y_i+z_i]=E[y_i]+E[z_i]=\\mu_i+\\mu_i'\n$$\n\n上式的结果根据期望的线性性质。因此，$y + z$的均值可以简单的写作$\\mu+\\mu'$。 同时，协方差矩阵的第$(i, j)$项由下式给出:\n\n$$\n\\begin{aligned}\n&E[(y_i+z_i)(y_j+z_j)]-E[y_i+z_i]E[y_j+z_j] \\\\\n&\\qquad=E[y_iy_j+z_iy_j+y_iz_j+z_iz_j]-(E[y_i]+E[z_i])(E[y_j]+E[z_j]) \\\\\n&\\qquad=E[y_iy_j]+E[z_iy_j]+E[y_iz_j]+E[z_iz_j]-E[y_i]E[y_j]-E[z_i]E[y_j]-E[y_i]E[z_j]-E[z_i]E[z_j] \\\\\n&\\qquad=(E[y_iy_j]-E[y_i]E[y_j])+(E[z_iz_j]-E[z_i]E[z_j]) \\\\\n&\\qquad\\qquad+(E[z_iy_j]-E[z_i]E[y_j])+(E[y_iz_j]-E[y_i]E[z_j]) \\\\\n\\end{aligned}\n$$\n\n利用$y$和$z$相互独立的事实，我们得到$E[z_iy_j]=E[z_i]E[y_j]$和$E[y_iz_j]=E[y_i]E[z_j]$。因此，最后两项消去了，剩下：\n\n$$\n\\begin{aligned}\n&E[(y_i+z_i)(y_j+z_j)]-E[y_i+z_i]E[y_j+z_j] \\\\\n&\\qquad=(E[y_iy_j]-E[y_i]E[y_j])+(E[z_iz_j]-E[z_i]E[z_j]) \\\\\n&\\qquad=\\Sigma_{ij}+\\Sigma_{ij}'\n\\end{aligned}\n$$\n\n由此，我们可以得出$y + z$的协方差矩阵可以简单的写作$\\Sigma+\\Sigma'$。\n\n此刻，让我们回顾一下刚刚我们做了什么？利用一些简单的期望和独立性的性质，我们计算出了$y + z$的均值和协方差矩阵。根据特点#1，我们可以立即写出$y + z$的概率密度函数，而不需要做卷积！$^3$\n\n>3 当然，我们首先需要知道$y + z$是高斯分布。\n\n##### 3.2 联合高斯分布的边缘分布是高斯分布\n\n本规则的正式表述为:\n\n假设\n\n$$\n\\begin{bmatrix}x_A\\\\x_B\\end{bmatrix}\\sim\\mathcal{N}\\begin{pmatrix}\\begin{bmatrix}\\mu_A\\\\\\mu_B\\end{bmatrix},\\begin{bmatrix}\\Sigma_{AA}&\\Sigma_{AB}\\\\\\Sigma_{BA}&\\Sigma_{BB}\\end{bmatrix}\\end{pmatrix}\n$$\n\n其中$x_A\\in R^m,x_B\\in R^n$并选择均值向量和协方差矩阵子块的维数与$x_A$和$x_B$进行匹配。则边缘概率密度函数如下所示：\n\n$$\np(x_A)=\\int_{x_B\\in R^n}p(x_A,x_B;\\mu,\\Sigma)dx_B \\\\\np(x_B)=\\int_{x_A\\in R^m}p(x_A,x_B;\\mu,\\Sigma)dx_A\n$$\n\n上面式子都是高斯分布：\n\n$$\nx_A\\sim\\mathcal{N}(\\mu_A,\\Sigma_{AA}) \\\\\nx_B\\sim\\mathcal{N}(\\mu_B,\\Sigma_{BB})\n$$\n\n为了证明这个规则，我们只关注变量$x_A$的边缘分布。\n\n>4 一般来说，对于一个高斯分布的随机向量$x$，只要我们对均值向量的项和协方差矩阵的行/列按对应的方式进行置换，则总是可以对$x$的项进行置换。因此，只看$x_A$就足够了，$x_B$的结果也立即得到了。\n\n首先，请注意计算边缘分布的均值和协方差矩阵很简单：只需从联合概率密度函数的均值和协方差矩阵中提取相应的子块。为了确保这是绝对清楚的，我们来看看$x_{A,i}$和$x_{A,j}$（$x_A$的第$i$个部分和$x_A$的第$j$个部分）之间的协方差。注意$x_{A,i}$和$x_{A,j}$同样也是下面式子的第$i$个和第$j$个部分：\n\n$$\n\\begin{bmatrix}x_A\\\\x_B\\end{bmatrix}\n$$\n\n（因为$x_A$出现在这个向量的上部分）。要找到它们的协方差，我们只需简单的使用下面式子的那个协方差矩阵的第$(i, j)$个元素即可：\n\n$$\n\\begin{bmatrix}\\Sigma_{AA}&\\Sigma_{AB}\\\\\\Sigma_{BA}&\\Sigma_{BB}\\end{bmatrix}\n$$\n\n第$(i, j)$个元素在可以在$\\Sigma_{AA}$子块矩阵中找到。事实上就是$\\Sigma_{AA,ij}$。对所有的$i,j\\in \\{1,\\dots,m\\}$使用这个参数，我们可以发现$x_A$的协方差矩阵可以简化为$\\Sigma_{AA}$。类似的方法可以用来求$x_A$的均值简化为$\\mu_A$。因此，上面的论证告诉我们，如果我们知道$x_A$的边缘分布是高斯分布，那么我们就可以用合适的均值子矩阵以及联合概率密度函数的协方差矩阵立即写出$x_A$的概率密度函数。\n\n上面的论证虽然简单，但多少有些不令人满意：我们如何才能真正确定$x_A$是一个多元高斯分布？关于这一点的论述有点冗长，因此，与其节外生枝，不如先列出我们的推导过程：\n\n1. 明确写出边缘概率密度函数的积分形式。\n2. 通过对逆协方差矩阵进行分块来重写积分。\n3. 使用“平方和”参数来计算$x_B$上的积分。\n4. 论述得到的概率密度函数是高斯的。\n\n下面让我们分别研究一下上面提到的每一个步骤。\n\n###### 3.2.1 边缘概率密度函数的积分形式\n\n假设我们想直接计算$x_A$的密度函数。然后，我们需要计算积分：\n\n$$\n\\begin{aligned}\np(x_A) &= \\int_{x_B\\in R^n}p(x_A,x_B;\\mu,\\Sigma)dx_B \\\\\n&= \\frac{1}{(2\\pi)^{\\frac{m+n}{2}} \\begin{vmatrix}\\Sigma_{AA}&\\Sigma_{AB}\\\\\\Sigma_{BA}&\\Sigma_{BB}\\end{vmatrix}^{1/2}}\\int_{x_B\\in R^n}\\exp\\left(-\\frac12\\begin{bmatrix}x_A-\\mu_A\\\\x_B-\\mu_B\\end{bmatrix}^T\\begin{bmatrix}\\Sigma_{AA}&\\Sigma_{AB}\\\\\\Sigma_{BA}&\\Sigma_{BB}\\end{bmatrix}^{-1}\\begin{bmatrix}x_A-\\mu_A\\\\x_B-\\mu_B\\end{bmatrix}\\right)dx_B\n\\end{aligned}\n$$\n\n###### 3.2.2 逆协方差矩阵的分块\n\n为了进一步推导，我们需要把指数中的矩阵乘积写成稍微不同的形式。特别地，让我们定义下面这个矩阵：\n\n$$\nV=\\begin{bmatrix}V_{AA}&V_{AB}\\\\V_{BA}&V_{BB}\\end{bmatrix}=\\Sigma^{-1}\n$$\n\n这里我们可能会有下面这种诱人的推导想法：\n\n$$\nV=\\begin{bmatrix}V_{AA}&V_{AB}\\\\V_{BA}&V_{BB}\\end{bmatrix}=\\begin{bmatrix}\\Sigma_{AA}&\\Sigma_{AB}\\\\\\Sigma_{BA}&\\Sigma_{BB}\\end{bmatrix}^{-1}=\\begin{bmatrix}\\Sigma_{AA}^{-1}&\\Sigma_{AB}^{-1}\\\\\\Sigma_{BA}^{-1}&\\Sigma_{BB}^{-1}\\end{bmatrix}\n$$\n\n然而，最右边的等号并不成立！我们将在稍后的步骤中讨论这个问题；不过，现在只要将$V$定义为上述形式就足够了，而不必担心每个子矩阵的实际内容是什么。\n\n利用$V$的这个定义，积分扩展到下面的式子：\n\n$$\n\\begin{aligned}\np(x_A)=\\frac 1Z\\int_{x_B\\in R^n}\\exp(-&[\\frac 12(x_A-\\mu_A)^TV_{AA}(x_A-\\mu_A)+\\frac 12(x_A-\\mu_A)^TV_{AB}(x_B-\\mu_B) \\\\\n& +\\frac 12(x_B-\\mu_B)^TV_{BA}(x_A-\\mu_A)+\\frac 12(x_B-\\mu_B)^TV_{BB}(x_B-\\mu_B)])dx_B\n\\end{aligned}\n$$\n\n其中$Z$是一个常数，不依赖于$x_A$或$x_B$，我们暂时忽略它。如果你以前没有使用过分块矩阵，那么上面的展开对你来说可能有点神奇。这类似于当定义一个二次形式基于某个矩阵$A$时，则可得：\n\n$$\nx^TAx=\\sum_i\\sum_jA_{ij}x_ix_j=x_1A_{11}x_1+x_1A_{12}x_2+x_2A_{21}x_1+x_2A_{22}x_2\n$$\n\n花点时间自己研究一下，上面的矩阵推广也适用。\n\n##### 3.2.3 $x_B$上的积分\n\n为了求积分，我们要对$x_B$积分。然而，一般来说，高斯积分是很难手工计算的。我们能做些什么来节省计算时间吗？事实上，有许多高斯积分的答案是已知的（见特点#2）。那么，本节的基本思想是将上一节中的积分转换为一种形式，在这种形式中，我们可以应用特点#2中的一个结果，以便轻松地计算所需的积分。\n\n这其中的关键是一个数学技巧，称为“配方法(completion of squares)”。考虑二次函数  。其中\n\n$$\n\\frac 12x^TAx+b^Tz+c=\\frac 12(z+A^{-1}b)^TA(z+A^{-1}b)+c-\\frac 12b^TA^{-1}b\n$$\n\n下面使用单变量代数中的“配方法”来泛华的多元变量的等式:\n\n$$\n\\frac 12az^2+bz+c=\\frac 12a(z+\\frac bz)^2+c-\\frac {b^2}{2a}\n$$\n\n若要将配方法应用于上述情形，令\n\n$$\n\\begin{aligned}\nz &= x_B-\\mu_B \\\\\nA &= V_{BB} \\\\\nb &=V_{BA}(x_A-\\mu_A) \\\\\nc &=\\frac 12(x_A-\\mu_A)^TV_{AA}(x_A-\\mu_A)\n\\end{aligned}\n$$\n\n然后，这个积分可以重写为\n\n$$\n\\begin{aligned}\np(x_A)=\\frac 1Z\\int_{x_B\\in R^n}exp(-&[\\frac 12(x_B-\\mu_B)^TV_{AA}(x_A-\\mu_A)+\\frac 12(x_A-\\mu_A)^TV_{AB}(x_B-\\mu_B) \\\\\n& +\\frac 12(x_B-\\mu_B)^TV_{BA}(x_A-\\mu_A)+\\frac 12(x_B-\\mu_B)^TV_{BB}(x_B-\\mu_B)])dx_B\n\\end{aligned}\n$$\n\n我们可以提出不包括$x_B$的项，\n\n$$\n\\begin{aligned}\np(x_{A})&=\\exp\\left(-\\frac{1}{2}\\left(x_{A}-\\mu_{A}\\right)^{T} V_{A A}\\left(x_{A}-\\mu_{A}\\right)+\\frac{1}{2}\\left(x_{A}-\\mu_{A}\\right)^{T} V_{A B} V_{B B}^{-1} V_{B A}\\left(x_{A}-\\mu_{A}\\right)\\right) \\\\ \n&\\quad \\cdot \\frac{1}{Z} \\int_{x_{B} \\in \\mathbb{R}^{n}} \\exp \\left(-\\frac{1}{2}\\left[\\left(x_{B}-\\mu_{B}+V_{B B}^{-1} V_{B A}\\left(x_{A}-\\mu_{A}\\right)\\right)^{T} V_{B B}\\left(x_{B}-\\mu_{B}+V_{B B}^{-1} V_{B A}\\left(x_{A}-\\mu_{A}\\right)\\right)\\right]\\right) d x_{B}\n\\end{aligned}\n$$\n\n现在，我们可以应用特点#2。特别的，我们知道通常情况下随机变量为$x$多元高斯分布，如果设均值$\\mu$，协方差矩阵$\\Sigma$，则概率密度函数可以得到如下式子：\n\n$$\n\\frac{1}{(2 \\pi)^{n / 2}|\\Sigma|^{1 / 2}} \\int_{\\mathbf{R}^{n}} \\exp \\left(-\\frac{1}{2}(x-\\mu)^{T} \\Sigma^{-1}(x-\\mu)\\right)=1\n$$\n\n或等价与下式：\n\n$$\n\\int_{R^{n}} \\exp \\left(-\\frac{1}{2}(x-\\mu)^{T} \\Sigma^{-1}(x-\\mu)\\right)=(2 \\pi)^{n / 2}|\\Sigma|^{1 / 2}\n$$\n\n我们用这个事实来消去表达式中剩下的积分以得到$p(x_A)$：\n\n$$\np\\left(x_{A}\\right)=\\frac{1}{Z} \\cdot(2 \\pi)^{n / 2}\\left|V_{B B}\\right|^{1 / 2} \\cdot \\exp \\left(-\\frac{1}{2}\\left(x_{A}-\\mu_{A}\\right)^{T}\\left(V_{A A}-V_{A B} V_{B B}^{-1} V_{B A}\\right)\\left(x_{A}-\\mu_{A}\\right)\\right)\n$$\n\n###### 3.2.4 论述得到的概率密度函数是高斯函数\n\n这时我们几乎已经完成了全部计算！忽略前面的归一化常数，我们看到$x_A$的概率密度函数是$x_A$的二次形的指数。我们可以很快意识到概率密度函数就是均值向量为$\\mu_A$，协方差矩阵为$(V_{A A}-V_{A B} V_{B B}^{-1} V_{B A})^{-1}$的高数分布。虽然协方差矩阵的形式看起来有点复杂，但是我们已经完成了我们开始想要展示的概念——即$x_A$有一个边缘高斯分布。利用前面的逻辑，我们可以得出这个协方差矩阵必须以某种方式消去$\\Sigma_{AA}$。\n\n但是，如果你好奇，也可以证明我们的推导与之前的证明是一致的。为此，我们对分块矩阵使用以下结果：\n\n$$\n\\left[ \\begin{array}{cc}{A} & {B} \\\\ {C} & {D}\\end{array}\\right]^{-1}=\\left[ \\begin{array}{cc}{M^{-1}} & {-M^{-1} B D^{-1}} \\\\ {-D^{-1} C M^{-1}} & {D^{-1}+D^{-1} C M^{-1} B D^{-1}}\\end{array}\\right]\n$$\n\n其中$M=A-B D^{-1} C$。这个公式可以看作是$2\\times 2$矩阵显式逆矩阵的多变量推广：\n\n$$\n\\left[ \\begin{array}{ll}{a} & {b} \\\\ {c} & {d}\\end{array}\\right]^{-1}=\\frac{1}{a d-b c} \\left[ \\begin{array}{cc}{d} & {-b} \\\\ {-c} & {a}\\end{array}\\right]\n$$\n\n用这个公式，可以得出：\n\n$$\n\\begin{aligned}\n\\left[ \\begin{array}{cc}{\\Sigma_{A A}} & {\\Sigma_{A B}} \\\\ {\\Sigma_{B A}} & {\\Sigma_{B B}}\\end{array}\\right] &=\\left[ \\begin{array}{ll}{V_{A A}} & {V_{A B}} \\\\ {V_{B A}} & {V_{B B}}\\end{array}\\right]^{-1} \\\\\n&=\\left[ \\begin{array}{cc}{\\left(V_{A A}-V_{A B} V_{B B}^{-1} V_{B A}\\right)^{-1}} & {-\\left(V_{A A}-V_{A B} V_{B B}^{-1} V_{B A}\\right)^{-1} V_{A B} V_{B B}^{-1}} \\\\ {-V_{B B}^{-1} V_{B A}\\left(V_{A A}-V_{A B} V_{B B}^{-1} V_{B A}\\right)^{-1}} & {\\left(V_{B B}-V_{B A} V_{A A}^{-1} V_{A B}\\right)^{-1}}\\end{array}\\right]\n\\end{aligned}\n$$\n\n正如我们所期望的那样，我们马上就能得出$\\left(V_{A A}-V_{A B} V_{B B}^{-1} V_{B A}\\right)^{-1}=\\Sigma_{A A}$。\n\n#### 3.3 联合高斯分布的条件分布是高斯分布\n\n本规则的正式表述为:\n\n假设：\n\n$$\n\\left[ \\begin{array}{l}{x_{A}} \\\\ {x_{B}}\\end{array}\\right]\\sim\\mathcal{N}\\left(\\left[ \\begin{array}{l}{\\mu_{A}} \\\\ {\\mu_{B}}\\end{array}\\right], \\left[ \\begin{array}{cc}{\\Sigma_{A A}} & {\\Sigma_{A B}} \\\\ {\\Sigma_{B A}} & {\\Sigma_{B B}}\\end{array}\\right]\\right)\n$$\n\n其中$x_{A} \\in \\mathbf{R}^{m}, x_{B} \\in \\mathbf{R}^{n}$，并选择均值向量和协方差矩阵子块的维数来匹配$x_A$和$x_B$。则条件概率密度函数为：\n\n$$\n\\begin{aligned} p\\left(x_{A} | x_{B}\\right) &=\\frac{p\\left(x_{A}, x_{B} ; \\mu, \\Sigma\\right)}{\\int_{x_{A} \\in \\mathbb{R}^{m}} p\\left(x_{A}, x_{B} ; \\mu, \\Sigma\\right) d x_{A}} \\\\ p\\left(x_{B} | x_{A}\\right) &=\\frac{p\\left(x_{A}, x_{B} ; \\mu, \\Sigma\\right)}{\\int_{x_{B} \\in \\mathbb{R}^{n}} p\\left(x_{A}, x_{B} ; \\mu, \\Sigma\\right) d x_{B}} \\end{aligned}\n$$\n\n同样是高斯分布：\n\n$$\n\\begin{array}{l}{x_{A}\\left|x_{B} \\sim \\mathcal{N}\\left(\\mu_{A}+\\Sigma_{A B} \\Sigma_{B B}^{-1}\\left(x_{B}-\\mu_{B}\\right), \\Sigma_{A A}-\\Sigma_{A B} \\Sigma_{B B}^{-1} \\Sigma_{B A}\\right)\\right.} \\\\ {x_{B} | x_{A} \\sim \\mathcal{N}\\left(\\mu_{B}+\\Sigma_{B A} \\Sigma_{A A}^{-1}\\left(x_{A}-\\mu_{A}\\right), \\Sigma_{B B}-\\Sigma_{B A} \\Sigma_{A A}^{-1} \\Sigma_{A B}\\right)}\\end{array}\n$$\n\n和之前一样，我们只研究条件分布$x_B|x_A$，另一个结果是对称的。我们的推导过程如下:\n\n1. 明确写出条件概率密度函数的表达式。\n2. 通过划分逆协方差矩阵重写表达式。\n3. 使用“平方和”参数。\n4. 论述得到的概率密度函数是高斯函数。\n\n下面让我们分别研究一下上面提到的每一个步骤。\n\n###### 3.3.1 明确写出条件概率密度函数的表达式\n\n假设我们想直接计算给定$x_A$下$x_B$的概率密度函数。则我们需要计算下式：\n\n$$\n\\begin{aligned}\np\\left(x_{B} | x_{A}\\right) &=\\frac{p\\left(x_{A}, x_{B} ; \\mu, \\Sigma\\right)}{\\int_{x_{B} \\in R^m} p\\left(x_{A}, x_{B} ; \\mu, \\Sigma\\right) d x_{A}} \\\\\n&=\\frac{1}{Z^{\\prime}} \\exp \\left(-\\frac{1}{2} \\left[ \\begin{array}{c}{x_{A}-\\mu_{A}} \\\\ {x_{B}-\\mu_{B}}\\end{array}\\right]^{T} \\left[ \\begin{array}{cc}{\\Sigma_{A A}} & {\\Sigma_{A B}} \\\\ {\\Sigma_{B A}} & {\\Sigma_{B B}}\\end{array}\\right]^{-1} \\left[ \\begin{array}{c}{x_{A}-\\mu_{A}} \\\\ {x_{B}-\\mu_{B}}\\end{array}\\right]\\right)\n\\end{aligned}\n$$\n\n其中$Z'$是一个归一化常数，我们用该常数表达不依赖于$x_B$的因子。注意，这一次，我们甚至不需要计算任何积分——积分的值不依赖于$x_B$，因此积分可以化简成归一化常数$Z'$。\n\n###### 3.3.2 通过划分逆协方差矩阵重写表达式\n\n和之前一样，我们用矩阵$V$重新参数化概率密度函数，由此得到下式：\n\n$$\n\\begin{aligned}\np\\left(x_{B} | x_{A}\\right) &=\\frac{1}{Z^{\\prime}} \\exp \\left(-\\frac{1}{2} \\left[ \\begin{array}{c}{x_{A}-\\mu_{A}} \\\\ {x_{B}-\\mu_{B}}\\end{array}\\right]^{T} \\left[ \\begin{array}{cc}{V_{A A}} & {V_{A B}} \\\\ {V_{B A}} & {V_{B B}}\\end{array}\\right] \\left[ \\begin{array}{c}{x_{A}-\\mu_{A}} \\\\ {x_{B}-\\mu_{B}}\\end{array}\\right]\\right) \\\\\n&=\\frac{1}{Z^{\\prime}} \\exp (-[\\frac{1}{2}\\left(x_{A}-\\mu_{A}\\right)^{T} V_{A A}\\left(x_{A}-\\mu_{A}\\right)+\\frac{1}{2}\\left(x_{A}-\\mu_{A}\\right)^{T} V_{A B}\\left(x_{B}-\\mu_{B}\\right) \\\\\n&\\qquad\\qquad\\qquad+\\frac{1}{2}\\left(x_{B}-\\mu_{B}\\right)^{T} V_{B A}\\left(x_{A}-\\mu_{A}\\right)+\\frac{1}{2}\\left(x_{B}-\\mu_{B}\\right)^{T} V_{B B}\\left(x_{B}-\\mu_{B}\\right) ] )\n\\end{aligned}\n$$\n\n###### 3.3.3 使用“平方和”参数\n\n回忆下面这个式子：\n\n$$\n\\frac{1}{2} z^{T} A z+b^{T} z+c=\\frac{1}{2}\\left(z+A^{-1} b\\right)^{T} A\\left(z+A^{-1} b\\right)+c-\\frac{1}{2} b^{T} A^{-1} b\n$$\n\n假设$A$是一个对称的非奇异矩阵。如前所述，要将平方的补全应用于上述情况，令：\n\n$$\n\\begin{aligned} \nz &=x_{B}-\\mu_{B} \\\\ \nA &=V_{B B} \\\\ \nb &=V_{B A}\\left(x_{A}-\\mu_{A}\\right) \\\\ \nc &=\\frac{1}{2}\\left(x_{A}-\\mu_{A}\\right)^{T} V_{A A}\\left(x_{A}-\\mu_{A}\\right) \n\\end{aligned}\n$$\n\n然后，可以将$p(x_B | x_A)$的表达式重写为：\n\n$$\n\\begin{array}{c}{p\\left(x_{B} | x_{A}\\right)=\\frac{1}{Z^{\\prime}} \\exp \\left(-\\left[\\frac{1}{2}\\left(x_{B}-\\mu_{B}+V_{B B}^{-1} V_{B A}\\left(x_{A}-\\mu_{A}\\right)\\right)^{T} V_{B B}\\left(x_{B}-\\mu_{B}+V_{B B}^{-1} V_{B A}\\left(x_{A}-\\mu_{A}\\right)\\right)\\right.\\right.} \\\\ \n{+\\frac{1}{2}\\left(x_{A}-\\mu_{A}\\right)^{T} V_{A A}\\left(x_{A}-\\mu_{A}\\right)-\\frac{1}{2}\\left(x_{A}-\\mu_{A}\\right)^{T} V_{A B} V_{B B}^{-1} V_{B A}\\left(x_{A}-\\mu_{A}\\right) ] )}\\end{array}\n$$\n\n将不依赖于$x_B$的指数部分化简到归一化常数中，得到：\n\n$$\np\\left(x_{B} | x_{A}\\right)=\\frac{1}{Z^{\\prime \\prime}} \\exp \\left(-\\frac{1}{2}\\left(x_{B}-\\mu_{B}+V_{B B}^{-1} V_{B A}\\left(x_{A}-\\mu_{A}\\right)\\right)^{T} V_{B B}\\left(x_{B}-\\mu_{B}+V_{B B}^{-1} V_{B A}\\left(x_{A}-\\mu_{A}\\right)\\right)\\right)\n$$\n\n###### 3.3.4 论述得到的概率密度函数是高斯函数\n\n看最后一个表达式，表达式$p(x_B|x_A)$是均值为$\\mu_B-V_{B B}^{-1} V_{B A}\\left(x_{A}-\\mu_{A}\\right)$，协方差矩阵为$V_{B B}^{-1}$的高斯概率密度函数。像往常一样，回忆一下矩阵等式：\n\n$$\n\\left[ \\begin{array}{cc}{\\Sigma_{A A}} & {\\Sigma_{A B}} \\\\ {\\Sigma_{B A}} & {\\Sigma_{B B}}\\end{array}\\right]=\n\\left[ \\begin{array}{c}{\\left(V_{A A}-V_{A B} V_{B B}^{-1} V_{B A}\\right)^{-1}}&-\\left(V_{A A}-V_{A B} V_{B B}^{-1} V_{B A}\\right)^{-1} V_{A B} V_{B B}^{-1} \\\\ {-V_{B B}^{-1} V_{B A}\\left(V_{A A}-V_{A B} V_{B B}^{-1} V_{B A}\\right)^{-1}}&\\left(V_{B B}-V_{B A} V_{A A}^{-1} V_{A B}\\right)^{-1}\\end{array}\\right]\n$$\n\n从上式可以推出：\n\n$$\n\\mu_{B | A}=\\mu_{B}-V_{B B}^{-1} V_{B A}\\left(x_{A}-\\mu_{A}\\right)=\\mu_{B}+\\Sigma_{B A} \\Sigma_{A A}^{-1}\\left(x_{A}-\\mu_{A}\\right)\n$$\n\n反过来，我们也可以利用矩阵恒等式得到：\n\n$$\n\\left[ \\begin{array}{cc}{V_{A A}} & {V_{A B}} \\\\ {V_{B A}} & {V_{B B}}\\end{array}\\right]=\n\\left[ \\begin{array}{c}{\\left(\\Sigma_{A A}-\\Sigma_{A B} \\Sigma_{B B}^{-1} \\Sigma_{B A}\\right)^{-1}}&-\\left(\\Sigma_{A A}-\\Sigma_{A B} \\Sigma_{A A}^{-1} \\Sigma_{B B}\\right)^{-1} \\Sigma_{A B} \\Sigma_{B B}^{-1} \\\\ {-\\Sigma_{B B}^{-1} \\Sigma_{B A}\\left(\\Sigma_{A A}-\\Sigma_{A B} \\Sigma_{B B}^{-1} \\Sigma_{B A}\\right)^{-1}}&\\left(\\Sigma_{B B}-\\Sigma_{B A} \\Sigma_{A A}^{-1} \\sum_{A B}\\right)^{-1}\\end{array} \\right]\n$$\n\n由此推出：\n\n$$\n\\Sigma_{B | A}=V_{B B}^{-1}=\\Sigma_{B B}-\\Sigma_{B A} \\Sigma_{A A}^{-1} \\Sigma_{A B}\n$$\n\n我们完成了!\n\n#### 4. 总结\n\n在本节的笔记中，我们使用了多元高斯的一些简单性质（加上一些矩阵代数技巧）来证明多元高斯分布满足许多封闭性质。一般来说，多元高斯分布是概率分布非常有用的表示形式，因为封闭性保证了这一点：即我们所希望的那样使用多元高斯分布执行的大多数类型的操作都可以以封闭形式完成。从分析的角度来看，涉及多元高斯的积分在实际应用中是往往是很好计算的，因为我们可以依赖于已知的高斯积分来避免自己进行积分。\n\n#### 5. 练习\n\n理解题：令$A\\in R^{n\\times n}$是对称非奇异方阵，$b\\in R^n,c$，证明：\n\n$$\n\\int_{x \\in \\mathbf{R}^{n}} \\exp \\left(-\\frac{1}{2} x^{T} A x-x^{T} b-c\\right) d x=\\frac{(2 \\pi)^{n / 2}}{|A|^{1 / 2} \\exp \\left(c-b^{T} A^{-1} b\\right)}\n$$\n\n##### 参考资料\n\n>有关多元高斯的更多信息，请参见:\nBishop, Christopher M. Pattern Recognition and Machine Learning. Springer,2006.\n\n### 参考","tags":["CS229"],"categories":["CS229"]},{"title":"[机器学习-CS229]多元高斯分布","url":"%2Fposts%2F96e2c326%2F","content":"# CS229 课程讲义中文翻译\nCS229 Section notes\n\n|原作者|翻译|\n|---|---|\n|Chuong B. Do|[XiaoDong_Wang](https://github.com/Dongzhixiao) |\n\n\n|相关链接|\n|---|\n|[Github 地址](https://github。com/Kivy-CN/Stanford-CS-229-CN)|\n|[知乎专栏](https://zhuanlan。zhihu。com/MachineLearn)|\n|[斯坦福大学 CS229 课程网站](http://cs229。stanford。edu/)|\n|[网易公开课中文字幕视频](http://open。163。com/movie/2008/1/M/C/M6SGF6VB4_M6SGHFBMC。html)|\n\n\n### 多元高斯分布\n\n#### 介绍\n\n我们称一个概率密度函数是一个均值为$\\mu\\in R^n$，协方差矩阵为$\\Sigma\\in S_{++}^n$的$^1$一个**多元正态分布（或高斯分布）(multivariate normal (or Gaussian) distribution)，** 其随机变量是向量值$X=[X_1\\dots X_n]^T$，该概率密度函数$^2$可以通过下式表达：\n\n<blockquote><details><summary>上一小段上标1,2的说明（详情请点击本行）</summary>\n\n1 回顾一下线性代数章节中介绍的$S_{++}^n$是一个对称正定的$n\\times n$矩阵空间，定义为：\n\n$$\nS_{++}^n=\\{A\\in R^{n\\times n}:A=A^T\\quad and\\quad x^TAx>0\\quad for\\quad all\\quad x\\in R^n\\quad such\\quad that\\quad x\\neq 0\\}\n$$\n\n2 在我们的这部分笔记中，不使用$f_X(\\bullet)$（如概率论笔记一节所述），而是使用符号$p(\\bullet)$代表概率密度函数。\n\n</details></blockquote>\n\n$$\np(x;\\mu,\\Sigma)=\\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)\\right)\n$$\n\n我们可以将其简写做$X\\sim\\mathcal{N}(\\mu,\\Sigma)$。在我们的这部分笔记中，我们描述了多元高斯函数及其一些基本性质。\n\n#### 1. 与单变量高斯函数的关系\n\n回忆一下，**一元正态分布（或高斯分布）(univariate normal (or Gaussian) distribution)** 的概率密度函数是由下式给出：\n\n$$\np(x;\\mu,\\sigma^2)=\\frac 1{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac 1{2\\sigma^2}(x-\\mu)^2\\right)\n$$\n\n这里，指数函数的自变量$-\\frac 1{2\\sigma^2}(x-\\mu)^2$是关于变量$x$的二次函数。此外，抛物线是向下的，因为二次项的系数是负的。指数函数前面的系数$\\frac 1{\\sqrt{2\\pi}\\sigma}$是不依赖$x$的常数。因此，我们可以简单地把这个系数当作保证下面的式子成立的“标准化因子”(normalization factor)。\n\n$$\n\\frac 1{\\sqrt{2\\pi}\\sigma}\\int_{-\\infin}^{\\infin} \\exp\\left(-\\frac 1{2\\sigma^2}(x-\\mu)^2\\right)=1\n$$\n\n![](https://github.com/Kivy-CN/Stanford-CS-229-CN/blob/master/img/cs229notegf1.png?raw=true)\n\n在多元高斯概率密度函数的情况下，指数函数的自变量$-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)$是一个以向量$x$为变量的**二次形(quadratic form)**。因为$\\Sigma$是正定矩阵，并且任何正定矩阵的逆也是正定矩阵，所以对于任何非零向量$z$，有$z\\Sigma^Tz>0$。这就表明了对于任何满足$x\\neq\\mu$的向量，有：\n\n$$\n(x-\\mu)^T\\Sigma^{-1}(x-\\mu)>0 \\\\\n-\\frac 12(x-\\mu)^T\\Sigma^{-1}(x-\\mu)<0\n$$\n\n就像在单变量的情况下类似，这里你可以把指数函数的参数看成是一个开口向下的二次碗，指数函数前面的系数（即，$\\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}}$）是一个比单变量情况下更复杂的一种形式。但是，它仍然不依赖于$x$，因此它只是一个用来保证下面的式子成立的标准化因子：\n\n$$\n\\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}}\\int_{-\\infin}^{\\infin}\\int_{-\\infin}^{\\infin}\\dots\\int_{-\\infin}^{\\infin}exp(-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu))dx_1dx_2\\dots dx_n=1\n$$\n\n#### 2. 协方差矩阵\n\n**协方差矩阵**的概念对于理解多元高斯分布是至关重要的。回忆一下，对于一对随机变量$X$和$Y$，它们的**协方差**定义为：\n\n$$\nCov[X,Y]=E[(X-E[X])(Y-E[Y])]=E[XY]-E[X]E[Y]\n$$\n\n当处理多个变量时，协方差矩阵提供了一种简洁的方法来表达所有变量对的协方差。特别注意我们通常协方差矩阵表示成一个$n\\times n$的矩阵$\\Sigma$，其中第$(i,j)$个元素代表$Cov[X_i,Y_j]$。\n\n下面的命题（其证明见附录A.1）给出了描述随机向量$X$的协方差矩阵的另一种方法：\n\n**命题 1.** 对于任意一个具有均值为$\\mu$的随机向量为$X$的协方差矩阵$\\Sigma$如下：\n\n$$\n\\Sigma=E[(X-\\mu)(X-\\mu)^T]=E[XX^T]-\\mu\\mu^T\n$$\n\n在多元高斯分布的定义中，我们要求协方差矩阵$\\Sigma$是对称正定矩阵（即，$\\Sigma\\in S_{++}^n$）。为什么存在这种限制？如下面命题所示，任意随机向量的协方差矩阵都必须是对称正半定的：\n\n**命题 2.** 假如$\\Sigma$是关于随机向量$X$的协方差矩阵。则$\\Sigma$是对称半正定矩阵。\n\n证明。$\\Sigma$的对称性直接来源于它的定义。然后对于任意向量$z\\in R^n$我们可以观察到：\n\n$$\n\\begin{aligned}\nz^T\\Sigma z &= \\sum_{i=1}^n\\sum_{j=1}^n(\\Sigma_{ij}z_iz_j)\\qquad\\qquad &(2) \\\\\n&= \\sum_{i=1}^n\\sum_{j=1}^n(Cov[X_i,X_j]\\cdot z_iz_j)   \\\\\n&= \\sum_{i=1}^n\\sum_{j=1}^n(E[(X_i-E[X_i])(X_j-E[X_j])] \\cdot z_iz_j)  \\\\\n&= E\\left[\\sum_{i=1}^n\\sum_{j=1}^n(X_i-E[X_i])(X_j-E[X_j])\\cdot z_iz_j\\right]&(3)\n\\end{aligned}\n$$\n\n这里，$(2)$式由二次形式的展开公式（参见线性代数部分章节）得到，$(3)$式由期望的线性性质得到（参见概率章节）。\n\n想要要完成证明，请注意括号内的量是形式$\\sum_{i=1}^n\\sum_{j=1}^nx_ix_jz_iz_j=(x^Tz)^2\\ge 0$（见问题设定#1）。因此，期望中的量总是非负的，即得到期望本身必须是非负的。我们可以断定$z^T\\Sigma z\\ge 0$\n\n从上面的命题可以推出，为了使$\\Sigma$成为一个有效的协方差矩阵，其必须是对称正半定的。然而，为了使$\\Sigma^{-1}$存在（如多元高斯密度的定义所要求的），则$\\Sigma$必须是可逆的，因此是满秩的。由于任何满秩对称正半定矩阵必然是对称正定的，因此$\\Sigma$必然是对称正定的。\n\n#### 3. 对角协方差矩阵的情况\n\n为了直观地理解多元高斯函数是什么，考虑一个简单的$n=2$并且协方差矩阵$\\Sigma$是对角阵的例子，即：\n\n$$\nx=\\begin{bmatrix}x_1\\\\x_2\\end{bmatrix}\\qquad\\qquad \\mu=\\begin{bmatrix}\\mu_1\\\\\\mu_2\\end{bmatrix}\\qquad\\qquad \\Sigma=\\begin{bmatrix}\\sigma_1^2&0\\\\0&\\sigma_2^2\\end{bmatrix}\n$$\n\n在这种情况下，多元高斯概率密度函数的形式如下：\n\n$$\n\\begin{aligned}\np(x;\\mu,\\Sigma) &=\\frac{1}{2\\pi\\begin{vmatrix}\\sigma_1^2&0\\\\0&\\sigma_2^2\\end{vmatrix}^{1/2}} \\exp\\left(-\\frac{1}{2}\\begin{bmatrix}x_1-\\mu_1\\\\x_2-\\mu_2\\end{bmatrix}^T\\begin{bmatrix}\\sigma_1^2&0\\\\0&\\sigma_2^2\\end{bmatrix}^{-1}\\begin{bmatrix}x_1-\\mu_1\\\\x_2-\\mu_2\\end{bmatrix}\\right) \\\\\n&= \\frac 1{2\\pi(\\sigma_1^2\\cdot \\sigma_2^2-0\\cdot 0)^{1/2}}\\exp\\left(-\\frac{1}{2}\\begin{bmatrix}x_1-\\mu_1\\\\x_2-\\mu_2\\end{bmatrix}^T\\begin{bmatrix}\\frac 1{\\sigma_1^2}&0\\\\0&\\frac 1{\\sigma_2^2}\\end{bmatrix}^{-1}\\begin{bmatrix}x_1-\\mu_1\\\\x_2-\\mu_2\\end{bmatrix}\\right)\n\\end{aligned}\n$$\n\n其中我们使用了一个$2\\times 2$矩阵$^3$的行列式的显式公式，并且使用了一个对角矩阵的逆就是通过取每个对角元素的倒数来得到的事实。之后可得：\n\n>3 即$\\begin{vmatrix}a&b\\\\c&d\\end{vmatrix}=ad-bc$\n\n$$\n\\begin{aligned}\np(x;\\mu,\\Sigma) &=\\frac{1}{2\\pi\\sigma_1\\sigma_2} \\exp\\left(-\\frac{1}{2}\\begin{bmatrix}x_1-\\mu_1\\\\x_2-\\mu_2\\end{bmatrix}^T\\begin{bmatrix}\\frac 1{\\sigma_1^2}(x_1-\\mu_1)\\\\\\frac 1{\\sigma_2^2}(x_2-\\mu_2)\\end{bmatrix}\\right) \\\\\n&= \\frac{1}{2\\pi\\sigma_1\\sigma_2} \\exp\\left(-\\frac 1{2\\sigma_1^2}(x_1-\\mu_1)^2-\\frac 1{2\\sigma_2^2}(x_2-\\mu_2)^2\\right) \\\\\n&= \\frac{1}{\\sqrt{2\\pi}\\sigma_1} \\exp\\left(-\\frac 1{2\\sigma_1^2}(x_1-\\mu_1)^2\\right)\\cdot \\frac{1}{\\sqrt{2\\pi}\\sigma_2} \\exp\\left(-\\frac 1{2\\sigma_2^2}(x_2-\\mu_2)^2\\right)\n\\end{aligned}\n$$\n\n最后一个等式是两个独立的高斯概率函数函数的乘积，其中一个具有均值$\\mu_1$，方差$\\sigma_1^2$。另一个具有均值$\\mu_2$，方差$\\sigma_2^2$。\n\n更一般地，我们可以证明$n$维具有为均值$\\mu\\in R^n$，对角协方差矩阵为$\\sigma=diag(\\sigma_1^2,\\sigma_2^2,\\dots,\\sigma_n^2)$高斯概率密度函数等于$n$个独立的随机变量分别是均值为$\\mu_i$，方差为$\\sigma_i^2$的高斯概率密度函数的乘积。\n\n#### 4. 等高线\n\n从概念上理解多元高斯函数的另一种方法是理解其**等高线**的形状。对于一个函数$f:R^2\\rightarrow R$，等高线集合数学表达形式如下：\n\n$$\n\\{x\\in R^2:f(x)=c\\}\n$$\n\n其中$c\\in R$。$^4$\n\n>4 等高线通常也称为**等值线(level curves)。** 更一般地说，函数的一组**水平集(level set)** $f:R^2\\rightarrow R$是一个对于一些$c\\in R$形式为$\\{x\\in R^2:f(x)=c\\}$的集合。\n\n##### 4.1 等高线的型状\n\n多元高斯函数的等高线是什么样的？和之前一样，我们考虑$n = 2$，协方差矩阵$\\Sigma$是对角阵的情况，即：\n\n$$\nx=\\begin{bmatrix}x_1\\\\x_2\\end{bmatrix}\\qquad\\qquad \\mu=\\begin{bmatrix}\\mu_1\\\\\\mu_2\\end{bmatrix}\\qquad\\qquad \\Sigma=\\begin{bmatrix}\\sigma_1^2&0\\\\0&\\sigma_2^2\\end{bmatrix}\n$$\n\n正如我们在上一节所展示的那样，有：\n\n$$\np(x;\\mu,\\Sigma) = \\frac{1}{2\\pi\\sigma_1\\sigma_2} \\exp\\left(-\\frac 1{2\\sigma_1^2}(x_1-\\mu_1)^2-\\frac 1{2\\sigma_2^2}(x_2-\\mu_2)^2\\right)\\qquad\\qquad(4)\n$$\n\n现在，让我们考虑由所有点组成的水平集，其中对于某个常数$c\\in R$来说$p(x;\\mu,\\sigma)=c$。 特别的，考虑所有$x_1,x_2\\in R$的集合，比如：\n\n$$\n\\begin{aligned}\nc&=\\frac{1}{2\\pi\\sigma_1\\sigma_2} \\exp\\left(-\\frac 1{2\\sigma_1^2}(x_1-\\mu_1)^2-\\frac 1{2\\sigma_2^2}(x_2-\\mu_2)^2\\right) \\\\\n2\\pi c\\sigma_1\\sigma_2 &= \\exp\\left(-\\frac 1{2\\sigma_1^2}(x_1-\\mu_1)^2-\\frac 1{2\\sigma_2^2}(x_2-\\mu_2)^2\\right)  \\\\\nlog(2\\pi c\\sigma_1\\sigma_2) &= -\\frac 1{2\\sigma_1^2}(x_1-\\mu_1)^2-\\frac 1{2\\sigma_2^2}(x_2-\\mu_2)^2 \\\\\nlog(\\frac 1{2\\pi c\\sigma_1\\sigma_2}) &= \\frac 1{2\\sigma_1^2}(x_1-\\mu_1)^2+\\frac 1{2\\sigma_2^2}(x_2-\\mu_2)^2 \\\\\n1 &= \\frac {(x_1-\\mu_1)^2}{2\\sigma_1^2log(\\frac 1{2\\pi c\\sigma_1\\sigma_2})}+\\frac {(x_2-\\mu_2)^2}{2\\sigma_2^2log(\\frac 1{2\\pi c\\sigma_1\\sigma_2})}\n\\end{aligned}\n$$\n\n定义：\n\n$$\nr_1= \\sqrt{2\\sigma_1^2log(\\frac 1{2\\pi c\\sigma_1\\sigma_2})}\\qquad\\qquad r_2= \\sqrt{2\\sigma_2^2log(\\frac 1{2\\pi c\\sigma_1\\sigma_2})}\n$$\n\n之后可得：\n\n$$\n1 = (\\frac {x_1-\\mu_1}{r_1})^2+(\\frac {x_2-\\mu_2}{r_2})^2\\qquad\\qquad (5)\n$$\n\n方程$(5)$在高中解析几何中应该很熟悉：它是一个**轴向椭圆(axis-aligned ellipse)** 的方程，其中心是$(\\mu_1,\\mu_2)$，并且$x_1$轴的长度是$2r_1$，$x_2$轴的长度是$2r_2$！\n\n![](https://github.com/Kivy-CN/Stanford-CS-229-CN/blob/master/img/cs229notegf2.png?raw=true)\n左边的图显示了一个热图，它表示具有均值为$\\mu=\\begin{bmatrix}3\\\\2\\end{bmatrix}$，对角协方差矩阵为$\\Sigma=\\begin{bmatrix}25&0\\\\0&9\\end{bmatrix}$的轴向多元高斯函数的概率密度函数值。注意到这个高斯分布的中心点为$(3,2)$，等高线均为椭圆形，长/短轴长之比为$5:3$。右边的图显示了一个热图，该图表示了一个非轴向对齐的具有平均值为$\\mu=\\begin{bmatrix}3\\\\2\\end{bmatrix}$协方差矩阵为$\\Sigma=\\begin{bmatrix}25&5\\\\5&5\\end{bmatrix}$的多元高斯概率密度函数值。这里，椭圆再次以$(3,2)$为中心，但现在通过线性变换旋转了主轴和副主轴。\n\n##### 4.2 轴的长度\n\n为了更好地理解等值线的形状是如何随着多元高斯分布的方差变化的，也许我们会对当$c$等于高斯密度峰值高度的为分数$1/e$时的$r_1$和$r_2$的值感兴趣。\n\n首先，观察式$(4)$式的最大值出现在$x_1=\\mu_1,x_2=\\mu_2$。将这些值代入式$(4)$，我们看到高斯密度的峰值高度为$\\frac 1{2\\pi\\sigma_1\\sigma_2}$。\n\n其次，我们将等式中的$r_1,r_2$的变量$c$替换为$c=\\frac 1e(\\frac 1{2\\pi\\sigma_1\\sigma_2})$可以得到：\n\n$$\nr_1= \\sqrt{2\\sigma_1^2log(\\frac 1{2\\pi \\sigma_1\\sigma_2\\cdot \\frac 1e(\\frac 1{2\\pi\\sigma_1\\sigma_2})})}=\\sigma_1\\sqrt2 \\\\\nr_2= \\sqrt{2\\sigma_2^2log(\\frac 1{2\\pi c\\sigma_1\\sigma_2\\cdot \\frac 1e(\\frac 1{2\\pi\\sigma_1\\sigma_2})})}=\\sigma_2\\sqrt2\n$$\n\n从上式可以得出，轴的长度需要达到高斯概率密度函数锋值高度的$1/e$，该高斯概率密度函数在第$i$个维度上与标准差$\\sigma_i$成正比增长。直观地说，这是有道理的：某个随机变量$x_i$的方差越小，在那个维度高斯分布的峰值越“紧密”，因此半径$r_i$越小。\n\n##### 4.3 非对角、高维的情况\n\n显然，上面的推导依赖于$\\Sigma$是对角矩阵的假设。然而，在非对角的情况下，情况并没有发生太大的变化。等高线不是一个轴向对齐的椭圆，而是简单地**旋转椭圆(rotated ellipses)。** 此外，在$n$维情况下，水平集形成的几何结构称为$R^n$的**椭球(ellipsoids)**。\n\n#### 5. 线性变换的解释\n\n在最后几节中，我们主要关注如何提供一个多元高斯分布与对角协方差矩阵的直观感觉。特别的，我们发现一个具有对角协方差矩阵的$n$维多元高斯分布可以被简单地看作是$n$个独立的随机变量分别是均值为$\\mu_i$，方差是$\\sigma_i^2$高斯分布的乘积。在本节中，我们将更深入地探讨并提供一个当协方差矩阵不是对角阵时多元高斯分布的定量解释。\n\n本节的关键结果是以下定理（参见附录A.2中的证明）。\n\n**定理 1** 给定$X\\sim\\mathcal{N}(\\mu,\\Sigma)$，其中$\\mu\\in R^n,\\Sigma\\in S_{++}^n$。则存在矩阵$B\\in R^{n\\times n}$如果我们定义$Z=B^{-1}(X-\\mu)$，则满足$Z\\sim\\mathcal{N}(0,I)$。\n\n为了理解这个定理的意义，注意到如果$Z\\sim\\mathcal{N}(0,I)$，则利用第$4$节的分析，$Z$可以看作是$n$个独立标准正态随机变量的集合（即，$Z_i\\sim\\mathcal{N}(0,1)$）。进一步，如果$Z=B^{-1}(X-\\mu)$，则根据简单的代数知识可得$X=BZ+\\mu$。\n\n因此，该定理表明：任何具有多元高斯分布的随机变量$X$都可以解释为对$n$个独立标准正态随机变量$(Z)$集合进行线性变换$(X=BZ+\\mu)$的结果。\n\n#### 附录 A.1\n\n证明。我们证明了$(1)$中的两个等式中的第一个等式；另一个等式的证明是相似的。\n\n$$\n\\begin{aligned}\n\\Sigma &= \\begin{bmatrix}Cov[X_1,X_1]&\\dots&Cov[X_1,X_n]\\\\\\vdots&\\ddots&\\vdots\\\\Cov[X_n,X_1]&\\dots&Cov[X_n,X_n]\\end{bmatrix} \\\\\n&= \\begin{bmatrix}E[(X_1-\\mu_1)^2]&\\dots&E[(X_1-\\mu_1)(X_n-\\mu_n)]\\\\\\vdots&\\ddots&\\vdots\\\\E[(X_n-\\mu_n)(X_1-\\mu_1)]&\\dots&E[(X_n-\\mu_n)^2]\\end{bmatrix} \\\\\n&= E\\begin{bmatrix}(X_1-\\mu_1)^2&\\dots&(X_1-\\mu_1)(X_n-\\mu_n)\\\\\\vdots&\\ddots&\\vdots\\\\(X_n-\\mu_n)(X_1-\\mu_1)&\\dots&(X_n-\\mu_n)^2\\end{bmatrix} &(6) \\\\\n&= E\\begin{bmatrix}\\begin{bmatrix}X_1-\\mu_1\\\\\\vdots\\\\X_n-\\mu_n\\end{bmatrix}[X_1-\\mu_1\\dots X_n-\\mu_n]\\end{bmatrix} &(7) \\\\\n&= E[(X-\\mu)(X-\\mu)^T]\n\\end{aligned}\n$$\n\n这里，公式中的$(6)$由“一个矩阵的期望仅仅是通过取每一项的分量期望而得到的矩阵”得到。同样，公式中的$(7)$由“对于任何向量$z\\in R^n$，下面的式子成立”而得到。\n\n$$\nzz^T=\\begin{bmatrix}z_1\\\\z_2\\\\\\vdots\\\\z_n\\end{bmatrix}[z_1\\quad z_2\\quad\\dots z_n]=\\begin{bmatrix}z_1z_1&z_1z_2&\\dots&z_1z_n\\\\z_2z_1&z_2z_2&\\dots&z_2z_n\\\\\\vdots&\\vdots&\\ddots&\\vdots\\\\z_nz_1&z_nz_2&\\dots&z_nz_n\\end{bmatrix}\n$$\n\n#### 附录 A.2\n\n我们重申以下定理：\n\n**定理 1** 给定$X\\sim\\mathcal{N}(\\mu,\\Sigma)$，其中$\\mu\\in R^n,\\Sigma\\in S_{++}^n$。则存在矩阵$B\\in R^{n\\times n}$如果我们定义$Z=B^{-1}(X-\\mu)$，则满足$Z\\sim\\mathcal{N}(0,I)$。\n\n这个定理的推导需要一些高级线性代数和概率论，仅学习本课程内容的话可以跳过。我们的论点将由两部分组成。首先，对于某个可逆矩阵$B$，我们会证明协方差矩阵$\\Sigma$可以因式分解为$\\Sigma=BB^T$。其次，我们将根据关系$Z=B^{-1}(X-\\mu)$执行从变量$X$到另一个向量值随机变量$Z$的变量“换元(change-of-variable)”。\n\n**第一步：分解协方差矩阵。** ：回忆一下线性代数$^5$笔记中对称矩阵的两个性质:\n\n>5 参见“对称矩阵的特征值和特征向量”一节。\n\n1. 任意实对称矩阵$A\\in R^{n\\times n}$总是可以表示为$A=U\\Lambda U^T$，其中$U$是一个满秩正交矩阵，其中$A$的特征向量作为它的列。$\\Lambda$是一个包含$A$的特征值的对角矩阵。\n2. 如果A是对称正定的，它的所有特征值都是正的。\n\n因为协方差矩阵$\\Sigma$是正定的，使用第一个性质，我们可以对于一些适当定义的矩阵$U,\\Lambda$写出$\\Sigma=U\\Lambda U^T$。利用第二个性质，我们可以定义$\\Lambda^{1/2}\\in R^{n\\times n}$是一个对角矩阵，它的元素是对应来自于$\\Lambda$元素的平方根。因为$\\Lambda=\\Lambda^{1/2}(\\Lambda^{1/2})^T$，我们可得：\n\n$$\n\\Sigma=U\\Lambda U^T=U\\Lambda^{1/2}(\\Lambda^{1/2})^TU^T=U\\Lambda^{1/2}(U\\Lambda^{1/2})^T=BB^T\n$$\n\n其中$B=U\\Lambda^{1/2}$。$^6$那么在这种情况下$\\Sigma^{-1}=B^{-T}B^{-1}$，所以我们可以把多元高斯函数的密度的标准公式重写为：\n\n>6 为了证明B是可逆的，只要观察到$U$是可逆矩阵，并且将$U$右乘一个对角矩阵（没有零对角元素）将重新排列它的列，但不会改变它的秩。\n\n$$\np(x;\\mu,\\Sigma)=\\frac{1}{(2\\pi)^{n/2}|BB^T|^{1/2}} \\exp\\left(-\\frac{1}{2}(x-\\mu)^TB^{-T}B^{-1}(x-\\mu)\\right)\\qquad\\qquad(8)\n$$\n\n**第二步:变量替换。** 现在，定义向量值随机变量$Z=B^{-1}(X-\\mu)$。概率论的一个基本公式是有关向量值随机变量的变量变换公式，我们在概率论的讲义中没有介绍这个公式。\n\n假设$X=[X_1\\dots X_n]^T\\in R^n$是联合概率密度函数$f_X:R^n\\rightarrow R$的向量值随机变量。如果$Z=H(X)\\in R^n$，其中$H$是一个一个双射的可微函数，则$Z$是一个联合概率密度函数$f_Z:R^n\\rightarrow R$，其中：\n\n$$\nf_Z(z)=f_X(x)\\cdot\\begin{vmatrix}det\\begin{pmatrix}\\begin{bmatrix}\\frac {\\partial x_1}{\\partial z_1}&\\dots&\\frac {\\partial x_1}{\\partial z_n}\\\\\\vdots&\\ddots&\\vdots\\\\\\frac{\\partial x_n}{\\partial z_1}&\\dots&\\frac{\\partial x_n}{\\partial z_n}\\end{bmatrix}\\end{pmatrix}\\end{vmatrix}\n$$\n\n使用变量变换公式，我们可以证明（经过一些我们将跳过的代数运算）向量变量$Z$的联合概率密度如下：\n\n$$\np_Z(z)=\\frac 1{(2\\pi)^{n/2}}\\exp\\left(-\\frac 12z^Tz\\right)\\qquad \\qquad (9)\n$$\n \n\n### 参考","tags":["CS229"],"categories":["CS229"]},{"title":"[机器学习-CS229]隐马尔可夫模型基础","url":"%2Fposts%2F48ac52cc%2F","content":"# CS229 课程讲义中文翻译\nCS229 Section notes\n\n|原作者|翻译|\n|---|---|\n|Daniel Ramage|[XiaoDong_Wang](https://github.com/Dongzhixiao) |\n\n\n|相关链接|\n|---|\n|[Github 地址](https://github。com/Kivy-CN/Stanford-CS-229-CN)|\n|[知乎专栏](https://zhuanlan。zhihu。com/MachineLearn)|\n|[斯坦福大学 CS229 课程网站](http://cs229。stanford。edu/)|\n|[网易公开课中文字幕视频](http://open。163。com/movie/2008/1/M/C/M6SGF6VB4_M6SGHFBMC。html)|\n\n\n### 隐马尔可夫模型基础\n\n#### 摘要\n\n我们如何将机器学习应用于随时间变化观察到的一系列数据中来？例如，我们可能对根据一个人讲话的录音来发现他所说的话的顺序感兴趣。或者，我们可能对用词性标记来注释单词序列感兴趣。本小节的内容对马尔可夫模型的概念进行了全面的数学介绍，该模型是一种关于状态随时间变化的推理一种学习形式。并且使用隐马尔可夫模型，我们希望从一系列观察数据中恢复这一系列模型的初始状态。最后一节包含一些特定参考资料，这些资料从其他角度介绍隐马尔可夫模型。\n\n#### 1. 马尔科夫模型\n\n给定一个状态集合$S=\\{s_1,s_2,\\dots,s_{|s|}\\}$，我们可以观察到一系列随时间变化的序列$\\vec{z}\\in S^T$。例如，我们也许有这样一个来自天气系统的状态集合$S=\\{sun,cloud,rain\\}$，显然$|S|=3$。在给定$T=5$的情况下我们可能会观察到这几天的天气情况的一个序列$\\{z_1=s_{sun},z_2=s_{cloud},z_3=s_{cloud},z_4=s_{rain},z_5=s_{cloud}\\}$\n\n我们上面的天气示例里面的观察状态可以表示随时间变化的一种随机过程的输出。如果没有进一步的假设，时间$t$下的状态$s_j$可以是自变量为任意数的一个函数，包括从时间$1$到$t-1$的所有状态，可能还有许多其它我们甚至没有建模的状态。然而，我们将做两个马尔可夫假设，这将允许我们对时间序列进行可以追溯的推断。\n\n有限地平线假设(limited horizon assumption)是$t$时刻处于状态的概率只取决于$t-1$时刻的状态。这个假设背后的直觉是，$t$时刻的状态代表对过去“足够”的总结，可以合理地预测未来。正式的公式如下:\n\n$$\nP(z_t|z_{t-1},z_{t-2},\\dots,z_1)=P(z_t|z_{t-1})\n$$\n\n平稳过程假设(stationary process assumption)是在给定当前状态的条件下，下一个状态的条件分布不随时间变化。正式的公式如下:\n\n$$\nP(z_t|z_{t-1})=P(z_2|z_1);t\\in 2\\dots T\n$$\n\n习惯上，我们还将假设存在一个初始状态和初始观察值$z_0\\equiv s_0$，其中$s_0$为$0$时刻状态的初始概率分布。这种符号定义可以使我们方便编码观察到第一个真实的状态$z_1$的先验概率的确信度，其可以用符号表示为$p(z_1|z_0)$。注意到公式$P(z_t|z_{t-1},\\dots,z_1)=P(z_t|z_{t-1},\\dots,z_1,z_0)$成立是因为我们为所有状态序列都定义了$z_0=s_0$。（HMMs的其它表示形式有时用向量$\\pi\\in R^{|S|}$表示这些先验确信度(prior believes)）\n\n我们通过定义一个状态转移矩阵$A\\in R^{(|S|+1)\\times(|S|+1)}$来参数化这些转移数据。矩阵中的值$A_{ij}$代表在任意时刻$t$从状态$i$转移到状态$j$的转移概率。对于我们太阳和雨的例子，可能有下面的状态转移矩阵：\n\n$$\nA=\\begin{matrix}\n\\ & s_0&s_{sun} & s_{cloud} & s_{rain}\\\\\ns_0 &  0 & .33 & .33 & .33 \\\\\ns_{sun} & 0 & .8 & .1 & .1 \\\\\ns_{cloud} & 0 & .2 & .6 & .2\\\\\ns_{rain} & 0 & .1 & .2 & .7\n\\end{matrix}\n$$\n\n请注意，这些数字（我自己编的）表明了天气是自相关的，这是因为：如果天气晴朗，它将趋向于保持晴朗，如果天气多云将保持多云等等。这种模式在许多马尔可夫模型中都很常见，可以作为转移矩阵中的强对角性来遵守。注意，在本例中，我们的初始状态$s_0$显示了过渡到天气系统中的三种状态的概率是一样的。\n\n##### 1.1 马尔可夫模型的两个问题\n\n结合马尔可夫假设和状态转移参数矩阵$A$，我们可以回答关于马尔可夫链中状态序列的两个基本问题。\n- 给定一个特定的状态序列$\\vec{z}$，其概率是多少？\n- 给定一个观测序列$\\vec{z}$，如何通过其进行最大似然估计得到状态转移参数矩阵$A$？\n\n###### 1.1.1 状态序列的概率\n\n我们可以利用概率的链式法则来计算某一特定状态序列$\\vec{z}$的概率：\n\n$$\n\\begin{aligned}\nP（\\vec{z}) &= P(z_t,z_{t-1},\\dots,z_1;A) \\\\\n&= P(z_t,z_{t-1},\\dots,z_1,z_0;A) \\\\\n&= P(z_t|z_{t-1},z_{t-2},\\dots,z_1;A)P(z_{t-1}|z_{t-2},\\dots,z_1;A)\\dots P(z_1|z_0;A) \\\\\n&= P(z_t|z_{t-1};A)P(z_{t-1}|z_{t-2};A)\\dots P(z_2|z_1;A)P(z_1|z_0;A) \\\\\n&= \\prod_{t=1}^TP(z_t|z_{t-1};A) \\\\\n&= \\prod_{t=1}^TA_{z_{t-1} z_t}\n\\end{aligned}\n$$\n\n在第二行，我们在联合概率密度的公式中引入$z_0$，这使得该式可以通过前面定义的$z_0$来计算。第三行的结果是通过将概率链式法则或贝叶斯规则的重复应用到该联合概率密度上得到的。第四行遵循马尔可夫假设，最后一行表明这些项都来自于状态转换矩阵$A$中的元素。\n\n我们计算一下前面例子中的时间序列的概率。通过式子表达的话，即我们想要计算$P(z_1 = s_{sun} , z_2 = s_{cloud} , z_3 = s_{rain} , z_4 = s_{rain} , z_5 = s_{cloud})$，这个式子可以通过分解来计算，即$P(s_{sun}|s_0)P(s_{cloud}|s_{sun})P(s_{rain}|s_{cloud})P(s_{rain}|s_{rain})P(s_{cloud}|s_{rain}) =.33 \\times .1 \\times .2 \\times .7 \\times .2$。\n\n###### 1.1.2 最大似然参数赋值\n\n从学习的角度来看，我们可以通过观察序列$\\vec{z}$的对数似然函数找到参数矩阵$A$。相应的找到从晴天到多云或者从晴天到晴天等转移的似然，最大化以使得观察集合发生的概率最大。让我们定义一个马尔科夫模型的对数似然函数：\n\n$$\n\\begin{aligned}\nl(A) &= logP(\\vec{z};A) \\\\\n&= log\\prod_{t=1}^TA_{z_{t-1} z_t} \\\\\n&= \\sum_{t=1}^TlogA_{z_{t-1} z_t} \\\\\n&= \\sum_{i=1}^{|S|}\\sum_{j=1}^{|S|}\\sum_{t=1}^{T}1\\{z_{t-1}=s_i\\wedge z_t=s_j\\}logA_{ij}\n\\end{aligned}\n$$\n\n在最后一行中，我们使用一个示性函数，当大括号内的条件满足时，它的值为$1$，否则为$0$，通过该函数在每个时间步长选择观察到的转换。在求解这一优化问题时，重要的是要保证所求解的参数矩阵$A$仍然是一个有效的转移矩阵。特别地，我们需要确保状态$i$的输出概率分布总是和为$1$，并且$A$的所有元素都是非负的。我们可以用拉格朗日乘子法来求解这个优化问题。\n\n$$\n\\begin{aligned}\n\\max_A\\qquad &l(A) \\\\\ns.t.\\qquad &\\sum_{j=1}^{|S|}A_{ij}=1,\\quad i=1..|S|\\\\\n&A_{ij}\\ge 0,\\quad i,j=1..|S|\n\\end{aligned}\n$$\n\n该约束优化问题可以用拉格朗日乘子法求得闭式解。我们将把等式约束带入拉格朗日方程，但不等式约束可以放心地忽略——因为优化解总能为$A_{ij}$产生一个正值。因此我们构建如下的拉格朗日函数：\n\n$$\n\\mathcal{L}(A,\\alpha)=\\sum_{i=1}^{|S|}\\sum_{j=1}^{|S|}\\sum_{t=1}^{T}1\\{z_{t-1}=s_i\\wedge z_t=s_j\\}logA_{ij}+\\sum_{i=1}^{|S|}\\alpha_i(1-\\sum_{j=1}^{|S|}A_{ij})\n$$\n\n求偏导数，令它们等于零可得:\n\n$$\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(A,\\alpha)}{\\partial A_{ij}} &=\\frac{\\partial}{\\partial A_{ij}}(\\sum_{t=1}^{T}1\\{z_{t-1}=s_i\\wedge z_t=s_j\\}logA_{ij}) + \\frac{\\partial}{\\partial A_{ij}}\\alpha_i(1-\\sum_{j=1}^{|S|}A_{ij}) \\\\\n&= \\frac 1{A_{ij}}\\sum_{t=1}^{T}1\\{z_{t-1}=s_i\\wedge z_t=s_j\\}-\\alpha_i\\equiv0\\\\\n&\\Rightarrow \\\\\nA_{ij} &=\\frac 1{\\alpha_i}\\sum_{t=1}^{T}1\\{z_{t-1}=s_i\\wedge z_t=s_j\\}\n\\end{aligned}\n$$\n\n回带原式，并令其对于$\\alpha$的偏导等于零可得：\n\n$$\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(A,\\alpha)}{\\partial \\alpha_i} &= 1-\\sum_{j=1}^{|S|}A_{ij} \\\\\n&= 1-\\sum_{j=1}^{|S|}\\frac 1{\\alpha_i}\\sum_{t=1}^{T}1\\{z_{t-1}=s_i\\wedge z_t=s_j\\}\\equiv0 \\\\\n&\\Rightarrow \\\\\n\\alpha_i &= \\sum_{j=1}^{|S|}\\sum_{t=1}^{T}1\\{z_{t-1}=s_i\\wedge z_t=s_j\\} \\\\\n&= \\sum_{t=1}^{T}1\\{z_{t-1}=s_i\\}\n\\end{aligned}\n$$\n\n把$\\alpha_i$的值带入相应表达式，我们推导出$A_{ij}$的最大似然参数值$\\hat{A_{ij}}$为：\n\n$$\n\\hat{A_{ij}} = \\frac{\\sum_{t=1}^T 1\\{z_{t-1}=s_i\\wedge z_t=s_j\\}}{\\sum_{t=1}^T 1\\{z_{t-1} = s_i\\}}\n$$\n\n这个公式结果表达的一个简单的解释是：从状态$i$到状态$j$转移的最大似然概率其实就是从状态$i$到状态$j$出现的次数数除以总次数。换句话说，就是最大似然参数等于我们从状态$i$到状态$j$的次数比上我们在状态$i$中的次数的分数。\n\n#### 2. 隐马尔科夫模型\n\n马尔可夫模型是对时间序列数据的一种强大抽象，但无法捕获非常常见的场景。如果我们不能观察状态本身，而只能观察这些状态的一些概率函数，我们怎么能对一系列状态进行推理呢？比如一个词性标注的场景，其中单词被观察到，但是词性标记没有被观察到。或者在语音识别的场景中，语音序列被观察到，但是生成它的单词没有被观察到。举个简单的例子，让我们借用Jason Eisner在2002[1]`参考资料[1]见文章最下方`年提出的设置，即“冰淇淋气候学”：\n\n情境：在2799年，你是一位气候学家，研究全球变暖的历史。你找不到巴尔的摩(Baltimore)天气的任何记录，但你找到了我（杰森·艾斯纳(Jason Eisner)）的日记。我勤奋地记录我每天吃了多少冰淇淋。关于那个夏天的天气情况，你能推断出什么？\n\n可以使用隐马尔可夫模型(HMM)来研究这个场景。我们不能观察状态的实际序列（每天天气情况的序列）。相反，我们只能观察每个天气状态产生的一些结果（那天吃了多少冰淇淋）。\n\n形式上，HMM是一个马尔可夫模型，我们有一系列观察到的输出$x=\\{x_1,x_2,\\dots,x_T\\}$，该输出来自于一组输出符号集(an output alphabet)$V=\\{v_1,v_2,\\dots,v_{|V|}\\}$，即$x_t\\in V,t=1..T$。和上一节一样，我们也假定了一系列状态的存在，这些状态来自于一个状态符号集合$S=\\{s_1,s_2,\\dots s_{|s|}\\},z_t\\in S,t=1..T$，但是在这种情况下，状态值是不可见的。状态$i$和$j$之间的转换将再次用状态转移矩阵$A_{ij}$中的对应值表示。\n\n我们还将生成输出观测值的概率作为隐状态的函数来建模。为此，我们做了输出无关的假设(output independence assumption)，同时定义$P(x_t=v_k|z_t=s_j)=P(x_t=v_k|x_1,\\dots,x_T,z_1,\\dots,z_T)=B_{jk}$。矩阵$B$编码了隐藏状态产生输出$v_k$的概率，$v_k$在相应时间产生的状态是$s_j$。\n\n回到天气的例子，假设你有四天的冰淇淋消费记录$\\vec{x}=\\{x_1=v_3,x_2=v_2,x_3=v_1,x_4=v_2\\}$。其中我们的观察集合仅仅有冰激凌消耗的数量，即$V=\\{v_1=1\\text{冰激凌},v_2=2\\text{冰激凌},v_3=3\\text{冰激凌}\\}$。HMM能给我们回答什么问题呢？\n\n##### 2.1 隐马尔科夫模型的三个问题\n\n我们可能会问HMM三个基本问题。观察到的序列的概率是多少（比如我们观察到消耗了$3,2,3,2$个冰淇淋）？最有可能产生观测结果的一系列状态是什么（那四天的天气如何）？我们如何学习给定数据时的隐马尔可夫模型参数$A$和$B$的值？\n\n##### 2.2 观测序列的概率：正演过程\n\n在HMM中，我们假设数据是由以下过程生成的：假设存在一系列基于我们时间训序列长度的状态$\\vec{z}$。该状态序列由状态转换矩阵$A$参数化的马尔可夫模型生成。在每个时间步$t$，我们选择一个输出$x_t$作为状态$z_t$出现下的函数。因此，为了得到一个观测序列的概率，我们需要将给定的每个可能状态序列的数据$\\vec{x}$的似然概率相加。\n\n$$\n\\begin{aligned}\nP(\\vec{x};A,B) &= \\sum_{\\vec{z}}P(\\vec{x},\\vec{z};A,B) \\\\\n&= \\sum_{\\vec{z}}P(\\vec{x}|\\vec{z};A,B)P(\\vec{z};A,B)\n\\end{aligned}\n$$\n\n上述公式适用于任何概率分布。然而，HMM假设允许我们进一步简化表达式：\n\n$$\n\\begin{aligned}\nP(\\vec{x};A,B) &= \\sum_{\\vec{z}}P(\\vec{x}|\\vec{z};A,B)P(\\vec{z};A,B) \\\\\n&= \\sum_{\\vec{z}}(\\prod_{t=1}^TP(x_t|z_t;B))(\\prod_{t=1}^TP(z_t|z_{t-1};A)) \\\\\n&= \\sum_{\\vec{z}}(\\prod_{t=1}^TB_{z_tx_t})(\\prod_{t=1}^TA_{z_{t-1}z_t})\n\\end{aligned}\n$$\n\n好消息是，上式是一个关于参数的简单表达式。推导过程遵循HMM假设：输出独立假设、马尔可夫假设和平稳过程假设，这三个假设都用于推导第二行。坏消息是所有可能的产生序列$\\vec{z}$情况的总和太大了。因为$z_t$在每个时间步都可能有$|S|$种可能情况，直接计算总和需要操作的时间复杂度是$O(|S|^T)$。\n\n<hr style=\"height:1px;border:none;border-top:3px solid black;\" />\n\n**算法 1** 前向算法计算$\\alpha_i(t)$\n\n<hr style=\"height:1px;border:none;border-top:1px solid black;\" />\n\n1. 基本情况：$\\alpha_i(0) = A_{0i},i=1..|s|$\n\n2.  递归： $\\alpha_j(t) = \\sum_{i=1}^{|S|}\\alpha_i(t-1)A_{ij}B_{jx_t},j=1..|S|,t=1..T$\n\n<hr style=\"height:1px;border:none;border-top:1px solid black;\" />\n\n幸运的是，可以根据一个名叫前向算法(Forward Procedure)的算法更快的计算$P(\\vec{x};A,B)$，该算法采用了动态规划的思想。首先让我们定义一个符号：$\\alpha_i(t)=P(x_1,x_1,\\dots,x_t,z_t=s_i;A,B)$。$\\alpha_i(t)$代表随时间$t$（通过任意状态指定）变化的所有观测值和我们在时间$t$进入状态$s_i$的联合概率。在我们有了这个符号之后，所有观察到对象的全集的概率$P(\\vec{x})$可以如下表达：\n\n$$\n\\begin{aligned}\nP(\\vec{x};A,B) &= P(x_1,x_2,\\dots,x_T;A,B) \\\\\n&= \\sum_{i=1}^{|S|}P(x_1,x_2,\\dots,x_T,z_T=s_i;A,B) \\\\\n&= \\sum_{i=1}^{|S|}\\alpha_i(T)\n\\end{aligned}\n$$\n\n算法$1$给出了一种有效的方法来计算$\\alpha_i(t)$。在每个时间步，我们进行计算的时间复杂度仅仅是$O(|S|)$，这样得到最终计算观察到的状态序列的总概率$P(\\vec{x};A,B)$算法的时间复杂度是$O(|S|\\times T)$。\n\n一个类似称为向后过程(Backward Procedure)的算法可以用来计算类似的概率$\\beta_i(t)=P(x_T,x_{T-1},\\dots,x_{t+1},z_t=s_i;A,B)$。\n\n##### 2.3 最大似然状态目标序列：维特比算法\n\n隐马尔可夫模型最常见的问题之一是想要知道在给定了一个观察到的输出序列$\\vec{x}\\in V^T$时，最有可能的状态序列$\\vec{z}\\in S^T$是什么。可以用如下公式表达：\n\n$$\narg\\max_{\\vec{z}}P(\\vec{z}|\\vec{x};A,B)=arg\\max_{\\vec{z}} \\frac{P(\\vec{x}, \\vec{z};A,B)}{\\sum_{\\vec{z}}P(\\vec{x}, \\vec{z};A,B)}=arg\\max_{\\vec{z}}P(\\vec{x}, \\vec{z};A,B)\n$$\n\n第一个化简遵循贝叶斯规则，第二个化简遵循分母不直接依赖$\\vec{z}$的观察结果。简而言之，我们这里模型的意思是尝试所有可能产生目标序列$\\vec{z}$，并取其中能使得联合概率最大的那个目标序列。然而，枚举一组可能的任务序列需要的时间复杂度是$O(|S|^T)$。在这一点上，你可能会想到使用上一小节的正向算法那样的动态规划方案来解决本节的问题可能会节约时间，没错。注意，如果将$arg\\max_{\\vec{z}}$替换为$\\sum_{\\vec{z}}$，那么我们当前的任务与前向算法的表达式完全类似。\n\n<hr style=\"height:1px;border:none;border-top:3px solid black;\" />\n\n**算法 2** 基于$EM$算法解决隐马尔可夫模型普通应用的算法： \n\n<hr style=\"height:1px;border:none;border-top:1px solid black;\" />\n\n（$E$步）对于每一个可能的序列$\\vec{z} \\in S^T$，设：\n\n$$\nQ(\\vec{z}):=p(\\vec{z}|\\vec{x};A, B)\n$$\n\n（$M$步）设：\n\n$$\n\\begin{aligned}\nA, B &:= arg\\max_{A,B}\\sum_{\\vec{z}}Q(\\vec{z})log\\frac{P(\\vec{x}, \\vec{z}; A, B)}{Q(\\vec{z})} \\\\\n&s.t.\\sum_{j=1}^{|S|}A_{ij}=1,i=1...|S|;A_{ij}\\ge0,\\quad i,j=1...|S| \\\\\n&\\quad\\sum_{k=1}^{|V|}B_{ik}=1,i=1...|S|;B_{ik}\\ge0,\\quad i=1...|S|,k=1...|V|\n\\end{aligned}\n$$\n\n<hr style=\"height:1px;border:none;border-top:1px solid black;\" />\n\n维特比算法(Viterbi Algorithm)与正向过程类似，不同之处在于，我们只需要跟踪最大概率并记录其对应的状态序列，而不是跟踪到目前为止所看到的生成观测结果的总概率。\n\n##### 2.4 参数学习：基于EM算法的隐马尔可夫模型\n\nHMM模型的最后一个问题是：给定一组观察序列的集合，使这组集合最有可能出现的状态转移概率矩阵(state transition probabilities)$A$和状态生成概率矩阵(output emission probabilities)$B$的值是多少？例如，基于语音识别数据集求解最大似然参数可以使我们有效地训练HMM模型，之后在需要求得候选语音信号的最大似然状态序列时使用该模型。\n\n在本节中，我们推导了隐马尔可夫模型的期望最大化算法。这个证明来自于CS229课堂讲稿中给出的$EM$的一般公式。算法$2$给出了基本的$EM$算法。注意，$M$步中的优化问题现在受到约束，使得$A$和$B$包含有效的概率。就像我们为（非隐）马尔可夫模型找到的最大似然解一样，我们将能够用拉格朗日乘子来解决这个优化问题。还要注意，$E$步和$M$步都需要枚举所有$|S|^T$种可能的序列$\\vec{z}$。我们将使用前面提到的前向和后向算法为我们的$E$步和$M$步计算一组有效的统计量。\n\n首先，我们用马尔可夫假设重写目标函数：\n\n$$\n\\begin{aligned}\nA,B &= arg\\max_{A,B}\\sum_{\\vec{z}}Q(\\vec{z})log\\frac{P(\\vec{x},\\vec{z};A,B)}{Q(\\vec{z})} \\\\\n&= arg\\max_{A,B}\\sum_{\\vec{z}}Q(\\vec{z})log P(\\vec{x},\\vec{z};A,B) \\\\\n&= arg\\max_{A,B}\\sum_{\\vec{z}}Q(\\vec{z})log (\\prod_{t=1}^TP(x_t|z_t;B))(\\prod_{t=1}^TP(z_t|z_{t-1};A)) \\\\\n&= arg\\max_{A,B}\\sum_{\\vec{z}}Q(\\vec{z})\\sum_{t=1}^TlogB_{z_tx_t}+logA_{z_{t-1}z_t} \\\\\n&= arg\\max_{A,B}\\sum_{\\vec{z}}Q(\\vec{z})\\sum_{i=1}^{|S|}\\sum_{j=1}^{|S|}\\sum_{k=1}^{|V|}\\sum_{t=1}^T1\\{z_t=s_j\\wedge x_t=v_k\\}logB_{jk}+1\\{z_{t-1}=s_i\\wedge z_t=s_j\\}logA_{ij}\n\\end{aligned}\n$$\n\n在第一行中，我们将对数除法分解为减法，注意分母的项不依赖于参数$A,B$。第$3$行应用了马尔可夫假设。第$5$行使用示性函数按状态索引$A$和$B$。\n\n对于可见马尔可夫模型的最大似然参数，忽略不等式约束是安全的，因为解的形式自然只产生正解。构造拉格朗日函数：\n\n$$\n\\begin{aligned}\n\\mathcal{L}(A,B,\\delta,\\epsilon) = &\\sum_{\\vec{z}}Q(\\vec{z})\\sum_{i=1}^{|S|}\\sum_{j=1}^{|S|}\\sum_{k=1}^{|V|}\\sum_{t=1}^T1\\{z_t=s_j\\wedge x_t=v_k\\}logB_{jk}+1\\{z_{t-1}=s_i\\wedge z_t=s_j\\}logA_{ij}\\\\\n&+ \\sum_{j=1}^{|S|}\\epsilon_j(1-\\sum_{k=1}^{|V|}logB_{jk})+\\sum_{i=1}^{|S|}\\delta_i(1-\\sum_{j=1}^{|S|}A_{ij})\n\\end{aligned}\n$$\n\n求偏导并使它们等于零：\n\n$$\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(A,B,\\delta,\\epsilon)}{\\partial A_{ij}} &= \\sum_{\\vec{z}}Q(\\vec{z})\\frac 1{A_{ij}}\\sum_{t=1}^T1\\{z_{t-1}=s_i\\wedge z_t=s_j\\}-\\delta_i\\equiv 0 \\\\\nA_{ij} &= \\frac 1{\\delta_i}\\sum_{\\vec{z}}Q(\\vec{z})\\sum_{t=1}^T1\\{z_{t-1}=s_i\\wedge z_t=s_j\\} \\\\\n\\frac{\\partial\\mathcal{L}(A,B,\\delta,\\epsilon)}{\\partial B_{jk}} &= \\sum_{\\vec{z}}Q(\\vec{z})\\frac 1{B_{jk}}\\sum_{t=1}^T1\\{z_t=s_j\\wedge x_t=v_k\\}-\\epsilon_j\\equiv 0 \\\\\nB_{jk} &= \\frac 1{\\epsilon_j}\\sum_{\\vec{z}}Q(\\vec{z})\\sum_{t=1}^T1\\{z_t=s_j\\wedge x_t=v_k\\}\n\\end{aligned}\n$$\n\n对拉格朗日乘子求导，代入上面$A_{ij}$和$B_{jk}$的值：\n\n$$\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(A,B,\\delta,\\epsilon)}{\\partial \\delta_i} &= 1 - \\sum_{j=1}^{|S|}A_{ij} \\\\\n&= 1 - \\sum_{j=1}^{|S|}\\frac 1{\\delta_i}\\sum_{\\vec{z}}Q(\\vec{z})\\sum_{t=1}^T1\\{z_{t-1}=s_i\\wedge z_t=s_j\\}\\equiv 0 \\\\\n\\delta_i &= \\sum_{j=1}^{|S|}\\sum_{\\vec{z}}Q(\\vec{z})\\sum_{t=1}^T1\\{z_{t-1}=s_i\\wedge z_t=s_j\\} \\\\\n&= \\sum_{\\vec{z}}Q(\\vec{z})\\sum_{t=1}^T1\\{z_{t-1}=s_i\\}  \\\\\n\\frac{\\partial\\mathcal{L}(A,B,\\delta,\\epsilon)}{\\partial \\epsilon_j} &= 1 - \\sum_{k=1}^{|V|}B_{jk} \\\\\n&= 1 - \\sum_{k=1}^{|V|}\\frac 1{\\epsilon_j}\\sum_{\\vec{z}}Q(\\vec{z})\\sum_{t=1}^T1\\{z_t=s_j\\wedge x_t=v_k\\}\\equiv 0 \\\\\n\\epsilon_j &= \\sum_{k=1}^{|V|}\\sum_{\\vec{z}}Q(\\vec{z})\\sum_{t=1}^T1\\{z_t=s_j\\wedge x_t=v_k\\} \\\\\n&= \\sum_{\\vec{z}}Q(\\vec{z})\\sum_{t=1}^T1\\{z_t=s_j\\}\n\\end{aligned}\n$$\n\n代回上面的表达式，我们得到参数$\\hat{A}$和$\\hat{B}$使我们对数据集的预测计数最大化：\n\n$$\n\\begin{aligned}\n\\hat{A}_{ij} &= \\frac{\\sum_{\\vec{z}}Q(\\vec{z})\\sum_{t=1}^T1\\{z_{t-1}=s_i\\wedge z_t=s_j\\}}{\\sum_{\\vec{z}}Q(\\vec{z})\\sum_{t=1}^T1\\{z_{t-1}=s_i\\}} \\\\\n\\hat{B}_{jk} &= \\frac{\\sum_{\\vec{z}}Q(\\vec{z})\\sum_{t=1}^T1\\{z_t=s_j\\wedge x_t=v_k\\}}{\\sum_{\\vec{z}}Q(\\vec{z})\\sum_{t=1}^T1\\{z_t=s_j\\}}\n\\end{aligned}\n$$\n\n不幸的是，这些总和都超过了所有可能的标签$\\vec{z}\\in S^T$。但是回忆一下在最后一个时间步时，在有参数矩阵分别为$A,B$的情况下，$Q(\\vec{z})$在E-step中被定义为$P(\\vec{z}|\\vec{x};A,B)$。首先，让我们来考虑如何根据向前向后概率，$\\alpha_i(t)$以及$\\beta_j(t)$来表达$\\hat{A}_{ij}$的分子。\n\n$$\n\\begin{aligned}\n& \\sum_{\\vec{z}}Q(\\vec{z})\\sum_{t=1}^T1\\{z_{t-1}=s_i\\wedge z_t=s_j\\} \\\\\n=& \\sum_{t=1}^T\\sum_{\\vec{z}}1\\{z_{t-1}=s_i\\wedge z_t=s_j\\}Q(\\vec{z}) \\\\\n=& \\sum_{t=1}^T\\sum_{\\vec{z}}1\\{z_{t-1}=s_i\\wedge z_t=s_j\\}P(\\vec{z}|\\vec{x};A,B) \\\\\n=& \\frac 1{P(\\vec{x};A,B)}\\sum_{t=1}^T\\sum_{\\vec{z}}1\\{z_{t-1}=s_i\\wedge z_t=s_j\\}P(\\vec{z},\\vec{x};A,B) \\\\\n=& \\frac 1{P(\\vec{x};A,B)}\\sum_{t=1}^T\\alpha_i(t)A_{ij}B_{jx_t}\\beta_j(t+1)\n\\end{aligned}\n$$\n\n在前两步骤中，我们重新数学符号，并在式中代入$Q$的定义，然后我们在第$4$行的推导中使用了贝叶斯规则，随后在第$5$行中代入对$\\alpha,\\beta,A$和$B$的定义。类似地，分母可以用分子对$j$求和来表示。\n\n$$\n\\begin{aligned}\n& \\sum_{\\vec{z}}Q(\\vec{z})\\sum_{t=1}^T1\\{z_{t-1}=s_i\\} \\\\\n=& \\sum_{j=1}^{|S|}\\sum_{\\vec{z}}Q(\\vec{z})\\sum_{t=1}^T1\\{z_{t-1}=s_i\\wedge z_t=s_j\\} \\\\\n=& \\frac 1{P(\\vec{x};A,B)}\\sum_{j=1}^{|S|}\\sum_{t=1}^T\\alpha_i(t)A_{ij}B_{jx_t}\\beta_j(t+1)\n\\end{aligned}\n$$\n\n结合这些表达式，我们可以充分描述我们的最大似然状态转换$\\hat{A}_{ij}$，而不需要枚举所有可能的标签：\n\n$$\n\\hat{A}_{ij} = \\frac{\\sum_{t=1}^T\\alpha_i(t)A_{ij}B_{jx_t}\\beta_j(t+1)}{\\sum_{j=1}^{|S|}\\sum_{t=1}^T\\alpha_i(t)A_{ij}B_{jx_t}\\beta_j(t+1)}\n$$\n\n同样，$\\hat{B}_{jk}$的分子可以表示为：\n\n$$\n\\begin{aligned}\n& \\sum_{\\vec{z}}Q(\\vec{z})\\sum_{t=1}^T1\\{z_t=s_j\\wedge x_t=v_k\\} \\\\\n=& \\frac 1{P(\\vec{x};A,B)}\\sum_{t=1}^T\\sum_{\\vec{z}}1\\{z_t=s_j\\wedge x_t=v_k\\}P(\\vec{z},\\vec{x};A,B) \\\\\n=& \\frac 1{P(\\vec{x};A,B)}\\sum_{i=1}^{|S|}\\sum_{t=1}^T\\sum_{\\vec{z}}1\\{z_{t-1}=s_i\\wedge z_t=s_j\\wedge x_t=v_k\\}P(\\vec{z},\\vec{x};A,B) \\\\\n=& \\frac 1{P(\\vec{x};A,B)}\\sum_{i=1}^{|S|}\\sum_{t=1}^T1\\{x_t=v_t\\}\\alpha_i(t)A_{ij}B_{jx_t}\\beta_j(t+1)\n\\end{aligned}\n$$\n\n$\\hat{B}_{jk}$的分母是：\n\n$$\n\\begin{aligned}\n& \\sum_{\\vec{z}}Q(\\vec{z})\\sum_{t=1}^T1\\{z_t=s_j\\} \\\\\n=& \\frac 1{P(\\vec{x};A,B)}\\sum_{i=1}^{|S|}\\sum_{t=1}^T\\sum_{\\vec{z}}1\\{z_{t-1}=s_i\\wedge z_t=s_j\\}P(\\vec{z},\\vec{x};A,B) \\\\\n=& \\frac 1{P(\\vec{x};A,B)}\\sum_{i=1}^{|S|}\\sum_{t=1}^T\\alpha_i(t)A_{ij}B_{jx_t}\\beta_j(t+1)\n\\end{aligned}\n$$\n\n结合这些表达式，得到最大似然发射概率的形式为：\n\n$$\n\\hat{B}_{jk}=\\frac{\\sum_{i=1}^{|S|}\\sum_{t=1}^T1\\{x_t=v_t\\}\\alpha_i(t)A_{ij}B_{jx_t}\\beta_j(t+1)}{\\sum_{i=1}^{|S|}\\sum_{t=1}^T\\alpha_i(t)A_{ij}B_{jx_t}\\beta_j(t+1)}\n$$\n\n<hr style=\"height:1px;border:none;border-top:3px solid black;\" />\n\n**算法 3** HMM参数学习的前向后向算法： \n\n<hr style=\"height:1px;border:none;border-top:1px solid black;\" />\n\n初始化：设$A$和$B$为随机有效的概率矩阵，其中$A_{i0}=0,B_{0k}=0,i=1..|S|,k=1..|V|$\n\n重复直到收敛：{\n\n（$E$步）运行前向和后向算法进行计算$\\alpha_i,\\beta_i,i=1..|S|$，然后设：\n\n$$\n\\gamma_t(i,j):=\\alpha_i(t)A_{ij}B_{jx_t}\\beta_j(t+1)\n$$\n\n（$M$步）重新估计最大似然参数为：\n\n$$\n\\begin{aligned}\nA_{ij} &:= \\frac{\\sum_{t=1}^T\\gamma_t(i,j)}{\\sum_{j=1}^{|S|}\\sum_{t=1}^T\\gamma_t(i,j)} \\\\\nB_{jk} &:= \\frac{\\sum_{i=1}^{|S|}\\sum_{t=1}^T1\\{x_t=v_k\\}\\gamma_t(i,j)}{\\sum_{i=1}^{|S|}\\sum_{t=1}^T\\gamma_t(i,j)}\n\\end{aligned}\n$$\n\n}\n\n<hr style=\"height:1px;border:none;border-top:1px solid black;\" />\n\n算法$3$展示了用于HMMs中参数学习的前向后向算法或Baum-Welch算法的变体。在$E$步，我们并没有对于所有的$\\vec{z}\\in S^T$来明确的计算$Q(\\vec{z})$，而是计算一个充分统计量$\\gamma_t(i,j):=\\alpha_i(t)A_{ij}B_{jx_t}\\beta_j(t+1)$。对于所有观察序列$\\vec{x}$，这个统计量正比于时间步$t$从状态$x_i$转移到状态$x_j$的概率。$A_{ij}$和$B_{jk}$导出的表达式在直观上很有吸引力。$A_{ij}$计算式是从状态$s_i$到$s_j$的期望数除以$s_i$出现的期望次数。同样，$B_{jk}$的计算式是$v_k$转移到$s_j$的期望数量除以$s_j$出现的预期数量。\n\n与许多$EM$算法应用一样，HMMs的参数学习是一个具有许多局部极大值的非凸问题。$EM$算法将根据其初始参数收敛到最大值，因此可能需要多次迭代。此外，通常重要的是$A$和$B$表示的概率分布的平滑计算，以便没有转移或发射被分配为$0$的概率。\n\n##### 2.5 扩展阅读\n\n学习隐马尔可夫模型有很多很好的资源。对于NLP的应用，我推荐查看Jurafsky & Martin's写的《Speech and Language Processing》$^1$第二版或Manning & Schütze 写的《Foundations of Statistical Natural Language Processing.》。此外，Eisner写的HMM-in-a-spreadsheet[1]`注：参考资料[1]见文章最下方`是一种轻量级的交互方式，可以学习只需要电子表格应用程序的HMM。\n\n>1 <a target='_blank' href='http://www.cs.colorado.edu/~martin/slp2.html'>http://www.cs.colorado.edu/~martin/slp2.html</a>\n\n##### 参考资料\n\n<blockquote id='[1]'>[1] Jason Eisner.<a target='_blank' href='https://dl.acm.org/citation.cfm?id=1118110'>An interactive spreadsheet for teaching the forward-backward algorithm.</a>In Dragomir Radev and Chris Brew, editors, Proceedings of the ACL Workshop on Effective Tools and Methodologies for Teaching NLP and CL, pages 10-18, 2002.</blockquote>\n\n### 参考","tags":["CS229"],"categories":["CS229"]},{"title":"[机器学习-CS229]凸优化2","url":"%2Fposts%2F2954f056%2F","content":"# CS229 课程讲义中文翻译\nCS229 Section notes\n\n|原作者|翻译|\n|---|---|\n|Chuong B. Do|[XiaoDong_Wang](https://github.com/Dongzhixiao) |\n\n\n|相关链接|\n|---|\n|[Github 地址](https://github。com/Kivy-CN/Stanford-CS-229-CN)|\n|[知乎专栏](https://zhuanlan。zhihu。com/MachineLearn)|\n|[斯坦福大学 CS229 课程网站](http://cs229。stanford。edu/)|\n|[网易公开课中文字幕视频](http://open。163。com/movie/2008/1/M/C/M6SGF6VB4_M6SGHFBMC。html)|\n\n\n### 凸优化概述\n\n在上一篇的笔记中，我们开始学习**凸优化，** 并且学习到了如下优化问题的数学形式：\n\n$$\n\\begin{aligned}\n\\min_{x\\in R^n}\\quad&f(x) \\\\\nsubject\\quad to\\quad &x\\in C \\qquad\\qquad (1)\n\\end{aligned}\n$$\n\n在凸优化问题的设定中，$x\\in R^n$是一个被称作**优化变量**的向量。$f:R^n\\rightarrow R$是我们想要优化的**凸函数。** $C\\subseteq R^n$是一个被称作可行域集合的**凸集。** 从计算的角度来看，凸优化问题的有趣之处在于，任何局部最优解总是保证其也是全局最优的解。在过去的几十年里，求解凸优化问题的通用方法也变得越来越可靠有效。\n\n在后面的课堂笔记中，我们继续探索凸优化领域。特别地，我们将探讨凸优化理论中一个强大的概念，称为拉格朗日对偶。重点研究拉格朗日对偶的主要思想和机制；特别地，我们将描述拉格朗日的概念以及它与原问题和对偶问题之间的关系，我们还会介绍Karush-Kuhn-Tucker (KKT)条件在提供凸优化问题最优解的充要条件中的作用。\n\n#### 1. 拉格朗日对偶\n\n一般来说，拉格朗日对偶理论是研究凸优化问题的最优解。正如我们在之前的课上看到的，当最小化一个关于$x\\in R^n$的可微凸函数$f(x)$时，使得解集$x^*\\in R^n$是全局最优解的一个充要条件是$\\nabla_xf(x^*)=0$。然而，在具有约束条件的凸优化问题的一般设置中，这种简单的最优性条件并不适用。对偶理论的一个主要目标是用严格的数学方法描述凸规划的最优点。\n\n在后面的笔记中，我们简要介绍了拉格朗日对偶性及其在给出了如下一般可微凸优化问题形式时的应用：\n\n$$\n\\begin{aligned}\n\\min_{x\\in R^n}\\quad&f(x) \\\\\nsubject\\quad to\\quad &g_i(x)\\le 0,\\quad i=1,\\dots,m, \\qquad\\qquad (OPT) \\\\\n&h_i(x)=0,\\quad i=1,\\dots,p,\n\\end{aligned}\n$$\n\n其中$x\\in R^n$是**优化变量**，$f:R^n\\rightarrow R$以及$g_i:R^n\\rightarrow R$是**可微凸函数$^1$。**$h_i:R^n\\rightarrow R$是**仿射函数。$^2$**\n\n>1 回忆一下我们称一个函数$f:S\\rightarrow R$是一个凸函数，需要满足给定任意$x,y\\in S$以及$0\\le\\theta\\le 1$，都有$f(\\theta x +(1-\\theta) y)\\le \\theta f(x)+(1-\\theta)f(y)$成立。如果函数$-f$是凸函数，则函数$f$是凹函数。\n\n>2 回忆一下仿射函数有如下的形式$f(x)=b^Tx+c$，满足$b\\in R^n,c\\in R$。由于仿射函数的海森矩阵是一个零矩阵（即该矩阵是半正定也是半负定矩阵），因此仿射函数即是凸函数，也是凹函数\n\n##### 1.1 拉格朗日函数\n\n在这一节中，我们介绍了拉格朗日对偶理论的基础——拉格朗日函数。给出了凸约束极小化问题的形式，（广义）拉格朗日是一个函数$\\mathcal{L}:R^n\\times R^m\\times R^p\\rightarrow R$，定义如下：\n\n$$\n\\mathcal{L}(x,\\alpha,\\beta)=f(x)+\\sum_{i=1}^m\\alpha_ig_i(x)+\\sum_{i=1}^p\\beta_ih_i(x)\\qquad\\qquad (2)\n$$\n\n这里，拉格朗日函数的第一个参数是一个向量$x\\in R^n$，其维数与原优化问题中的优化变量的维数相匹配；根据习惯，我们称$x$为拉格朗日函数的**原始变量(primal variables)。** 拉格朗日函数的第二个参数是一个向量$\\alpha\\in R^m$，对应于原优化问题中的$m$个凸不等式约束，每个约束都有一个变量$\\alpha_i$。拉格朗日函数的第三个参数是是个向量$\\beta\\in R^p$，对应于原始优化问题的$p$个仿射不等式，每一个约束都有一个变量$\\beta_i$。这些$\\alpha,\\beta$元素合起来被称作拉格朗日函数或**拉格朗日乘数(Lagrange multipliers)** 的**对偶变量(dual variables)**。 \n\n直观地，拉格朗日函数可以看作是原始凸优化问题的目标函数的一个修正版本，该优化问题考虑了每个约束条件。作为拉格朗日乘数的$\\alpha_i,\\beta_i$可以认为是违反不同的限制条件的“代价”。拉格朗日对偶理论背后的关键直觉如下：\n\n对于任何凸优化问题，总是存在对偶变量的设置，使得拉格朗日关于原变量的无约束极小值（保持对偶变量不变）与原约束极小化问题的解一致。\n\n我们在第1.6节描述KKT条件时将这种直觉形式化。\n\n##### 1.2 原问题与对偶问题\n\n为了说明拉格朗日问题与原凸优化问题之间的关系，我们引入了与拉格朗日问题相关的原问题和对偶问题的概念：\n\n<u>原问题</u>\n\n考虑如下的优化问题：\n\n$$\n\\min_x\\underbrace{[\\max_{\\alpha,\\beta:\\alpha_i\\ge0,\\forall i}\\mathcal{L}(x,\\alpha,\\beta)]}_{\\text{which is called }\\theta_\\mathcal{P}(x)}=\\min_x\\theta_\\mathcal{P}(x)\\qquad\\qquad(P)\n$$\n\n上面的等式中，函数$\\theta_\\mathcal{P}:R^n\\rightarrow R$被称作**原目标(primal objective,)，** 右边的无约束极小化问题称为**原问题(primal problem)。** 在通常情况下，当$g_i(x)\\le 0,i=1,\\dots,m$以及$h_i(x)=0,i=1,\\dots,p$时，一个点$x\\in R^n$被称作**原可行域(primal feasible)。** 我们通常使用向量$x^*\\in R^n$代表$(P)$式的解。我们令$p^*=\\theta_\\mathcal{P}(x^*)$代表原目标的最优值。\n\n<u>对偶问题</u>\n\n通过对上述最小化和最大化顺序的转换，我们得到了一个完全不同的优化问题：\n\n$$\n\\max_{\\alpha,\\beta:\\alpha_i\\ge0,\\forall i}\\underbrace{[\\min_x\\mathcal{L}(x,\\alpha,\\beta)]}_{\\text{which is called }\\theta_\\mathcal{D}(\\alpha,\\beta)}=\\max_{\\alpha,\\beta:\\alpha_i\\ge 0,\\forall i}\\theta_\\mathcal{D}(\\alpha,\\beta)\\qquad\\qquad(D)\n$$\n\n这里的函数$\\theta_\\mathcal{D}:R^m\\times R^p\\rightarrow R$被称作**对偶目标(dual objective)，** 右边的约束最大化问题称为**对偶问题(dual problem)。** 通常情况下，当$\\alpha_i\\ge 0,i=1,\\dots,m$时，我们称$(\\alpha,\\beta)$为**对偶可行域(dual feasible)。** 我们通常使用向量对$(\\alpha^*,\\beta^*)\\in R^m\\times R^p$代表$(D)$式的解。我们令$\\theta_\\mathcal{D}(\\alpha^*,\\beta^*)$代表对偶目标的最优值。\n\n##### 1.3 原问题的解释\n\n首先观察到，原目标函数$\\theta_\\mathcal{P}(x)$是一个关于$x$的凸函数$^3$。为了解释原问题，注意到：\n\n<blockquote><details><summary>3 原目标函数是凸函数的原因（详情请点击本行）</summary>\n为了解释原因，注意到：\n\n$$\n\\theta_\\mathcal{P}(x) = \\max_{\\alpha,\\beta:\\alpha_i\\ge0,\\forall i}\\mathcal{L}(x,\\alpha,\\beta) = \\max_{\\alpha,\\beta:\\alpha_i\\ge0,\\forall i}[f(x)+\\sum_{i=1}^m\\alpha_ig_i(x)+\\sum_{i=1}^p\\beta_ih_i(x)]\\qquad\\qquad(3)\n$$\n\n从这个式子中，我们可以观察到$g_i(x)$是一个关于$x$的凸函数。因为$\\alpha_i$被限制为非负数，所以对于所有的$i$，都满足$\\alpha_ig_i(x)$是凸函数。类似的，因为$h_i(x)$是线性函数，所以每一个$\\beta_ih_i(x)$都是关于$x$（不用管$\\beta_i$的符号）的凸函数。由于凸函数的和总是凸函数，我们可以得出括号内的整体是一个关于$x$的凸函数。最后，凸函数集合的最大值也是一个凸函数（自己证明一下！），因此我们可以得出$\\theta_\\mathcal{P}(x)$是一个关于$x$的凸函数。\n</details></blockquote>\n\n$$\n\\begin{aligned}\n\\theta_\\mathcal{P}(x) &= \\max_{\\alpha,\\beta:\\alpha_i\\ge0,\\forall i}\\mathcal{L}(x,\\alpha,\\beta) \\qquad\\qquad&(4)\\\\\n&= \\max_{\\alpha,\\beta:\\alpha_i\\ge0,\\forall i}[f(x)+\\sum_{i=1}^m\\alpha_ig_i(x)+\\sum_{i=1}^p\\beta_ih_i(x)]&(5)\\\\\n&=f(x)+\\max_{\\alpha,\\beta:\\alpha_i\\ge0,\\forall i}[\\sum_{i=1}^m\\alpha_ig_i(x)+\\sum_{i=1}^p\\beta_ih_i(x)]&(6)\n\\end{aligned}\n$$\n\n可以看到这样一个事实：函数$f(x)$不依赖于$\\alpha$或者$\\beta$。只考虑括号内的符号，可以注意到：\n\n- 如果任意$g_i(x)>0$，则使括号内表达式最大化需要使对应的$\\alpha_i$为任意大的正数；但是，如果$g_i(x)\\le 0$，且需要$\\alpha_i$非负，这就意味着调节$\\alpha_i$达到整体最大值的设置为$\\alpha_i= 0$，此时的最大值为$0$。\n\n- 类似地，如果任意$h_i(x) \\ne 0$，则要使括号内表达式最大化，需要选择与$h_i(x)$符号相同且任意大的对应$\\beta_i$；但是，如果$h_i(x)=0$，则最大值与$\\beta_i$无关，只能取$0$。\n\n把这两种情况放在一起，我们看到如果$x$是在原可行域内（即$g_i(x)\\le 0,i=1,\\dots,m$以及$h_i(x)=0,i=1,\\dots,p$）的，则括号内表达式的最大值为$0$，但如果违反任何约束，则最大值为$\\infin$。根据前面的讨论，我们可以写出以下的式子：\n\n$$\n\\theta_\\mathcal{P}(x) = \\underbrace{f(x)}_{原目标(original\\quad objective)} + \\underbrace{\\begin{cases}\n0& 如果x在原始可行域内\\\\\n\\infin& 如果x不在原始可行域内\n\\end{cases}\n}_{为了“跨域(carving\\quad away)”不可行解的障碍函数}\\qquad(7)\n$$\n\n因此，我们可以将原始目标$\\theta_\\mathcal{P}(x)$理解为原问题凸目标函数的一个修正版本，其区别是不可行解（即那些违法限制条件的$x$的集合）含有目标值$\\infin$。直观地说，我们可以考虑如下式子：\n\n$$\n\\max_{\\alpha,\\beta:\\alpha_i\\ge0,\\forall i}[\\sum_{i=1}^m\\alpha_ig_i(x)+\\sum_{i=1}^p\\beta_ih_i(x)]=\\begin{cases}\n0& 如果x在原始问题的可行域内\\\\\n\\infin& 如果x不在原始问题可行域内\n\\end{cases}\\qquad(8)\n$$\n\n作为一种障碍函数(“barrier” function)，它防止我们将不可行点作为优化问题的候选解。\n\n##### 1.4 对偶问题的解释\n\n对偶目标函数$\\theta_\\mathcal{D}(\\alpha,\\beta)$是一个关于$\\alpha$和$\\beta$的凸函数$^4$。为了解释对偶问题，我们首先做如下观察:\n\n<blockquote><details><summary>4 对偶目标函数是凸函数的原因（详情请点击本行）</summary>\n为了解释原因，注意到：\n\n$$\n\\theta_\\mathcal{D}(\\alpha,\\beta) = \\min_{x}\\mathcal{L}(x,\\alpha,\\beta) = \\min_{x}[f(x)+\\sum_{i=1}^m\\alpha_ig_i(x)+\\sum_{i=1}^p\\beta_ih_i(x)]\\qquad\\qquad(9)\n$$\n\n从这个式子中，我们可以观察到对于任意固定值$x$，等式中括号里面是一个关于$\\alpha$和$\\beta$的仿射函数，因此是凸函数。由于凹函数集合的最小值也是凹函数，我们可以得出结论$\\theta_\\mathcal{D}(\\alpha,\\beta)$是一个关于$\\alpha$和$\\beta$的凸函数\n</details></blockquote>\n\n**引理 1** 如果$(\\alpha,\\beta)$为对偶可行域(dual feasible)。则$\\theta_\\mathcal{D}(\\alpha,\\beta)\\le p^*$。\n\n<i>证明。</i>观察如下的式子：\n\n$$\n\\begin{aligned}\n\\theta_\\mathcal{D}(\\alpha,\\beta) &= \\min_{x}\\mathcal{L}(x,\\alpha,\\beta) \\qquad\\qquad&(10)\\\\\n&\\le \\mathcal{L}(x^*,\\alpha,\\beta)&(11)\\\\\n&= f(x^*)+\\sum_{i=1}^m\\alpha_ig_i(x^*)+\\sum_{i=1}^p\\beta_ih_i(x^*)&(12)\\\\\n&\\le f(x^*)=p^*&(13)\n\\end{aligned}\n$$\n\n这里，第一步和第三步分别直接遵循对偶目标函数和拉格朗日函数的定义。第二步是根据不能等号前面的表达式的意思是$x$在的所有可能值上使得函数$\\mathcal{L}(x,\\alpha,\\beta)$为最小的那个值，最后一步是根据$x^*$是在原可行域内的这个事实得出的，并且等式$(8)$也暗示等式$(12)$的后两项必须是非正数。\n\n引理表明，给定任何对偶可行的$(\\alpha,\\beta)$，对偶目标$\\theta_\\mathcal{D}(\\alpha,\\beta)$提供了原问题优化值$p^*$的一个下界。由于对偶问题涉及到在所有对偶可行域$(\\alpha,\\beta)$上使对偶目标最大化。因此，对偶问题可以看作是对可能的最紧下界$p^*$的搜索。这就对任何原始和对偶优化问题对产生了一个性质，这个性质被称为**弱对偶(weak duality)：**\n\n**引理 2** （弱对偶）。对于任何一对原始的和对偶的问题，$d^*\\le p^*$。\n\n显然，弱对偶性是引理$1$使用$(\\alpha,\\beta)$作为对偶可行点的结果。对于一些原始/对偶优化问题，一个更强的可以成立的结果，其被称为**强对偶性(strong duality)：**\n\n**引理 3** （强对偶）。对于满足一定被称为**约束规定(constraint qualifications)** 的技术条件的任何一对原问题和对偶问题， 可得到$d^*= p^*$。\n\n存在许多不同的约束条件，其中最常调用的约束条件称为**Slater条件(Slater’s condition)：** 如果存在一个所有不等式约束（即$g_i(x)<0,i=1,\\dots,m$）都严格满足的可行原始解$x$，则原始/对偶问题对满足Slater条件。在实际应用中，几乎所有的凸问题都满足某种约束条件，因此原问题和对偶问题具有相同的最优值。\n\n##### 1.5 互补松弛性 \n\n凸优化问题强对偶性的一个特别有趣的结果是**互补松弛性(complementary slackness)**（或KKT互补）：\n\n**引理 4** （互补松弛性）。如果强对偶成立，则对于每一个$i=1,\\dots,m$都有$\\alpha_i^*g(x_i^*)=0$\n\n<i>证明。</i>假设强对偶性成立。主要是复制上一节的证明，注意这下面的式子：\n\n$$\n\\begin{aligned}\np^*=d^*=\\theta_\\mathcal{D}(\\alpha^*,\\beta^*) &= \\min_{x}\\mathcal{L}(x,\\alpha^*,\\beta^*) \\qquad\\qquad&(14)\\\\\n&\\le \\mathcal{L}(x^*,\\alpha^*,\\beta^*)&(15)\\\\\n&= f(x^*)+\\sum_{i=1}^m\\alpha_i^*g_i(x^*)+\\sum_{i=1}^p\\beta_i^*h_i(x^*)&(16)\\\\\n&\\le f(x^*)=p^*&(17)\n\\end{aligned}\n$$\n\n由于这个一系列式子中的第一个和最后一个表达式是相等的，因此每个中间表达式也是相等的。从式子$(16)$减去式子$(17)$的左半边，我们得到：\n\n$$\n\\sum_{i=1}^m\\alpha_i^*g_i(x^*)+\\sum_{i=1}^p\\beta_i^*h_i(x^*)=0\\qquad\\qquad(18)\n$$\n\n但是，回忆一下由于$x^*$和$(\\alpha^*,\\beta^*)$分别都在原可行域和对偶可行域内，所以每个$\\alpha_i^*$是非负的，每个$g_i(x^*)$是非负的，以及每个$h_i(x^*)$都是零。因此，$(18)$表示的是所有非正项之和等于零的一个式子。很容易得出结论，求和中的所有单独项本身都必须为零（因为如果不为零，求和中就没有允许总体和保持为零的补偿正项）。\n\n互补松弛性可以用许多等价的方式来表示。一种特别的方法是如下的条件对：\n\n$$\n\\begin{aligned}\n\\alpha_i^*>0 &\\Longrightarrow g_i(x^*)=0\\qquad\\qquad &(19) \\\\\ng_i(x^*)<0 &\\Longrightarrow\\alpha_i^*=0\\qquad\\qquad &(20)\n\\end{aligned}\n$$\n\n在这个形式中，我们可以看到，无论何时任意$\\alpha_i^*$都严格大于零，因此这就意味着相应的不等式约束必须保证等式成立。我们将其称为**有效约束(active constraint)。** 在支持向量机(SVMs)的情况下，有效约束也称为**支持向量(support vectors)。**\n\n##### 1.6 KKT条件\n\n最后，根据到目前为止的所有条件，我们就可以描述原始对偶优化对的最优条件。我们有如下的定理：\n\n**定理 1.1** 假设$x^*\\in R^n,\\alpha^*\\in R^m,\\beta^*\\in R^p$满足以下条件：\n\n1. （原始的可行性）$g_i(x^*)\\le 0,i=1,\\dots,m$以及$h_i(x^*)=0,i=1,\\dots,p$，\n2. （对偶可行性）$\\alpha_i^*\\ge 0,i= 1,\\dots,m$，\n3. （互补松弛性）$\\alpha_i^*g_i(x^*)=0,i= 1,\\dots,m$，\n4. （拉格朗日稳定性）$\\nabla_x\\mathcal{L}(x^*,\\alpha^*,\\beta^*)=0$。\n\n$x^*$是原优化，$(\\alpha^*,\\beta^*)$是对偶优化。更进一步，如果强对偶成立，则任意原优化$x^*$以及对偶优化$(\\alpha^*,\\beta^*)$必须满足条件$1$到条件$4$。\n\n这些条件被称为Karush-Kuhn-Tucker (KKT)条件。$^5$\n\n>5 顺便提一下，KKT定理有一段有趣的历史。这个结果最初是由卡鲁什在1939年的硕士论文中推导出来的，但直到1950年被两位数学家库恩和塔克重新发现，才引起人们的注意。约翰在1948年也推导出了本质上相同结果的一个变体。关于为什么这个结果在近十年中有如此多的迭代版本都被忽视的有趣历史解释，请看这篇论文：\nKjeldsen, T.H. (2000) A contextualized historical analysis of the Kuhn-Tucker Theorem in nonlinear programming: the impact of World War II. Historica Mathematics 27: 331-361.\n\n#### 2 一个简单的对偶实例\n\n作为对偶的一个简单应用，在本节中，我们将展示如何形成一个简单凸优化问题的对偶问题。考虑如下的凸优化问题：\n\n$$\n\\begin{aligned}\n\\min_{x\\in R^2}\\quad &x_1^2+x_2 \\\\\nsubject\\quad to \\quad&2x_1+x_2\\ge 4 \\\\\n& x_2\\ge 1\n\\end{aligned}\n$$\n\n首先，我们将优化问题重写为标准形式：\n\n$$\n\\begin{aligned}\n\\min_{x\\in R^2}\\quad &x_1^2+x_2 \\\\\nsubject\\quad to \\quad&4-2x_1-x_2\\le 0 \\\\\n& 1-x_2\\le 0\n\\end{aligned}\n$$\n\n拉格朗日函数是：\n\n$$\n\\mathcal{L}(x,\\alpha)=x_1^2+x_2+\\alpha_1(4-2x_1-x_2)+\\alpha_2(1-x_2),\\qquad\\qquad (21)\n$$\n\n对偶问题的目标定义为：\n\n$$\n\\theta_\\mathcal{D}(\\alpha)=\\min_x\\mathcal{L}(x,\\alpha)\n$$\n\n为了用只依赖于$\\alpha$（而不是$x$）的形式来表示对偶目标，我们首先观察到拉格朗日函数关于$x$是可微的，事实上，$x_1$和$x_2$（即我们可以分别求出它们的最小值）是可以分离的。\n\n为了使函数关于$x_1$最小化，可以观察到拉格朗日函数是关于$x_1$的严格凸二次函数，因此通过将导数设为零可以找到关于$x_1$的最小值：\n\n$$\n\\frac{\\partial}{\\partial x_1}\\mathcal{L}(x,\\alpha)=2x_1-2\\alpha_1=0\\Longrightarrow x_1=\\alpha_1\\qquad\\qquad (22)\n$$\n\n为了使函数关于$x_2$最小化，可以观察到拉格朗日函数是$x_2$的仿射函数，其中线性系数恰好是拉格朗日系数关于$x_2$的导数：\n\n$$\n\\frac{\\partial}{\\partial x_2}\\mathcal{L}(x,\\alpha)=1-\\alpha_1-\\alpha_2\\qquad\\qquad (23)\n$$\n\n如果线性系数非零，则目标函数可以通过选择与线性系数符号相反的$x_2$和任意大的增幅使其任意小。然而，如果线性系数为零，则目标函数不依赖于$x_2$。\n\n把以上这些观察结果放在一起，我们得到：\n\n$$\n\\begin{aligned}\n\\theta_\\mathcal{D}(\\alpha)&=\\min_x\\mathcal{L}(x,\\alpha) \\\\\n&=\\min_{x_2}[\\alpha_1^2+x_2+\\alpha_1(4-2x_1-x_2)+\\alpha_2(1-x_2)] \\\\\n&=\\min_{x_2}[-\\alpha_1^2+4\\alpha_1+\\alpha_2+x_2(1-\\alpha_1-\\alpha_2)] \\\\\n&=\\begin{cases}\n-\\alpha_1^2+4\\alpha_1+\\alpha_2 \\quad &如果1-\\alpha_1-\\alpha_2=0\\\\\n-\\infin &其他情况\n\\end{cases}\n\\end{aligned}\n$$\n\n所以对偶问题由下式给出:\n\n$$\n\\begin{aligned}\n\\max_{x\\in R^2}\\quad &\\theta_\\mathcal{D}(\\alpha) \\\\\nsubject\\quad to \\quad&\\alpha_1\\ge 0 \\\\\n& \\alpha_2\\ge 0\n\\end{aligned}\n$$\n\n最后，我们可以通过观察使对偶约束显式$^6$的化简对偶问题：\n\n>6 这就是说，我们把使$\\theta_\\mathcal{D}(\\alpha)$为$-\\infin$的条件移到对偶优化问题的约束集中。\n\n$$\n\\begin{aligned}\n\\max_{x\\in R^2}\\quad &-\\alpha_1^2+4\\alpha_1+\\alpha_2 \\\\\nsubject\\quad to \\quad&\\alpha_1\\ge 0 \\\\\n& \\alpha_2\\ge 0 \\\\\n& 1-\\alpha_1-\\alpha_2=0\n\\end{aligned}\n$$\n\n注意对偶问题是以为$\\alpha$变量的一个凹二次规划问题。\n\n#### 3 SVM$L_1$范数的软边界\n\n为了看到一个更复杂的拉格朗日对偶例子，我们来推导以前课堂上给出的SVM$L_1$范数的软边界的原对偶问题，以及相应的KKT互补（即，互补松弛）条件。我们有：\n\n$$\n\\begin{aligned}\n\\min_{w,b,\\xi} \\quad & \\frac 12 \\parallel w\\parallel^2+C\\sum^m_{i=1}\\xi_i \\\\\nsubject\\quad to \\quad& y^{(i)}(w^Tx^{(i)}+b) \\geq1-\\xi_i,\\quad &i=1,...,m\\\\\n& \\xi_i \\geq 0, &i=1,...,m\n\\end{aligned}\n$$\n\n首先，我们使用“$\\le 0$”的不等式形式把它化成标准形式：\n\n$$\n\\begin{aligned}\n\\min_{w,b,\\xi} \\quad & \\frac 12 \\parallel w\\parallel^2+C\\sum^m_{i=1}\\xi_i \\\\\nsubject\\quad to \\quad& 1-\\xi_i-y^{(i)}(w^Tx^{(i)}+b) \\le 0,\\quad &i=1,...,m\\\\\n& -\\xi_i \\le 0, &i=1,...,m\n\\end{aligned}\n$$\n\n接下来，我们构造广义拉格朗日函数：$^7$\n\n> 7 在这里，非常重要的一点是要注意到全体$(w,b,\\xi)$在“$x$”原变量中占据的角色。类似的，要注意到全体$(\\alpha,\\beta)$在“$\\alpha$”对偶变量中占据的角色，通常是用于不等式约束的。因为在本问题中并没有仿射不等式约束，因此在这里就没有“$\\beta$”对偶变量。\n\n$$\n\\mathcal{L}(w,b,\\xi,\\alpha,\\beta)=\\frac 12 \\parallel w\\parallel^2+C\\sum^m_{i=1}\\xi_i+\\sum^m_{i=1}\\alpha_i(1-\\xi_i-y^{(i)}(w^Tx^{(i)}+b))-\\sum_{i=1}^m\\beta_i\\xi_i\n$$\n\n上式给出了原始和对偶优化问题：\n\n$$\n\\begin{aligned}\n\\max_{\\alpha,\\beta:\\alpha_i\\ge0,\\beta_i\\ge0}\\theta_{\\mathcal{D}}(\\alpha,\\beta) \\qquad&其中\\qquad\\theta_{\\mathcal{D}}(\\alpha,\\beta) := \\min_{w,b,\\xi}\\mathcal{L}(w,b,\\xi,\\alpha,\\beta),\\quad&(SVM-D)\\\\\n\\min_{w,b,\\xi}\\theta_{\\mathcal{P}}(w,b,\\xi)\\qquad&其中\\qquad\\theta_{\\mathcal{P}}(w,b,\\xi) := \\max_{\\alpha,\\beta:\\alpha_i\\ge0,\\beta_i\\ge0}\\mathcal{L}(w,b,\\xi,\\alpha,\\beta),\\quad&(SVM-P)\n\\end{aligned}\n$$\n\n不过，要把对偶问题化成讲义中所示的形式，我们还有一些工作要做。特别是,\n\n1. **消去原始变量。** 为了消除对偶问题中的原始变量，通过下面的式子计算$\\theta_{\\mathcal{D}}(\\alpha,\\beta)$：\n\n$$\n\\theta_{\\mathcal{D}}(\\alpha,\\beta)=\\min_{w,b,\\xi}\\quad \\mathcal{L}(w,b,\\xi,\\alpha,\\beta)\n$$\n\n上式是一个无约束优化问题，其中目标函数$\\mathcal{L}(w,b,\\xi,\\alpha,\\beta)$是可微的。拉格朗日函数是关于$w$的严格凸二次函数，到目前为止，对于任意给定的$(\\alpha,\\beta)$来说，如果有$(\\hat{w},\\hat{b},\\hat{\\xi})$使得拉格朗日函数最小化，必须满足下式：\n\n$$\n\\nabla_w\\mathcal{L}(\\hat{w},\\hat{b},\\hat{\\xi},\\alpha,\\beta) = \\hat{w}-\\sum_{i=1}^m\\alpha_iy^{(i)}x^{(i)}=0\\qquad\\qquad(24)\n$$\n\n跟进一步来说，拉格朗日函数是关于$b$和$\\xi$的线性函数。通过类似于前一节简单对偶例子中描述的推理。我们可以设置对$b$和$\\xi$求导等于零，并将得到的条件作为显式约束添加到对偶优化问题中：\n\n$$\n\\frac{\\partial}{\\partial_b}\\mathcal{L}(\\hat{w},\\hat{b},\\hat{\\xi},\\alpha,\\beta)=-\\sum_{i=1}^m\\alpha_iy^{(i)}=0\\qquad\\qquad(25) \\\\\n\\frac{\\partial}{\\partial_{\\xi_i}}\\mathcal{L}(\\hat{w},\\hat{b},\\hat{\\xi},\\alpha,\\beta)=C-\\alpha_i-\\beta_i=0\\qquad\\qquad(26)\n$$\n\n我们可以用这些条件来计算对偶目标为：\n\n$$\n\\begin{aligned}\n\\theta_{\\mathcal{D}}(\\alpha,\\beta) &= \\mathcal{L}(\\hat{w},\\hat{b},\\hat{\\xi})\\\\\n&= \\frac 12 \\parallel \\hat{w}\\parallel^2+C\\sum^m_{i=1}\\hat{\\xi_i}+\\sum^m_{i=1}\\alpha_i(1-\\hat{\\xi}_i-y^{(i)}(\\hat{w}^Tx^{(i)}+\\hat{b}))-\\sum_{i=1}^m\\beta_i\\hat{\\xi_i} \\\\\n&= \\frac 12 \\parallel \\hat{w}\\parallel^2+C\\sum^m_{i=1}\\hat{\\xi_i}+\\sum^m_{i=1}\\alpha_i(1-\\hat{\\xi}_i-y^{(i)}(\\hat{w}^Tx^{(i)}))-\\sum_{i=1}^m\\beta_i\\hat{\\xi_i} \\\\\n&= \\frac 12 \\parallel \\hat{w}\\parallel^2 + \\sum^m_{i=1}\\alpha_i(1-y^{(i)}(\\hat{w}^Tx^{(i)}))\n\\end{aligned}\n$$\n\n其中第一个等式来自于给定$(\\alpha,\\beta)$最优的$(\\hat{w},\\hat{b},\\hat{\\xi})$，第二个等式使用广义拉格朗日函数的定义，第三个等式和第四个等式分别来自$(25)$和$(26)$。最后，使用$(24)$，可得：\n\n$$\n\\begin{aligned}\n\\frac 12 \\parallel \\hat{w}\\parallel^2 + \\sum^m_{i=1}\\alpha_i(1-y^{(i)}(\\hat{w}^Tx^{(i)})) &= \\sum^m_{i=1}\\alpha_i + \\frac 12 \\parallel \\hat{w}\\parallel^2 - \\hat{w}^T\\sum^m_{i=1}\\alpha_iy^{(i)}x^{(i)} \\\\\n&= \\sum^m_{i=1}\\alpha_i + \\frac 12 \\parallel \\hat{w}\\parallel^2 - \\parallel \\hat{w}\\parallel^2 \\\\\n&= \\sum^m_{i=1}\\alpha_i - \\frac 12 \\parallel \\hat{w}\\parallel^2 \\\\\n&= \\sum^m_{i=1}\\alpha_i - \\frac 12\\sum^m_{i=1}\\sum^m_{j=1}\\alpha_i\\alpha_iy^{(i)}y^{(j)}<x^{(i)},x^{(j)}>\n\\end{aligned}\n$$\n\n因此，我们的对偶问题（因为没有更多原始变量和所有的显式约束）就很简单了：\n\n$$\n\\begin{aligned}\n\\min_{\\alpha,\\beta} \\quad & \\sum^m_{i=1}\\alpha_i - \\frac 12\\sum^m_{i=1}\\sum^m_{j=1}\\alpha_i\\alpha_iy^{(i)}y^{(j)}<x^{(i)},x^{(j)}> \\\\\nsubject\\quad to \\quad&\\alpha_i \\geq 0,\\quad &i=1,...,m\\\\\n& \\beta_i \\geq 0, &i=1,...,m \\\\\n& \\alpha_i + \\beta_i = C, &i=1,...,m \\\\\n&\\sum^m_{i=1}\\alpha_iy^{(i)}=0\n\\end{aligned}\n$$\n\n2. **KKT互补性。** KKT互补要求对任何原始最优解$(w^*,b^*,\\xi^*)$和对偶最优解$(\\alpha^*,\\beta^*)$都满足： \n\n$$\n\\begin{aligned}\n\\alpha_i^*(1-\\xi_i^*-y^{(i)}(w^{*T}x^{(i)}+b^*)) &= 0 \\\\\n\\beta_i^*\\xi_i^* &= 0\n\\end{aligned}\n$$\n\n对于$i = 1,\\dots,m$。根据第一个条件我们可以得出如果$\\alpha_i^*>0$那么为了使乘积为零，则$1-\\xi_i^*-y^{(i)}(w^{*T}x^{(i)}+b^*)=0$。由此断定：\n\n$$\ny^{(i)}(w^{*T}x^{(i)}+b^*)\\le 1\n$$\n\n根据原可行性有$\\xi^*\\ge 0$，类似的，如果$\\beta_i^*>0$，则需要$\\xi_i^*=0$来确保互补性。根据原约束条件$y^{(i)}(w^{*T}x^{(i)}+b^*)\\ge 1-\\xi_i$，可以得出：\n\n$$\ny^{(i)}(w^{*T}x^{(i)}+b^*)\\ge 1\n$$\n\n最后，因为$\\beta_i^*>0$等价于$\\alpha_i^*<C$（因为$\\alpha^* + \\beta_i^* = C$），我们可以将KKT条件总结为如下式子：\n\n$$\n\\alpha_i^*<C\\quad\\Rightarrow\\quad y^{(i)}(w^{*T}x^{(i)}+b^*)\\ge 1, \\\\\n\\alpha_i^*>0\\quad\\Rightarrow\\quad y^{(i)}(w^{*T}x^{(i)}+b^*)\\le 1\n$$\n\n或者等价的表示为：\n\n$$\n\\alpha_i^*=0\\quad\\Rightarrow\\quad y^{(i)}(w^{*T}x^{(i)}+b^*)\\ge 1, \\\\\n0<\\alpha_i^*<C\\quad\\Rightarrow\\quad y^{(i)}(w^{*T}x^{(i)}+b^*)= 1, \\\\\n\\alpha_i^*=C\\quad\\Rightarrow\\quad y^{(i)}(w^{*T}x^{(i)}+b^*)\\le 1\n$$\n\n3. **简化。** 通过观察下面数学形式的每一对约束，我们可以稍微整理一下对偶问题：\n\n$$\n\\beta_i\\ge 0\\qquad\\qquad \\alpha_i + \\beta_i = C\n$$\n\n上面的式子等价于一个单约束$\\alpha_i\\le C$；也就是说，如果我们相约解决下面的约束问题：\n\n$$\n\\begin{aligned}\n\\min_{\\alpha,\\beta} \\quad & \\sum^m_{i=1}\\alpha_i - \\frac 12\\sum^m_{i=1}\\sum^m_{j=1}\\alpha_i\\alpha_iy^{(i)}y^{(j)}<x^{(i)},x^{(j)}> \\\\\nsubject\\quad to \\quad&0\\le\\alpha_i \\le C,\\quad &i=1,...,m,\\qquad\\qquad(27)\\\\\n&\\sum^m_{i=1}\\alpha_iy^{(i)}=0\n\\end{aligned}\n$$\n\n并且随后设置$\\beta_i=C-\\alpha_i$，则$(\\alpha,\\beta)$对于前面的对偶问题是最优的。最后一种形式实际上是课堂讲稿中给出的软边界SVM对偶形式。\n\n#### 4 后续研究方向\n\n在许多实际任务中，$90\\%$的挑战都涉及如何以凸优化的形式来改写优化问题。一旦找到了正确的形式，就可以使用软件包。因为许多已有的用于凸优化的软件包已经进行了很好的调优，以处理不同类型的优化问题。下面是一小部分可用的软件包：\n\n- 商业包：CPLEX, MOSEK\n- 基于matlab的包：CVX，优化工具箱(linprog, quadprog)， SeDuMi\n- 库：CVXOPT (Python)、GLPK (C)、COIN-OR (C)\n- 支持向量机：LIBSVM SVM-light\n- 机器学习：Weka (Java)\n\n我们特别指出CVX作为一个易于使用的通用工具可以基于MATLAB求解凸优化问题，还有CVXOPT作为一个强大的基于Python库，其独立于MATLAB运行。$^8$ 如果你对上诉列表中出现的其他包感兴趣的话，可以很容易的在web中搜索到。简而言之，如果你需要一个特定的凸优化算法，现有的软件包提供了一种快速的原型化方法来让你实现该算法，而无需你自己完整的完成凸优化的所有数值计算。\n\n>8 CVX 在网址 http://cvxr.com/cvx/ 可以找到。CVXOPT 在网址 http://cvxopt.org/ 可以找到。\n\n另外，如果你觉得本材料很有趣，一定要看看Stephen Boyd的课程EE364: Optimization I，它将在冬季学期提供。EE364的课程教材（在参考资料[1]`注：参考资料[1]见文章最下方`中列出）包含丰富的凸优化知识，可以在线浏览。\n\n##### 参考资料\n\n<blockquote id='[1]'>[1] Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge UP, 2004. Online: <a target='_blank' href='http://www.stanford.edu/~boyd/cvxbook/'>http://www.stanford.edu/~boyd/cvxbook/</a></blockquote>\n\n### 参考","tags":["CS229"],"categories":["CS229"]},{"title":"[机器学习-CS229]凸优化1","url":"%2Fposts%2Fb05da1ec%2F","content":"# CS229 课程讲义中文翻译\nCS229 Section notes\n\n|原作者|翻译|\n|---|---|\n|Zico Kolter|[XiaoDong_Wang](https://github.com/Dongzhixiao) |\n\n\n|相关链接|\n|---|\n|[Github 地址](https://github。com/Kivy-CN/Stanford-CS-229-CN)|\n|[知乎专栏](https://zhuanlan。zhihu。com/MachineLearn)|\n|[斯坦福大学 CS229 课程网站](http://cs229。stanford。edu/)|\n|[网易公开课中文字幕视频](http://open。163。com/movie/2008/1/M/C/M6SGF6VB4_M6SGHFBMC。html)|\n\n\n### 凸优化概述\n\n#### 1. 介绍\n\n在很多时候，我们进行机器学习算法时希望优化某些函数的值。即，给定一个函数$f:R^n\\rightarrow R$，我们想求出使函数$f(x)$最小化（或最大化）的原像$x\\in R^n$。我们已经看过几个包含优化问题的机器学习算法的例子，如：最小二乘算法、逻辑回归算法和支持向量机算法，它们都可以构造出优化问题。\n\n在一般情况下，很多案例的结果表明，想要找到一个函数的全局最优值是一项非常困难的任务。然而，对于一类特殊的优化问题——**凸优化问题，** 我们可以在很多情况下有效地找到全局最优解。在这里，有效率既有实际意义，也有理论意义：它意味着我们可以在合理的时间内解决任何现实世界的问题，它意味着理论上我们可以在一定的时间内解决该问题，而时间的多少只取决于问题的多项式大小。**（译者注：即算法的时间复杂度是多项式级别$O(n^k)$，其中$k$代表多项式中的最高次数）**\n\n这部分笔记和随附课程的目的是对凸优化领域做一个非常简要的概述。这里的大部分材料（包括一些数字）都是基于斯蒂芬·博伊德(Stephen Boyd)和利文·范登伯格(lieven Vandenberghe)的著作《凸优化》（凸优化[1]`注：参考资料[1]见文章最下方`在网上<a target='_blank' href='https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf'>免费提供下载</a>）以及斯蒂芬·博伊德(Stephen Boyd)在斯坦福教授的课程EE364。如果你对进一步研究凸优化感兴趣，这两种方法都是很好的资源。\n\n#### 2. 凸集\n\n我们从**凸集**的概念开始研究凸优化问题。\n\n**定义2.1** 我们定义一个集合是凸的，当且仅当任意$x,y\\in C$ 且 $\\theta\\in R, 0\\le\\theta\\le 1$,\n\n$$\n\\theta x + (1-\\theta)y\\in C\n$$\n\n实际上，这意味着如果我们在集合$C$中取任意两个元素，在这两个元素之间画一条直线，那么这条直线上的每一点都属于$C$。图$1$显示了一个示例的一个凸和一个非凸集。其中点$\\theta x +(1-\\theta) y$被称作点集$x,y$的**凸性组合(convex combination)。**\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229notecof1.png)\n\n##### 2.1 凸集的实例\n\n- **全集$R^n$** 是一个凸集 **（译者注：即$n$维向量空间或线性空间中所有元素组成的集合）。** 因为以下结论显而易见：任意$x,y\\in R^n$，则$\\theta x +(1-\\theta) y\\in R^n$。**（译者注：$n$维向量空间对加法和数乘封闭，显然其线性组合也封闭）**\n\n- **非负的象限$R^n_+$组成的集合** 是一个凸集。$R^n$中所有非负的元素组成非负象限：$R^n_+=\\{x:x_i\\ge 0\\quad\\forall i=1,\\dots,n \\}$。为了证明$R^n_+$是一个凸集，只要给定任意$x,y\\in R^n_+$，并且$0\\le\\theta\\le 1$，\n\n$$\n(\\theta x +(1-\\theta) y)_i = \\theta x_i + (1 - \\theta)y_i\\ge 0\\quad\\forall i\n$$\n\n- **范数球**是一个凸集。设$\\parallel\\cdot\\parallel$是$R^n$中的一个范数（例如，欧几里得范数，即二范数$\\parallel x\\parallel_2=\\sqrt{\\sum_{i=1}^nx_i^2}$）。则集合$\\{x:\\parallel x\\parallel\\le 1\\}$是一个凸集。为了证明这个结论，假设$x,y\\in R^n$，其中$\\parallel x\\parallel\\le 1,\\parallel y\\parallel\\le 1,0\\le\\theta\\le 1$，则：\n\n$$\n\\parallel \\theta x +(1-\\theta) y\\parallel\\le \\parallel\\theta x\\parallel+\\parallel(1-\\theta) y\\parallel = \\theta\\parallel x\\parallel+(1-\\theta)\\parallel y\\parallel\\le 1\n$$\n\n我们用了三角不等式和范数的正同质性(positive homogeneity)。\n\n- **仿射子空间和多面体** 是一个凸集。给定一个矩阵$A\\in R^{m\\times n}$和一个向量$b\\in R^m$，一个仿射子空间可以表示成一个集合$\\{x\\in R^n:Ax=b\\}$（注意如果$b$无法通过$A$列向量的线性组合得到时，结果可能是空集）。类似的，一个多面体（同样，也可能是空集）是这样一个集合$\\{x\\in R^n:Ax\\preceq b\\}$，其中‘$\\preceq$’代表分量不等式(componentwise inequality)（也就是，$Ax$得到的向量中的所有元素都小于等于$b$向量对应位置的元素）$^1$。为了证明仿射子空间和多面体是凸集，首先考虑$x,y\\in R^n$，这样可得$Ax=Ay=b$。则对于$0\\le\\theta\\le 1$，有：\n\n>1 类似的，对于两个向量$x,y\\in R^n$，$x\\succeq y$代表，向量$x$中的每一个元素都大于等于向量$y$对应位置的元素。注意有时候文中使用符号‘$\\le$’和‘$\\ge$’代替了符号‘$\\preceq$’和‘$\\succeq$’，则符号的实际意义必须根据上下文来确定（也就是如果等式两边都是向量的时候，文中使用的是常规的不等号‘$\\le$’和‘$\\ge$’，则我们自己心中要知道用后两个符号‘$\\preceq$’和‘$\\succeq$’的意义代替之）\n\n$$\nA(\\theta x +(1-\\theta) y) = \\theta Ax + (1-\\theta)Ay=\\theta b + (1-\\theta)b=b\n$$\n\n类似的，对于$x,y\\in R^n$，满足$Ax\\le b$以及$Ay\\le b,0\\le\\theta\\le 1$，则：\n\n$$\nA(\\theta x +(1-\\theta) y) = \\theta Ax + (1-\\theta)Ay\\le\\theta b + (1-\\theta)b=b\n$$\n\n- **凸集之间的交集**还是凸集。假设$C_1,C_2,\\dots,C_k$都是凸集，则它们的交集：\n\n$$\n\\bigcap_{i=1}^kC_i=\\{x:x\\in C_i\\quad\\forall i=1,\\dots,k\\}\n$$\n\n同样也是凸集。为了证明这个结论，考虑$x,y\\in \\bigcap_{i=1}^k C_i$以及$0\\le\\theta\\le 1$，则：\n\n$$\n\\theta x +(1-\\theta) y\\in C_i\\quad\\forall i=1,\\cdots,k\n$$\n\n因此，根据凸集的定义可得：\n\n$$\n\\theta x +(1-\\theta) y\\in\\bigcap_{i=1}^kC_i\n$$\n\n然而要注意在通常情况下，凸集之间的并集并不是一个凸集。\n\n- **半正定矩阵**是一个凸集。所有对称半正定矩阵的集合，常称为半正定锥，记作$S^n_+$，其是一个凸集（通常情况下，$S^n\\subset R^{n\\times n}$代表$n\\times n$对称矩阵的集合）。回忆一个概念，我们说一个矩阵$A\\in R^{n\\times n}$是对称半正定矩阵，当且仅当该矩阵满足$A=A^T$，并且给定任意一个$n$维向量$x\\in R^n$，满足$x^TAx\\ge 0$。现在考虑两个对称半正定矩阵$A,B\\in S^n_+$，并且有$0\\le\\theta\\le 1$。给定任意$n$维向量$x\\in R^n$，则：\n\n$$\nx^T(\\theta A +(1-\\theta) B)x=\\theta x^TAx+(1-\\theta)x^TBx\\ge 0\n$$\n\n同样的逻辑可以用来证明所有正定、负定和半负定矩阵的集合也是凸集。\n\n#### 3. 凸函数\n\n凸优化的一个核心要素是**凸函数**的概念。\n\n**定义 $3.1$** 我们称一个函数$f:R^n\\rightarrow R$是一个凸函数，需要满足其定义域（记作$\\mathcal{D}(f)$）是一个凸集，同时给定任意$x,y\\in \\mathcal{D}(f)$以及$\\theta\\in R,0\\le\\theta\\le 1$，满足：\n\n$$\nf(\\theta x +(1-\\theta) y)\\le \\theta f(x)+(1-\\theta)f(y)\n$$\n\n**（译者注：注意这里函数的凸凹性和我们本科《高等数学》上册里面微分中值定理与导数应用章节中曲线的凸凹性是相反的，不过这里定义的凸凹的方向在机器学习中更常见）**\n\n直观地，考虑这个定义的方法是如果我们在凸函数的图上取任意两点并在两点之间画一条直线，那么函数在这两点之间的部分就会在这条直线下面。这种情况如图$2^2$所示。\n\n>2 不要太担心$f$的定义域是凸集的要求，这个要求仅仅是在技术上保证$f(\\theta x +(1-\\theta) y)$有定义（如果定义域$\\mathcal{D}(f)$不是凸集，则即使$x,y\\in\\mathcal{D}(f)$，$f(\\theta x +(1-\\theta) y)$也有可能没有意义）\n\n如果在定义$3.1$的基础上增加严格的不等的条件$x\\ne y$和$0<\\theta<1$，则可以说一个函数是**严格凸函数**。如果$f$是凸函数则我们可以得到$-f$是**凹函数，** 同理如果$f$是严格凸函数则$-f$是**严格凹函数。**\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229notecof2.png)\n\n##### 3.1 凸性的一阶条件\n\n假设函数$f:R^n\\rightarrow R$可微（即其梯度$^3\\nabla_xf(x)$在函数$f$的定义域内处处存在）。则$f$是一个凸函数，只要满足$\\mathcal{D}(f)$是一个凸集，同时对所有的$x,y\\in\\mathcal{D}(f)$，有：\n\n>3 回忆一下梯度定义为$\\nabla_xf(x)\\in R^n,(\\nabla_xf(x))_i=\\frac{\\partial f(x)}{\\partial x_i}$。有关梯度和海森函数的知识，请参阅前面关于线性代数的部分的章节笔记。\n\n$$\nf(y)\\ge f(x)+\\nabla_xf(x)^T(y-x)\n$$\n\n函数$f(x)+\\nabla_xf(x)^T(y-x)$称为函数$f(x)$在点$x$处的**一阶近似(first-order approximation)。** 直觉上来说，这个函数可以近似的认为是函数$f$在点$x$处的切线。凸性的一阶条件就是阐明了，$f$是凸函数当且仅当该函数的切线是一个全局下估计(global underestimator)。换句话说，如果我们根据函数的特性在任意一点绘制该函数的切线，那么这条直线上的每一点将低于函数$f$在相应位置的点。\n\n与凸性的定义类似，当严格不等条件成立时$f$是严格凸函数，当不等式符号颠倒时$f$是凹函数，当颠倒的不等式的严格不等条件成立时$f$是严格凹函数。\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229notecof3.png)\n\n##### 3.2 凸性的二阶条件\n\n假设函数$f:R^n\\rightarrow R$二阶可微（即其海森矩阵$^4\\nabla_x^2f(x)$在函数$f$的定义域内处处存在）。则$f$是一个凸函数，只要满足$\\mathcal{D}(f)$是一个凸集，同时对所有的$x,y\\in\\mathcal{D}(f)$，有：\n\n>4 回忆一下海森矩阵定义为$\\nabla_x^2f(x)\\in R^{n\\times n},(\\nabla_x^2f(x))_{ij}=\\frac{\\partial^2 f(x)}{\\partial x_i\\partial x_j}$\n\n$$\n\\nabla_x^2f(x)\\succeq 0\n$$\n\n当符号“$\\succeq$”在这里与矩阵结合使用时，指的是半正定矩阵，而不是分量不等式(componentwise inequality)$^5$。在一维中，这等价于二阶导数$f''(x)$总是非负的（即函数始终为绝对的非负值(positive non-negative)）。\n\n>5 与对称矩阵$X\\in S^n$类似，$X\\preceq 0$代表$X$是负定矩阵。对于向量不等式来说，有时可以用符号‘$\\le$’和‘$\\ge$’代替符号‘$\\preceq$’和‘$\\succeq$’。尽管这里的符号类似向量的符号，但是意义非常不一样。特别的，矩阵$X\\succeq 0$并不意味着对于所有矩阵的元素下标$i,j$都有元素$X_{ij}\\ge 0$\n\n同样，与凸性的定义以及一阶条件类似，如果海森矩阵是正定的，则函数$f$是严格凸函数，如果是半负定则函数是凹函数，是负定则函数是严格凹函数。\n\n##### 3.3 Jensen不等式\n\n假设我们从凸函数的基本定义中的不等式开始：\n\n$$\nf(\\theta x +(1-\\theta) y)\\le \\theta f(x)+(1-\\theta)f(y)\\quad 其中\\quad 0\\le\\theta\\le 1\n$$\n\n使用归纳法，这可以相当容易地扩展到多个点的凸组合：\n\n$$\nf(\\sum_{i=1}^k\\theta_ix_i)\\le\\sum_{i=1}^k\\theta_if(x_i)\\quad 其中\\quad\\sum_{i=1}^k\\theta_i=1,\\theta_i\\ge 0\\quad\\forall i\n$$\n\n事实上，这也可以推广到无穷和或积分。在后一种情况下，不等式可以写成：\n\n$$\nf(\\int p(x)xdx)\\le\\int p(x)f(x)dx\\quad 其中\\quad\\int p(x)dx=1,p(x)\\ge0\\quad\\forall x\n$$\n\n由于$p(x)$积分为$1$，通常把它看作概率密度，在这种情况下，前面的方程可以用期望来表示：\n\n$$\nf(E[x])\\le E[f(x)]\n$$\n\n最后一个不等式叫做Jensen不等式，后面的课上会讲到。$^6$\n\n>6 事实上，这四个方程有时都被称为Jensen不等式，因为它们都是等价的。但是，对于这门课，我们将使用该术语来具体指这里给出的最后一个不等式。\n\n##### 3.4 水平集\n\n凸函数产生一种特别重要的凸集称为 **$\\alpha-sublevel$集。** 给出了凸函数$f:R^n\\rightarrow R$和一个实数$\\alpha\\in R$，$\\alpha-sublevel$集被定义为：\n\n$$\n\\{x\\in\\mathcal{D}(f):f(x)\\le\\alpha\\}\n$$\n\n换句话说，$\\alpha-sublevel$集是所有满足$f(x)\\le\\alpha$的点$x$的集合。\n\n为了证明这是一个凸集，考虑任意$x,y\\in\\mathcal{D}(f)$，并且$f(x)\\le\\alpha,f(y)\\le\\alpha$，则：\n\n$$\nf(\\theta x +(1-\\theta) y)\\le \\theta f(x)+(1-\\theta)f(y)\\le\\theta\\alpha+(1-\\theta)\\alpha=\\alpha\n$$\n\n##### 3.5 凸函数判断的实例\n\n我们从几个单变量凸函数的简单例子开始，然后继续讨论多元函数。\n\n- **指数函数**是凸函数。有函数$f:R\\rightarrow R$，任意$a\\in R$使得$f(x)=e^{ax}$。为了证明$f$是凸函数，我们可以简单的考虑二阶导数$f''(x)=a^2e^{ax}$，对于所有$x$都是正的。\n\n- **负对数函数**是凸函数。函数$f:R\\rightarrow R,f(x)=-logx$，有定义域$\\mathcal{D}(f)=R_{++}$（这里的$R_{++}$代表严格正实数的集合$\\{x:x>0\\}$）。则对于所有的$x$都满足$f''(x)=1/x^2>0$。\n\n- **仿射函数**。函数$f:R\\rightarrow R,f(x)=b^Tx+c$，满足$b\\in R^n,c\\in R$。在这种情况下对于所有的$x$，该函数的海森矩阵$\\nabla^2_xf(x)=0$。应为零矩阵即是半正定也是半负定矩阵，因此函数$f$即是凸函数，也是凹函数。事实上，这种形式的仿射函数是唯一既凸又凹的函数。\n\n- **二次函数**。函数$f:R\\rightarrow R,f(x)=\\frac12x^TAx+b^Tx+c$，系数的对称矩阵为$A\\in S^n,b\\in R^n$以及$c\\in R$。在先前线性代数笔记中，我们展示了这个函数的海森函数为：\n\n$$\n\\nabla^2_xf(x)=A\n$$\n\n因此，函数$f$的凸性或非凸性完全取决于A是否为半正定矩阵：如果$A$为半正定，则函数为凸函数（严格凸、凹、严格凹函数同样类比）；如果$A$是不定的矩阵，那么$f$既不是凸函数也不是凹函数。\n\n注意，平方欧几里得范数$f(x) = \\parallel x\\parallel^2_2=x^Tx$是二次函数的一个特例，其中$a = I, b = 0, c = 0$，因此它是一个严格凸函数。\n\n- **范数函数**是凸函数。函数$f:R\\rightarrow R$为$R^n$上的某个范数。然后根据三角不等式和正范数的同质性，对于$x,y\\in R^n,0\\le\\theta\\le 1$，有：\n\n$$\nf(\\theta x +(1-\\theta) y)\\le f(\\theta x)+f((1-\\theta)y) =\\theta f(x)+(1-\\theta)f(y)\n$$\n\n这是一个凸函数的例子，由于范数不是处处可微的（例如，$1$范数，$\\parallel x\\parallel_1 = \\sum_{i=1}^n|x_i|$，在任意$x_i = 0$的点上都是不可微的），因此无法根据二阶或一阶条件证明凸性。\n\n- **凸函数的非负加权和**是凸函数。令$f_1,f_2,\\dots,f_k$是凸函数，并且有$w_1,w_2,\\dots,w_k$都是非负实数，则：\n\n$$\nf(x) = \\sum_{i=1}^kw_if_i(x)\n$$\n\n是一个凸函数，因为：\n\n$$\n\\begin{aligned}\nf(\\theta x +(1-\\theta) y)&=\\sum_{i=1}^kw_if_i(\\theta x +(1-\\theta) y) \\\\\n&\\le\\sum_{i=1}^kw_i(\\theta f_i(x))+(1-\\theta)f_i(y)) \\\\\n&=\\theta\\sum_{i=1}^kw_if_i(x)+(1-\\theta)\\sum_{i=1}^kw_if_i(y) \\\\\n&=\\theta f(x) + (1-\\theta)f(x)\n\\end{aligned}\n$$\n\n#### 4 凸优化问题\n\n利用凸函数和集合的定义，我们现在可以考虑**凸优化问题。** 正式的定义为：一个凸优化问题在一个最优化问题中的形式如下：\n\n$$\nminimize\\quad f(x) \\\\\nsubject\\quad to\\quad x\\in C\n$$\n\n其中$f$为凸函数，$C$为凸集，$x$为优化变量。然而，由于这样写可能有点不清楚，我们通常把它写成\n\n$$\n\\begin{aligned}\nminimize\\quad &f(x) \\\\\nsubject\\quad to\\quad &g_i(x)\\le 0,\\quad i=1,\\cdots,m \\\\\n&h_i(x)=0,\\quad i=1,\\cdots,p\n\\end{aligned}\n$$\n\n其中$f$为凸函数，$g_i$为凸函数，$h_i$为仿射函数，$x$为优化变量。\n\n注意这些不等式的方向很重要：凸函数$g_i$必须小于零。这是因为$g_i$的$0-sublevel$集是一个凸集，所以<a target='_blank' href='https://baike.baidu.com/item/%E5%8F%AF%E8%A1%8C%E5%9F%9F/4930167'>可行域</a>，是许多凸集的交集，其也是凸集（回忆前面讲过的仿射子空间也是凸集）。如果我们要求某些凸函数$g_i$的不等式为$g_i\\ge 0$，那么可行域将不再是一个凸集，我们用来求解这些问题的算法也不再保证能找到全局最优解。还要注意，只有仿射函数才允许为等式约束。直觉上来说，你可以认为一个等式约束$h_i= 0$等价于两个不等式约束$h_i\\le 0$和$h_i\\ge 0$。然而，当且仅当$h_i$同时为凸函数和凹函数时，这两个约束条件才都是有效的，因此$h_i$一定是仿射函数。\n\n优化问题的**最优值**表示成$p^*$（有时表示为$f^*$），并且其等于目标函数在可行域$^7$内的最小可能值。\n\n>7 数学专业的学生可能会注意到，下面出现的最小值更应该用符号$inf$。这里我们不需要担心这些技术问题，为了简单起见，我们使用符号$min$。\n\n$$\np^* = min\\{f(x):g_i(x)\\le 0,i=1,\\dots,m,h_i(x)=0,i=1,\\dots,p\\}\n$$\n\n当问题是不可行（即可行域是空的）时或无下界（即存在这样的可行点使得$f(x)\\rightarrow -\\infin$）时，我们允许$p^*$取值为$+\\infin$和$-\\infin$。当$f(x^*)=p^*$时，我们称$x^*$是一个**最优点(optimal point)。** 注意，即使最优值是有限的，也可以有多个最优点。\n\n##### 4.1 凸问题的全局最优性\n\n在说明凸问题中的全局最优性结果之前，让我们正式定义局部最优和全局最优的概念。直观地说，如果一个函数目标值附近没有令该函数值较低的可行点，则该可行点 **（译者注：即该函数目标值的原像）** 被称为**局部最优。** 类似地，如果一个函数的全局都没有比目标值更低的可行点，则该可行点称为**全局最优。** 为了更形式化一点，我们给出了以下两个定义。\n\n**定义$4.1$** 如果在可行域（即，满足优化问题的约束条件）内存在某些$R > 0$的数，使得所有可行点$z$，当满足$\\parallel x-z\\parallel_2\\le R$时，均可以得到$f(x)\\le f(z)$，则我们称点$x$是局部最优的。\n\n**定义$4.2$** 如果在可行域所有可行点$z$，都满足$f(x)\\le f(z)$，则我们称点$x$是全局最优的。\n\n现在我们来讨论凸优化问题的关键元素，它们的大部作用都来自于此。其核心思想是**对于一个凸优化问题，所有局部最优点都是全局最优的。**\n\n让我们用反证法来快速证明这个性质。假设$x$是局部最优点而不是全局最优点，即，存在这样一个可行点$y$使得$f(x)>f(y)$。根据局部最优性的定义，不存在$\\parallel x-z\\parallel_2\\le R$和$f(z) < f(x)$的可行点$z$。现在假设我们选择这个点\n\n$$\nz = \\theta y +(1-\\theta)x\\quad有\\quad\\theta=\\frac{R}{2\\parallel x-y\\parallel_2}\n$$\n\n则：\n\n$$\n\\begin{aligned}\n\\parallel x-z\\parallel_2 &= \\parallel x-(\\frac{R}{2\\parallel x-y\\parallel_2}y+(1-\\frac{R}{2\\parallel x-y\\parallel_2})x)\\parallel_2 \\\\\n&= \\parallel \\frac{R}{2\\parallel x-y\\parallel_2}(x-y)\\parallel_2 \\\\\n&= \\frac R2\\le R\n\\end{aligned}\n$$\n\n另外，通过$f$的凸性，我们可得：\n\n$$\nf(z)=f(\\theta y +(1-\\theta)x)\\le\\theta f(y)+(1-\\theta)f(x)< f(x)\n$$\n\n此外，由于可行域的集合是凸集，同时$x$和$y$都是可行的，因此$z =\\theta y +(1-\\theta)$也会是可行的。因此，$z$是一个可行点，满足$\\parallel x-z\\parallel_2\\le R$以及$f(z) < f(x)$。这与我们的假设相矛盾，表明$x$不可能是局部最优的。\n\n##### 4.2 凸问题的特殊情况\n\n由于各种原因，通常考虑一般凸规划公式的特殊情况比较方便。对于这些特殊情况，我们通常可以设计出非常高效的算法来解决非常大的问题，正因为如此，当人们使用凸优化技术时，你可能会看到这些特殊情况。\n\n- **线性规划。** 如果目标函数$f$和不等式约束$g_i$都是仿射函数，那么凸优化问题就是一个**线性规划(linear program,LP)** 问题。换句话说，这些问题都有如下形式：\n\n$$\n\\begin{aligned}\nminimize\\quad &c^Tx+d \\\\\nsubject\\quad to\\quad &Gx\\preceq h \\\\\n&Ax=b\n\\end{aligned}\n$$\n\n其中，$x\\in R^n$是优化变量，$c\\in R^n,d\\in R,G\\in R^{m\\times n},h\\in R^m,A\\in R^{p\\times n},b\\in R^p$这些变量根据具体问题具体定义，符号‘$\\preceq$’代表（多维向量中）各个元素不相等。\n\n- **二次规划。** 如果不等式约束（跟线性规划）一样是仿射的，而目标函数$f$是凸二次函数，则凸优化问题是一个**二次规划(quadratic program,QP)** 问题。换句话说，这些问题都有如下形式：\n\n$$\n\\begin{aligned}\nminimize\\quad &\\frac 12x^TPx+c^Tx+d \\\\\nsubject\\quad to\\quad &Gx\\preceq h \\\\\n&Ax=b\n\\end{aligned}\n$$\n\n其中，$x\\in R^n$是优化变量，$c\\in R^n,d\\in R,G\\in R^{m\\times n},h\\in R^m,A\\in R^{p\\times n},b\\in R^p$这些变量根据具体问题具体定义，但是这里我们还有一个对称半正定矩阵$P\\in R^n_+$\n\n- **二次约束二次规划。** 如果目标函数$f$和不等式约束条件$g_i$都是凸二次函数，那么凸优化问题就是一个**二次约束的二次规划(quadratically constrained quadratic program,QCQP)**问题，形式如下：\n\n$$\n\\begin{aligned}\nminimize\\quad &\\frac 12x^TPx+c^Tx+d \\\\\nsubject\\quad to\\quad &\\frac 12x^TQ_ix+r_i^Tx+s_i\\le 0,\\quad i=1,\\dots,m \\\\\n&Ax=b\n\\end{aligned}\n$$\n\n跟二次规划一样，其中的$x\\in R^n$是优化变量，并且有$c\\in R^n,d\\in R,A\\in R^{p\\times n},b\\in R^p,P\\in R^n_+$，与之不同的是这里还有$Q_i\\in S^n_+,r_i\\in R^n,s_i\\in R$，其中$i=1,...,m$。\n\n- **半定规划。** 最后一个示例比前面的示例更复杂，所以如果一开始不太理解也不要担心。但是，半定规划在机器学习许多领域的研究中正变得越来越流行，所以你可能在以后的某个时候会遇到这些问题，所以提前了解半定规划的内容还是比较好的。我们说一个凸优化问题是**半定规划(SDP)** 的，则其形式如下所示：\n\n$$\n\\begin{aligned}\nminimize\\quad &tr(CX) \\\\\nsubject\\quad to\\quad &tr(A_iX)=b_i,\\quad i=1,\\cdots,p \\\\\n&X\\succeq 0\n\\end{aligned}\n$$\n\n其中对称矩阵$X\\in S^n$是优化变量，对称矩阵$C,A_1,\\cdots,A_p\\in S^n$根据具体问题具体定义，限制条件$X\\succeq 0$意味着$X$是一个半正定矩阵。以上这些看起来和我们之前看到的问题有点不同，因为优化变量现在是一个矩阵而不是向量。如果你好奇为什么这样的公式可能有用，你应该看看更高级的课程或关于凸优化的书。\n\n从定义可以明显看出，二次规划比线性规划更具有一般性（因为线性规划只是$P = 0$时的二次规划的特殊情况），同样，二次约束二次规划比二次规划更具有一般性。然而，不明显的是，半定规划实际上比以前的所有类型都更一般，也就是说，任何二次约束二次规划（以及任何二次规划或线性规划）都可以表示为半定规划。在本文当中，我们不会进一步讨论这种关系，但是这个结论可能会让你对半定规划为何有用有一个小小的概念。\n\n##### 4.3 实例\n\n到目前为止，我们已经讨论了凸优化背后大量枯燥的数学以及形式化的定义。接下来，我们终于可以进入有趣的部分：使用这些技术来解决实际问题。我们在课堂上已经遇到过一些这样的优化问题，而且几乎在每个领域，都有很多情况需要人们应用凸优化来解决一些问题。\n\n- **支持向量机(SVM)。** 支持向量机分类器是凸优化方法在机器学习中最常见的应用之一。如课堂上所讨论的，寻找支持向量分类器（在松弛变量的情况下）可以表示为如下所示的优化问题：\n\n$$\n\\begin{aligned}\nminimize \\quad & \\frac 12 \\parallel w\\parallel_2 ^2+C\\sum^m_{i=1}\\xi_i \\\\\nsubject\\quad to \\quad& y^{(i)}(w^Tx^{(i)}+b) \\geq1-\\xi_i,\\quad &i=1,...,m\\\\\n& \\xi_i \\geq 0, &i=1,...,m\n\\end{aligned}\n$$\n\n其中$w\\in R^n,\\xi\\in R^m,b\\in R$是优化变量，$C\\in R,x^{(i)},y^{(i)},i=1,\\cdots,m$根据具体问题具体定义。这是一个二次规划的例子，我们下面通过将问题转换成上一节中描述的形式来展示它。特别的，当我们定义$k=m+n+1$时，则优化变量为：\n\n$$\nx\\in R^k=\\left[\n\\begin{matrix}\nw \\\\\n\\xi \\\\\nb\n\\end{matrix}\n\\right]\n$$\n\n然后定义矩阵：\n\n$$\nP\\in R^{k\\times k}=\\left[\n\\begin{matrix}\nI&0&0 \\\\\n0&0&0 \\\\\n0&0&0\n\\end{matrix}\n\\right],\\quad c\\in R^k=\\left[\n\\begin{matrix}\n0 \\\\\nC\\cdot 1 \\\\\n0\n\\end{matrix}\n\\right], \\\\\nG\\in R^{2m\\times k}=\\left[\n\\begin{matrix}\n-diag(y)X&-I&-y \\\\\n0&-I&0\n\\end{matrix}\n\\right],\\quad h\\in R^{2m}=\\left[\n\\begin{matrix}\n-1 \\\\\n0\n\\end{matrix}\n\\right]\n$$\n\n其中$I$是单位矩阵，$1$是所有元素都是$1$的向量，$X$和$y$跟课程中定义的一样：\n\n$$\nX\\in R^{m\\times n}=\\left[\n\\begin{matrix}\nx^{(1)T} \\\\\nx^{(2)T} \\\\\n\\vdots \\\\\nx^{(m)T}\n\\end{matrix}\n\\right],\\quad y\\in R^m=\\left[\n\\begin{matrix}\ny^{(1)} \\\\\ny^{(2)} \\\\\n\\vdots \\\\\ny^{(m)}\n\\end{matrix}\n\\right]\n$$\n\n你有理由相信在使用上述定义的矩阵时，上一节描述的二次规划与SVM优化问题是等价的。事实上，这里很容易看到支持向量机优化问题的二次优化目标项以及线性约束项，所以我们通常不需要把它化为标准形式“证明”它是一个二次规划(QP)问题，只有在遇到的现成解决方案中要求输入必须为标准形式时，我们才会这样做。\n\n- **约束最小二乘法。** 在课堂上，我们也遇到了最小二乘问题，在这个问题中，我们想要在已知某些矩阵$A\\in R^{m\\times n}$以及$b\\in R^m$时最小化$\\parallel Ax=b\\parallel_2^2$。正如我们所看到的，这个特殊的问题可以通过正规方程得到解析解。但是，假设我们还希望将解决方案中的$x$限制在一些预定义的范围内。换句话说，假设我们要解最优化如下的问题：\n\n$$\n\\begin{aligned}\nminimize\\quad&\\frac 12\\parallel Ax-b\\parallel_2^2 \\\\\nsubject\\quad to\\quad &l\\preceq x\\preceq\\mu\n\\end{aligned}\n$$\n\n$x$是优化变量，$A\\in R^{m\\times n},b\\in R^m,l\\in R^n$根据具体问题具体定义。这看起来像是一个简单的附加约束，但事实证明，这个问题将不再存在一个解析解。但是，你应该相信这个优化问题是一个二次规划问题，它的矩阵由如下式子定义：\n\n$$\nP\\in R^{n\\times n}=\\frac 12A^TA,\\quad c\\in R^n=-b^TA,\\quad d\\in R=\\frac 12b^Tb, \\\\\nG\\in R^{2n\\times 2n}=\\left[\n\\begin{matrix}\n-I&0 \\\\\n0&I \n\\end{matrix}\n\\right],\\quad h\\in R^{2n}=\\left[\n\\begin{matrix}\n-l \\\\\nu \n\\end{matrix}\n\\right]\n$$\n\n- **最大似然逻辑回归。** 作业一要求你需要证明逻辑回归模型中数据的对数似然函数是凹的。逻辑回归的对数似然函数如下：\n\n$$\nl(\\theta)=\\sum_{i=1}^n \\{y^{(i)}lng(\\theta^Tx^{(i)})+(1-y^{(i)})ln(1-g(\\theta^Tx^{(i)}))\\}\n$$\n\n其中$g(z)$表示逻辑回归函数$g(z) = 1/(1 + e^{-z})$，求出最大似然估计是使对数似然最大化的任务（或者等价的最小化负对数似然函数，其是一个凸函数），即：\n\n$$\nminimize\\quad -l(\\theta)\n$$\n\n优化变量为$\\theta\\in R^n$，并且没有约束。\n\n与前两个示例不同，将这个问题转化为标准形式优化问题并不容易。尽管如此，你们在作业中已经看到这是一个凹函数，这意味着你们可以非常有效地使用一些算法，如：牛顿法来找到全局最优解。\n\n##### 4.4 实现:使用CVX实现线性SVM\n\n利用CVX、Sedumi、CPLEX、MOSEK等现有软件包可以解决许多凸优化问题。因此，在许多情况下，一旦你确定了凸优化问题，就不必担心如何实现算法来解决它，而这一点这对于快速原型开发特别有用。$^8$\n\n>8 然而，根据优化问题的不同，这些现成的凸优化求解器会比最佳实现慢得多；因此，有时你可能不得不使用更定制的解决方案或实现自己的解决方案。\n\n在这些软件包中，我们以CVX[2]`注：参考资料[2]见文章最下方`为例。CVX是一种自由的基于matlab的求解一般凸优化问题的软件包;它可以解决多种凸优化问题，如LP、QP、QCQP、SDP等。作为一个例子，我们通过使用习题集1中的数据为二分类问题实现一个线性SVM分类器来结束本节。对于使用其他非线性内核的一般设置，也可以使用CVX求解对偶公式。\n\n```\n% load data\nload q1x.dat\nload q1y.dat\n% define variables\nX = q1x;\ny = 2*(q1y-0.5);\nC = 1;\nm = size(q1x,1);\nn = size(q1x,2);\n% train svm using cvx\ncvx_begin\nvariables w(n) b xi(m)\nminimize 1/2*sum(w.*w) + C*sum(xi)\ny.*(X*w + b) >= 1 - xi;\nxi >= 0;\ncvx_end\n% visualize\nxp = linspace(min(X(:,1)), max(X(:,1)), 100);\nyp = - (w(1)*xp + b)/w(2);\nyp1 = - (w(1)*xp + b - 1)/w(2); % margin boundary for support vectors for y=1\nyp0 = - (w(1)*xp + b + 1)/w(2); % margin boundary for support vectors for y=0\nidx0 = find(q1y==0);\nidx1 = find(q1y==1);\nplot(q1x(idx0, 1), q1x(idx0, 2), ’rx’); hold on\n```\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229notecof4.png)\n\n```\nplot(q1x(idx1, 1), q1x(idx1, 2), ’go’);\nplot(xp, yp, ’-b’, xp, yp1, ’--g’, xp, yp0, ’--r’);\nhold off\ntitle(sprintf(’decision boundary for a linear SVM classifier with C=%g’, C));\n```\n\n##### 参考资料\n\n<blockquote id='[1]'>[1] Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge UP, 2004. Online: <a target='_blank' href='http://www.stanford.edu/~boyd/cvxbook/'>http://www.stanford.edu/~boyd/cvxbook/</a></blockquote>\n\n<blockquote id='[2]'>[2] M. Grant and S. Boyd. CVX: Matlab software for disciplined convex programming (web page and software). <a target='_blank' href='http://cvxr.com/'>http://cvxr.com/</a>, September 2008.</blockquote>\n\n### 参考","tags":["CS229"],"categories":["CS229"]},{"title":"[机器学习-CS229]集成学习","url":"%2Fposts%2Fbd98a08b%2F","content":"# CS229 课程讲义中文翻译\nCS229 Lecture notes\n\n|原作者|翻译|校对|\n|---|---|---|\n| Raphael John Lamarre Townshend|[CycleUser](https://www.zhihu.com/people/cycleuser/columns)|[XiaoDong_Wang](https://github.com/Dongzhixiao) |\n\n|相关链接|\n|---|\n|[Github 地址](https://github.com/Kivy-CN/Stanford-CS-229-CN)|\n|[知乎专栏](https://zhuanlan.zhihu.com/MachineLearn)|\n|[斯坦福大学 CS229 课程网站](http://cs229.stanford.edu/)|\n|[网易公开课中文字幕视频](http://open.163.com/movie/2008/1/M/C/M6SGF6VB4_M6SGHFBMC.html)|\n\n# 集成学习(Ensembling Methods)\n\n现在要讲的方法可以来整合训练模型的输出。这里要用到偏差-方差(Bias-Variance)分析，以及决策树的样本来探讨一下每一种方法所做的妥协权衡。\n\n要理解为什么从继承方法推导收益函数(benefit)，首先会议一些基本的概率论内容。加入我们有n个独立同分布(independent, identically distributed,缩写为i.i.d.) 的随机变量$X_i$，其中的$0\\le i<n$。假如对所有的$X_i$有$Var(X_i)=\\sigma^2$。然后就可以得到均值(mean)的方差(variance)是:\n\n$$\nVar(\\bar x)=Var(\\frac{1}{n}\\sum_iX_i)=\\frac{\\sigma^2}{n}\n$$\n\n现在就去掉了独立性假设(所以变量就只是同分布的，即i.d.)，然后我们说变量$X_i$是通过一个因子(factor)$\\rho$而相关联的，可以得到:\n\n$$\n\\begin{aligned}\n   Var(\\bar X) &= Var (\\frac{1}{n}\\sum_iX_i) & \\quad\\text{(1)}\\\\\n    &= \\frac{1}{n^2}\\sum_{ij}Cov(X_i,X_j)  & \\quad\\text{(2)}\\\\\n    &= \\frac{n\\sigma^2}{n^2} +\\frac{n(n-1)\\rho\\sigma^2}{n^2}  & \\quad\\text{(3)}\\\\\n    &=  \\rho\\sigma^2+ \\frac{1-\\rho}{n}\\sigma^2&  \\quad\\text{(4)}\\\\\n\\end{aligned}\n$$\n\n在第3步中用到了皮尔逊相关系数(pearson correlation coefficient)的定义$P_{X,Y}=\\frac{Cov(X,Y)}{\\sigma_x\\sigma_y}$，而其中的$Cov(X,X)=Var(X)$。\n\n现在如果我们设想每个随机变量都是一个给定模型的误差，就可以看到用到的模型的数目在增长(导致第二项消失)，另外模型间的相关性在降低(使得第一项消失并且得到一个独立同分布的定义)，最终导致了集成方法误差的方差的总体降低。\n\n有好几种方法能够生成去除先关的模型(de-correlated models)，包括:\n\n* 使用不同算法\n* 使用不同训练集\n* 自助聚合(Bagging)\n* Boosting\n\n前两个方法很直接，就是需要大规模的额外工作。接下来讲一下后两种技术，boosting和bagging，以及在决策树情境下这两者的使用。\n\n## 1 袋装(Bagging)\n\n### 1.1 自助(Bootstrap)\n\nBagging这个词的意思是\"Boostrap Aggregation\"的缩写(可以直接翻译成 自助聚合)，是一个方差降低(variance reduction)的集成学习方法(ensembling method)。Bootstrap这个方法是传统统计中用于测量某个估计器(比如均值mean)的不确定性的。\n\n加入我们有一个真实的群体$P$，想要对其计算一个估计器，然后又一个从$P$中取样得到的训练集$S$，$S\\sim P$。虽然可以通过对$S$计算估计器(estimator)来找到一个近似，但还是不知道对应真实值的误差是多少。这需要我们多次从$P$中进行独立采样得到多个训练集$S_1,S_2,...$。\n\n可是如果我们假设$S=P$，就可以生成一个Bootstrap集合$Z$，从$S$中进行有放回的采样($Z\\sim S,|Z|=|S|$)。实际上我们看医生称很多这样的样本$Z_1,Z_2,...,Z_M$。然后就可以看一下各个Bootstrap集合上估计的方差(variability)来得到对误差(error)的衡量。\n\n### 1.2 聚合 (Aggregation)\n\n现在回到集成学习的方法上，我们取得每个$Z_m$，然后对每个都训练一个机器学习模型$G_m$，然后定义一个聚合预测器(aggregate predictor):\n\n$$\nG(X)=\\sum_m\\frac{G_m(x)}{M}\n$$\n\n这样上面的过程就叫做袋装(Bagging)。回到等式$(4)$，就有了$M$个相关联的预测器的方差是:\n\n$$\nVar(\\bar X)=\\rho\\sigma^2+\\frac{1-\\rho}{M}\\sigma^2\n$$\n\n通过袋装可以得到比简单在$S$上训练更低相关的预测器(predictors)。虽然每个独立的预测器的偏差(bias)都增长了，由于每个Bootstrap集合都没有全部的训练样本可用，但实践中方差的降低带来的优势远超过偏差增加的劣势。另外要注意增加预测器个数$M$并不会引起额外的过拟合，这是由于$\\rho$对$M$不敏感，因此总体上方差依然会降低。\n\n袋装的另一个优势就是包外估计(out-of-bag estimation)。可以证明每个Bootstrap的样本都只包含大概$\\frac{2}{3}$的$S$，因此可以用额外的$\\frac{1}{3}$来对误差(error)进行估计，叫做包外误差(out-of-bag error)。在极端情况下，$M\\rightarrow \\infty$，包外误差就会等价于留一法交叉验证(leave-one-out cross-validation)。\n\n### 1.3 袋装决策树 (Bagging+Decision Trees)\n\n回想一下完全生长的决策树是高方差低偏差的模型，因此Bagging的降低方差的效果很适合与上述方法相结合。袋装还允许处理缺失特征:如果一个特征丢失了，排除在树中使用该特征的树。如果特定特征是很重要的预测器，它们虽然不会被包含在全部树内，也还是会在大部分树内。\n\n袋装决策树的一个缺点是失去了单独决策树中继承来的可解释性。有一种叫做变量重要性衡量(variable importance measure)的方法可以来对洞察力(insight)进行一定的恢复。对每个特征，在集成学习中查找每一个用到这个特征的分割，然后在所有分割上将损失函数的降低平均分摊。要注意这和衡量确实这个特征导致的性能下降多少不同，因为其他特征可能是相关的，也可以替补。\n\n袋装决策树的最后一个重要的内容就是一种叫做随机森林(random forests)的方法。如果我们的训练集有一个很强的预测器，然后袋装树就会总是用这个特征来切分并且最终相关。使用随机森林，就可以只允许特征的一个自己来用在每一个切分上。这样就可以得到一个对相关性(correlation)$\\rho$的下降(decrease)，最终能导致方差的降低。在此要强调的是这也会导致偏差(bias)的增高因为特征空间的约束，但就和常规的袋装决策树一样这个通常并不会带来什么问题。最后，即便是强大的预测器也不会在每一个树中都出现(假设树有足够的数目以及每个切分都对特征有充分的约束)，这就能对缺失的预测器有更好地控制了。\n\n### 1.4 本节概要\n\n总结来看，对于决策树来说，袋装发的主要优势是:\n\n* 降低了方差(随机森林更能显著)\n* 更好的精度\n* 自由验证集\n* 支持缺失值\n\n当然也有一些缺点，包括:\n\n* 偏差增大(随机森林里面这种增大更严重)\n* 难以解释\n* 依然缺乏加性\n* 更昂贵(估计是计算成本)\n\n## 2 推进法(Boosting)\n\n### 2.1 直观理解 (Intuition)\n\n上文所讲的袋装法(Bagging)是一种将地方插的技术，而这次要讲的推进发(Boosting)则是降低偏差的(bias-reduction)。因此我们想要高偏差低方差的模型，也就是弱学习模型(weak learners)。考虑到在介绍决策树的时候做的解释，可以将决策树在进行预测之前只允许进行一次决策，就能使之成为弱学习模型;这样就成了决策树桩(decision stumps)。\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229noteemf1.png)\n\n上面的例子可以用于直观理解一下推进法(Boosting)背后的思想。开始时是左边的数据及，然后进行单词决策树桩的训练，如中间图所示。关键思想是接下来要看那些样品被分类错了，然后增加他们和其他正确分类的样品相比的相对权重。然后在训练一个新的决策树桩，这个就会更倾向于将这些特殊值(hard negatives)进行正确的分类。然后继续这样，逐渐在每一步中对样品重新评估权重，最后输出的就是一系列这些弱学习模型(weak learners)的结合，就是一个集成分类器(classi\fer)了。\n\n### 2.2 Adaboost 算法\n\n简单介绍了直观理解之后，接下来看一种最流行的推进法(boosting)算法:Adaboost，过程如下所示:\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229noteemf2.png)\n\n每个样本的权重开始的时候都是偶数(begin out even)，误分类样本会在每一步被进一步更新权重，以累积的形式(in a cumulative fashion)。最终得到的累加分类器是对所有弱学习模型(weak learners)的总和(summation)，权重是加权误差(weighted error)的负对数概率(negative log-odds)。\n\n由于最终求和，还可以看到这个集成学习的方法允许对加性项目建模，整体上增强了最终模型的兼容性(以及方差)。每个新的弱学习模型都不再与之前序列中的模型独立，这就意味着增加$M$就会导致增加过拟合的风险。\n\nAdaboost中用到的确切权重似乎第一眼看上去很随意，但实际上可以被证明是经过良好调节的。接下来的章节我们将要通过一个更通用的模块来对此进行了解，Adaboost只是其中一个特例。\n\n### 2.3 正向累加建模(Forward Stagewise Additive Modeling)\n\n这个算法如下所示，也是一个集成学习方法:\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229noteemf3.png)\n\n闭合检查(close inspection)表明对目前的学习问题进行了很少的假设，主要的就都是集成学习算法的可加性本质以及在一个给定步骤后之前所有的权重和参数的固定。这次还是要用到弱分类器$G(x)$，虽然这时候我们会用参数$\\gamma$来进行参数化。在每一步我们都是要找到下一个若学习模型的参数和权重来使得当前的集成学习算法能够最适合剩余的误差。\n\n作为这个算法的集中实现，使用平方损失函数就等价于将单个分类器拟合到残差$y_i-f_{m-1}(x_i)$。另外还可以证明的是Adaboost是这个公式的一个特例，具体来说是在二分类的分类问题和指数损失函数的情况下的特例:\n\n$$\nL(y,\\hat y)=\\exp(-y\\hat y)\n$$\n\n关于Adaboost和正向累加建模的更多联系，感兴趣的读者可以去参考10.4 统计学习的要素(Elements of Statistical Learning)。\n\n### 2.4 梯度推进法 (Gradient Boosting)\n\n一般来说，在正向累加建模中出现的最小化问题写出一个闭合形式的解总是容易的。包括xgboost在内的高性能方法都可以将这个问题转换成数值优化问题。\n\n这样做的最显著办法就是对损失函数取积分然后运行梯度下降法。不过复杂的地方在于我们被限制在只能在模型类内进行上述步骤，只能增加参数化的弱学习模型$G(x,\\gamma)$,而不能在输入空间内进行任意移动。\n\n在梯度推进法中，我们改为计算每个样本点对应当前预测期的梯度(通常就是一个决策树桩):\n\n$$\ng_i=\\frac{\\partial L(y,f(x_i))}{\\partial f(x_i)}\n$$\n\n然后可以训练一个新的回归分类器来匹配这个梯度并且用来进行梯度步骤.在正向累加建模中，这就得到了:\n\n$$\n\\gamma_i =\\arg \\min_\\gamma \\sum^N_{i=1}(g_i-G(x_i;\\gamma))^2\n$$\n\n### 2.5 本节概要\n\n总结一下，推进法的主要优势有:\n\n* 降低偏差(bias)\n* 更好的精度\n* 可加性建模\n\n当然也有缺陷:\n\n* 方差增长了\n* 容易过拟合\n\n更多推进法背后的理论，推荐阅读John Duchi的补充材料。\n\n\n### 参考","tags":["CS229"],"categories":["CS229"]},{"title":"[机器学习-CS229]决策树","url":"%2Fposts%2Fd22611f2%2F","content":"# CS229 课程讲义中文翻译\nCS229 Lecture notes\n\n|原作者|翻译|校对|\n|---|---|---|\n| Raphael John Lamarre Townshend|[CycleUser](https://www.zhihu.com/people/cycleuser/columns)| [XiaoDong_Wang](https://github.com/Dongzhixiao) |\n\n\n|相关链接|\n|---|\n|[Github 地址](https://github.com/Kivy-CN/Stanford-CS-229-CN)|\n|[知乎专栏](https://zhuanlan.zhihu.com/MachineLearn)|\n|[斯坦福大学 CS229 课程网站](http://cs229.stanford.edu/)|\n|[网易公开课中文字幕视频](http://open.163.com/movie/2008/1/M/C/M6SGF6VB4_M6SGHFBMC.html)|\n\n# 决策树(Decision Trees)\n\n接下来就要讲决策树了，这是一类很简单但很灵活的算法。首先要考虑决策树所具有的非线性/基于区域(region-based)的本质，然后要定义和对比基于区域算则的损失函数，最后总结一下这类方法的具体优势和不足。讲完了这些基本内容之后，接下来再讲解通过决策树而实现的各种集成学习方法，这些技术很适合这些场景。\n\n## 1 非线性(Non-linearity)\n\n决策树是我们要讲到的第一种内在非线性的机器学习技术(inherently non-linear machine\nlearning techniques)，与之形成对比的就是支持向量机(SVMs)和通用线性模型(GLMs)这些方法。严格来说，如果一个方法满足下面的特性就称之为线性方法：对于一个输入$x\\in R^n$（截距项 intercept term $x_0=1$），只生成如下形式的假设函数(hypothesis functions)h:\n\n$$\nh(x)=\\theta^Tx\n$$\n\n其中的$\\theta\\in R^n$。不能简化成上面的形式的假设函数(hypothesis functions)就都是非线性的(non-linear)，如果一个方法产生的是非线性假设函数，那么这个方法也就是非线性的。之前我们已经看到过，对一个线性方法核化(kernelization)就是一种通过特征映射$\\phi(x)$来实现非线性假设函数的方法。\n\n决策树算法则可以直接产生非线性假设函数，不用去先生成合适的特征映射。接下来这个例子很振奋人心(加拿大风格的)，假设要构建一个分类器，给定一个时间和一个地点，要来预测附近能不能滑雪。为了简单起见，时间就用一年中的月份来表示，而地点就用纬度(latitude)来表示(北纬南纬，-90°表示南极，0°表示迟到，90°表示南极)。\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229notedtf1.png)\n\n有代表性的数据结构如上图左侧所示。不能划分一个简单的线性便捷来正确区分数据集。不过可以很明显可以发现数据集中的空间中有可以鼓励出来的不同区域，一种划分方式如上图中右侧所示。上面这个分区过程(partitioning)开通过对输入空间$x$分割成不相连接的自己(或者区域)$R_i$:\n\n$$\n\\begin{aligned}\nX&= \\bigcup ^n_{i=0} R_i\\\\\ns.t.\\qquad R_i \\bigcap R_j&= \\not{0} \\quad\\text{for}\\quad i \\ne j\n\\end{aligned}\n$$\n\n其中$n\\in Z^+$。\n\n## 2 区域选择(Selecting Regions)\n\n通常来说，选择最有区域是很难的(intractable)。决策树会生通过**贪心法，从头到尾，递归分区(greedy, top-down, recursive partitioning)** 成一种近似的解决方案。这个方法是**从头到尾(top-down)** 是因为是从原始输入空间$X$开始，先利用单一特征为阈值切分成两个子区域。然后对两个子区域选择一个再利用一个新阈值来进行分区。然后以递归模式来持续对模型的训练，总是选择一个叶节点(leaf node)，一个特征(feature)，以及一个阈值(threshold)来生成新的分割(split)。严格来说，给定一个父区域$R_p$，一个特征索引$j$以及一个阈值$t\\in R$，就可以得到两个子区域$R_1,R_2$，如下所示:\n\n$$\n\\begin{aligned}\nR_1 &= \\{ X|X_j<t,X\\in R_p\\}\\\\\nR_2 &= \\{ X|X_j \\ge t,X\\in R_p\\}\\\\\n\\end{aligned}\n$$\n\n下面就着滑雪数据集来应用上面这样的过程。在步骤a当中，将输入空间$\\mathcal{X}$根据地理位置特征切分，阈值设置的是$15$，然后得到了子区域$R_1,R_2$。在步骤b，选择一个子区域(例子中选的是$R_2$)来递归进行同样的操作，选择时间特征，阈值设为$3$，然后生成了二级子区域$R_{21},R_{22}$。在步骤c，对剩下的叶节点($R_1,R_{21},R_{22}$)任选一饿，然后继续上面的过程，知道遇到了一个给定的停止条件(stop criterion，这个稍后再介绍)，然后再预测每个节点上的主要类别。\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229notedtf2.png)\n\n## 3 定义损失函数(Defining a Loss Function)\n\n这时候很自然的一个问题就是怎么去选择分割。首先要定义损失函数$L$，这个函数是在区域$R$上的一个集合函数(set function)。对一个父区域切分成两个子区域$R_1,R_2$之后，可以计算福区域的损失函数$L(R_p)$，也可以计算子区域的基数加权(cardinality-weighted)损失函数$\\frac{|R_1|L(R_1)+|R_2|L(R_2)}{|R_1|+|R_2|}$。在贪心分区框架(greedy partitioning framework)下，我们想要选择能够最大化损失函数减少量(decrease)的叶区域/特征和阈值:\n\n$$\nL(R_p)-\\frac{|R_1|L(R_1)+|R_2|L(R_2)}{|R_1|+|R_2|}\n$$\n\n对一个分类问题，我们感兴趣的是**误分类损失函数(misclassification loss)** $L_{misclass}$。对于一个区域$R$，设$\\hat p_c$是归于类别$c$的$R$中样本的分区。在$R$上的误分类损失函数可以写为:\n\n$$\nL_{misclass}(R)=1-\\max_c(\\hat p_c)\n$$\n\n这个可以理解为在预测区域$R$上的主要类别中发生错误分类的样本个数。虽然误分类损失函数是我们关心的最终值，但这个指标的对类别概率的变化并不敏感。举个例子，如下图所示的二值化分类。我们明确描述了父区域$R_p$，也描述了每个区域上的正负值的个数。\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229notedtf3.png)\n\n第一个切分就是讲搜有的正值分割开来，但要注意到:\n\n$$\nL(R_p)=\\frac{|R_1|L(R_1)+|R_2|L(R_2)}{|R_1|+|R_2|}=\\frac{|R_1'|L(R_1')+|R_2'|L(R_2')}{|R_1'|+|R_2'|}=100\n$$\n\n这样不仅能使两个切分的损失函数相同，还能使得任意一个分割都不会降低在父区域上的损失函数。(Thus, not only can we not only are the losses of the two splits identical, but neither of the splits decrease the loss over that of the parent.)\n\n因此我们有兴趣去定义一个更敏感的损失函数。前面已经给出过一些损失函数了，这里要用到的是**交叉熵(cross-entropy)** 损失函数$L_{cross}$:\n\n$$\nL_{cross}(R)=-\\sum_c \\hat p_c \\log_2 \\hat p_c\n$$\n\n如果$\\hat p_c=0$，则$\\hat p_c \\log_2 \\hat p_c\\equiv 0$。从信息论角度来看，交叉熵要衡量的是给定已知分布的情况下要确定输出所需要的位数(number of bits)。更进一步说，从父区域到子区域的损失函数降低也就是信息增益(information gain)。\n\n要理解交叉熵损失函数比误分类损失函数相对更敏感，我们来看一下对同样一个二值化分类案例这两种损失函数的投图。从这些案例中，我们能够对损失函数进行简化，使之仅依赖一个区域$R_i$中正值分区的样本$\\hat p_i$:\n\n$$\n\\begin{aligned}\nL_{misclass}(R)= L_{misclass}(\\hat p)=1-\\max(\\hat p,1-\\hat p)\\\\\nL_{cross}(R)=L_{cross}(\\hat p)=- \\hat p\\log \\hat p -(1-\\hat p)\\log(1-\\hat p)\\\\\n\\end{aligned}\n$$\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229notedtf4.png)\n\n上图左侧中看到的是交叉熵损失函数在$\\hat p$上的投图。从前文中样本中的第一个分割得到的区域$(R_p,R_1,R_2)$然后也对其损失函数进行了投图。交叉熵损失函数是严格的凹函数(concave)，可以从投图中明显看出(证明起来也很容易)只要$\\hat p_1 \\ne \\hat p_2$以及两个子区域都是非空的，则子区域损失函数的加权和总会小于父区域。\n\n误分类损失函数，则不是严格凹函数，因此也不能保证子区域损失函数的加权和能够比父区域的小，如上图右侧所示，虽然有同样的分区方案。由于这一点额外的敏感性，交叉熵损失函数（或者与之密切相关的基尼损失函数(Gini loss)）用于用于分类的生长决策树(growing decision trees)。\n\n在谈论其他内容之前，我们先简单介绍一些决策树的回归设定。对每个数据点$x_i$，我们现在有了一个关联值(associated value)也就是我们要去预测的$y_i\\in R$。大部分树的生长过程都是相同的，区别只是在对于一个区域$R$的最终预测是所有值的平均：\n\n$$\n\\hat y =\\frac{\\sum_{i\\in R}y_i}{|R|}\n$$\n\n然后在这个例子中直接使用**平方损失函数(squared loss)** 来选择切分:\n\n$$\nL_{squared}(R)=\\frac{\\sum_{i\\in R}(y_i -\\hat y)^2}{|R|}\n$$\n\n## 4 其他考虑(Other Considerations)\n\n决策树的流行很大一部分是由于其好解释也好理解，另外就是高度的可解释性：我们可以看到生成的阈值集合来理解为什么模型做出了具体的预测。不过这还不仅仅是全部，接下来我们要讲到一些额外的优点。\n\n### 4.1 分类变量(Categorical Variables)\n\n决策树的一大优点就是很适合用于处理分类变量。例如在滑雪数据集里面的地点就可以替代城任意的分类数据(Northern Hemisphere,\nSouthern Hemisphere, 或者 Equator （也就是$loc in\\{N,S,E\\}$）。相比独热编码(one-hot encoding)或者类似的预处理步骤来讲数据转换到定量特征，这些方法对于之前见过的其他算法来说是必要的，但对于决策树算法来说，可以直接探测子集成员。本章第2节中的最终树形可以写成如下的形式:\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229notedtf5.png)\n\n需要注意的就是要留神避免变量有太多的分类。对于一个分类集合$S$，我们的潜在问题集合及就是幂集合(power set)$P(S)$，基数(cardinality)为$2^{|S|}$。因此分类类别太多了就可能是问题选择的计算变得很难。对于二值化分类来说优化倒是可能的，虽然即便这时候也应该考虑将特征重新格式化成定量的而不是使用大规模的潜在阈值然后任它们严重过拟合。\n\n### 4.2 规范化(Regularization)\n\n在第2节中我们顺便提到了各种停止条件(stopping criteria)来决定是否终止一个树的生长。最简单的判断条件就是让树\"完全\"生长:一直继续分类知道叶区域包含的只有单个的训练样本点。这个方法会导致高方差和低偏差的模型，因此我们改用其他的各种启发式的方法来规范化。下面是一些常用方法:\n\n- **最小叶规模(Minimum Leaf Size)** —— 直到基数(cardinality)落到一个固定的阈值内才停止切割$R$。\n- **最大深度(Maximum Depth)** —— 如果到达$R$已经经过了超过一个固定估值的阈值，就停止切割$R$。\n- **最大节点数(Maximum Number of Nodes)** —— 如果一个树到达了超过一个的叶节点阈值就停止。\n\n一种启发式方法是在切分之后使用损失函数的最小降低发。这是一个有问题的方法，因为对决策树的贪心的每次单特正的方法可能意味着损失掉高阶互动(higher order interactions)。如果我们需要对多个特征设置阈值来达到一个良好的分割，就可能没办法在初始分割的损失上达到一个好的降低(decrease)，因此就可能过早停止(prematurely terminate)。一种更好的方法是将树完全生长出来，然后修剪掉那些最小程度上减少错误分类或平方误差的节点，就跟在验证集上进行判断的方法一样。\n\n### 4.3 运行(Runtime)\n\n我们简短地来看一下决策树的运行。为了易于分析，考虑一个有$n$个样本，$f$个特征，决策树深度为$d$的二值化分类问题。在测试的时候，对一个数据点我们会贯穿(traverse)整个树直到到达一个叶节点(leaf node)然后输出预测，运算时间是$O(d)$。要注意这里的书是平衡的$d=O(\\log n)$，因此测试时间的运算还挺快的。\n\n在训练的时候，我们要注意每个数据节点都只会在最多$O(d)$节点中出现。通过排序和对中间值的智能获取，就可以在每个节点对每个数据点的每一个特征构建一个分摊时间为$O(1)$的运算。因此总的运行时间就是$O(nfd)$-鉴于数据矩阵规模是$nf$，这是一个比较快的运行速度了。\n\n### 4.4 加性结构缺失(Lack of Additive Structure)\n\n决策树的一个重要缺陷就是不能轻易捕获加性结构。例如如下图作图所示，简单的分裂便捷形式$x_1+x_2$只能用于对很多分类模型进行近似，每个分类都可能每次产生一个 $x_1$ 或者 $x_2$ 。而下图右侧的线性模型则直接推导出了这个边界。\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229notedtf6.png)\n\n要让决策边界同时从多个特征中寻找因子，还需要很多的工作，随着未来变量的增加，可解释性就会降低，这也是个缺陷。\n\n## 5 本章概要\n\n总结一下，本节关于决策树的有点是:\n\n* 好解释\n* 解释性好\n* 分类变量支持的好\n* 速度快\n\n缺陷则包括:\n* 方差大\n* 对加性模型建模很差\n\n很不幸，这些问题都使得单独使用决策树会在整体上导致较低的预测精度。解决这个问题的常见（也是可行）的办法就是通过集成学习(ensembling methods)——留在下一章再讲了。\n\n\n### 参考","tags":["CS229"],"categories":["CS229"]},{"title":"[机器学习-CS229]反向传播","url":"%2Fposts%2Fd60bdbaa%2F","content":"# CS229 课程讲义中文翻译\nCS229 Lecture notes\n\n|原作者|翻译|校对|\n|---|---|---|\n| [Andrew Ng  吴恩达](http://www.andrewng.org/),Kian Katanforoosh |[CycleUser](https://www.zhihu.com/people/cycleuser/columns)| [XiaoDong_Wang](https://github.com/Dongzhixiao) |\n\n\n|相关链接|\n|---|\n|[Github 地址](https://github.com/Kivy-CN/Stanford-CS-229-CN)|\n|[知乎专栏](https://zhuanlan.zhihu.com/MachineLearn)|\n|[斯坦福大学 CS229 课程网站](http://cs229.stanford.edu/)|\n|[网易公开课中文字幕视频](http://open.163.com/movie/2008/1/M/C/M6SGF6VB4_M6SGHFBMC.html)|\n\n\n# 关于反向传播(Backpropagation)的附加讲义\n\n## 1 正向传播(Forward propagation)\n\n回忆一下，给出一个输入特征$x$的时候，我们定义了$a^{[0]}=x$。然后对于层(layer)$l=1,2,3,\\dots,N$，其中的$N$是网络中的层数，则有：\n\n1. $z^{[l]}=W^{[l]}a^{[l-1]}+b^{[l]}$\n2. $a^{[l]}=g^{[l]}(z^{[l]})$\n\n在讲义中都是假设了非线性特征$g^{[l]}$对除了第$N$层以外的所有层都相同。这是因为我们可能要在输出层进行回归(regression) [因此就可能使用$g(x)=x$]，或者进行二值化分类(binary classification) [这时候用$g(x)=sigmoid(x)$]，或者是多类分类(multiclass classification) [这就要用$g(x)=softmax(x)$]。因此要将$g^{[N]}$和$g$相区分开，然后假设$g$使用在除了第$N$层外的其他所有层。\n\n最终，给网络$a^{[N]}$的输出，为了简单记作$\\hat y$，就可以衡量损失函数$J(W,b)=\\mathcal{L}(a^{[N]},y)=\\mathcal{L}(\\hat y,y)$。例如，对于实数值的回归可以使用下面的平方损失函数(squared loss)：\n\n$$\n\\mathcal{L}(\\hat y,y)=\\frac{1}{2} (\\hat y-y)^2\n$$\n\n对使用逻辑回归(logistic regression)的二值化分类(binary classification)，我们可以使用下面的损失函数：\n\n$$\n\\mathcal{L}(\\hat y,y) =-[y\\log \\hat y+(1-y)\\log(1-\\hat y)]\n$$\n\n或者也可以使用负对数似然函数(negative log-likelihood)。最后是对于有$k$类的柔性最大回归(softmax regression)，使用交叉熵损失函数(cross entropy loss)：\n\n$$\n\\mathcal{L}(\\hat y,y)=-\\sum^k_{j=1}1\\{y=j\\}\\log\\hat y_j\n$$\n\n上面这个其实就是将负对数似然函数简单扩展到多类情景而已。要注意这时候的$\\hat y$是一个$k$维度向量。如果使用$y$来表示这个$k$维度向量，其中除了第$l$个位置是$1$，其他地方都是$0$。也就是为真的分类标签(label)就是$l$，这时候也可以将交叉熵损失函数以如下方式表达：\n\n$$\n\\mathcal{L}(\\hat y,y)=- \\sum^k_{j=1}y_j\\log \\hat y_j\n$$\n\n## 2 反向传播(Backpropagation)\n\n然后咱们再定义更多的记号用于反向传播。$^1$首先定义:\n\n>1 这部分的讲义内容基于 [斯坦福大学的监督学习中关于多层神经网络算法的内容](http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/)\nScribe: Ziang Xie\n\n$$\n\\delta ^{[l]} =\\nabla _{z^{[l]}}\\mathcal{L}(\\hat y,y)\n$$\n\n然后可以按照后续内容所示来定义一个计算每个$W^{[l]},b^{[l]}$所对应的梯度的三个步骤：\n\n1. 对于输出层$N$有:\n   \n   $$\n   \\delta ^{[N]} =\\nabla _{z^{[N]}}\\mathcal{L}(\\hat y,y)\n   $$\n\n   有时候我们可能要直接计算$\\nabla _{z^{[N]}}L(\\hat y,y)$（比如$g^{[N]}$是柔性最大函数(softmax function)），而其他时候（比如$g^{[N]}$是S型函数(sigmoid function)$\\sigma$）则可以应用链式法则(chain rule)：\n\n   $$\n   \\nabla _{z^{[N]}}L(\\hat y,y)= \\nabla _{\\hat y}L(\\hat y,y) \\circ (g^{[N]})'(z^{[N]})\n   $$\n\n   注意$(g^{[N]})'(z^{[N]})$表示的是关于$z^{[N]}$的按元素的导数(elementwise derivative)。\n\n2. 对$l=N-1,N-1,\\dots,1$则有：\n   \n   $$\n   \\delta^{[l]} = ( {W^{[l+1]}}^T \\delta^{[l+1]} )  \\circ g' ( z^{[l]}  )\n   $$\n\n3. 最终就可以计算第$l$层的梯度(gradies)：\n   \n   $$\n   \\begin{aligned}\n    \\nabla_{W^{[l]}}J(W,b)&= \\delta^{[l]}{a^{[l-1]}  }^T  \\\\\n    \\nabla_{b^{[l]}}J(W,b)&= \\delta^{[l]}    \\\\\n   \\end{aligned}\n   $$\n\n上文中的小圆圈负号$\\circ$表示元素积(elementwise product)。要注意上面的过程是对应单个训练样本的。\n\n你可以将上面的算法用到逻辑回归(logistic regression)里面（$N=1,g^{[1]}$是S型函数(sigmoid function)$\\sigma$）来测试步骤$1$和$3$。还记得$\\sigma'(z)=\\sigma(z)\\circ (1-\\sigma(z))$以及$\\sigma(z^{[1]})$就是$a^{[1]}$。注意对于逻辑回归的情景，如果$x$是一个实数域内$R^{n\\times 1}$的列向量(column vector)，然后$W^{[1]}\\in R^{1\\times n}$,因此则有$\\nabla_{W^{[1]}}J(W,b)\\in R^{1\\times n}$。代码样本如[http://cs229.stanford.edu/notes/backprop.py](http://cs229.stanford.edu/notes/backprop.py)所示。\n\n（译者注：为了方便我直接把上面链接中的代码贴到下面了。）\n\n\n```python\n#http://cs229.stanford.edu/notes/backprop.py\nimport numpy as np\nfrom copy import copy\n\n# Example backpropagation code for binary classification with 2-layer\n# neural network (single hidden layer)\n\nsigmoid = lambda x: 1 / (1 + np.exp(-x))\n\ndef fprop(x, y, params):\n  # Follows procedure given in notes\n  W1, b1, W2, b2 = [params[key] for key in ('W1', 'b1', 'W2', 'b2')]\n  z1 = np.dot(W1, x) + b1\n  a1 = sigmoid(z1)\n  z2 = np.dot(W2, a1) + b2\n  a2 = sigmoid(z2)\n  loss = -(y * np.log(a2) + (1-y) * np.log(1-a2))\n  ret = {'x': x, 'y': y, 'z1': z1, 'a1': a1, 'z2': z2, 'a2': a2, 'loss': loss}\n  for key in params:\n    ret[key] = params[key]\n  return ret\n\ndef bprop(fprop_cache):\n  # Follows procedure given in notes\n  x, y, z1, a1, z2, a2, loss = [fprop_cache[key] for key in ('x', 'y', 'z1', 'a1', 'z2', 'a2', 'loss')]\n  dz2 = (a2 - y)\n  dW2 = np.dot(dz2, a1.T)\n  db2 = dz2\n  dz1 = np.dot(fprop_cache['W2'].T, dz2) * sigmoid(z1) * (1-sigmoid(z1))\n  dW1 = np.dot(dz1, x.T)\n  db1 = dz1\n  return {'b1': db1, 'W1': dW1, 'b2': db2, 'W2': dW2}\n\n# Gradient checking\n\nif __name__ == '__main__':\n  # Initialize random parameters and inputs\n  W1 = np.random.rand(2,2)\n  b1 = np.random.rand(2, 1)\n  W2 = np.random.rand(1, 2)\n  b2 = np.random.rand(1, 1)\n  params = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n  x = np.random.rand(2, 1)\n  y = np.random.randint(0, 2)  # Returns 0/1\n\n  fprop_cache = fprop(x, y, params)\n  bprop_cache = bprop(fprop_cache)\n\n  # Numerical gradient checking\n  # Note how slow this is! Thus we want to use the backpropagation algorithm instead.\n  eps = 1e-6\n  ng_cache = {}\n  # For every single parameter (W, b)\n  for key in params:\n    param = params[key]\n    # This will be our numerical gradient\n    ng = np.zeros(param.shape)\n    for j in range(ng.shape[0]):\n      for k in xrange(ng.shape[1]):\n        # For every element of parameter matrix, compute gradient of loss wrt\n        # that element numerically using finite differences\n        add_eps = np.copy(param)\n        min_eps = np.copy(param)\n        add_eps[j, k] += eps\n        min_eps[j, k] -= eps\n        add_params = copy(params)\n        min_params = copy(params)\n        add_params[key] = add_eps\n        min_params[key] = min_eps\n        ng[j, k] = (fprop(x, y, add_params)['loss'] - fprop(x, y, min_params)['loss']) / (2 * eps)\n    ng_cache[key] = ng\n\n  # Compare numerical gradients to those computed using backpropagation algorithm\n  for key in params:\n    print key\n    # These should be the same\n    print(bprop_cache[key])\n    print(ng_cache[key])\n```\n\n\n### 参考","tags":["CS229"],"categories":["CS229"]},{"title":"[机器学习-CS229]深度学习","url":"%2Fposts%2Ffc734306%2F","content":"# CS229 课程讲义中文翻译\nCS229 Lecture notes\n\n|原作者|翻译|校对|\n|---|---|---|\n|[Andrew Ng  吴恩达](http://www.andrewng.org/),Kian Katanforoosh|[CycleUser](https://www.zhihu.com/people/cycleuser/columns)|[XiaoDong_Wang](https://github.com/Dongzhixiao) |\n\n|相关链接|\n|---|\n|[Github 地址](https://github.com/Kivy-CN/Stanford-CS-229-CN)|\n|[知乎专栏](https://zhuanlan.zhihu.com/MachineLearn)|\n|[斯坦福大学 CS229 课程网站](http://cs229.stanford.edu/)|\n|[网易公开课中文字幕视频](http://open.163.com/movie/2008/1/M/C/M6SGF6VB4_M6SGHFBMC.html)|\n\n# 深度学习(Deep Learning)\n\n现在开始学深度学习。在这部分讲义中，我们要简单介绍神经网络，讨论一下向量化以及利用反向传播(backpropagation)来训练神经网络。\n\n## 1 神经网络(Neural Networks)\n\n我们将慢慢的从一个小问题开始一步一步的构建一个神经网络。回忆一下本课程最开始的时就见到的那个房价预测问题：给定房屋的面积，我们要预测其价格。\n\n在之前的章节中，我们学到的方法是在数据图像中拟合一条直线。现在咱们不再拟合直线了，而是通过设置绝对最低价格为零来避免有负值房价出现。这就在图中让直线拐了个弯，如图1所示。\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229notedlf1.png)\n\n我们的目标是输入某些特征$x$到一个函数$f(x)$中，然后输出房子$y$的价格。规范来表述就是：$f:x\\rightarrow y$。可能最简单的神经网络就是定义一个单个神经元(neuron)的函数$f(x)$，使其满足$f(x)=\\max(ax+b,0)$，其中的$a,b$是参数(coefficients)。这个$f(x)$所做的就是返回一个单值：要么是$(ax+b)$，要么是$0$，就看哪个更大。在神经网络的领域，这个函数叫做一个ReLU（英文读作'ray-lu'），或者叫整流线性单元(rectified linear unit)。更复杂的神经网络可能会使用上面描述的单个神经元然后堆栈(stack)起来，这样一个神经元的输出就是另一个神经元的输入，这就得到了一个更复杂的函数。\n\n现在继续深入房价预测的例子。除了房子面积外，假如现在你还知道了卧房数目，邮政编码，以及邻居的财富状况。构建神经网络的过程和乐高积木(Lego bricks)差不多：把零散的砖块堆起来构建复杂结构而已。同样也适用于神经网络：选择独立的神经元并且堆积起来创建更复杂的神经元网络。\n\n有了上面提到的这些特征（面积，卧房数，邮编，社区财富状况），就可以决定这个房子的价格是否和其所能承担的最大家庭规模有关。假如家庭规模是房屋面积和卧室数目的一个函数（如图2所示）。邮编(zip code)则可以提供关于邻居走动程度之类的附加信息（比如你能走着去杂货店或者去哪里都需要开车）。结合邮编和邻居的财富状况就可以预测当地小学的教育质量。给了上面这三个推出来的特征（家庭规模，交通便利程度，学校教育质量），就可以依据这三个特征来最终推断房价了。\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229notedlf2.png)\n\n我们就已经描述了上面这个神经网络了，就如同读者应该已经理解了确定这三个因素来最终影响房屋。神经网络的一个神奇之处就在于你只需要有输入特征向量$x$以及输出$y$，而其他的具体过程都交给神经网络自己来完成。用神经网络学习中介特征(intermediate features)的这个过程叫做端到端学习(end-to-end learning)。\n\n参考上面关于房价的例子，严格来说，输入到神经网络的是一个输入特征的集合$x_1,x_2,x_3,x_4$。我们将这四个特征连接到三个神经元(neurons)。这三个“内部(internal)”神经元叫做隐藏单元(hidden units)。这个神经网络的目标是要去自动判定三个相关变量来借助着三个变量来预测房屋价格。我们只需要给神经网络提供充足数量的训练样本$(x^{(i)},y^{(i)})$。很多时候，神经网络能够发现对预测输出很有用的一些复杂特征，但这些特征对于人类来说可能不好理解，因为可能不具备通常人类所理解的常规含义。因此有人就把神经网络看作是一个黑箱（black box，意思大概就是说内部过程不透明），因为神经网络内部发掘的特征可能是难以理解的。\n\n接下来我们将神经网络的概念以严格术语进行表述。假设我们有三个输入特征$x_1,x_2,x_3$,这三个特征共同称为输入层(input layer)，然后又四个隐藏单元(hidden units)共同称为隐藏层(hidden layer)，一个输出神经元叫做输出层(output layer)。隐藏层之所以称之为隐藏，是因为不具备足够的事实依据或者训练样本值来确定这些隐藏单元。这是受到输入和输出层限制的，对输入输出我们所了解的基本事实就是$(x^{(i)},y^{(i)})$。\n\n第一个隐藏单元需要输入特征$x_1,x_2,x_3$,然后输出一个记作$a_1$的输出值。我们使用字母$a$是因为这个可以表示神经元的“激活(activation)”的值。在这个具体的案例中，我们使用了一个单独的隐藏层，但实际上可能有多个隐藏层。假设我们用$a_1^{[1]}$来表示第一个隐藏层中的第一个隐藏单元。对隐藏层用从零开始的索引来指代层号。也就是输入的层是第$0$层，第一层隐藏层是第$1$层，输出层是第二层。再次强调一下，更复杂的神经网络就可能有更多的隐藏层。有了上述数学记号，第$2$层的输出就表达做$a_1^{[2]}$。统一记号就得到了:\n\n$$\n\\begin{aligned}\nx_1 &= a^{[0]}_1 &\\quad\\text{(1.1)}\\\\\nx_2 &= a^{[0]}_2 &\\quad\\text{(1.2)}\\\\\nx_3 &= a^{[0]}_3 &\\quad\\text{(1.3)}\\\\\n\\end{aligned}\n$$\n\n这里要说清的是，用方括号$[1]$上标的元素表示一切和第$1$层相关的，带圆括号的$x^{(i)}$表示的则是第$i$个训练样本，而$a^{[l]}_j$表示的是第$j$个单元在第$l$层的激活。可以将逻辑回归函数$g(x)$看做一个单个神经元（如图3所示）:\n\n$$\ng(x)=\\frac{1}{1+\\exp(-w^Tx)}\n$$\n\n向上面逻辑回归函数$g(x)$中输入的就是三个特征$x_1,x_2,x_3$,而输出的是对$y$的估计值。可以将上面这个$g(x)$表示成神经网络中的一个单个神经元。可以将这个函数拆解成两个不同的计算：$(1)z=w^Tx+b$；$(2)a=\\sigma(z),\\sigma(z)=\\frac{1}{1+e^{-z}}$。要注意这里的记号上的差别：之前我们使用的是$z=\\theta^Tx$但现在使用的是$z=w^Tx+b$，这里面的$w$是一个向量。后面的讲义中会看到如果是表示矩阵就用大写字母$W$了。这里的记号差别是为了遵循标准的神经网络记号。更通用的写法，还要加上$a=g(z)$，这里面这个$g(z)$可以试试某种激活函数。举几个例子，激活函数可以包括下面几种:\n\n$$\n\\begin{aligned}\ng(z) &= \\frac{1}{1+e^{-z}} &\\quad\\text{(sigmoid)}\\quad\\text{(1.4)}\\\\\ng(z) &= \\max(z,0) &\\quad\\text{(ReLU)}\\quad\\text{(1.5)}\\\\\ng(z) &= \\frac{e^z-e^{-z}}{e^z+e^{-z}} &\\quad\\text{(tanh)}\\quad\\text{(1.6)}\\\\\n\\end{aligned}\n$$\n\n一般来说，$g(z)$都是非线性函数(non-linear function)。\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229notedlf3.png)\n\n\n回到前面的那个神经网络，第一隐藏层的第一个隐藏单元会进行下面的计算:\n\n$$\nz^{[1]}_1={W^{[1]}_1}^T x+b^{[1]}_1\\quad,\\quad a^{[1]}_1=g(z^{[1]}_1)   \\quad\\text{(1.7)}\n$$\n\n上式中的$W$是一个参数矩阵，而$W_1$指的是这个矩阵的第$1$行(row)。和第一个隐藏单元相关联的参数包括了向量$W_1^{[1]} \\in R^3$和标量$b_1^{[1]} \\in R^3$。对第$1$个隐藏层的第二和第三个隐藏单元，计算定义为:\n\n$$\n\\begin{aligned}\nz^{[1]}_2&={W^{[1]}_2}^T x+b^{[1]}_2\\quad,\\quad a^{[1]}_2&=g(z^{[1]}_2)  \\\\\nz^{[1]}_3&={W^{[1]}_3}^T x+b^{[1]}_3\\quad,\\quad a^{[1]}_3&=g(z^{[1]}_3)  \\\\\n\\end{aligned}\n$$\n\n其中每个隐藏单元都有各自对应的参数$W$和$b$。接下来就是输出层的计算：\n\n$$\nz^{[2]}_1={W^{[2]}_1}^T a^{[1]}+b^{[2]}_1\\quad,\\quad a^{[2]}_1=g(z^{[2]}_1)   \\quad\\text{(1.8)}\n$$\n\n上面式子中的$a^{[1]}$定义是所有第一层激活函数的串联(concatenation)：\n\n$$\na^{[1]}=\\begin{bmatrix} a^{[1]}_1 \\\\\na^{[1]}_2 \\\\\na^{[1]}_3 \\\\\na^{[1]}_4 \\\\\n \\end{bmatrix} \\quad\\text{(1.9)}\n$$\n\n激活函数$a^{[2]}_1$来自第二层，是一个单独的标量，定义为$a^{[2]}_1=g(z^{[2]}_1)$，表示的是神经网络的最终输出预测结果。要注意对于回归任务来说，通常不用严格为正的非线性函数（比如ReLU或者Sigmoid），因为对于某些任务来说，基本事实$y$的值实际上可能是负值。\n\n## 2 向量化(Vectorization)\n\n为了使用合理的计算速度来实现一个神经网络，我们必须谨慎使用循环(loops)。要计算在第一层中的隐藏单元的激活函数，必须要计算出来$z_1,...,z_4$和$a_1,...,a_4$。\n\n$$\n\\begin{aligned}\nz^{[1]}_1&={W^{[1]}_1}^T x+b^{[1]}_1\\quad,\\quad a^{[1]}_1=g(z^{[1]}_1) \\qquad&(2.1) \\\\\n&\\vdots\\qquad\\qquad\\qquad\\vdots\\qquad\\qquad\\qquad\\vdots&(2.2) \\\\\nz^{[1]}_4&={W^{[1]}_4}^T x+b^{[1]}_4\\quad,\\quad a^{[1]}_4=g(z^{[1]}_4) &(2.3) \\\\\n\\end{aligned}\n$$\n\n最自然的实现上述目的的方法自然是使用for循环了。深度学习在机器学习领域中最大的特点就是深度学习的算法有更高的算力开销。如果你用了for循环，代码运行就会很慢。\n\n这就需要使用向量化了。向量化和for循环不同，能够利用矩阵线性代数的优势，还能利用一些高度优化的数值计算的线性代数包（比如BLAS），因此能使神经网络的计算运行更快。在进行深度学习领域之前，小规模数据使用for循环可能就足够用了，可是对现代的深度学习网络和当前规模的数据集来说，for循环就根本不可行了。\n\n### 2.1 输出计算向量化(Vectorizing the Output Computation)\n\n接下来将的方法是不使用for循环来计算$z_1,...,z_4$。使用矩阵线性代数方法，可以用如下方法计算状态:\n\n$$\n\\begin{aligned}\n\\underbrace{  \\begin{bmatrix} z^{[1]}_1 \\\\  \\vdots\\\\ \\vdots\\\\ z^{[1]}_4 \\end{bmatrix} }_{z^{[1]} \\in  R^{4\\times 1}}\n\n=\\underbrace{  \\begin{bmatrix}-&{W{[1]}_1}^T - \\\\ -&{W{[1]}_2}^T -\\\\&\\vdots\\\\ -&{W{[1]}_4}^T -\\end{bmatrix}}_{W^{[1]}\\in  R^{4\\times 3}}\n\n\\underbrace{ \\begin{bmatrix}x_1\\\\x_2\\\\x_3 \\end{bmatrix}}_{x\\in  R^{3\\times 1}} + \n\n\\underbrace{ \\begin{bmatrix} b^{[1]}_1 \\\\ b^{[1]}_2 \\\\ \\vdots\\\\ b^{[1]}_4\\end{bmatrix}  }_{b^{[1]}\\in  R^{4\\times 1}}\\quad\\text{(2.4)}\n\\end{aligned}\n$$\n\n上面的矩阵下面所标注的$R^{n\\times m}$表示的是对应矩阵的维度。直接用矩阵记号表示是这样的:$z^{[1]}= W^{[1]}x+b^{[1]}$。要计算$a^{[1]}$而不实用for循环，可以利用MATLAB/Octave或者Python里面先有的向量化库，这样通过运行分布式的针对元素的运算就可以非常快速的计算出$a^{[1]}=g(z^{[1]})$。数学上可以定义一个S型函数(sigmoid function)$g(z)$:\n\n$$\ng(z)=\\frac{1}{1+e^{-1}} \\quad\\text{, } z\\in R \\quad\\text{(2.5)}\n$$\n\n不够，这个S型函数不尽力针对标量(scalars)来进行定义，也可以对向量(vectors)定义。以MATLAB/Octave风格的伪代码，就可以如下方式定义这个函数:\n\n$$\ng(z)=1 ./(1+\\exp(-z)) \\quad\\text{, } z\\in R \\quad\\text{(2.6)}\n$$\n\n上式中的$./$表示的是元素对除。这样有了向量化的实现后，$a^{[1]}=g(z^{[1]})$就可以很快计算出来了。\n\n总结一下目前位置对神经网络的了解，给定一个输入特征$x\\in R^3$，就可以利用$z^{[1]}=W^{[1]}x+b^{[1]}$和$a^{[1]}=g(z^{[1]})$计算隐藏层的激活，要计算输出层的激活状态(也就是神经网络的输出)，要用:\n\n$$\\begin{aligned}\n\n\\underbrace{ z^{[2]} } _{ 1\\times 1}\n\n&=\\underbrace{ W^{[2]} } _{ 1\\times 4} \\underbrace{ a^{[1]}} _{ 4\\times 1} +\\underbrace{ b^{[2]} } _{ 1\\times 1}\\quad,\\quad \\underbrace{  a^{[2]}} _{ 1\\times 1}&=g(\\underbrace{  z^{[2]} } _{ 1\\times 1})\\quad\\text{(2.7)}\n\\end{aligned}\n$$\n\n为什么不对$g(z)$使用同样的函数呢?为啥不用$g(z)=z$呢?假设$b^{[1]}$和$b^{[2]}$都是零值的。利用等式$(2.7)$就得到了:\n\n$$\\begin{aligned}\nz^{[2]}&=W^{[2]}a^{[1]} &\\text{} \\quad\\text{(2.8)}\\\\\n&= W^{[2]} g(z^{[1]}) &\\text{根据定义} \\quad\\text{(2.9)}\\\\\n&= W^{[2]}z^{[1]}&\\text{因为}g(z)=z \\quad\\text{(2.10)}\\\\\n&= W^{[2]}W^{[1]}x&\\text{参考等式(2.4)} \\quad\\text{(2.11)}\\\\\n&= \\tilde W x&\\text{其中的}\\tilde W  =W^{[2]}W^{[1]} \\quad\\text{(2.12)}\\\\\n\\end{aligned}\n$$\n\n这样之前的$W^{[2]}W^{[1]}$就合并成了$\\tilde W$。这是因为对一个线性函数应用另一个线性函数会得到原结果的一个线性函数(也就是你可以构建一个$\\tilde W$来使得$\\tilde W  x=W^{[2]}W^{[1]}x)$。这也使得神经网络失去了很多的代表性，因为有时候我们要预测的输出和输入可能是存在非线性关系的。没有了非线性激活函数，神经网络就只是简单地进行线性回归了。\n\n### 2.2 训练样本集向量化(Vectorization Over Training Examples)\n\n假如你有了一个三个样本组成的训练集。每个样本的激活函数如下所示:\n\n$$\n\\begin{aligned}\nz^{[1](1)} &= W^{[1]}x^{(1)}+b^{[1]}\\\\\nz^{[1](2)} &= W^{[1]}x^{(2)}+b^{[1]}\\\\\nz^{[1](3)} &= W^{[1]}x^{(3)}+b^{[1]}\\\\\n\\end{aligned}\n$$\n\n要注意上面的括号是有区别的，方括号[]内的数字表示的是层数(layer number)，而圆括号()内的数字表示的是训练样本编号(training example number)。直观来看，似乎可以用一个for循环实现这个过程。但其实也可以通过向量化来实现。首先定义:\n\n$$\nX=\\begin{bmatrix} |&|&|&\\\\ \nx^{(1)}&x^{(2)}&x^{(3)}\\\\ \n|&|&|&\\\\ \n\\end{bmatrix}\\quad\\text{(2.13)}\n$$\n\n注意，我们是在列上排放训练样本而非在行上。然后可以将上面的式子结合起来用单个的统一公式来表达:\n\n$$\nZ^{[1]}=\\begin{bmatrix} |&|&|&\\\\ \nz^{[1](1)}&z^{[1](2)}&z^{[1](3)}\\\\ \n|&|&|&\\\\ \n\\end{bmatrix} =W^{[1]}X+b^{[1]}\\quad\\text{(2.14)}\n$$\n\n你或许已经注意到了我们试图在$W^{[1]}X \\in R^{4\\times 3}$的基础上添加一个$b^{[1]}\\in R^{4\\times 1}$。严格按照线性代数规则的话，这是不行的。不过在实际应用的时候，这个加法操作是使用广播(boradcasting)来实现的。创建一个中介$\\tilde b^{[1]}\\in R^{4\\times 3}$:\n\n$$\n\\tilde b^{[1]} =\\begin{bmatrix} |&|&|&\\\\ \nb^{[1]}&b^{[1]}&b^{[1]}\\\\ \n|&|&|&\\\\ \n\\end{bmatrix}\\quad\\text{(2.15)}\n$$\n\n然后就可以计算：$Z^{[1]}= =W^{[1]}X+\\tilde b^{[1]}$。通常都没必要去特地构建一个$\\tilde b^{[1]}$。检查一下等式(2.14)，你就可以假设$b^{[1]}\\in R^{4\\times 1}$能够正确广播(broadcast)到$W^{[1]}X \\in R^{4\\times 3}$上。\n\n综上所述：假如我们有一个训练集：$(x^{(1)},y^{(1)}),...,(x^{(m)},y^{(m)})$，其中的$x^{(i)}$是一个图形而$y^{(i)}$是一个二值化分类标签，标示的是一个图片是否包含一只猫（比如y=1就表示是一只猫）。\n\n首先要将参数$W^{[1]},b^{[1]},W^{[2]},b^{[2]}$初始化到比较小的随机值。对每个样品，都计算其从$S$型函数(sigmoid function)$a^{[2](i)}$的输出概率。然后利用逻辑回归(logistic regression)对数似然函数(log likelihood):\n\n$$\n\\sum^m_{i=1}(y^{(i)}\\log a^{[2](i)} +(1-y^{(i)}\\log(1-a^{[2](i)})\\quad\\text{(2.16)}\n$$\n\n最终，利用梯度上升法(gradient ascent)将这个函数最大化。这个最大化过程对应的就是对神经网络的训练。\n\n## 3 反向传播(Backpropagation)\n\n现在我们不再使用上面的房价预测的例子，要面对一个新问题了。假如我们要检测在一个图片中是否包含足球。给定一张图片$x^{(i)}$作为输入，我们希望能够输出一个二值化的预测，如果图中包含足球就输出$1$，反之就输出$0$。\n\n备注：图像可以表示成一个矩阵的形式，矩阵的元素等于图像的像素数。不过彩色的图像是以张量/体积(volume)的形式来进行数字化表示的(也就是说有三个通道，或者说是三个矩阵堆叠到一起)。这里有三个矩阵是对应着显示器上面的三原色红绿蓝(RGB)的值。在下面的案例中，我们有一个$64\\times 64\\times 3$数值规模的图像，其中包含了一个足球。拉平(flattened)之后就成了一个单独的向量，包含有$12288$个元素。\n\n一个神经网络模型包含两个成分:$(i)$网络结构，定义了有多少层，多少个神经元，以及多少个神经元彼此链接;$(ii)$参数（数值values;也称作权重weights）。在这一节，我们要讲一下如何学习这些参数。首先我们要讲一下参数初始化和优化，以及对这些参数的分析。\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229notedlf4.png)\n\n### 3.1 参数初始化(ParameterInitialization)\n\n设想一个双层神经网络。左侧的输入是一个拉平的图像向量$x^{(1)},...,x^{(n)}$。在第一个隐藏层中，要注意所有的输入如何连接到下一层中的所有神经元上。这就叫做全连接层(fully connected layer)。\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229notedlf5.png)\n\n下一步就是计算在这个神经网络中有多少个参数。一种方法是手动向前计算(forward propagation by hand)。\n\n$$\n\\begin{aligned}\nz^{[1]} &= W^{[1]}x^{(i)}+b^{[1]}\\quad&\\text{(3.1)}\\\\\na^{[1]} &= g(z^{[1]})\\quad&\\text{(3.2)}\\\\\nz^{[2]} &= W^{[2]}a^{(1)}+b^{[2]}\\quad&\\text{(3.3)}\\\\\na^{[2]} &= g(z^{[2]}) \\quad&\\text{(3.4)}\\\\\nz^{[3]} &= W^{[3]}a^{(2)}+b^{[3]}\\quad&\\text{(3.5)}\\\\\n\\hat y ^{[i]} &= a^{[3]}=g(z^{[3]})\\quad&\\text{(3.6)}\\\\\n\\end{aligned}\n$$\n\n已知其中的$z^{[1]},a^{[1]}\\in R^{3\\times 1}, z^{[2]},a^{[2]}\\in R^{2\\times 1},z^{[3],a^{[3]}\\in R^{1\\times 1}}$。现在不知道的就是$W^{[1]}$的规模。不过这个可以算出来。\n\n已知$x\\in R^{n\\times 1}$。因此则有:\n\n$$\nz^{[1]}=W^{[1]}x^{(i)} =R^{3\\times 1} \\quad\\text{可以写作} R^{3\\times 1}=R^{?\\times ?} \\times R^{n\\times  1}\\quad\\text{(3.7)}\n$$\n\n根据矩阵乘法，可以推断出上面式子中的$?\\times ?$应该是$3\\times n$。另外还能推断出基(原文写的bias莫不是偏差?)的规模应该是$3\\times 1$，因为必须能够匹配$W^{1]}x^{(i)}$。对每个隐藏层重复上面的这一过程。这样就得到了:\n\n$$\nW^{[2]}\\in R^{2\\times 3},b^{[2]}\\in R^{2\\times 1}\\quad\\text{以及}\\quad W^{[3]}\\in R^{1\\times 2},b^{[3]}\\in R^{1\\times 1}\\quad\\text{(3.8)}\n$$\n\n加到一起，就知道在第$1$层是$3n+3$，第$2$层是$2\\times 3+2$，第$3$层就是$2+1$。这样参数一共就是$3n+14$个。\n\n在我们开始训练神经网络之前，必须先对上面这么多参数赋予初始值。这里不能使用零值作为初始值。因为第一层的输出这样就总是相同的了，因为$W^{[1]}x^{(i)}+b^{[1]}=0^{3\\times1}$，其中的$0^{3\\times1}$表示的是全部值为零的形状为$n\\times m$的矩阵。这样会给后续的参数更新带来麻烦（也就是所有的梯度都是相同的）。解决办法是对所有参数以极小的随机值来进行初始化（比如一般可以使用在0附近正态分布的值$\\mathcal{N}(0,0.1)$）。单输出时候了之后，就可以开始利用梯度下降法(gradient descent)来开始训练这个神经网络了。\n\n训练过程中接下来的一步就是更新参数。在通过整个神经网络的一次正向过程之后，输出就是一个预测值$\\hat y$。可以计算损失函数$\\mathcal{L}$，这个案例中我们使用对数损失函数(log loss):\n\n$$\n\\mathcal{L}(\\hat y,y)=-[(1-y)\\log(1-\\hat y)+y\\log \\hat y]\\quad\\text{(3.9)}\n$$\n\n上面的损失函数$\\mathcal{L}(\\hat y,y)$就会产生单个的一个表两只。简单起见，我们将这个损失函数值也记作$\\mathcal{L}$。有了这个值，就必须对神经网络中各层的所有参数进行更新。对任意的一个给定的层次$l$，更新方法如下所示:\n\n$$\\begin{aligned}\nW^{[l]} &= W^{[l]}-\\alpha\\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}}\\quad&\\text{(3.10)}\\\\\nb^{[l]} &= b^{[l]}-\\alpha\\frac{\\partial \\mathcal{L}}{\\partial b^{[l]}}\\quad&\\text{(3.11)}\\\\\n\\end{aligned}\n$$\n\n上式中的$\\alpha$是学习速率(learning rate).另外还必须计算对应参数的梯度:$\\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}}$和$\\frac{\\partial \\mathcal{L}}{\\partial b^{[l]}}$.\n\n要记住，上面是决定了不去将所有参数都设置为零值。那么如果就是全部用零值来初始化会怎么样?我明知道$z^{[3]}=w^{[3]}a^{[2]}+b^{[3]}$会等于零，因为$W^{[3]}$和$b^{[3]}$都是零值了。不过神经网络的输出则定义成$a^{[3]}=g(z^{[3]})$。还记得上面的$g(\\cdot)$的定义是一个S型函数(sigmoid function)。这就一诶这$a^{[3]}=g(z^{[3]})=0.5$。也就是不论提供什么样的$x^{(i)}$的值，网络的输出都将会是$\\hat y=0.5$。\n\n要是我们将所有参数都用非零值但是都是同一个值来初始化会怎么样?这时候，考虑第1层的激活状态函数:\n\n$$\na^{[1]}=g(z^{[1]})=g(W^{[1]}x^{(i)}+b^{[1]})\\quad\\text{(3.12)}\n$$\n\n如此一来激活向量$a^{[1]}$的每一个元素都是相同的(因为$W^{[1]}$包含的是全部相同的值)。这一情况也会发生在神经网络中的其他所有层上。结果就是，在计算题度的时候，一个层里面的所有神经元对最终的损失函数都具有同等的贡献。这个性质叫做对称(symmetry)。这就意味着(一个层内)的每个神经元都接受完全相同的梯度更新至(也就是所有的神经元都进行同样的学习)。\n\n在实践中，会有一种比随机值初始化更好的方法。叫做Xavier/He 初始化，对权重(weights)进行的初始化如下\n\n$$\nw^{[l]}\\sim \\mathcal{N}(0,\\sqrt{\\frac{2}{n^{[l]}+n^{[l-1]}}}\\quad\\text{(3.13)}\n$$\n\n上式中的$n^{[l]}$表示的是第$l$层的神经元个数。这种操作是一种最小规范化技术(mini-normalization technique)。对于单层而言，设该层的输入(input)的方差(variance)是$\\sigma^{(in)}$而输出(也就是激活状态函数，activations)的方差是$\\sigma^{(out)}$.Xavier/He 初始化就是让$\\sigma^{(in)}$尽量接近$\\sigma^{(out)}$。\n\n### 3.2 优化(Optimization)\n\n回忆一下咱们这个神经网络的参数:$W^{[1]},b^{[1]},W^{[2]},b^{[2]},W^{[3]},b^{[3]}$。要对这些进行更新，可以利用等式$(3.10)$和$(3.11)$里面的更新规则来实现随机梯度下降(stochastic gradient descent，缩写为SGD)。首先计算对应$W^{[3]}$的梯度。这是因为$W^{[1]}$对损失函数的影响比$W^{[3]}$更复杂。这是由于在计算的次序上来说，$W^{[3]}$更接近(closer)输出$\\hat y$。\n\n$$\\begin{aligned}\n\\frac{\\partial \\mathcal{L}}{\\partial W^{[3]}}&=   \\frac{\\partial  }{\\partial W^{[3]}} ((1-y)\\log(1-\\hat y)+y\\log \\hat y)  quad&\\text{(3.14)}\\\\\n&=  -(1-y) \\frac{\\partial  }{\\partial W^{[3]}}  \\log(1-g(W^{[3]}a^{[2]}+b^{[3]}))   \\quad&\\text{(3.15)}\\\\\n& \\qquad -y  \\frac{\\partial  }{\\partial W^{[3]}}  \\log(g(W^{[3]}a^{[2]}+b^{[3]}))     \\quad&\\text{(3.16)}\\\\\n&=  -(1-y)\\frac{1}{1-g(W^{[3]}a^{[2]}+b^{[3]})}(-1)g'(W^{[3]}a^{[2]}+b^{[3]}){a^{[2]}}^T   \\quad&\\text{(3.17)}\\\\\n&\\qquad -y\\frac{1}{g(W^{[3]}a^{[2]}+b^{[3]})}g'(W^{[3]}a^{[2]}+b^{[3]}){a^{[2]}}^T    \\quad&\\text{(3.18)}\\\\\n&= (1-y)\\sigma(W^{[3]}a^{[2]}+b^{[3]})  {a^{[2]}}^T  -y(1-\\sigma (W^{[3]}a^{[2]}+b^{[3]}) ) {a^{[2]}}^T  \\quad&\\text{(3.19)}\\\\\n&= (1-y)a^{[3]}{a^{[2]}}^T  -y(1-a^{[3]}){a^{[2]}}^T      \\quad&\\text{(3.20)}\\\\\n&=  (a^{[3]}-y){a^{[2]}}^T     \\quad&\\text{(3.21)}\\\\\n\\end{aligned}\n$$\n\n注意上面用的函数$g(\\cdot)$是S型函数(sigmoid)。质疑函数的导数(derivative)是$g'=\\sigma'=\\sigma(1-\\sigma)$。另外有 $a^{[3]} =\\sigma(W^{[3]}a^{[2]}+b^{[3]})$。这时候就结束了对一个参数$W^{[3]}$的梯度计算过程。\n\n接下来要计算$W^{[2]}$的梯度。这里不再推导$\\frac{\\partial \\mathcal{L}}{\\partial W^{[2]}}$，可以利用微积分里面的链式规则(chain rule)。已知$L$依赖于$\\hat y=a^{[3]}$。\n\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial W^{[2]}} = \\frac{\\partial \\mathcal{L}}{?} \\frac{?}{\\partial W^{[2]}} \\quad \\text{(3.22)}\n$$\n\n如果观察正向传播(forward propagation)，就知道$\\mathcal{L}$依赖于$\\hat y=a^{[3]}$。利用链式规则就可以插入$\\frac{\\partial a^{[3]}}{\\partial a^{[3]}}$:\n\n$$  \n\\frac{\\partial \\mathcal{L}}{\\partial W^{[2]}} = \\frac{\\partial \\mathcal{L}}{\\partial a^{[3]}}  \\frac{\\partial a^{[3]}}{?} \\frac{?}{\\partial W^{[2]}}\n\\quad \\text{(3.23)}\n$$\n\n我们已知$a^{[3]}$和$z^{[3]}$直接相关。\n\n$$  \n\\frac{\\partial \\mathcal{L}}{\\partial W^{[2]}} = \\frac{\\partial \\mathcal{L}}{\\partial a^{[3]}}  \\frac{\\partial a^{[3]}}{\\partial z^{[3]}}  \\frac{\\partial z^{[3]}}  {?} \\frac{?}{\\partial W^{[2]}}\n\\quad \\text{(3.24)}\n$$\n\n接下来，我们知道$z^{[3]}$和$a^{[2]}$直接相关。要注意不能使用$W^{[2]}$或者$b^{[2]}$，因为$a^{[2]}$十字等式$(3.5)$和$(3.6)$之间唯一的共有元素(common element)。在反向传播(Backpropagation)中需要用共有元素。\n\n$$  \n\\frac{\\partial \\mathcal{L}}{\\partial W^{[2]}} = \\frac{\\partial \\mathcal{L}}{\\partial a^{[3]}}  \\frac{\\partial a^{[3]}}{\\partial z^{[3]}}  \\frac{\\partial z^{[3]}} {\\partial a^{[2]}}  \\frac{\\partial a^{[2]}}  {?} \\frac{?}{\\partial W^{[2]}}\n\\quad \\text{(3.25)}\n$$\n\n再次用到$a^{[2]}$和$z^{[2]}$直接相关，而$z^{[2]}$直接依赖于$W^{[2]}$，这使得我们可以计算整个链:\n\n$$  \n\\frac{\\partial \\mathcal{L}}{\\partial W^{[2]}} = \\frac{\\partial \\mathcal{L}}{\\partial a^{[3]}}  \\frac{\\partial a^{[3]}}{\\partial z^{[3]}}  \\frac{\\partial z^{[3]}} {\\partial a^{[2]}}  \\frac{\\partial a^{[2]}}  {\\partial z^{[2]}} \\frac{\\partial z^{[2]}}{\\partial W^{[2]}}\n\\quad \\text{(3.26)}\n$$\n\n回忆一下之前的$\\frac{\\partial \\mathcal{L}} {\\partial W^{[3]}}$:\n\n$$\n\\frac{\\partial \\mathcal{L}} {\\partial W^{[3]}} = (a^{[3]}-y){a^{[2]}}^T  \\quad \\text{(3.27)}\n$$\n\n因为我们首先计算出了$\\frac{\\partial \\mathcal{L}} {\\partial W^{[3]}}$，就知道了$a^{[2]}= \\frac{\\partial z^{[3]}} {\\partial W^{[3]}}$。类似地就也有$(a^{[3]}-y)=\\frac{\\partial \\mathcal{L}} {\\partial W^{[3]}}$。这些可以帮我们计算出$\\frac{\\partial \\mathcal{L}} {\\partial W^{[2]}}$。将这些值带入到等式$(3.26)$。这就得到了:\n\n$$  \n\\frac{\\partial L}{\\partial W^{[2]}} = \n\n\\underbrace{ \n    \\frac{\\partial L}{\\partial a^{[3]}}  \\frac{\\partial a^{[3]}}{\\partial z^{[3]}} \n}_{(a^{[3]}-y)} \n\n\\underbrace{  \n    \\frac{\\partial z^{[3]}} {\\partial a^{[2]}} \n}_{W^{[3]}} \n\n\\underbrace{  \n     \\frac{\\partial a^{[2]}}  {\\partial z^{[2]}}\n}_{g'(z^{[2]})} \n\n\\underbrace{ \n    \\frac{\\partial z^{[2]}}{\\partial W^{[2]}}\n}_{a^{[1]}} \n    \n    \n =(a^{[3]}-y)W^{[3]}g'(z^{[2]})a^{[1]}\n\\quad \\text{(3.28)}\n$$\n\n虽然已经大幅度简化了这个过程，但还没有完成。因为要计算更高维度的导数(derivatives)，要计算等式$(3.28)$所要求的矩阵乘法的确切的次序(order)还不清楚。必须在等式$(3.28)$中对各个项目进行重排列，使其维度相符合(align)。首先将每个项目的维度标记出来:\n\n$$\n\\underbrace{ \n    \\frac{\\partial L}{\\partial W^{[2]}} \n}_{2\\times3}\n =  \n \\underbrace{ (a^{[3]}-y)\n }_{1\\times1}\n \n \\underbrace{ W^{[3]}\n }_{1\\times2}\n \n\\underbrace{  g'(z^{[2]})\n }_{2\\times1}\n \n \\underbrace{ a^{[1]}\n }_{3\\times1}\n\\quad \\text{(3.29)}\n$$\n\n要注意上面各项目并没有根据形状进行妥善排列。因此必须利用矩阵线性代数的形制将其重排列，使矩阵运算能够产生一个有正确形态的输出结果。正确的排序如下所示:\n\n\n$$\n\\underbrace{ \n    \\frac{\\partial L}{\\partial W^{[2]}} \n}_{2\\times3}\n =  \n \n  \\underbrace{ {W^{[3]}}^T\n }_{2\\times1}\n\n \\circ \n \n\\underbrace{  g'(z^{[2]})\n }_{2\\times1}\n\n \n \\underbrace{ (a^{[3]}-y)\n }_{1\\times1}\n \n \\underbrace{ {a^{[1]}}^T\n }_{1\\times3}\n\\quad \\text{(3.30)}\n$$\n\n\n其余的梯度计算就作为练习交给读者自行完成了。在计算剩余参数的梯度的嘶吼，利用已经计算的$\\frac{\\partial \\mathcal{L}}{\\partial W^{[2]}}$和$\\frac{\\partial \\mathcal{L}}{\\partial W^{[3]}}$作为中介结果是很重要的，因为这两者都可以直接用于梯度计算中。\n\n回到优化上，我们之前讨论过随机梯度下降法(stochastic gradient descent，缩写为SGD)了。接下来要将的是梯度下降(gradient descent)。对任意一个单层$l$，更新规则的定义为:\n\n$$\nW^{[l]}=W^{[l]}-\\alpha \\frac{\\partial J}{\\partial W^{[l]}}\\quad \\text{(3.31)}\n$$\n\n上式中的$J$是成本函数(cost function)$J=\\frac{1}{m}\\sum^m_{i=1}L^{(i)}$，而其中的$L^{(i)}$是对单个样本的损失函数值(loss)。梯度下降更新规则和随机梯度下降更新规则的区别是成本函数$j$给出的是更精确的梯度，而损失函数$L^{(i)}$可能是有噪音的。随机梯度下降法视图从全部梯度下降中对梯度进行近似。梯度下降的弱点是很难在依次向前或向后传播阶段(phase)中计算所有样本的所有状态函数。\n\n在实践中，研究和应用一般都使用小批量梯度下降(mini-batch gradient descent)。这个方法是梯度下降和随机梯度下降之间的一个折中方案。在这个方法中，成本函数$J_{mb}$定义如下:\n\n$$\nJ_{mb}=\\frac{1}{B}\\sum^B_{i=1}L^{(i)}\\quad \\text{(3.32)}\n$$\n\n上式中的$B$是指最小批量(mini-batch)中的样本数目。\n\n还有一种优化方法叫做动量优化(momentum optimization)。设想最小批量随机梯度下降。对于任意个单一层$l$，更新规则如下所示:\n\n$$\n\\begin{cases}\nv_{dW^{[l}} &= \\beta v_{dW^{[l}} +(1-\\beta) \\frac{\\partial J}{\\partial W^{[l}}\\\\\nW^{[l]}   &= W^{[l]}  -\\alpha v_{dW^{[l}}\n\\end{cases}\n\\quad \\text{(3.33)}\n$$\n\n注意这里的更新规则有两步，而不是之前的单步了。权重(weight)的更新现在依赖于在这一更新步骤的成本函数$J$以及速度(velocity)$v_{dW^{[l]}}$。相对重要程度(relative importance)受到$\\beta$的控制。设想模拟一个人开车。在懂得时候，汽车有栋梁(momentum)。如果踩了刹车或者油门，汽车由于有动量会继续移动。回到优化上，速度(velocity)$v_{dW^{[l}}$就会在时间上跟踪梯度。这个技巧对训练阶段的神经网络有很大帮助。\n\n### 3.3 参数分析(Analyzing the Parameters)\n\n这时候已经初始化过参数了，并且也优化出了这些参数。加入我们对训练出来的模型进行应用评估发现在训练集商贸能够达到96%的准确率，但在测试集上准确率只有64%。解决的思路包括:收集更多数据，进行规范化，或者让模型更浅(shallower)。下面简单讲解一下规范化的技术。\n\n#### 3.3.1 L2规范化(Regularization)\n\n设下面的$W$表示的是一个模型中的所有参数。在神经网络中，你可能会想到对所有层权重$W^{[l]}$添加第二项。为了简单，就简写成$W$。对成本函数进行$L2$规范化街上另一项就得到了:\n\n$$\n\\begin{aligned}\nJ_{L2}&= J+\\frac{\\lambda}{2} ||W||^2   \\quad\\text{(3.34)} \\\\  \n&=J+\\frac{\\lambda}{2}  \\sum_{ij}|W_{ij}|^2      \\quad\\text{(3.35)} \\\\  \n&=J+\\frac{\\lambda}{2}   W^TW     \\quad\\text{(3.36)} \\\\  \n\\end{aligned}\n$$\n\n上式中的$J$是前文提到过的标准成本函数，$\\lambda$是一个任意值，越大表示更加规范化，而$W$包含所有的权重矩阵(weight matrices)，等式$(3.34)(3.35)(3.36)$是完全等价的。这样L2规范化的更新规则就成了:\n$$\n\\begin{aligned}\n W &=   W-\\alpha\\frac{\\partial J}{\\partial W}  -\\alpha\\frac{\\lambda}{2}    \\frac{\\partial W^TW}{\\partial W}      \\quad\\text{(3.37)} \\\\  \n &= (1-\\alpha\\lambda)W-\\alpha \\frac{\\partial J}{\\partial W}        \\quad\\text{(3.38)} \\\\  \n\\end{aligned}\n$$\n\n当使用梯度下降更新参数的时候，并没有$(1-\\alpha\\lambda)W$这一项。这就意味着通过L2规范化，每个更新会泽都会加入某一个惩罚项(penalization)，这个惩罚项依赖于$W$。这个惩罚项(penalization)会增加成本函数(cost)$J$的值，这样可以鼓励单个参数值在程度上(in magnitude)尽量缩小，这是一种降低过拟合(overfitting)发生概率的办法。\n\n#### 3.3.2 参数共享(Parameter Sharing)\n\n回忆一下逻辑回归(logistic regression)。也可以表示成一个神经网络，如图3所示。参数向量$\\theta = (\\theta_1,...,\\theta_n)$必须和输入向量$x=(x_1,...,x_n)$有同样的元素个数。在检测图像中是否包含足球这个例子中，这就意味着$\\theta_1$必须总要查看图像的左上角的像素。不过我们知道足球可能出现在图像的任意一个区域而不一定总在中心位置。很可能$\\theta_1$从没被选镰刀图片左上角有足球。结果就是在测试的时候，只要一个图像中足球出现在左上角，逻辑回归就很可能预测没有足球。这就是个问题了。\n\n因此我们就要试试卷积神经网络(convolutional neural networks)。设$\\theta$不再是一个向量而本身就是一个矩阵。比如在足球这个样例中，设$\\theta=R^{4\\times4}$。\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229notedlf6.png)\n\n\n为了简单起见，我们展示一个$64\\times 64$的图像但还记得图像实际上是三维的包含了三个通道。现在将参数矩阵$\\theta$分散(slide)在图像上。这就如上图中所示在图像左上角位置的加粗方框。要计算激活函数$a$，就要计算$\\theta$和$x_{1:4,1:4}$按元素求积(element-wise product)，其中x的下标表示的是从图像x的左上方$4\\times4$个像素中取值。然后将按元素求积得到的所有元素加到一起来将矩阵压缩(collapse)成一个单独标量。具体形式为:\n\n$$ \na=\\sum^4_{i=1}\\sum^4_{j=1}\\theta_{ij}x_{ij}\\quad\\text{(3.39)} \n$$\n\n然后将这个窗口向图像右侧轻微移动，然后重复上面的国产。一旦到达了行尾了，就从第二行的开头部位开始。\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229notedlf7.png)\n\n一旦到达了图像末尾，参数$\\theta$就已经\"检视\"过了图片的全部像素了：$\\theta_1$就不再只是和左上像素相关联了。结果就是，如论足球出现的位置是图像的右下角或者左上角，神经网络都可以成功探测到。\n\n\n### 参考","tags":["CS229"],"categories":["CS229"]},{"title":"[机器学习-CS229]线性二次调节,微分动态规划,线性二次高斯分布","url":"%2Fposts%2F6c658c3d%2F","content":"# CS229 课程讲义中文翻译\nCS229 Lecture notes\n\n|原作者|翻译|校对|\n|---|---|---|\n|Dan Boneh ， [Andrew Ng  吴恩达](http://www。andrewng。org/)|[CycleUser](https://www。zhihu。com/people/cycleuser/columns)|[XiaoDong_Wang](https://github.com/Dongzhixiao) |\n\n\n|相关链接|\n|---|\n|[Github 地址](https://github。com/Kivy-CN/Stanford-CS-229-CN)|\n|[知乎专栏](https://zhuanlan。zhihu。com/MachineLearn)|\n|[斯坦福大学 CS229 课程网站](http://cs229。stanford。edu/)|\n|[网易公开课中文字幕视频](http://open。163。com/movie/2008/1/M/C/M6SGF6VB4_M6SGHFBMC。html)|\n\n\n# 第十三章\n\n### 第十四部分 线性二次调节，微分动态规划，线性二次高斯分布\n\n上面三个名词的英文原版分别为:\n1. Linear Quadratic Regulation，缩写为LQR；\n2. Differential Dynamic Programming，缩写为DDP；\n3. Linear Quadratic Gaussian，缩写为LQG。\n\n#### 1 有限范围马尔科夫决策过程(Finite-horizon MDPs)\n\n前面关于强化学习(Reinforcement Learning)的章节中，我们定义了马尔科夫决策过程(Markov Decision Processes，缩写为MDPs)，还涉及到了简单情景下的值迭代(Value Iteration)和策略迭代(Policy Iteration)。还具体介绍了**最优贝尔曼方程(optimal Bellman equation)，** 这个方程定义了对应最优策略(optimal policy)$\\pi^*$的最优值函数(optimal value function)$V^{\\pi^*}$。\n\n$$\nV^{\\pi^*}(s)=R(s)+\\max_{a \\in \\mathcal{A}} \\gamma \\sum_{s' \\in \\mathcal{S}} P_{sa}(s')V^{\\pi^*}(s')\n$$\n\n通过优化值函数，就可以恢复最优策略$\\pi^*$:\n\n$$\n\\pi^*(s)=\\arg\\max_{a\\in \\mathcal{A}} \\sum_{s'\\in \\mathcal{S}} P_{sa} (s')V^{\\pi^*}(s')\n$$\n\n本章的讲义将会介绍一个更通用的情景:\n\n1. 这次我们希望写出来的方程能够对离散和连续的案例都适用。因此就要用期望$E_{s' \\sim P_{sa}}[V^{\\pi^*}(s')]$替代求和$\\sum_{s'\\in S} P_{sa}(s')V^{\\pi^*}(s')$。这就意味着在下一个状态中使用值函数的期望(exception)。对于离散的有限案例，可以将期望写成对各种状态的求和。在连续场景，可以将期望写成积分(integral)。上式中的记号$s'\\sim P_{sa}$的意思是状态$s'$是从分布$P_{sa}$中取样得到的。\n\n2. 接下来还假设奖励函数(reward)**同时依赖状态(states)和动作(actions)。** 也就是说，$R:\\mathcal{S}\\times\\mathcal{A} \\rightarrow R$。这就意味着前面计算最优动作的方法改成了\n\n$$\n\\pi^*(s)=\\arg\\max_{a\\in A} R(s，a)+\\gamma E_{s'\\sim P_{sa}}[V^{\\pi^*}(s')]\n$$\n\n3. 以前我们考虑的是一个无限范围马尔科夫决策过程(infinite horizon MDP)，这回要改成**有限范围马尔科夫决策过程(finite horizon MDP)，** 定义为一个元组(tuple):\n\n$$\n(\\mathcal{S},\\mathcal{A},P_{sa},T,R)\n$$\n\n其中的$T>0$的**时间范围(time horizon)，** 例如$T=100$。这样的设定下，支付函数(payoff)就变成了:\n\n$$\nR(s_0,a_0)+R(s_1,a_1)+\\dots+R(s_T,a_T)\n$$\n\n而不再是之前的:\n\n$$\n\\begin{aligned}\n& R(s_0,a_0)+\\gamma R(s_1,a_1) + \\gamma^2 R(s_2,a_2)+\\dots\\\\\n& \\sum^\\infty_{t=0}R(s_t,a_t)\\gamma^t\n\\end{aligned}\n$$\n\n折扣因子(discount factor)$\\gamma$哪去了呢？还记得当初引入这个$\\gamma$的一部分原因就是由于要保持无穷项求和(infinite sum)是有限值(\ffinite)并且好定义(well-defined)的么？如果奖励函数(rewards)绑定了一个常数$\\bar R$，则支付函数(payoff)也被绑定成:\n\n$$\n|\\sum^{\\infty}_{t=0}R(s_t)\\gamma^t|\\le \\bar R \\sum^{\\infty}_{t=0}\\gamma^t\n$$\n\n这就能识别是一个几何求和(geometric sum)！现在由于支付函数(payoff)是一个有限和(finite sum)了，那折扣因子(discount factor)$\\gamma$就没有必要再存在了。\n\n在这种新环境下，事情就和之前不太一样了。首先是最优策略(optimal policy)$\\pi^*$可能是非稳定的(non-stationary)，也就意味着它可能**随着时间步发生变化。** 也就是说现在有:\n\n$$\n\\pi^{(t)}:\\mathcal{S}\\rightarrow\\mathcal{A}\n$$\n\n上面括号中的$(t)$表示了在第$t$步时候的策略函数(policy)。遵循策略$\\pi^{(t)}$的有限范围马尔科夫决策过程如下所示：开始是某个状态$s_0$，然后对应第$0$步时候的策略$\\pi^{(0)}$采取某种行为$a_0:= \\pi^{(0)}(s_0)$。然后马尔科夫决策过程(MDP)转换到接下来的$s_1$，根据$P_{s_0a_0}$来进行调整。然后在选择遵循第$1$步的新策略$\\pi^{(1)}$的另一个行为$a_1:= \\pi^{(1)}(s_1)$。依次类推进行下去。\n\n为什么在有限范围背景下的优化策略函数碰巧就是非稳定的呢？直观来理解，由于我们只能够选择有限的应对行为，我们可能要适应不同环境的不同策略，还要考虑到剩下的时间(步骤数)。设想有一个网格，其中有两个目标，奖励值分别是$+1$和$+10$。那么开始的时候我们的行为肯定是瞄准了最高的奖励$+10$这个目标。但如果过了几步之后，我们更靠近$+1$这个目标而没有足够的剩余步数去到达$+10$这个目标，那更好的策略就是改为瞄准$+1$了。\n\n4. 这样的观察就使得我们可以使用对**时间依赖的方法(time dependent dynamics):**\n\n$$\ns_{t+1} \\sim P^{(t)}_{s_t,a_t}\n$$\n\n这就意味着变换分布(transition distribution)$P^{(t)}_{s_t,a_t}$随着时间而变化。对$R^{(t)}$而言也是如此。要注意，现在这个模型就更加符合现实世界的情况了。比如对一辆车来说，油箱会变空，交通状况会变化，等等。结合前面提到的内容，就可以使用下面这个通用方程(general formulation)来表达我们的有限范围马尔科夫决策过程(fi\fnite horizon MDP):\n\n$$\n(\\mathcal{S},\\mathcal{A},P^{(t)}_{sa},T,R^{(t)})\n$$\n\n**备注：** 上面的方程其实和在状态中加入时间所得到的方程等价。\n\n在时间$t$对于一个策略$\\pi$的值函数也得到了定义，也就是从状态$s$开始遵循策略$\\pi$生成的轨道(trajectories)的期望(expectation)。\n\n$$\nV_t(s)=E[R^{(t)}(s_t,a_t)+\\dots+R^{(T)}(s_T,a_T)|s_t=s,\\pi ]\n$$\n\n现在这个方程就是：在有限范围背景下，如何找到最优值函数(optimal value function):\n\n$$\nV^*_t(s)=\\max_{\\pi}V^{\\pi}_t(s)\n$$\n\n结果表明对值迭代(Value Iteration)的贝尔曼方程(Bellman's equation)正好适合**动态规划(Dynamic Programming)。** 这也没啥可意外的，因为贝尔曼(Bellman)本身就是动态规划的奠基人之一，而贝尔曼方程(Bellman equation)和这个领域有很强的关联性。为了理解为啥借助基于迭代的方法(iteration-based approach)就能简化问题，我们需要进行下面的观察:\n\n1. 在游戏终结（到达步骤$T$）的时候，最优值(optimal value)很明显就是\n\n$$\n\\forall s\\in \\mathcal{S}: V^*_T(s):=\\max_{a\\in A} R^{(T)}(s,a) \\qquad(1)\n$$\n\n2. 对于另一个时间步$0\\le t <T$，如果假设已经知道了下一步的最优值函数$V^*_{t+1}$，就有:\n\n$$\n\\forall t<T，s \\in \\mathcal{S}: V^*_t (s):= \\max_{a\\in A} [R^{(t)}(s,a)+E_{s'\\sim P^{(t)}_{sa}}[V^*_{t+1}(s')]] \\qquad (2)\n$$\n\n观察并思考后，就能想出一个聪明的算法来解最优值函数了:\n\n1. 利用等式$(1)$计算$V^*_T$。\n2. for $t= T-1,\\dots,0$:\n&emsp;&emsp;使用$V^*_{t+1}$利用等式$(2)$计算$V^*_t$。\n  \n**备注:** 可以将标准值迭代(standard value iteration)看作是上述通用情况的一个特例，就是不用记录时间(步数)。结果表明在标准背景下，如果对$T$步骤运行值迭代，会得到最优值迭代的一个$\\gamma^T$的近似(几何收敛，geometric convergence)。参考习题集4中有对下列结果的证明:\n\n<u>定理：</u>设$B$表示贝尔曼更新函数(Bellman update)，以及$||f(x)||_\\infty:= \\sup_x|f(x)|$。如果$V_t$表示在第$t$步的值函数，则有:\n\n$$\n\\begin{aligned}\n||V_{t+1}-V^*||_\\infty &=||B(V_t)-V^*||_\\infty\\\\\n&\\le \\gamma||V_t-V^*||_\\infty\\\\\n&\\le \\gamma^t||V_1-V^*||_\\infty\n\\end{aligned}\n$$\n\n也就是说贝尔曼运算器$B$成了一个$\\gamma$收缩算子(contracting operator)。\n\n#### 2 线性二次调节(Linear Quadratic Regulation，缩写为LQR)\n\n在本节，我们要讲一个上一节所提到的有限范围(finite-horizon)背景下**精确解(exact solution)** 很容易处理的特例。这个模型在机器人领域用的特别多，也是在很多问题中将方程化简到这一框架的常用方法。\n\n首先描述一下模型假设。考虑一个连续背景，都用实数集了:\n\n$$\n\\mathcal{S}=R^n,\\quad\\mathcal{A}=R^d\n$$\n\n然后设有噪音(noise)的**线性转换(linear transitions):**\n\n$$\ns_{t+1}=A_ts_t+B_ta_t+w_t\n$$\n\n上式中的$A_t\\in R^{n\\times n}，B_t\\in R^{n\\times d}$实矩阵，而$w_t\\sim N(0，\\Sigma_t)$是某个高斯分布的噪音（均值为**零**）。我们接下来要讲的内容就表明：只要噪音的均值是$0$，就不会影响最优化策略。\n\n另外还要假设一个**二次奖励函数(quadratic rewards):**\n\n$$\nR^{(t)}(s_t,a_t)=-s_t^TU_ts_t-a_t^TW_ta_t\n$$\n\n上式中的$U_t\\in R^{n\\times n}，W_t\\in R^{n\\times d}$都是正定矩阵(positive definite matrices)，这就意味着奖励函数总是**负的(negative)。**\n\n**要注意**这里的奖励函数的二次方程(quadratic formulation)就等价于无论奖励函数是否更高我们都希望能接近原始值(origin)。例如，如果$U_t=I_n$就是$n$阶单位矩阵(identity matrix)，而$W_t=I_d$为一个$d$阶单位矩阵，那么就有$R_t=-||s_t||^2-||a_t||^2$，也就意味着我们要采取光滑行为(smooth actions)（$a_t$的范数(norm)要小）来回溯到原始状态（$s_t$的范数(norm)要小）。这可以模拟一辆车保持在车道中间不发生突发运动。\n\n接下来就可以定义这个线性二次调节(LQR)模型的假设了，这个LQR算法包含两步骤:\n\n**第一步**设矩阵$A，B，\\Sigma$都是未知的。那就得估计他们，可以利用强化学习课件中的值估计(Value Approximation)部分的思路。首先是从一个任意策略(policy)收集转换(collect transitions)。然后利用线性回归找到$\\arg\\min_{A,B}\\sum^m_{i=1}\\sum^{T-1}_{t=0}||s^{(i_)}_{t+1}- ( As^{(i)}_t +Ba^{(i)}_t)||^2$。最后利用高斯判别分析(Gaussian Discriminant Analysis，缩写为GDA)中的方法来学习$\\Sigma$。\n\n**(译者注:原文这里第一步的第二行公式中用的是$U_T$，应该是写错了，结合上下文公式推导来看，分明应该是$U_t$)**\n\n**第二步**假如模型参数已知了，比如可能是给出了，或者用上面第一步估计出来了，就可以使用动态规划(dynamic programming)来推导最优策略(optimal policy)了。\n   \n也就是说，给出了:\n\n$$\n\\begin{cases}\ns_{t+1} &= A_ts_t+B_ta_t+w_t\\qquad 已知A_t,B_t,U_t,W_t,\\Sigma_t\\\\\nR^{(t)}(s_t，a_t)&= -s_t^TU_ts_t-a^T_tW_ta_t\n\\end{cases}\n$$\n\n然后要计算出$V_t^*$。如果回到第一步，就可以利用动态规划，就得到了:\n\n1. **初始步骤(Initialization step)**\n\n&emsp;&emsp;对最后一次步骤$T$，\n   \n$$\n\\begin{aligned}\nV^*_T(s_T)&=\\max_{a_T\\in A}R_T(s_T,a_T)\\\\\n&=\\max_{a_T\\in A}-s^T_TU_ts_T - a^T_TW_ta_T\\\\\n&= -s^T_TU_ts_T\\qquad\\qquad(\\text{对}a_T=0\\text{最大化})\n\\end{aligned}\n$$\n\n2. **递归步骤(Recurrence step)**\n\n&emsp;&emsp;设$t<T$。加入已经知道了$V^*_{t+1}$。\n\n<u>定理1:</u>很明显如果$V^*_{t+1}$是$s_t$的一个二次函数，则$V_t^*$也应该是$s_t$的一个二次函数。也就是说，存在某个矩阵$\\Phi$以及某个标量$\\Psi$满足:\n\n$$\n\\begin{aligned}\n\\text{if} \\quad V^*_{t+1}(s_{t+1}) &= s^T_{t+1}\\Phi_{t+1}s_{t+1}+\\Psi_{t+1}\\\\\n\\text{then} \\quad V^*_t(s_t)&=s^T_t\\Phi_ts_t+\\Psi_t\n\\end{aligned}\n$$\n\n对时间步骤$t=T$，则有$\\Phi_t=-U_T，\\Psi_T=0$。\n\n<u>定理2:</u>可以证明最优策略是状态的一个线性函数。\n\n已知$V^*_{t+1}$就等价于知道了$\\Phi_{t+1}，\\Psi_{t+1}$，所以就只需要解释如何从$\\Phi_{t+1}，\\Psi_{t+1}$去计算$\\Phi_{t}，\\Psi_{t}$，以及问题中的其他参数。\n\n$$\n\\begin{aligned}\nV^*_t(s_t)&=  s_t^T\\Phi_ts_t+\\Psi_t \\\\\n&= \\max_{a_t}[R^{(t)}(s_t,a_t)+E_{s_{t+1}\\sim P^{(t)}_{s_t,a_t}}[V^*_{t+1}(s_{t+1})]]  \\\\\n&= \\max_{a_t}[-s_t^TU_ts_t-a_t^TV_ta_t+E_{s_{t+1}\\sim N(A_ts_t+B_ta_t,\\Sigma_t)}  [s_{t+1}^T\\Phi_{t+1}s_{t+1}+\\Psi_{t+1}] ]  \\\\\n\\end{aligned}\n$$\n\n上式中的第二行正好就是最优值函数(optimal value function)的定义，而第三行是通过代入二次假设和模型方法。注意最后一个表达式是一个关于$a_t$的二次函数，因此很容易就能优化掉$^1$。然后就能得到最优行为(optimal action)$a^*_t$:\n\n>1 这里用到了恒等式(identity)$E[w_t^T\\Phi_{t+1}w_t] =Tr(\\Sigma_t\\Phi_{t+1})，\\quad \\text{其中} w_t\\sim N(0，\\Sigma_t)$)。\n\n$$\n\\begin{aligned}\na^*_t&= [(B_t^T\\Phi_{t+1}B_t-V_t)^{-1}B_t\\Phi_{t+1}A_t]\\cdot s_t\\\\\n&= L_t\\cdot s_t\\\\\n\\end{aligned}\n$$\n\n上式中的\n$$\nL_t := [(B_t^T\\Phi_{t+1}B_t-V_t)^{-1}B_t\\Phi_{t+1}A_t]\n$$\n\n这是一个很值得注意的结果(impressive result)：优化策略(optimal policy)是关于状态$s_t$的**线性函数。** 对于给定的$a_t^*$，我们就可以解出来$\\Phi_t$和$\\Psi_t$。最终就得到了**离散里卡蒂方程(Discrete Ricatti equations):**\n\n$$\n\\begin{aligned}\n\\Phi_t&= A^T_t(\\Phi_{t+1}-\\Phi_{t+1}B_t(B^T_t\\Phi_{t+1}B_t-W_t)^{-1}B_t\\Phi_{t+1})A_t-U_t\\\\\n\\Psi_t&= -tr(\\Sigma_t\\Phi_{t+1})+\\Psi_{t+1}\\\\\n\\end{aligned}\n$$\n\n<u>定理3:</u>要注意$\\Phi_t$既不依赖$\\Psi_t$也不依赖噪音项$\\Sigma_t$！由于$L_t$是一个关于$A_t，B_t，\\Phi_{t+1}$的函数，这就暗示了最优策略也**不依赖噪音！** （但$\\Psi_t$是依赖$\\Sigma_t$的，这就暗示了最优值函数$V^*_t$也是依赖噪音$\\Sigma_t$的。）\n\n然后总结一下，线性二次调节(LQR)算法就如下所示:\n\n1. 首先，如果必要的话，估计参数$A_t,B_t,\\Sigma_t$。\n2. 初始化$\\Phi_T:=-U_T,\\quad \\Psi_T:=0$。\n3. 从$t=T-1,\\dots,0$开始迭代，借助离散里卡蒂方程(Discrete Ricatti equations)来利用$\\Phi_{t+1},\\Psi_{t+1}$来更新$\\Phi_{t},\\Psi_{t}$，如果存在一个策略能朝着$0$方向推导状态，收敛就能得到保证。\n\n利用<u>定理3</u>，我们知道最优策略不依赖与$\\Psi_t$而只依赖$\\Phi_t$，这样我们就可以**只** 更新$\\Phi_t$，从而让算法运行得更快一点！\n\n#### 3 从非线性方法(non-linear dynamics)到线性二次调节(LQR)\n\n很多问题都可以化简成线性二次调节(LDR)的形式，包括非线性的模型。LQR是一个很好的方程，因为我们能够得到很好的精确解，但距离通用还有一段距离。我们以倒立摆(inverted pendulum)为例。状态的变换如下所示:\n\n$$\n\\begin{pmatrix}\nx_{t+1}\\\\\n\\dot x_{t+1}\\\\\n\\theta_{t+1}\\\\\n\\dot \\theta_{t+1}\n\\end{pmatrix}=F\\begin{pmatrix} \\begin{pmatrix} x_t\\\\\n \\dot x_t\\\\\n  \\theta_t\\\\\n   \\dot\\theta_t \\end{pmatrix}，a_t\\end{pmatrix}\n$$\n\n其中的函数$F$依赖于角度余弦等等。然后这个问题就成了：\n\n**我们能将这个系统线性化么?**\n\n##### 3.1 模型的线性化(Linearization of dynamics)\n\n假设某个时间$t$上，系统的绝大部分时间都处在某种状态$\\bar s_t$上，而我们要选取的行为大概就在$\\bar a_t$附近。对于倒立摆问题，如果我们达到了某种最优状态，就会满足：行为很小并且和竖直方向的偏差不大。\n\n这就要用到泰勒展开(Taylor expansion)来将模型线性化。简单的情况下状态是一维的，这时候转换函数$F$就不依赖于行为，这时候就可以写成:\n\n$$\ns_{t+1}=F(s_t)\\approx F(\\bar s_t)+ F'(\\bar s_t)\\cdot (s_t-\\bar s_t)\n$$\n\n对于更通用的情景，方程看着是差不多的，只是用梯度(gradients)替代简单的导数(derivatives):\n\n$$\ns_{t+1}\\approx F(\\bar s_t,\\bar a_t)+\\nabla _sF(\\bar s_t,\\bar a_t)\\cdot (s_t-\\bar s_t)+\\nabla_aF(\\bar s_t,\\bar a_t)\\cdot (a_t-\\bar a_t) \\qquad \\text{(3)}\n$$\n\n现在$s_{t+1}$就是关于$s_t，a_t$的线性函数了，因为可以将等式$(3)$改写成下面的形式：\n\n$$\ns_{t+1}\\approx As_t+Ba_t+k\n$$\n\n**(译者注:原文这里的公式应该是写错了，写成了$s_{t+1}\\approx As_t+Bs_t+k$)**\n\n上式中的$k$是某个常数，而$A，B$都是矩阵。现在这个写法就和在LQR里面的假设非常相似了。这时候只要摆脱掉常数项$k$就可以了!结果表明只要任意增长一个维度就可以将常数项吸收进$s_t$中区。这和我们在线性回归的课程里面用到的办法一样。\n\n##### 3.2 微分动态规划(Differential Dynamic Programming，缩写为DDP)\n\n如果我们的目标就是保持在某个状态$s^*$，上面的方法都能够很适合所选情景（比如倒立摆或者一辆车保持在车道中间）。不过有时候我们的目标可能要更复杂很多。\n\n本节要讲的方法适用于要符合某些轨道的系统（比如火箭发射）。这个方法将轨道离散化称为若干离散的时间步骤，然后运用前面的方法创建中间目标!这个方法就叫做**微分动态规划(Differential Dynamic Programming，缩写为DDP)。** 主要步骤包括：\n\n**第一步**利用简单控制器(naive controller)创建一个标称轨道(nominal trajectory)，对要遵循轨道进行近似。也就是说，我们的控制器可以用如下方法来近似最佳轨道：\n\n$$\ns^*_0,a^*_0\\rightarrow s^*_1,a^*_1\\rightarrow\\dots\n$$\n\n**第二步**在每个轨道点(trajectory point)$s^*_t$将模型线性化，也就是:\n\n$$\ns_{t+1}\\approx F(s^*_t,a^*_t)+\\nabla_s F(s^*_t,a^*_t)(s_t-s^*_t)+\\nabla_aF(s^*_t,a^*_t)(a_t-a^*_t)\n$$\n\n上面的$s_t,a_t$是当前的状态和行为。现在已经在每个轨道点都有了线性估计了，就可以使用前面的方法将其改写成:\n\n$$\ns_{t+1}=A_t\\cdot s_t+B_t\\cdot a_t\n$$\n\n（要注意在这个情况下，我们可以使用在本章一开头所提到的非稳定动力学模型背景。）\n\n**注意，** 这里我们可以对奖励函数(reward)$R^{(t)}$推导一个类似的积分(derivation)，使用一个二阶泰勒展开(second-order Taylor expansion)就可以了。\n\n$$\n\\begin{aligned}\nR(s_t，a_t)& \\approx R(s^*_t，a^*_t)+\\nabla_s R(s^*_t，a^*_t)(s_t-s^*_t) +\\nabla_a R(s^*_t，a^*_t)(a_t-a^*_t) \\\\\n& + \\frac{1}{2}(s_t-s^*_t)^TH_{ss}(s_t-s^*_t)+(s_t-s^*_t)^TH_{sa}(a_t-a^*_t)\\\\\n&  + \\frac{1}{2}(a_t-a^*_t)^TH_{aa}(a_t-a^*_t) \\\\\n\\end{aligned}\n$$\n\n上式中的$H_{xy}$表示的 $R$ 的海森矩阵(Hessian)项，对应的$x$和$y$是在$(s^*_t,a^*_t)$中得到的（略去不表）。这个表达式可以重写成:\n\n$$\nR_t(s_t，a_t)= -s_t^TU_ts_t-a_t^TW_ta_t\n$$\n\n对于某些矩阵$U_t,W_t$，可以再次使用扩展维度的方法。注意:\n\n$$\n\\begin{pmatrix} 1&x \\end{pmatrix}\\cdot \\begin{pmatrix} a& b\\\\c&d \\end{pmatrix} \\cdot \\begin{pmatrix} 1\\\\x \\end{pmatrix} = a+2bx+cx^2\n$$\n\n**第三步**现在你就能够相信这个问题可以**严格**写成LQR框架的形式了吧。然后就可以利用线性二次调节(LQR)来找到最优策略$\\pi_t$。这样新的控制器就会更好些！\n\n*8注意:** 如果LQR轨道和线性近似的轨道偏离太远，可能会出现一些问题，不过这些都可以通过调节奖励函数形态来进行修正...\n\n**第四步**现在就得到了一个新的控制器了（新的策略$\\pi_t$），使用这个新控制器来产生一个新的轨道:\n\n$$\ns^*_0,\\pi_0(s^*_0)\\rightarrow s^*_1,\\pi_1(s^*_1)\\rightarrow \\quad \\rightarrow s^*_T\n$$\n\n注意当我们生成了这个新的轨道的时候，使用真实的$F$而不是其线性估计来计算变换，这就意味着:\n\n$$\ns^*_{t+1}=F(s^*_t，a^*_t)\n$$\n\n然后回到第二步，重复，直到达到某个停止条件(stopping criterion)。\n\n#### 4 线性二次高斯分布(Linear Quadratic Gaussian，缩写为LQG)\n\n在现实是集中我们可能没办法观测到全部的状态$s_t$。例如一个自动驾驶的汽车只能够从一个相机获取一个图像，这就是一次观察了，而不是整个世界的全部状态。目前为止都是假设所有状态都可用。可是在现实世界的问题中并不见得总是如此，我们需要一个新工具来对这种情况进行建模：部分观测的马尔科夫决策过程(Partially Observable MDPs，缩写为POMDP)。\n\nPOMDP是一个带有了额外观察层的马尔科夫决策过程(MDP)。也就是说要加入一个新变量$o_t$，在给定的当前状态下这个$o_t$遵循某种条件分布:\n\n$$\no_t|s_t\\sim O(o|s)\n$$\n\n最终，一个有限范围的部分观测的马尔科夫决策过程(finite-horizon POMDP)就是如下所示的一个元组(tuple):\n\n$$\n(\\mathcal{S},\\mathcal{O},\\mathcal{A},P_{sa},T,R)\n$$\n\n在这个框架下，整体的策略就是要在观测$o_1,o_2,\\dots,o_t$的基础上，保持一个**置信状态（belief state，对状态的分布）。** 这样在PDMDP中的策略就是从置信状态到行为的映射。\n\n在本节，我们队LQR进行扩展以适应新的环境。假设我们观测的是$y_t\\in R^m$，其中的$m<n$，且有:\n\n$$\n\\begin{cases}\ny_t &= C\\cdot s_t +v_t\\\\\ns_{t+1} &=  A\\cdot s_t+B\\cdot a_t+ w_t\\\\\n\\end{cases}\n$$\n\n上式中的$C\\in R^{m\\times n}$是一个压缩矩阵(compression matrix)，而$v_t$是传感器噪音（和$w_t$类似也是高斯分布的）。要注意这里的奖励函数$R^{(t)}$是左侧不变的，因为是关于状态（而不是观察）和行为的函数。另外，由于分布都是高斯分布，置信状态就也将是高斯分布的。在这样的新框架下，看看找最优策略的方法:\n\n**第一步**首先计算可能装填（置信状态）的分布，以已有观察为基础。也就是说要计算下列分布的均值$s_{t|t}$以及协方差$\\Sigma_{t|t}$:\n\n$$\ns_t|y_1 ,\\dots, y_t \\sim \\mathcal{N}(s_{t|t},\\Sigma_{t|t})\n$$\n\n为了进行时间效率高的计算，这里要用到卡尔曼滤波器算法(Kalman Filter algorithm)（阿波罗登月舱上就用了这个算法）。\n\n**第二步**然后就有了分布了，接下来就用均值$s_{t|t}$来作为对$s_t$的最佳近似。\n\n**第三步**然后设置行为$a_t:= L_ts_{t|t}$，其中的$L_t$来自正规线性二次调节算法(regular LQR algorithm)。\n\n从直觉来理解，这样做为啥能管用呢？要注意到$s_{t|t}$是滴$s_t$的有噪音近似（等价于在LQR的基础上增加更多噪音），但我们已经证明过了LQR是独立于噪音的!\n\n第一步就需要解释一下。这里会给出一个简单情境，其中在我们的方法里没有行为依赖性（但整体上这个案例遵循相同的思想）。设有:\n\n$$\n\\begin{cases}\ns_{t+1}  &= A\\cdot s_t+w_t,\\quad w_t\\sim N(0,\\Sigma_s)\\\\\ny_t  &= C\\cdot s_t+v_t,\\quad v_t\\sim N(0,\\Sigma_y)\\\\\n\\end{cases}\n$$\n\n由于噪音是高斯分布的，可以很明显证明联合分布也是高斯分布:\n\n$$\n\\begin{pmatrix}\ns_1\\\\\n\\vdots\\\\\ns_t\\\\\ny_1\\\\\n\\vdots\\\\\ny_t\n\\end{pmatrix} \\sim \\mathcal{N}(\\mu，\\Sigma) \\quad\\text{for some } \\mu,\\Sigma\n$$\n\n然后利用高斯分布的边缘方程(参考因子分析(Factor Analysis)部分的讲义)，就得到了:\n\n$$\ns_t|y_1,\\dots，y_t\\sim \\mathcal{N}(s_{t|t},\\Sigma_{t|t})\n$$\n\n可是这里使用这些方程计算边缘分布的参数需要很大的算力开销!因为这需要对规模为$t\\times t$的矩阵进行运算。还记得对一个矩阵求逆需要的运算时$O(t^3)$吧，这要是在时间步骤数目上进行重复，就需要$O(t^4)$的算力开销!\n\n**卡尔曼滤波器算法(Kalman filter algorithm)** 提供了计算均值和方差的更好的方法，只用在时间$t$上以一个**固定的时间(constant time)** 来更新！卡尔曼滤波器算法有两个基础步骤。加入我们知道了分布$s_t|y_1,\\dots,y_t$:\n\n$$\n\\begin{aligned}\n\\text{预测步骤(predict step) 计算} & s_{t+1}|y_1,\\dots,y_t\n\\\\\n\\text{更新步骤(update step) 计算} & s_{t+1}|y_1,\\dots,y_{t+1}\n\\end{aligned}\n$$\n\n然后在时间步骤上迭代！预测和更新这两个步骤的结合就更新了我们的置信状态，也就是说整个过程大概类似:\n\n$$\n(s_{t}|y_1,\\dots,y_t)\\xrightarrow{predict} (s_{t+1}|y_1,\\dots,y_t)\n \\xrightarrow{update} (s_{t+1}|y_1,\\dots,y_{t+1})\\xrightarrow{predict}\\dots\n$$\n\n**预测步骤** 假如我们已知分布:\n\n$$\ns_{t}|y_1,\\dots,y_t\\sim \\mathcal{N}(s_{t|t},\\Sigma_{t|t})\n$$\n\n然后在下一个状态上的分布也是一个高斯分布:\n\n$$\ns_{t+1}|y_1,\\dots,y_t\\sim \\mathcal{N}(s_{t+1|t},\\Sigma_{t+1|t})\n$$\n\n其中有:\n\n$$\n\\begin{cases}\ns_{t+1|t}&=  A\\cdot s_{t|t}\\\\\n\\Sigma_{t+1|t} &= A\\cdot \\Sigma_{t|t}\\cdot A^T+\\Sigma_s\n\\end{cases}\n$$\n\n**更新步骤** 给定了$s_{t+1|t}$和$\\Sigma_{t+1|t}$，则有:\n\n$$\ns_{t+1}|y_1,\\dots,y_t\\sim \\mathcal{N}(s_{t+1|t},\\Sigma_{t+1|t})\n$$\n可以证明有:\n\n$$\ns_{t+1}|y_1,\\dots,y_{t+1}\\sim \\mathcal{N}(s_{t+1|t+1},\\Sigma_{t+1|t+1})\n$$\n\n其中有:\n\n$$\n\\begin{cases}\ns_{t+1|t+1}&= s_{t+1|t}+K_t(y_{t+1}-Cs_{t+1|t})\\\\\n\\Sigma_{t+1|t+1} &=\\Sigma_{t+1|t}-K_t\\cdot C\\cdot \\Sigma_{t+1|t}\n\\end{cases}\n$$\n\n上式中的\n\n$$\nK_t:= \\Sigma_{t+1|t} C^T (C \\Sigma_{t+1|t} C^T + \\Sigma_y)^{-1}\n$$\n\n这个矩阵$K_t$就叫做**卡尔曼增益(Kalman gain)。**\n\n现在如果我们仔细看看方程就会发现根本不需要对时间步骤 $t$ 有观测先验。更新步骤只依赖与前面的分布。综合到一起，这个算法最开始向前运行传递计算$K_t,\\Sigma_{t|t},s_{t|t}$（有时候可能值得是$\\hat s$）。然后就向后运行（进行LQR更新）来计算变量$\\Phi_t,\\Psi_t,L_t$了，最终就得到了最优策略$a^*_t=L_Ts_{t|t}$。\n\n\n### 参考","tags":["CS229"],"categories":["CS229"]},{"title":"[机器学习-CS229]强化学习和控制","url":"%2Fposts%2F332056ea%2F","content":"# CS229 课程讲义中文翻译\n\n| 原作者 | 翻译 | 校对 |\n| --- | --- | --- |\n| [Andrew Ng  吴恩达](http://www.andrewng.org/) | [CycleUser](https://www.zhihu.com/people/cycleuser/columns) | [XiaoDong_Wang](https://github.com/Dongzhixiao) |\n\n\n|相关链接|\n|---|\n|[Github 地址](https://github.com/Kivy-CN/Stanford-CS-229-CN)|\n|[知乎专栏](https://zhuanlan.zhihu.com/MachineLearn)|\n|[斯坦福大学 CS229 课程网站](http://cs229.stanford.edu/)|\n|[网易公开课中文字幕视频](http://open.163.com/movie/2008/1/M/C/M6SGF6VB4_M6SGHFBMC.html)|\n\n\n# 第十二章\n\n### 第十三部分 强化学习（Reinforcement Learning）和控制（Control）\n\n这一章我们就要学习强化学习（reinforcement learning）和适应性控制（adaptive control）了。\n\n在监督学习（supervised learning）中，我们已经见过的一些算法，输出的标签类 $y$ 都是在训练集中已经存在的。这种情况下，对于每个输入特征 $x$，都有一个对应的标签作为明确的“正确答案（right answer）”。与之相反，在很多的连续判断（sequential decisions making）和控制（control）的问题中，很难提供这样的明确的显示监督（explicit supervision）给学习算法。例如，假设咱们制作了一个四条腿的机器人，然后要编程让它能走路，而我们并不知道怎么去采取“正确”的动作来进行四条腿的行走，所以就不能给他提供一个明确的监督学习算法来进行模仿。\n\n在强化学习（reinforcement learning）的框架下，我们就并不提供监督学习中那种具体的动作方法，而是只给出一个奖励函数（reward function），这个函数会告知学习程序（learning agent） 什么时候的动作是好的，什么时候的是不好的。在四腿机器人这个样例中，奖励函数会在机器人有进步的时候给出正面回馈，即奖励，而有退步或者摔倒的时候给出负面回馈，可以理解成惩罚。接下来随着时间的推移，学习算法就会解决如何选择正确动作以得到最大奖励。\n\n强化学习（Reinforcement learning，下文中缩写为 RL）已经成功用于多种场景了，例如无人直升机的自主飞行，机器人用腿来运动，手机的网络选择，市场营销策略筛选，工厂控制，高效率的网页索引等等。我们对强化学习的探索，要先从**马尔可夫决策过程（Markov decision processes，缩写为 MDP）** 开始，这个概念给出了强化学习问题的常见形式。\n\n#### 1 马尔可夫决策过程（Markov decision processes）\n\n一个马尔可夫决策过程（Markov decision process）由一个元组（tuple） $(S, A, \\{P_{sa}\\}, \\gamma, R)$组成，其中元素分别为：\n\n- $S$ 是一个**状态**集合（a set of states）。（例如，在无人直升机飞行的案例中，$S$ 就可以是直升机所有的位置和方向的集合。）\n- $A$ 是一个**动作**集合（a set of actions）。（例如，还以无人直升机为例，$A$ 就可以是遥控器上面能够操作的所有动作方向。） \n- $P_{sa}$ 为状态转移概率（state transition probabilities）。对于每个状态 $s \\in S$ 和动作 $a \\in A$， $P_{sa}$ 是在状态空间上的一个分布（a distribution over the state space）。后面会再详细讲解，不过简单来说， $P_{sa}$ 给出的是在状态 $s$ 下进行一个动作 $a$ 而要转移到的状态的分布。 \n- $\\gamma \\in [0, 1)$ 叫做**折扣因子（discount factor）。**\n- $R : S × A → R$ 就是**奖励函数（reward function）。**（奖励函数也可以写成仅对状态 $S$ 的函数，这样就可以写成 $R : S → R$。）\n\n马尔可夫决策过程（MDP）的动力学（dynamics）过程如下所示：于某个起始状态 $s_0$ 启动，然后选择某个动作 $a_0 \\in A$ 来执行 MDP 过程。根据所选的动作会有对应的结果，MDP 的状态则转移到某个后继状态（successor state），表示为 $s_1$，根据 $s_1 \\sim P_{s_0a_0}$ 得到。然后再选择另外一个动作 $a_1$，接下来又有对应这个动作的状态转移，状态则为 $s_2 \\sim P_{s_1a_1}$。接下来再选择一个动作 $a_2$，就这样进行下去。如果将这个过程绘制出来的话，结果如下所示：\n\n$$\ns_0\\xrightarrow{a_0}s_1\\xrightarrow{a_1}s_2\\xrightarrow{a_2}s_3\\xrightarrow{a_3}\\dots\n$$\n\n通过序列中的所有状态 $s_0, s_1, \\dots$  和对应的动作 $a_0, a_1,\\dots$，你就能得到总奖励值，即总收益函数（total payoff）为 \n\n$$\nR(s_0,a_0) + \\gamma R(s_1,a_1) + \\gamma^2 R(s_2,a_2) + \\dots\n$$\n\n如果把奖励函数作为仅与状态相关的函数，那么这个值就简化成了 \n\n$$\nR(s_0) + \\gamma R(s_1) + \\gamma^2 R(s_2) + \\dots\n$$\n\n多数情况下，我们都用后面这种仅为状态的函数$R(s)$这种形式，虽然扩展到对应状态-动作两个变量的函数 $R(s,a)$ 也并不难。\n\n强化学习的目标就是找到的一组动作，能使得总收益函数（total payoff）的期望值最大：\n\n$$\nE[R(s_0) + \\gamma R(s_1) + \\gamma^2 R(s_2) + \\dots]\n$$\n\n注意，在时间步长（timestep） $t$ 上的奖励函数（reward）通过一个参数（factor）$\\gamma^t$ 而进行了**缩减（discounted）。** 因此，要使得期望最大化，就需要尽可能早积累符号为正的奖励（positive rewards），而尽量推迟负面奖励（negative rewards，即惩罚）的出现。在经济方面的应用中，其中的 $R(·)$ 就是盈利金额（amount of money made），$\\gamma$ 也可以理解为利润率（interest rate）的表征，这样有自然的解释（natural interpretation），例如今天的一美元就比明天的一美元有更多价值。\n\n有一种**策略（policy），** 是使用任意函数 $\\pi : S → A$，从状态（states）到动作（actions）进行映射（mapping）。如果在状态 $s$，采取动作 $a = \\pi(s)$，就可以说正在**执行（executing）** 某种策略（policy） $\\pi$。然后还可以针对策略函数（policy）$\\pi$ 来定义一个**值函数（value function）：**\n\n$$\nV^\\pi(s)=E[R(s_0) + \\gamma R(s_1) + \\gamma^2 R(s_2) + \\dots | s_0=s,\\pi]\n$$\n\n$V^\\pi(s)$ 就是从状态 $s$ 开始，根据 $\\pi^1$ 给出的动作来积累的部分奖励函数（discounted rewards）的期望总和（expected sum）。\n\n>1 实际上这里我们用 $\\pi$ 这个记号来表示，严格来说不太正确，因为 $\\pi$ 并不是一个随机变量，不过在文献里面这样表示很多，已经成了某种事实上的标准了。\n\n给定一个固定的策略函数（policy） $\\pi$，则对应的值函数 $V^\\pi$ 满足**贝尔曼等式（Bellman equations）：**\n\n$$\nV^\\pi(s)=R(s)+\\gamma \\sum_{s'\\in S}P_{s\\pi(s)}(s')V^\\pi(s')\n$$\n\n这也就意味着，从状态 $s$ 开始的这个部分奖励（discounted rewards）的期望总和（expected sum） $V^\\pi(s)$ 由两部分组成：首先是在状态 $s$ 时候当时**立即获得的奖励函数值** $R(s)$，也就是上面式子的第一项；另一个就是第二项，即后续的部分奖励函数值（discounted rewards）的期望总和（expected sum）。对第二项进行更深入的探索，就能发现这个求和项（summation term）可以写成 $E_{s'\\sim P_{s\\pi(s)}} [V^\\pi(s')]$ 的形式。这种形式也就是从状态 $s'$ 开始的这个部分奖励（discounted rewards）的期望总和（expected sum） $V^\\pi(s')$，此处的 $s'$ 是根据 $P_{s\\pi(s)}$ 分布的，在 MDP 过程中从状态 $s$ 采取第一个动作 $\\pi(s)$ 之后，确定了这个分布所在的空间。因此，上面的第二项实际上也就是给出了在 MDP 过程中第一步之后的部分奖励（discounted rewards）的期望总和（expected sum）。\n\n贝尔曼等式（Bellman’s equations）可以有效地解出 $V^\\pi$。尤其是在一个有限状态的 MDP 过程中，即 $(|S| < \\infty)$，我们可以把每个状态 $s$ 对应的 $V^\\pi (s)$ 的方程写出来。这样就得到了一系列的 $|S |$ 个线性方程，有 $|S |$ 个变量（也就是对应每个状态的未知的 $V^\\pi(s)$ ），这些 $V^\\pi(s)$ 都很容易解出来。\n\n然后可以定义出**最优值函数（optimal value function）**\n\n$$\nV^*(s)=\\max_\\pi V^\\pi(s)\\qquad(1)\n$$\n\n换一种说法，这个值也就是能用任意一种策略函数（policy）来获得的，最佳的可能部分奖励（discounted rewards）的期望总和（expected sum）。另外对于最优值函数（optimal value function），也有一个版本的贝尔曼等式（Bellman’s equations）：\n\n$$\nV^*(s)=R(s)+\\max_{a\\in A}\\gamma\\sum_{s'\\in S}P_{sa}(s')V^*(s')\\qquad(2)\n$$\n\n上面这个等式中的第一项，还是跟之前一样的，还是即时奖励函数值。第二项是在采取了动作 $a$ 之后的所有动作 $a$ 的部分奖励（discounted rewards）的未来期望总和（expected future sum）的最大值。要确保理解这个等式，并且要明白为什么这个等式有意义。\n`译者注：抱歉，这里的这个 discounted rewards 弄得我不知道怎么翻译才顺，意思表达得很狗，非常抱歉。`\n\n另外还定义了一个策略函数（policy） $\\pi^* : S \\to A$，如下所示\n\n$$\n\\pi^*(s)=arg\\max_{a\\in A}\\sum_{s'\\in S}P_{sa}(s')V^*(s')\\qquad(3)\n$$\n\n注意，这里的 $\\pi^*(s)$ 给出的动作 $a$ 实现了上面等式$(2)$当中能够使 “max” 项取到最大值。\n\n事实上，对于每个状态 $s$ 和每个策略函数（policy）$\\pi$，我们都可以得出：\n\n$$\nV^*(s)=V^{\\pi^*}(s)\\ge V^\\pi(s)\n$$\n\n上面的第一个等式关系表明，在任何状态 $s$ 下，对应策略函数（policy） $V^{\\pi^*}$的值函数（value function）$\\pi^*$ 等于最优值函数 $V^*$。右边的不等式则表明，$\\pi^*$ 的值至少也等于任意其他策略函数的值。也就是说，上面在等式$(3)$当中定义的这个 $\\pi^*$ 就是最佳策略函数（optimal policy）。\n\n注意，这个 $\\pi^*$ 有一个有趣的特性，它是所有状态 $s$ 下的最佳策略。具体来讲，并不是说只是从某些状态 $s$ 开始的MDP过程才使得这个$\\pi^*$是对应这些状态的最佳策略，而如果从某些别的状态 $s'$ 开始就有其他的最佳策略。而是对于所有的状态 $s$，都是同样的一个策略函数 $\\pi^*$ 能够使得等式$(1)$中的项目取得最大值。这也就意味着无论 MDP 过程的初始状态（initial state）如何，都可以使用同样的策略函数 $\\pi^*$。\n\n#### 2 值迭代（Value iteration）和策略迭代（policy iteration） \n\n现在我们要讲两种算法，都能很有效地解决有限状态的马尔可夫决策过程问题（finite-state MDPs）。目前为止，我们只考虑有限状态和动作空间的马尔可夫决策过程，也就是状态和动作的个数都是有限的，即$|S| < \\infty, |A| < \\infty$。\n\n第一种算法，**值迭代（value iteration），** 过程如下所述：\n1. 对每个状态 $s$, 初始化 $V (s) := 0$.\n2. 重复直到收敛 {\n\n&emsp;&emsp;对每个状态，更新规则$V(s):=R(s)+\\max_{a\\in A}\\gamma\\sum_{s'}P_{sa}(s')V(s')$ \n\n}\n\n这个算法可以理解成，利用贝尔曼等式（Bellman Equations）$(2)$重复更新估计值函数（estimated value function）。\n\n在上面的算法的内部循环体中，有两种进行更新的方法。首先，我们可以为每一个状态 $s$ 计算新的值 $V(s)$，然后用新的值覆盖掉所有的旧值。这也叫做**同步更新（synchronous update）。** 在这种情况下，此算法可以看做是实现（implementing）了一个“贝尔曼备份运算符（Bellman backup operator）”，这个运算符接收值函数（value function）的当前估计（current estimate），然后映射到一个新的估计值（estimate）。（更多细节参考作业题目中的内容。）另外一种方法，即我们可以使用**异步更新（asynchronous updates）。** 使用这种方法，就可以按照某种次序来遍历（loop over）所有的状态，然后每次更新其中一个的值。\n\n无论是同步还是异步的更新，都能发现最终值迭代（value iteration）会使 $V$ 收敛到 $V^*$ 。找到了 $V^*$ 之后，就可以利用等式$(3)$来找到最佳策略（optimal policy）。\n\n除了值迭代（value iteration）之外，还有另外一种标准算法可以用来在马尔可夫决策过程（MDP）中寻找一个最佳策略（optimal policy）。这个**策略迭代（policy iteration）** 算法如下所述：\n\n1. 随机初始化 $\\pi$。\n2. 重复直到收敛{\n\n&emsp;&emsp;$(a)$ 令 $V := V^\\pi$. \n\n&emsp;&emsp;$(b)$ 对每个状态 $s$，令 $\\pi(s):=arg\\max_{a\\in A}\\sum_{s'}P_{sa}(s')V(s')$\n\n}\n\n因此，在循环体内部就重复计算对于当前策略（current policy）的值函数（value function），然后使用当前的值函数（value function）来更新策略函数（policy）。（在步骤 $(b)$ 中找到的策略 $\\pi$ 也被称为对应 $V$ 的**贪心策略(greedy with respect to V)** ）注意，步骤 $(a)$ 可以通过解贝尔曼等式（Bellman’s equation）来实现，之前已经说过了，在固定策略（fixed policy）的情况下，这个等式只是一系列有 $|S|$ 个变量（variables）的 $|S|$ 个线性方程（linear equations）。\n\n在上面的算法迭代了某个最大迭代次数之后，$V$ 将会收敛到 $V^*$，而 $\\pi$ 会收敛到 $\\pi^*$。\n\n值迭代（value iteration）和策略迭代（policy iteration）都是解决马尔可夫决策过程（MDPs）问题的标准算法， 而且目前对于这两个算法哪个更好，还没有一个统一的一致意见。对小规模的 MDPs 来说，策略迭代（policy iteration）通常非常快，迭代很少的次数就能瘦脸。然而，对有大规模状态空间的 MDPs，确切求解 $V^\\pi$就要涉及到求解一个非常大的线性方程组系统，可能非常困难。对于这种问题，就可以更倾向于选择值迭代（value iteration）。因此，在实际使用中，值迭代（value iteration）通常比策略迭代（policy iteration）更加常用。\n\n#### 3 学习一个马尔可夫决策过程模型（Learning a model for an MDP） \n\n目前为止，我们已经讲了 MDPs，以及用于 MDPs 的一些算法，这都是基于一个假设，即状态转移概率（state transition probabilities）以及奖励函数（rewards）都是已知的。在很多现实问题中，却未必知道这两样，而是必须从数据中对其进行估计。（通常 $S，A 和 \\gamma$ 都是知道的。）\n\n例如，加入对倒立摆问题（inverted pendulum problem，参考习题集 $4$），在 MDP 中进行了一系列的试验，过程如下所示：\n\n$$\n\\begin{aligned}\n&s_0^{(1)}\\xrightarrow{a_0^{(1)}}s_1^{(1)}\\xrightarrow{a_1^{(1)}}s_2^{(1)}\\xrightarrow{a_2^{(1)}}s_3^{(1)}\\xrightarrow{a_3^{(1)}}\\dots \\\\\n&s_0^{(2)}\\xrightarrow{a_0^{(2)}}s_1^{(2)}\\xrightarrow{a_1^{(2)}}s_2^{(2)}\\xrightarrow{a_2^{(2)}}s_3^{(2)}\\xrightarrow{a_3^{(2)}}\\dots  \\\\\n&\\cdots\n\\end{aligned}\n$$\n\n其中 $s_i^{(j)}$ 表示的是第 $j$ 次试验中第 $i$ 次的状态，而 $a_i^{(j)}$ 是该状态下的对应动作。在实践中，每个试验都会运行到 MDP 过程停止（例如在倒立摆问题（inverted pendulum problem）中杆落下（pole falls）），或者会运行到某个很大但有限的一个数的时间步长（timesteps）。\n\n有了在 MDP 中一系列试验得到的“经验”，就可以对状态转移概率（state transition probabilities）推导出最大似然估计（maximum likelihood estimates）了：\n\n$$\nP_{sa}(s')= \\frac{\\text{在状态 s 执行动作 a 而到达状态 s' 花的时间}}{\\text{在状态 s 执行动作 a 花的时间}}\\qquad(4)\n$$\n\n或者，如果上面这个比例出现了$0/0$的情况，对应的情况就是在状态 $s$ 之前没进行过任何动作 $a$，这样就可以简单估计 $P_{sa}(s')$ 为 $1/|S|$。（也就是说把 $P_{sa}$ 估计为在所有状态上的均匀分布（uniform distribution）。）\n\n注意，如果在 MDP 过程中我们能获得更多经验信息（观察更多次数），就能利用新经验来更新估计的状态转移概率（estimated state transition probabilities），这样很有效率。具体来说，如果我们保存下来等式$(4)$中的分子（numerator）和分母（denominator）的计数（counts），那么观察到更多的试验的时候，就可以很简单地累积（accumulating）这些计数数值。计算这些数值的比例，就能够给出对 $P_{sa}$ 的估计。\n\n利用类似的程序（procedure），如果奖励函数（reward） $R$ 未知，我们也可以选择在状态 $s$ 下的期望即时奖励函数（expected immediate reward） $R(s)$ 来当做是在状态 $s$ 观测到的平均奖励函数（average reward）。\n\n学习了一个 MDP 模型之后，我们可以使用值迭代（value iteration）或者策略迭代（policy iteration），利用估计的状态转移概率（transition probabilities）和奖励函数，来去求解这个 MDP 问题。例如，结合模型学习（model learning）和值迭代（value iteration），就可以在未知状态转移概率（state transition probabilities）的情况下对 MDP 进行学习，下面就是一种可行的算法：\n\n1. 随机初始化 $\\pi$ 。 \n2. 重复 {\n\n&emsp;&emsp;$(a)$ 在 MDP 中执行 $\\pi$ 作为若干次试验（trials）。\n\n&emsp;&emsp;$(b)$ 利用上面在 MDP 积累的经验（accumulated experience），更新对 $P_{sa}$ 的估计（如果可以的话也对奖励函数 $R$ 进行更新）。\n\n&emsp;&emsp;$(c)$ 利用估计的状态转移概率（estimated state transition probabilities）和奖励函数\n（rewards），应用值迭代（value iteration），得到一个新的估计值函数（estimated value function） $V$。\n\n&emsp;&emsp;$(d)$ 更新 $\\pi$ 为与 $V$ 对应的贪婪策略（greedy policy）。\n\n}\n我们注意到，对于这个特定的算法，有一种简单的优化方法（optimization），可以让该算法运行得更快。具体来说，在上面算法的内部循环中，使用了值迭代（value iteration），如果初始化迭代的时候不令 $V = 0$ 启动，而是使用算法中上一次迭代找到的解来初始化，这样就有了一个更好的迭代起点，能让算法更快收敛。\n\n#### 4 连续状态的马尔可夫决策过程（Continuous state MDPs）\n\n目前为止，我们关注的都是有限个状态（a finite number of states）的马尔可夫决策过程（MDPs）。接下来我们要讲的就是有无限个状态（an infinite number of states）的情况下的算法。例如，对于一辆车，我们可以将其状态表示为 $(x, y, \\theta, \\dot x,\\dot y,\\dot\\theta)$，其中包括位置（position）  $(x, y)$，方向（orientation）$\\theta$， 在 $x$ 和 $y$ 方向的速度分量 $\\dot x$ 和 $\\dot y$，以及角速度（angular velocity）$\\dot\\theta$。这样，$S = R^6$ 就是一个无限的状态集合，因为一辆车的位置和方向的个数是有无限可能$^2$。与此相似，在习题集 $4$ 中看到的倒立摆问题（inverted pendulum）中，状态也有$(x,\\theta,\\dot x,\\dot\\theta)$，其中的 $\\theta$ 是杆的角度。在直升机飞行的三维空间中，状态的形式则为$(x,y,x,\\phi,\\theta,\\psi,\\dot x,\\dot y,\\dot z,\\dot\\phi,\\dot\\theta,\\dot\\psi)$，其中包含了滚动角（roll）$\\phi$，俯仰角（pitch）$\\theta$，以及偏航角（yaw）$\\psi$，这几个角度确定了直升机在三维空间中的运动方向。在本节中，我们考虑状态空间为 $S = R^n$ 的情况，并描述此种情况下解决 MDPs 的方法。\n\n>2 从理论上讲，$\\theta$ 是一个方向（orientation），所以更应当把 $\\theta$ 的取值空间写为 $\\theta \\in [\\pi, \\pi)$，而不是写为实数集合 $\\theta \\in R$；不过在我们讨论的问题中，这种区别不要紧。\n\n##### 4.1 离散化（Discretization）\n\n解决连续状态 MDP 问题最简单的方法可能就是将状态空间（state space）离散化（discretize），然后再使用之前讲过的算法，比如值迭代（value iteration）或者策略迭代（policy iteration）来求解。\n\n例如，假设我们有一个二维状态空间$(s_1,s_2)$，就可以用下面的网格（grid）来将这个状态空间离散化：\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note12f1.png)\n\n如上图所示，每个网格单元（grid cell）表示的都是一个独立的离散状态 $\\overline s$。这样就可以把一个连续状态 MDP 用一个离散状态的 $(\\overline S, A, \\{P_{\\overline sa}\\}, \\gamma, R)$ 来进行逼近，其中的$\\overline S$ 是离散状态集合，而$\\{P_{\\overline sa}\\}$ 是此离散状态上的状态转移概率（state transition probabilities），其他项目同理。然后就可以使用值迭代（value iteration）或者策略迭代（policy iteration）来求解出离散状态的 MDP $(\\overline S, A, \\{P_{\\overline sa}\\}, \\gamma, R)$ 的 $V^*(\\overline s)$ 和 $\\pi^*(\\overline s)$。当真实系统是某种连续值的状态 $s \\in S$，而有需要选择某个动作来执行，就可以计算对应的离散化的状态 $\\overline s$，然后执行对应的动作 $\\pi^*(\\overline s)$。\n\n这种离散化方法（discretization approach）可以解决很多问题。然而，也有两个缺陷（downsides）。首先，这种方法使用了对 $V^*$ 和 $\\pi^*$ 相当粗糙的表征方法。具体来说，这种方法中假设了在每个离散间隔（discretization intervals）中的值函数（value function）都是一个常数值（也就是说，值函数是在每个网格单元中分段的常数。）。\n\n要更好理解这样表征的的局限性，可以考虑对下面这一数据集进行函数拟合的监督学习问题：\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note12f2.png)\n\n很明显，上面这个数据适合使用线性回归。然而，如果我们对 $x$ 轴进行离散化，那么在每个离散间隔中使用分段常数表示，对同样的数据进行拟合，得到的曲线则如下所示：\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note12f3.png)\n\n这种分段常数表示，对于很多的光滑函数，都不能算好。这会导致输入值缺乏平滑（little smoothing over the inputs），而且在不同的望各单元中间也没有进行扩展（generalization）。使用这种表示方法，我们还需要一种非常精细的离散化过程（也就是网格单元要非常小），才能得到一个比较好的近似估计。\n\n第二个缺陷可以称之为**维度的诅咒（curse of dimensionality）。** 设 $S = R^n$ ，然后我们队每个 $n$ 维度状态离散成 $k$ 个值。这样总共的离散状态的个数就是 kn。在状态空间 $n$ 的维度中，这个值会呈指数级增长，对于大规模问题就不好缩放了。例如，对于一个 $10$ 维的状态，如果我们把每个状态变量离散化成为 $100$ 个值，那么就会有 $100^{10} = 10^{20}$ 个离散状态，这个维度太大了，远远超过了当前桌面电脑能应付的能力之外。\n\n根据经验法则（rule of thumb），离散化通常非常适合用于 $1$ 维和 $2$ 维的问题（而且有着简单和易于快速实现的优势）。对于 $4$ 维状态的问题，如果使用一点小聪明，仔细挑选离散化方法，有时候效果也不错。如果你超级聪明，并且还得有点幸运，甚至也有可能将离散化方法使用于 $6$ 维问题。不过在更高维度的问题中，就更是极其难以使用这种方法了。\n\n##### 4.2 值函数近似（Value function approximation）\n\n现在我们来讲另外一种方法，能用于在连续状态的 MDPs 问题中找出策略，这种方法也就是直接对进行近似 $V^*$，而不使用离散化。这个方法就叫做值函数近似（value function approximation），在很多强化学习的问题中都有成功的应用。\n\n###### 4.2.1 使用一个模型或模拟器（Using a model or simulator）\n\n要开发一个值函数近似算法，我们要假设已经有一个对于 MDP 的**模型，** 或者**模拟器。** 简单来看，一个模拟器就是一个黑箱子（black-box），接收输入的任意（连续值的）状态 $s_t$ 和动作 $a_t$，然后输出下一个状态 $s_{t+1}$，这个新状态是根据状态转移概率（state transition probabilities） $P_{s_ta_t}$ 取样（sampled）得来：\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note12f4.png)\n\n有很多种方法来获取这样的一个模型。其中一个方法就是使用物理模拟（physics simulation）。 例如，在习题集 $4$ 中倒立摆模拟器，就是使用物理定律，给定当前时间 $t$ 和采取的动作 $a$，假设制导系统的所有参数，比如杆的长度、质量等等，来模拟计算在 $t+1$ 时刻杆所处的位置和方向。另外也可以使用现成的物理模拟软件包，这些软件包将一个机械系统的完整物理描述作为输入，当前状态 $s_t$ 和动作 $a_t$，然后计算出未来几分之一秒的系统状态 $s_{t+1}$。$^3$\n\n>3 开放动力引擎（Open Dynamics Engine，`http://www.ode.com`）就是一个开源物理模拟器，可以用来模拟例如倒立摆这样的系统，在强化学习研究领域中，已经相当流行了。\n\n另外一个获取模型的方法，就是从 MDP 中收集的数据来学习生成一个。例如，加入我们在一个 MDP 过程中重复进行了 $m$ 次**试验（trials），** 每一次试验的时间步长（time steps）为 $T$。这可以用如下方式实现，首先是随机选择动作，然后执行某些特定策略（specific policy），或者也可以用其他方法选择动作。接下来就能够观测到 $m$ 个状态序列，如下所示：\n\n$$\n\\begin{aligned}\n&s_0^{(1)}\\xrightarrow{a_0^{(1)}}s_1^{(1)}\\xrightarrow{a_1^{(1)}}s_2^{(1)}\\xrightarrow{a_2^{(1)}}\\dots\\xrightarrow{a_{T-1}^{(1)}}s_T^{(1)} \\\\\n&s_0^{(2)}\\xrightarrow{a_0^{(2)}}s_1^{(2)}\\xrightarrow{a_1^{(2)}}s_2^{(2)}\\xrightarrow{a_2^{(2)}}\\dots\\xrightarrow{a_{T-1}^{(2)}}s_T^{(2)}  \\\\\n&\\cdots \\\\\n&s_0^{(m)}\\xrightarrow{a_0^{(m)}}s_1^{(m)}\\xrightarrow{a_1^{(m)}}s_2^{(m)}\\xrightarrow{a_2^{(m)}}\\dots\\xrightarrow{a_{T-1}^{(m)}}s_T^{(m)}\n\\end{aligned}\n$$\n\n然后就可以使用学习算法，作为一个关于 $s_t$  和 $a_t$ 的函数来预测 $s_{t+1}$。\n\n例如，对于线性模型的学习，可以选择下面的形式：\n\n$$\ns_{t+1}=As_t+Ba_t\\qquad(5)\n$$\n\n然后使用类似线性回归（linear regression）之类的算法。上面的式子中，模型的参数是两个矩阵 $A$ 和 $B$，然后可以使用在 $m$ 次试验中收集的数据来进行估计，选择：\n\n$$\narg\\min_{A,B}\\sum_{i=1}^m\\sum_{t=0}^{T-1}||s_{t+1}^{(i)}-(As_t^{(i)}+Ba_t^{(i)})||^2\n$$\n\n（这对应着对参数（parameters）的最大似然估计（maximum likelihood estimate）。）\n\n通过学习得到 $A$ 和 $B$ 之后，一种选择就是构建一个**确定性** 模型（deterministic model），在此模型中，给定一个输入 $s_t$ 和 $a_t$，输出的则是固定的 $s_{t+1}$。具体来说，也就是根据上面的等式$(5)$来计算 $s_{t+1}$。或者用另外一种办法，就是建立一个**随机** 模型（stochastic model），在这个模型中，输出的 $s_{t+1}$ 是关于输入值的一个随机函数，以如下方式建模：\n\n$$\ns_{t+1}=As_t+Ba_t+\\epsilon_t\n$$\n\n上面式子中的 $\\epsilon_t$ 是噪音项（noise term），通常使用一个正态分布来建模，即 $\\epsilon_t\\sim N (0, \\Sigma)$。（协方差矩阵（covariance matrix） $\\Sigma$ 也可以从数据中直接估计出来。）\n\n这里，我们把下一个状态 $s_{t+1}$ 写成了当前状态和动作的一个线性函数；不过当然也有非线性函数的可能。比如我们学习一个模型 $s_{t+1} = A\\phi_s(s_t) + B\\phi_a(a_t)$，其中的 $\\phi_s$ 和 $\\phi_a$ 就可以使某些映射了状态和动作的非线性特征。另外，我们也可以使用非线性的学习算法，例如局部加权线性回归（locally weighted linear regression）进行学习，来将 $s_{t+1}$ 作为关于 $s_t$ 和 $a_t$ 的函数进行估计。 这些方法也可以用于建立确定性的（deterministic）或者随机的（stochastic）MDP 模拟器。\n\n###### 4.2.2 拟合值迭代（Fitted value iteration）\n\n接下来我们要讲的是**拟合值迭代算法（fitted value iteration algorithm），** 作为对一个连续状态 MDP 中值函数的近似。在这部分钟，我们假设学习问题有一个连续的状态空间 $S = R^n$，而动作空间 $A$ 则是小规模的离散空间。$^4$\n\n>4 在实践中，大多数的 MDPs 问题中，动作空间都要远远比状态空间小得多。例如，一辆汽车可能有 $6$维的状态空间，但是动作空间则只有 $2$维，即转向和速度控制；倒立的摆有 $4$维状态空间，而只有 $1$维的动作空间；一架直升机有 $12$维状态空间，只有 $4$维的动作空间。所以对动作空间进行离散化，相比对状态空间进行离散化，遇到的问题通常会少得多。\n\n回忆一下值迭代（value iteration），其中我们使用的更新规则如下所示：\n\n$$\n\\begin{aligned}\nV(s) &:= R(s)+\\gamma\\max_a \\int_{s'}P_{sa}(s')V(s')ds' \\qquad&(6)\\\\\n&= R(s)+\\gamma\\max_a E_{s'\\sim P_{sa}}[V(s')]\\qquad&(7)\n\\end{aligned}\n$$\n\n（在第二节当中，我们把值迭代的更新规则写成了求和（summation）的形式：$V(s) := R(s)+\\gamma\\max_a\\sum_{s'}P_{sa}(s')V(s')$而没有像刚刚上面这样写成在状态上进行积分的形式；这里采用积分的形式来写，是为了表达我们现在面对的是连续状态的情况，而不再是离散状态。）\n\n拟合值迭代（fitted value iteration）的主要思想就是，在一个有限的状态样本 $s^{(1)}, ...  s^{(m)}$ 上，近似执行这一步骤。具体来说，要用一种监督学习算法（supervised learning algorithm），比如下面选择的就是线性回归算法（linear regression），以此来对值函数（value function）进行近似，这个值函数可以使关于状态的线性或者非线性函数：\n\n$$\nV(s)=\\theta^T\\phi(s)\n$$\n\n上面的式子中，$\\phi$ 是对状态的某种适当特征映射（appropriate feature mapping）。对于有限个 $m$ 状态的样本中的每一个状态 $s$，拟合值迭代算法将要首先计算一个量 $y^{(i)}$，这个量可以用 $R(s)+\\gamma\\max_aE_{s'\\sim P_{sa}}[V(s')]$ 来近似（根据等式$(7)$的右侧部分）。然后使用一个监督学习算法，通过逼近 $R(s) + \\gamma\\max_a E_{s'\\sim P_{sa}}[V (s')]$ 来得到$V(s)$（或者也可以说是通过逼近到 $y^{(i)}$ 来获取 $V(s)$）。\n\n具体来说，算法如下所示：\n\n1. 从 $S$ 中随机取样 $m$ 个状态 $s^{(1)}, s^{(2)}, . . . s^{(m)}\\in S$。\n2. 初始化 $\\theta := 0$.\n3. 重复 {\n\n&emsp;&emsp;对 $i = 1, ... , m$ {\n\n&emsp;&emsp;&emsp;&emsp;对每一个动作 $a \\in A$ {\n\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;取样  $s_1',... , s_k'\\sim P_{s^{(i)}a}$   (使用一个 MDP 模型)\n\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;设$q(a)=\\frac 1k\\sum_{j=1}^kR(s^{(i)})+\\gamma V(s_j')$\n\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;// 因此， $q(a)$ 是对$R(s)^{(i)}+\\gamma E_{s'\\sim P_{sa}}[V(s')]$的估计。\n\n&emsp;&emsp;&emsp;&emsp;}\n\n&emsp;&emsp;&emsp;&emsp;设$y^{(i)} = \\max_a q(a)$.\n\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;// 因此， $y^{(i)}$是对$R(s^{(i)})+\\gamma\\max_aE_{s'\\sim P_{sa}}[V(s')]$的估计。\n\n&emsp;&emsp;&emsp;&emsp;}\n\n&emsp;&emsp;&emsp;&emsp;// 在原始的值迭代算法（original value iteration algorithm）中，（离散状态的情况 ）\n\n&emsp;&emsp;&emsp;&emsp;// 是根据 $V(s^{(i)}) := y^{(i)}$ 来对值函数（value function）进行更新。\n\n&emsp;&emsp;&emsp;&emsp;// 而在这里的这个算法中，我们需要的让二者近似相等，即 $V(s^{(i)}) \\approx y^{(i)}$，\n\n&emsp;&emsp;&emsp;&emsp;// 这可以通过使用监督学习算法（线性回归）来实现。\n\n&emsp;&emsp;&emsp;&emsp;设 $\\theta := arg\\min_\\theta \\frac 12\\sum_{i=1}^m(\\theta^T\\phi(s^{(i)})-y^{(i)})^2$\n\n&emsp;&emsp;}\n\n以上，我们就写出了一个拟合值迭代算法（fitted value iteration），其中使用线性回归作为算法（linear regression），使 $V (s^{(i)})$ 逼近 $y^{(i)}$。这个步骤完全类似在标准监督学习问题（回归问题）中面对 $m$ 个训练集 $(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})$ ，而要利用学习得到从$x$ 到 $y$ 的映射函数的情况；唯一区别无非是这里的 $s$ 扮演了当时 $x$ 的角色。虽然我们上面描述的算法是线性回归，很显然其他的回归算法（例如局部加权线性回归）也都可以使用。\n\n与离散状态集合上进行的的值迭代（value iteration）不同，拟合值迭代（fitted value iteration）并不一定总会收敛（converge）。然而，在实践中，通常都还是能收敛的（或者近似收敛），而且能解决大多数问题。另外还要注意，如果我们使用一个 MDP 的确定性模拟器/模型的话，就可以对拟合值迭代进行简化，设置算法中的 $k = 1$。这是因为等式$(7)$当中的期望值成为了对确定性分布（deterministic distribution）的期望，所以一个简单样本（single example）就足够计算该期望了。否则的话，在上面的算法中，就还要取样出 $k$ 个样本，然后取平均值，来作为对期望值的近似（参考在算法伪代码中的 $q(a)$ 的定义）。\n\n最后，拟合值迭代输出的 $V$，也就是对 $V^*$ 的一个近似。这同时隐含着对策略函数（policy）的定义。 具体来说，当我们的系统处于某个状态 $s$ 的时候，需要选择一个动作，我们可能会选择的动作为：\n\n$$\narg\\max_a E_{s'\\sim P_{sa}}[V(s')]\\qquad(8)\n$$\n\n这个计算/近似的过程很类似拟合值迭代算法的内部循环体，其中对于每一个动作，我们取样 $s_1',...,s_k'\\sim P_{sa}$ 来获得近似期望值（expectation）。（当然，如果模拟器是确定性的，就可以设 $k = 1$。）\n\n在实际中，通常也有其他方法来实现近似这个步骤。例如，一种很常用的情况就是如果模拟器的形式为 $s_{t+1} = f(s_t,a_t) + \\epsilon_t$，其中的 $f$ 是某种关于状态 $s$ 的确定性函数（例如 $f(s_t,a_t) = As_t + Ba_t$），而 $\\epsilon$ 是均值为 $0$ 的高斯分布的噪音。在这种情况下，可以通过下面的方法来挑选动作：\n\n$$\narg\\max_a V(f(s,a))\n$$\n\n也就是说，这里只是设置 $\\epsilon_t = 0$（即忽略了模拟器中的噪音项），然后设 $k = 1$。同样地，这也可以通过在等式$(8)$中使用下面的近似而推出：\n\n$$\n\\begin{aligned}\nE_{s'}[V(s')] &\\approx V(E_{s'}[s']) &(9) \\\\\n&= V(f(s,a)) &(10)\n\\end{aligned}\n$$\n\n这里的期望是关于随机分布 $s'\\sim P_{sa}$ 的。所以只要噪音项目 $\\epsilon_t$ 很小，这样的近似通常也是合理的。 \n\n然而，对于那些不适用于这些近似的问题，就必须使用模型，取样 $k|A|$ 个状态，以便对上面的期望值进行近似，当然这在计算上的开销就很大了。\n\n\n### 参考","tags":["CS229"],"categories":["CS229"]},{"title":"[机器学习-CS229]独立成分分析","url":"%2Fposts%2F1c66e34a%2F","content":"# CS229 课程讲义中文翻译\n\n| 原作者 | 翻译 | 校对 |\n| --- | --- | --- |\n| [Andrew Ng  吴恩达](http://www.andrewng.org/) | [CycleUser](https://www.zhihu.com/people/cycleuser/columns) | [XiaoDong_Wang](https://github.com/Dongzhixiao) |\n\n\n|相关链接|\n|---|\n|[Github 地址](https://github.com/Kivy-CN/Stanford-CS-229-CN)|\n|[知乎专栏](https://zhuanlan.zhihu.com/MachineLearn)|\n|[斯坦福大学 CS229 课程网站](http://cs229.stanford.edu/)|\n|[网易公开课中文字幕视频](http://open.163.com/movie/2008/1/M/C/M6SGF6VB4_M6SGHFBMC.html)|\n\n\n# 第十一章\n\n### 第十二部分 独立成分分析（Independent Components Analysis ）\n\n接下来我们要讲的主体是独立成分分析（Independent Components Analysis，缩写为 ICA）。这个方法和主成分分析（PCA）类似，也是要找到一组新的基向量（basis）来表征（represent）样本数据。然而，这两个方法的目的是非常不同的。\n\n还是先用“鸡尾酒会问题（cocktail party problem）”为例。在一个聚会场合中，有 $n$ 个人同时说话，而屋子里的任意一个话筒录制到底都只是叠加在一起的这 $n$ 个人的声音。但如果假设我们也有 $n$ 个不同的话筒安装在屋子里，并且这些话筒与每个说话人的距离都各自不同，那么录下来的也就是不同的组合形式的所有人的声音叠加。使用这样布置的 $n$ 个话筒来录音，能不能区分开原始的 $n$ 个说话者每个人的声音信号呢？\n\n把这个问题用方程的形式来表示，我们需要先假设有某个样本数据 $s \\in R^n$，这个数据是由 $n$ 个独立的来源（independent sources）生成的。我们观察到的则为：\n\n$$x = As,$$\n\n上面式子中的 $A$ 是一个未知的正方形矩阵（square matrix），叫做**混合矩阵（mixing matrix）。** 通过重复的观察，我们就得到了训练集 $\\{x^{(i)} ; i = 1, . . . , m\\}$，然后我们的目的是恢复出生成这些样本 $x^{(i)} = As^{(i)}$ 的原始声音源 $s^{(i)}$ 。\n\n在咱们的“鸡尾酒会问题”中，$s^{(i)}$ 就是一个 $n$ 维度向量，而 $s_j^{(i)}$ 是第 $j$ 个说话者在第 $i$ 次录音时候发出的声音。$x^{(i)}$ 同样也是一个 $n$ 维度向量，而 $x_j^{(i)}$是第 $j$ 个话筒在第 $i$ 次录制到的声音。\n\n设混合矩阵 $A$ 的逆矩阵 $W = A^{-1}$ 是混合的逆向过程，称之为**还原矩阵（unmixing matrix）。** 那么咱们的目标就是找出这个 $W$，这样针对给定的话筒录音 $x^{(i)}$，我们就可以通过计算 $s^{(i)} = Wx^{(i)}$ 来还原出来声音源。为了方便起见，我们就用 $w_i^T$ 来表示 $W$ 的第 $i$ 行，这样就有：\n\n$$\nw=\\begin{bmatrix}\n-w_1^T- \\\\\n\\vdots \\\\\n-w_n^T-\n\\end{bmatrix}\n$$\n\n这样就有 $w_i \\in R^n$，通过计算 $s_j^{(i)} = w_j^T x^{(i)}$ 就可以恢复出第 $j$ 个声源了。\n\n#### 1 独立成分分析（ICA）的模糊性（ambiguities）\n\n$W = A^{-1}$ 能恢复到怎样的程度呢？如果我们对声源和混合矩阵都有预先的了解（prior knowledge），那就不难看出，混合矩阵 $A$ 当中存在的某些固有的模糊性，仅仅给定了 $x^{(i)}$ 可能无法恢复出来。 \n\n例如，设 $p$ 是一个 $n×n$ 的置换矩阵（permutation matrix）。这就意味着矩阵 $P$ 的每一行和每一列都只有一个 $1$。下面就是几个置换矩阵的样例：\n\n$$\nP=\\begin{bmatrix}\n0&1&0 \\\\\n1&0&0 \\\\\n0&0&1\n\\end{bmatrix};\\quad\nP=\\begin{bmatrix}\n0&1 \\\\\n1&0\n\\end{bmatrix};\\quad\nP=\\begin{bmatrix}\n1&0 \\\\\n0&1\n\\end{bmatrix}\n$$\n\n如果 $z$ 是一个向量，那么 $Pz$ 就是另外一个向量，这个向量包含了 $z$ 坐标的置换版本（permuted version）。如果只给出 $x^{(i)}$，是没有办法区分出 $W$ 和 $PW$ 的。具体来说，原始声源的排列（permutation）是模糊的（ambiguous），这一点也不奇怪。好在大多数情况下，这个问题都并不重要。\n\n进一步来说，就是没有什么办法能恢复出 $w_i$ 的正确的缩放规模。例如，如果把$A$ 替换成了 $2A$，那么每个 $s^{(i)}$ 都替换成了 $(0.5)s^{(i)}$，那么观测到的 $x^{(i)} = 2A · (0.5)s^{(i)}$ 还是跟原来一样的。再进一步说，如果 $A$ 当中的某一列，都用一个参数 $\\alpha$ 来进行缩放，那么对应的音源就被缩放到了 $1/\\alpha$，这也表明，仅仅给出 $x^{(i)}$，是没办法判断这种情况是否发生的。因此，我们并不能还原出音源的“正确”缩放规模。然而，在我们应用的场景中，例如本文提到的这个“鸡尾酒会问题”中，这种不确定性并没有关系。具体来说，对于一个说话者的声音信号 $s^{(i)}$ 的缩放参数 $\\alpha$ 只影响说话者声音的大小而已。另外，符号变换也没有影响，因为$s_j^{(i)}$  和 $-s_j^{(i)}$  都表示了扬声器中同样的声音大小。所以，如果算法找到的 $w_i$ 被乘以任意一个非零数进行了缩放，那么对应的恢复出来的音源 $s_i = w_i^T x$ 也进行了同样的缩放；这通常都不要紧。（这些考量也适用于课堂上讨论的对 Brain/MEG 数据使用的 ICA 算法。）\n\n上面这些是 ICA 算法模糊性的唯一来源么？还真是这样，只要声源 $s_i$ 是非高斯分布（non-Gaussian）的即可。如果是高斯分布的数据（Gaussian data），例如一个样本中，$n = 2$，而 $s\\sim N(0,I)$ 。**（译者注：即 $s$ 是一个以 $0$ 和 $I$ 为参数的正态分布，正态分布属于高斯分布）** 其中的 $I$ 是一个 $2×2$ 的单位矩阵（identity matrix）。要注意，这是一个标准正态分布，其密度（density）轮廓图（contour）是以圆点为中心的圆，其密度是旋转对称的（rotationally symmetric）。\n\n接下来，假如我们观测到了某个 $x = As$，其中的$A$ 就是混合矩阵（mixing matrix）。这样得到的 $x$ 也是一个高斯分布的，均值为 $0$，协方差 $E[xx^T ] = E[Ass^T A^T ] = AA^T$。 然后设 $R$ 为任意的正交矩阵（不太正式地说，也可以说成是旋转（rotation）矩阵或者是反射（reflection）矩阵），这样则有 $RR^T = R^TR = I$，然后设 $A' = AR$。如果使用 $A'$ 而不是 $A$ 作为混合矩阵，那么观测到的数据就应该是 $x' = A's$。这个 $x'$  也还是个高斯分布，依然是均值为 $0$，协方差为 $E[x'(x')^T ] = E[A'ss^T (A')^T ] = E[ARss^T (AR)^T ] = ARR^T A^T = AA^T$。看到没，无论混合矩阵使用 $A$ 还是 $A'$ ，得到的数据都是一个正态分布 $N (0, AA^T )$**（以 0 为均值，协方差为 $AA^T$）** 。这样就根本不能区分出来混合矩阵使用的是 $A$ 还是 $A'$。所以，只要混合矩阵中有一个任意的旋转分量（arbitrary rotational component），并且不能从数据中获得，那么就不能恢复出原始数据源了。\n\n上面这些论证，是基于多元标准正态分布（multivariate standard normal distribution）是旋转对称（rotationally symmetric）的这个定理。这些情况使得 ICA 面对高斯分布的数据（Gaussian data）的时候很无力，但是只要数据不是高斯分布的，然后再有充足的数据，那就还是能恢复出 $n$ 个独立的声源的。\n\n\n#### 2 密度（Densities）和线性变换（linear transformations）\n\n在继续去推导 ICA 算法之前，我们先来简要讲一讲对密度函数进行线性变换的效果（effect）。\n\n加入我们有某个随机变量 $s$，可以根据某个密度函数 $p_s(s)$ 来绘制。简单起见，咱们现在就把 $s$ 当做是一个实数，即 $s \\in R$。然后设 $x$ 为某个随机变量，定义方式为 $x = As$ （其中 $x \\in R, A \\in R$）。然后设 $p_x$ 是 $x$ 的密度函数。那么这个 $p_x$ 是多少呢？\n\n设 $W = A^{-1}$。要计算 $x$ 取某一个特定值的“概率（probability）”，可以先计算对于 $s = Wx$，在这一点上的 $p_s$，然后推导出$p_x(x) = p_s(Wx)$。然而，这是错误的。例如，假设 $s\\sim Uniform[0, 1]$，即其密度函数 $p_s(s) = 1\\{0 ≤ s ≤ 1\\}$。然后设 $A = 2$，这样 $x = 2s$。很明显， $x$ 在 $[0,2]$ 这个区间均匀分布（distributed uniformly）。所以其密度函数也就是 $p_x(x) = (0.5)1\\{0 ≤ x ≤ 2\\}$。这并不等于 $p_s(W x)$，其中的 $W = 0.5 = A^{-1}$。所以正确的推导公式应该是 $p_x(x) = p_s(Wx)|W|$。\n\n推广一下，若 $s$ 是一个向量值的分布，密度函数为 $p_s$，而 $x = As$，其中的 $A$ 是一个可逆的正方形矩阵，那么 $x$ 的密度函数则为：\n\n$$\npx(x) = p_s(Wx) · |W|\n$$\n\n上式中 $W = A^{-1}$。\n\nRemark. If you’ve seen the result that A maps [0, 1]n to a set of volume |A|, then here’s another way to remember the formula for px given above, that also generalizes our previous 1-dimensional example. \n\n**备注。** 可能你已经看到了用 $A$ 映射 $[0, 1]^n$ 得到的就是一个由 $volume |A|$ 组成的集合（译者注：这里的 volume 我不确定该怎么翻译），然后就又有了一个办法可以记住上面给出的关于 $p_x$的公式了，这也是对之前讨论过的 $1$ 维样例的一个泛化扩展。具体来说，设给定了 $A \\in R^{n×n}$，然后还按照惯例设 $W = A^{-1}$。接着设 $C_1 = [0, 1]^n$ 是一个 $n$ 维度超立方体，然后设 $C_2 =\\{As:s\\in C1\\}\\subseteq R^n$ 为由 $A$ 给定的映射下的 $C_1$ 的投影图像。这就是线性代数里面，用 $|A|$ 来表示 $C_2$ 的体积的标准结果，另外也是定义行列式（determinants）的一种方式。接下来，设 $s$在  $[0, 1]^n$ 上均匀分布（uniformly distributed），这样其密度函数为 $p_s(s) = 1\\{s \\in C_1\\}$。然后很明显，$x$ 也是在 $C_2$ 内均匀分布（uniformly distributed）。因此可以知道其密度函数为 $p_x(x) = 1\\{x \\in C_2\\}/vol(C_2)$，必须在整个 $C_2$ 累积为$1$（integrate to $1$，这是概率的性质）。但利用逆矩阵的行列式等于行列式的倒数这个定理，就有了 $1/vol(C_2) = 1/|A| = |A^{-1}| = |W|$。所以则有 $p_x(x) = 1\\{x \\in C_2\\}|W| = 1\\{Wx \\in C_1\\}|W | = p_s(W x)|W |$。\n\n#### 3 独立成分分析算法（ICA algorithm）\n\n现在就可以推导 ICA 算法了。我们这里描述的算法来自于 Bell 和 Sejnowski，然后我们对算法的解释也是基于他们的算法，作为一种最大似然估计（maximum likelihood estimation）的方法。（这和他们最初的解释不一样，那个解释里面要涉及到一个叫做最大信息原则（infomax principal） 的复杂概念，考虑到对 ICA 的现代理解，推导过程已经不需要那么复杂了。）\n\n我们假设每个声源的分布 $s_i$ 都是通过密度函数 $p_s$ 给出，然后联合分布 $s$ 则为：\n\n$$\np(s)=\\prod_{i=1}^n p_s(s_i)\n$$\n\n这里要注意，通过在建模中将联合分布（joint distribution）拆解为边界分布（marginal）的乘积（product），就能得出每个声源都是独立的假设（assumption）。利用上一节推导的共识，这就表明对 $x = As = W^{-1}s$ 的密度函数为：\n\n$$\np(s)=\\prod_{i=1}^n p_s(w_i^T x)\\cdot |w|\n$$\n\n剩下的就只需要去确定每个独立的声源的密度函数 $p_s$ 了。\n\n回忆一下，给定某个实数值的随机变量 $z$，其累积分布函数（cumulative distribution function，cdf）$F$ 的定义为$F(z_0)=P(z\\le z_0)=\\int_{-\\infty}^{z_0}p_z(z)dz$。然后，对这个累积分布函数求导数，就能得到 z 的密度函数：$p_z(z) = F'(z)$。\n\n因此，要确定 $s_i$ 的密度函数，首先要做的就是确定其累积分布函数（cdf）。这个 $cdf$ 函数必然是一个从 $0$ 到 $1$ 的单调递增函数。根据我们之前的讨论，这里不能选用高斯分布的 $cdf$，因为 ICA 不适用于高斯分布的数据。所以这里我们选择一个能够保证从 $0$ 增长到 $1$ 的合理的“默认（default）” 函数就可以了，比如 $s$ 形函数（sigmoid function） $g(s) = 1/(1 + e^{-s})$。这样就有，$p_s(s) = g'(s)$。$^1$\n\n>1 如果你对声源的密度函数的形式有了事先的了解，那么在这个位置替换过来就是个很好的办法。不过如果没有这种了解，就可以用 $s$ 形函数（sigmoid function），可以把这个函数当做是一个比较合理的默认函数，在很多问题中，这个函数用起来效果都不错。另外这里讲述的是假设要么所有的数据 $x^{(i)}$ 已经被证明均值为 $0$，或者可以自然预期具有 $0$ 均值，比如声音信号就是如此。这很有必要，因为我们的假设 $p_s(s) = g'(s)$ 就意味着期望 $E[s] = 0$（这个逻辑函数（logistic function）的导数是一个对称函数，因此给出的就是均值为 $0$ 的随机变量对应的密度函数），这也意味着 $E[x] = E[As] = 0$。\n\n$W$ 是一个正方形矩阵，是模型中的参数。给定一个训练集合  $\\{x^{(i)};i = 1,...,m\\}$，然后对数似然函数（log likelihood）则为：\n\n$$\nl(W)=\\sum_{i=1}^m(\\sum_{j=1}^n log g'(w_j^Tx^{(i)})+log|W|))\n$$\n\n我们要做的就是上面这个函数找出关于 $W$ 的最大值。通过求导，然后利用前面讲义中给出的定理 $\\nabla_W|W| = |W|(W^{-1})^T$，就可以很容易推导出随机梯度上升（stochastic gradient ascent）学习规则（leaR^ning rule）。对于一个给定的训练样本 $x^{(i)}$，这个更新规则为：\n\n$$\nW:=W+\\alpha\\begin{pmatrix}\n\\begin{bmatrix}\n1-2g(w_1^T x^{(i)}) \\\\\n1-2g(w_2^T x^{(i)}) \\\\\n\\vdots \\\\\n1-2g(w_n^T x^{(i)})\n\\end{bmatrix}x^{(i)T} + (W^T)^{-1}\n\\end{pmatrix}\n$$\n\n上式中的 $\\alpha$ 是学习速率（leaR^ning rate）。\n\n在算法收敛（converges）之后，就能计算出 $s^{(i)} = Wx^{(i)}$，这样就能恢复出原始的音源了。\n\n**备注。** 在写下数据的似然函数的时候，我们隐含地假设了这些 $x^{(i)}$ 都是彼此独立的（这里指的是对于不同的 $i$ 值来说彼此独立；注意这个问题并不是说 $x^{(i)}$ 的不同坐标是独立的），这样对训练集的似然函数则为$\\prod_i p(x^{(i)};W)$。很显然，对于语音数据和其他 $x^{(i)}$ 有相关性的时间序列数据来说，这个假设是不对的，但是这可以用来表明，只要有充足的数据，那么有相关性的训练样本并不会影响算法的性能。但是，对于成功训练的样本具有相关性的问题，如果我们把训练样本当做一个随机序列来进行访问，使用随机梯度上升（stochastic gradient ascent）的时候，有时候也能帮助加速收敛。（也就是说，在训练集的一个随机打乱的副本中运行随机梯度上升。）\n\n\n### 参考","tags":["CS229"],"categories":["CS229"]},{"title":"[机器学习-CS229]主成分分析","url":"%2Fposts%2F51615712%2F","content":"# CS229 课程讲义中文翻译\n\n| 原作者 | 翻译 | 校对 |\n| --- | --- | --- |\n| [Andrew Ng  吴恩达](http://www.andrewng.org/) | [CycleUser](https://www.zhihu.com/people/cycleuser/columns) | [XiaoDong_Wang](https://github.com/Dongzhixiao) |\n\n\n|相关链接|\n|---|\n|[Github 地址](https://github.com/Kivy-CN/Stanford-CS-229-CN)|\n|[知乎专栏](https://zhuanlan.zhihu.com/MachineLearn)|\n|[斯坦福大学 CS229 课程网站](http://cs229.stanford.edu/)|\n|[网易公开课中文字幕视频](http://open.163.com/movie/2008/1/M/C/M6SGF6VB4_M6SGHFBMC.html)|\n\n\n# 第十章\n\n### 第十一部分 主成分分析（Principal components analysis） \n\n前面我们讲了因子分析（factor analysis），其中在某个 $k$ 维度子空间对 $x \\in R^n$ 进行近似建模，$k$ 远小于 $n$，即 $k \\ll n$。具体来说，我们设想每个点 $x^{(i)}$ 用如下方法创建：首先在 $k$ 维度仿射空间（affine space） $\\{\\Lambda z + \\mu; z \\in R^k\\}$ 中生成某个 $z^{(i)}$ ，然后增加 $Ψ$-协方差噪音（covariance noise）。因子分析（factor analysis）是基于一个概率模型（probabilistic model），然后参数估计（parameter estimation）使用了迭代期望最大化算法（iterative EM algorithm）。\n\n在本章讲义中，我们要学习一种新的方法，主成分分析（Principal Components Analysis，缩写为 PCA），这个方法也是用来对数据近似（approximately）所处的子空间（subspace）进行判别（identify）。然而，主成分分析算法（PCA）会更加直接，只需要进行一种特征向量（eigenvector）计算（在 Matlab 里面可以通过 eig 函数轻松实现），并且不需要再去使用期望最大化（EM）算法。\n\n假如我们有一个数据集 $\\{x^{(i)}; i = 1, . . ., m\\}$，其中包括了 $m$ 种不同汽车的属性，例如最大速度（maximum speed），转弯半径（turn radius）等等。设其中每个 $i$ 都有 $x^{(i)} \\in R^n，(n \\ll m)$。但对于两个不同的属性，例如 $x_i$ 和 $x_j$，对应着以英里每小时（mph）为单位的最高速度和以公里每小时（kph）为单位的最高速度。因此这两个属性应该基本是线性相关（linearly dependent）的，只在对 $mph$ 和 $kph$ 进行四舍五入时候会有引入一些微小差异。所以，这个数据实际上应该是近似处于一个 $n-1$ 维度的子空间中的。我们如何自动检测和删除掉这一冗余（redundancy）呢？\n\n举一个不那么麻烦的例子，设想有一个数据集，其中包含的是对一个无线电遥控直升机（radio-controlled helicopters）飞行员协会进行调查得到的数据，其中的 $x_1^{(i)}$ 指代的是飞行员 $i$ 的飞行技能的度量，而 $x_2^{(i)}$ 指代的是该飞行员对飞行的喜爱程度。无线电遥控直升机是很难操作的，只有那些非常投入，并且特别热爱飞行的学生，才能成为好的飞行员。所以，上面这两个属性 $x_1$ 和 $x_2$ 之间的相关性是非常强的。所以我们可以认为在数据中沿着对角线方向（也就是下图中的 $u_1$ 方向）表征了一个人对飞行投入程度的内在“源动力（karma）”，只有少量的噪音脱离这个对角线方向。如下图所示，我们怎么来自动去计算出 $u_1$ 的方向呢？\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note10f1.png)\n\n我们接下来很快就要讲到主成分分析算法（PCA algorithm）了。但在运行 PCA 之前，我们首先要进行一些预处理（pre-process），正则化（normalize）数据的均值（mean）和方差（variance），如下所示：\n\n1. 设$\\mu=\\frac 1m\\sum_{i=1}^m x^{(i)}$  \n2. 将每个 $x^{(i)}$ 替换成 $x^{(i)} - \\mu$ \n3. 设$\\sigma_j^2=\\frac 1m\\sum_{i} (x_j^{(i)})^2$\n4. 将每个 $x_j^{(i)}$ 替换成 $x_j^{(i)}/\\sigma_j$. \n\n第$(1-2)$步把数据的平均值清零（zero out），然后可以省略掉所有有零均值的数据（例如，对应语音或者其他声学信号的时间序列）。第$(3-4)$步将每个坐标缩放，使之具有单位方差（unit variance），这确保了不同的属性（attributes）都在同样的“尺度（scale）”上来进行处理。例如，如果 $x_1$ 是汽车的最大速度（以 mph 为单位，精确到十位），然后 $x_2$ 是汽车的座位数量（取值一般在 2-4），这样这个重新正则化（renormalization）就把不同的属性（attributes）进行了缩放（scale），然后这些不同属性就更具有对比性（more comparable）。如果我们事先已经知道不同的属性在同一尺度上，就可以省略第$(3-4)$步。例如，如果每个数据点表示灰度图像（grayscale image）中的每个数据点，而每个 $x_j^{(i)}$ 就从 $\\{0, 1, . . . , 255\\}$ 中取值，对应的也就是在图像 $i$ 中像素 $j$ 位置的灰度值（intensity value）。\n\n接下来，进行了正则化之后，对数据近似所处的方向，也就是“主要变异轴（major axis of variation）”$u$，该如何去计算呢？一种方法是找出一个单位向量（unit vector）$u$，使得数据投影在 $u$ 的方向上的时候，投影的数据的方差（variance）最大。直观来看，在这个方向上，数据开始有一定规模的方差（variance）/信息量（information）。我们要选择的是这样一个方向的单位向量 $u$：数据能近似投放到与单位向量 $u$ 一致的方向（direction）/子空间（subspace），并且尽可能多地保留上面的方差（variance）。\n\n设下面的数据集，我们已经进行了正则化步骤（normalization steps）：\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note10f2.png)\n\n现在，加入我们选择的单位向量 $u$ 对应了下图中所示的方向。下图中的圆点表示的就是原始数据在这条线上面的投影（projections）。\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note10f3.png)\n\n可以看到，上面投影得到的数据依然有还算比较大的方差，而这些点距离零点也都比较远。反面样本则如下图所示，我们选择了另外一个方向的单位向量：\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note10f4.png)\n\n上面这幅图的投影中的方差就明显小了很多，而且投影得到的点位置也距离原点更近很多。\n我们希望能自动地选择出来如上面两幅图中第一幅那样的方向的单位向量 $u$。要对这个过程进行方程化（formalize），要注意到给定一个向量 $u$ 和一个点 $x$，$x$ 投影到 $u$ 上的投影长度就可以用 $x^T u$ 来得到。也就是说，如果 $x^{(i)}$ 是我们数据集中的一个点（上面几个图中画叉的 $x$ 点中的一个），那么这个点在 $u$ 上的投影（对应的是图中的圆点）就是从原点到 $x^T u$ 的距离。因此，要最大化投影的方差，就要找到一个能够将下面式子最大化的单位长度向量 $u$：\n\n$$\n\\begin{aligned}\n\\frac 1m\\sum_{i=1}^m (x^{(i)T}u)^2 &= \\frac 1m\\sum_{i=1}^m u^Tx^{(i)}x^{(i)T}u \\\\\n&= u^T(\\frac 1m\\sum_{i=1}^m x^{(i)}x^{(i)T})u\n\\end{aligned}\n$$\n\n很容易就能发现，要让上面的式子最大化，$||u||_2 = 1$ 给出了 的主特征向量（principal eigenvector），而这也正好就是数据的经验协方差矩阵（假设有零均值）。$^1$\n\n>1 如果以前没见到过这种形式，可以用拉格朗日乘数法将 $u^T \\Sigma u$ 最大化，使得 $u^T u = 1$。你应该能发现对于某些 $\\lambda，\\Sigma u = \\lambda u$，这就意味着向量 $u$ 是 $\\Sigma$ 的特征向量（eigenvector），特征值（eigenvalue）为 $\\lambda$。\n\n总结一下，如果我们要找一个 1维度子控件来近似数据，就要选择 $\\Sigma$ 的主特征向量（principal eigenvector）作为单位向量 $u$。更广义地理解，就是如果要讲数据投影到一个 $k$ 维度子空间（$k < n$），就应当选择 $\\Sigma$ 的 $k$ 个特征向量（eigenvectors） 来作为单位向量 $u_1, . . ., u_k$。这里的 $u_i$ 就形成了数据的一组新的正交基（orthogonal basis）。$^2$\n\n>2 由于 $\\Sigma$ 是对称的，所以向量 $u_i$ 就总是（或者总能选出来）彼此正交的（orthogonal）\n\n然后，要使用这组正交基来表示 $x^{(i)}$，只需要计算对应的向量：\n\n$$\ny^{(i)}=\\begin{bmatrix}\nu_1^T x^{(i)}\\\\ u_2^T x^{(i)}\\\\ \\vdots\\\\ u_k^T x^{(i)}\n\\end{bmatrix} \\in R^k\n$$\n\n因此，$x^{(i)} \\in R^n$，向量 $y^{(i)}$就是对 $x^{(i)}$ 的近似/表示。因此，主成分分析算法（PCA）也被称为是一种**维度降低** 算法（dimensionality reduction algorithm）。而其中的单位向量 $u_1,...,u_k$ 也就叫做数据集的前 $k$ 个**主成分（principal components）。**\n\n**备注。** 虽然我们已经正式表述了，仅当 $k = 1$ 的情况下，使用特征向量（eigenvectors）的众所周知的特性，很明显，在所有可能的正交基（orthogonal bases）当中，我们选择的那一组就能使得取最大值。因此，我们对基向量（basis）的选择应当是尽可能保留原始数据的方差信息（variability）。\n\n在习题集 $4$ 中，你会发现主成分分析算法（PCA）也可以有另外一种推导方式：将数据投影到数据所张成的 $k$ 维度子空间中，选择一组基向量，使得投影引起的近似误差（approximation error）最小。\n\n主成分分析算法（PCA）有很多用法；我们接下来收尾这部分就来给出若干样例。首先是压缩—用更低维度的 $y^{(i)}$ 来表示 $x^{(i)}$ ，这很明显就是一种用途了。如果我们把高维度的数据降维到 $k = 2$ 或者 $3$，那么就可以将 $y^{(i)}$ 进行可视化了。例如，如果我们把汽车数据降维到 $2$ 维，那么就可以把压缩后的数据投图（例如这时候投图中的一二点可能就代表了骑车的类型），来看看哪些车彼此相似，以及这些车可以聚集成那些组。\n\n另一个常用应用就是在使用 $x^{(i)}$ 作为输入特征进行监督学习算法（supervised learning algorithm）之前降低数据维度的预处理步骤。除了有利于缓解计算性能压力之外，降低数据维度还可以降低假设类（hypothesis class）的复杂度（complexity），然后避免过拟合（overfitting）（例如，低维度的输入特征控件上的线性分类器（linear classifiers）会有更小的 $VC$ 维度）。\n\n最后，正如在遥控直升机飞行员那个样例，我们可以把 PCA 用作为一种降噪算法（noise reduction algorithm）。在那个例子中，算法从对遥控飞行技巧和热爱程度的有噪音的衡量中估计了直观的“遥控飞行原动力（piloting karma）”。在课程中，我们还看到了把这种思路用于人脸图像，得到的就是面部特征算法（eigenface method）。其中每个点 $x^{(i)} \\in R^{100×100}$ 都是一个 10000 维度的向量，每个坐标对应的是一个 100x100 的人脸图像中的一个像素灰度值。使用主特征分析算法（PCA），我们就可以用更低维度的 $y^{(i)}$ 来表示每个图像 $x^{(i)}$。在这个过程中，我们希望主成分（principal components）能够保存有趣的信息、面孔之间的系统变化（systematic variations），以便能捕获到一个人看上去的模样，而不是由于细微的光线变化、轻微的拍摄状况差别等而引起的图像中的“噪音（noise）”。然后我们通过降低纬度然后计算 $||y^{(i)} - y^{(j)}||_2$ 来测量面孔 $i$ 和 $j$ 之间的距离。这样就能得到一个令人惊艳的面部匹配和检索算法（face-matching and retrieval algorithm）。\n\n\n### 参考","tags":["CS229"],"categories":["CS229"]},{"title":"[机器学习-CS229]因子分析","url":"%2Fposts%2F24419787%2F","content":"# CS229 课程讲义中文翻译\n\n| 原作者 | 翻译 | 校对 |\n| --- | --- | --- |\n| [Andrew Ng  吴恩达](http://www.andrewng.org/) | [CycleUser](https://www.zhihu.com/people/cycleuser/columns) | [XiaoDong_Wang](https://github.com/Dongzhixiao) |\n\n\n|相关链接|\n|---|\n|[Github 地址](https://github.com/Kivy-CN/Stanford-CS-229-CN)|\n|[知乎专栏](https://zhuanlan.zhihu.com/MachineLearn)|\n|[斯坦福大学 CS229 课程网站](http://cs229.stanford.edu/)|\n|[网易公开课中文字幕视频](http://open.163.com/movie/2008/1/M/C/M6SGF6VB4_M6SGHFBMC.html)|\n\n\n# 第九章\n\n### 第十部分 因子分析（Factor analysis）\n如果有一个从多个高斯混合模型（a mixture of several Gaussians）而来的数据集 $x^{(i)} \\in R^n$ ，那么就可以用期望最大化算法（EM algorithm）来对这个混合模型（mixture model）进行拟合。这种情况下，对于有充足数据（sufficient data）的问题，我们通常假设可以从数据中识别出多个高斯模型结构（multiple-Gaussian structure）。例如，如果我们的训练样本集合规模（training set size） $m$ 远远大于（significantly larger than）数据的维度（dimension） $n$，就符合这种情况。\n\n然后来考虑一下反过来的情况，也就是 $n$ 远远大于 $m$，即 $n \\gg m$。在这样的问题中，就可能用单独一个高斯模型来对数据建模都很难，更不用说多个高斯模型的混合模型了。由于 $m$ 个数据点所张成（span）的只是一个 $n$ 维空间 $R^n$ 的低维度子空间（low-dimensional subspace），如果用高斯模型（Gaussian）对数据进行建模，然后还是用常规的最大似然估计（usual maximum likelihood estimators）来估计（estimate）平均值（mean）和方差（covariance），得到的则是：\n\n$$\n\\begin{aligned}\n&\\mu = \\frac 1m\\sum_{i=1}^m x^{(i)} \\\\\n&\\Sigma = \\frac 1m\\sum_{i=1}^m (x^{(i)}-\\mu)(x^{(i)}-\\mu)^T\n\\end{aligned}\n$$\n\n我们会发现这里的 $\\Sigma$ 是一个奇异（singular）矩阵。这也就意味着其逆矩阵 $\\Sigma^{-1}$ 不存在，而 $1/|\\Sigma|^{1/2} = 1/0$。 但这几个变量都还是需要的，要用来计算一个多元高斯分布（multivariate Gaussian distribution）的常规密度函数（usual density）。还可以用另外一种方法来讲述清楚这个难题，也就是对参数（parameters）的最大似然估计（maximum likelihood estimates）会产生一个高斯分布（Gaussian），其概率分布在由样本数据$^1$所张成的仿射空间（affine space）中，对应着一个奇异的协方差矩阵（singular covariance matrix）。\n\n>1 这是一个点集，对于某些 $\\alpha_i$，此集合中的点 $x$ 都满足 $x = \\sum_{i=1}^m \\alpha_i x^{(i)}$, 因此 $\\sum_{i=1}^m \\alpha_1 = 1$。\n\n通常情况下，除非 $m$ 比 $n$ 大出相当多（some reasonable amount），否则最大似然估计（maximum likelihood estimates）得到的均值（mean）和方差（covariance）都会很差（quite poor）。尽管如此，我们还是希望能用已有的数据，拟合出一个合理（reasonable）的高斯模型（Gaussian model），而且还希望能识别出数据中的某些有意义的协方差结构（covariance structure）。那这可怎么办呢？\n\n在接下来的这一部分内容里，我们首先回顾一下对 $\\Sigma$ 的两个可能的约束（possible restrictions），这两个约束条件能让我们使用小规模数据来拟合 $\\Sigma$，但都不能就我们的问题给出让人满意的解（satisfactory solution）。然后接下来我们要讨论一下高斯模型的一些特点，这些后面会用得上，具体来说也就是如何找到高斯模型的边界和条件分布。最后，我们会讲一下因子分析模型（factor analysis model），以及对应的期望最大化算法（EM algorithm）。\n\n#### 1 $\\Sigma$ 的约束条件（Restriction）\n如果我们没有充足的数据来拟合一个完整的协方差矩阵（covariance matrix），就可以对矩阵空间 $\\Sigma$ 给出某些约束条件（restrictions）。例如，我们可以选择去拟合一个对角（diagonal）的协方差矩阵 $\\Sigma$。这样，读者很容易就能验证这样的一个协方差矩阵的最大似然估计（maximum likelihood estimate）可以由对角矩阵（diagonal matrix） $\\Sigma$ 满足：\n\n$$\n\\Sigma_{jj} = \\frac 1m \\sum_{i=1}^m (x_j^{(i)}-\\mu_j)^2\n$$\n\n因此，$\\Sigma_{jj}$ 就是对数据中第 $j$ 个坐标位置的方差值的经验估计（empirical estimate）。\n \n回忆一下，高斯模型的密度的形状是椭圆形的。对角线矩阵 $\\Sigma$ 对应的就是椭圆长轴（major axes）对齐（axis- aligned）的高斯模型。\n\n有时候，我们还要对这个协方差矩阵（covariance matrix）给出进一步的约束，不仅设为对角的（major axes），还要求所有对角元素（diagonal entries）都相等。这时候，就有 $\\Sigma = \\sigma^2I$，其中 $\\sigma^2$ 是我们控制的参数。对这个 $\\sigma^2$ 的最大似然估计则为：\n\n$$\n\\sigma^2 = \\frac 1{mn} \\sum_{j=1}^n\\sum_{i=1}^m (x_j^{(i)}-\\mu_j)^2\n$$\n\n这种模型对应的是密度函数为圆形轮廓的高斯模型（在二维空间也就是平面中是圆形，在更高维度当中就是球（spheres）或者超球体（hyperspheres））。\n\n如果我们对数据要拟合一个完整的，不受约束的（unconstrained）协方差矩阵 $\\Sigma$，就必须满足 $m \\ge n + 1$，这样才使得对 $\\Sigma$ 的最大似然估计不是奇异矩阵（singular matrix）。在上面提到的两个约束条件之下，只要 $m \\ge 2$，我们就能获得非奇异的（non-singular） $\\Sigma$。 \n\n然而，将 $\\Sigma$ 限定为对角矩阵，也就意味着对数据中不同坐标（coordinates）的 $x_i，x_j$建模都将是不相关的（uncorrelated），且互相独立（independent）。通常，还是从样本数据里面获得某些有趣的相关信息结构比较好。如果使用上面对 $\\Sigma$ 的某一种约束，就可能没办法获取这些信息了。在本章讲义里面，我们会提到因子分析模型（factor analysis model），这个模型使用的参数比对角矩阵 $\\Sigma$ 更多，而且能从数据中获得某些相关性信息（captures some correlations），但也不能对完整的协方差矩阵（full covariance matrix）进行拟合。\n\n#### 2 多重高斯模型（Gaussians ）的边界（Marginal）和条件（Conditional）\n在讲解因子分析（factor analysis）之前，我们要先说一下一个联合多元高斯分布（joint multivariate Gaussian distribution）下的随机变量（random variables）的条件（conditional）和边界（marginal）分布（distributions）。\n\n假如我们有一个值为向量的随机变量（vector-valued random variable）：\n\n$$\nx=\\begin{bmatrix}\nx_1 \\\\ x_2\n\\end{bmatrix}\n$$\n\n其中 $x_1 \\in R^r, x_2 \\in R^s$，因此 $x \\in R^{r+s}$。设 $x\\sim N(\\mu,\\Sigma)$，则这两个参数为： \n\n$$\n\\mu=\\begin{bmatrix}\n\\mu_1 \\\\ \\mu_2\n\\end{bmatrix}\\qquad\n\\Sigma = \\begin{bmatrix}\n\\Sigma_{11} & \\Sigma_{12} \\\\ \\Sigma_{21} & \\Sigma_{22}\n\\end{bmatrix}\n$$\n\n其中， $\\mu_1 \\in R^r, \\mu_2 \\in R^s, \\Sigma_{11} \\in R^{r\\times r}, \\Sigma_{12} \\in R^{r\\times s}$，以此类推。由于协方差矩阵（covariance matrices）是对称的（symmetric），所以有 $\\Sigma_{12} = \\Sigma_{21}^T$。\n\n基于我们的假设，$x_1$ 和 $x_2$ 是联合多元高斯分布(jointly multivariate Gaussian)。 那么 $x_1$ 的边界分布是什么？不难看出 $x_1$ 的期望 $E[x_1] = \\mu_1$ ，而协方差 $Cov(x_1) = E[(x_1 - \\mu_1)(x_1 - \\mu_1)] = \\Sigma_{11}$。接下来为了验证后面这一项成立，要用 $x_1$ 和 $x_2$的联合方差的概念：\n\n$$\n\\begin{aligned}\nCov(x) &= \\Sigma \\\\\n&= \\begin{bmatrix}\n\\Sigma_{11} & \\Sigma_{12} \\\\ \\Sigma_{21} & \\Sigma_{22}\n\\end{bmatrix} \\\\\n&= E[(x-\\mu)(x-\\mu)^T] \\\\\n&= E\\begin{bmatrix}\n\\begin{pmatrix}x_1-\\mu_1 \\\\ x_2-\\mu_2\\end{pmatrix}  & \n\\begin{pmatrix}x_1-\\mu_1 \\\\ x_2-\\mu_2\\end{pmatrix}^T \n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}(x_1-\\mu_1)(x_1-\\mu_1)^T & (x_1-\\mu_1)(x_2-\\mu_2)^T\\\\\n(x_2-\\mu_2)(x_1-\\mu_1)^T & (x_2-\\mu_2)(x_2-\\mu_2)^T\n\\end{bmatrix}\n\\end{aligned}\n$$\n\n在上面的最后两行中，匹配（Matching）矩阵的左上方子阵（upper-left sub blocks），就可以得到结果了。\n\n高斯分布的边界分布（marginal distributions）本身也是高斯分布，所以我们就可以给出一个正态分布 $x_1\\sim N(\\mu_,\\Sigma_{11})$ 来作为 $x_1$ 的边界分布（marginal distributions）。\n\n此外，我们还可以提出另一个问题，给定 $x_2$ 的情况下 $x_1$ 的条件分布是什么呢？通过参考多元高斯分布的定义，就能得到这个条件分布 $x_1|x_2 \\sim N (\\mu_{1|2}, \\Sigma_{1|2})$为：\n\n$$\n\\begin{aligned}\n&\\mu_{1|2} = \\mu_1 + \\Sigma_{12}\\Sigma_{22}^{-1}(x_2-\\mu_2)\\qquad&(1) \\\\\n&\\Sigma_{1|2} = \\Sigma_{11} - \\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21}&(2)\n\\end{aligned}\n$$\n\n在下一节对因子分析模型（factor analysis model）的讲解中，上面这些公式就很有用了，可以帮助寻找高斯分布的条件和边界分布（conditional and marginal distributions）。\n\n#### 3 因子分析模型（Factor analysis model）\n在因子分析模型（factor analysis model）中，我们制定在 $(x, z)$ 上的一个联合分布，如下所示，其中 $z \\in R^k$ 是一个潜在随机变量（latent random variable）：\n\n$$\n\\begin{aligned}\nz &\\sim N(0,I) \\\\\nx|z &\\sim N(\\mu+\\Lambda z,\\Psi)\n\\end{aligned}\n$$\n\n上面的式子中，我们这个模型中的参数是向量 $\\mu \\in R^n$ ，矩阵 $\\Lambda \\in R^{n×k}$，以及一个对角矩阵 $\\Psi \\in R^{n×n}$。$k$ 的值通常都选择比 $n$ 小一点的。\n \n这样，我们就设想每个数据点 $x^{(i)}$ 都是通过在一个 $k$ 维度的多元高斯分布 $z^{(i)}$ 中取样获得的。然后，通过计算 $\\mu+\\Lambda z^{(i)}$，就可以映射到实数域 $R^n$ 中的一个 $k$ 维仿射空间（k-dimensional affine space），在 $\\mu + \\Lambda z^{(i)}$ 上加上协方差 $\\Psi$ 作为噪音，就得到了 $x^{(i)}$。\n\n反过来，咱们也就可以来定义因子分析模型（factor analysis model），使用下面的设定：\n\n$$\n\\begin{aligned}\nz &\\sim N(0,I) \\\\\n\\epsilon &\\sim N(0,\\Psi) \\\\\nx &= \\mu + \\Lambda z + \\epsilon \n\\end{aligned}\n$$\n\n其中的 $\\epsilon$ 和 $z$ 是互相独立的。\n\n然后咱们来确切地看看这个模型定义的分布（distribution our）。其中，随机变量 $z$ 和 $x$ 有一个联合高斯分布（joint Gaussian distribution）：\n\n$$\n\\begin{bmatrix}\nz\\\\x\n\\end{bmatrix}\\sim N(\\mu_{zx},\\Sigma)\n$$\n\n然后咱们要找到 $\\mu_{zx}$ 和 $\\Sigma$。\n\n我们知道 $z$ 的期望 $E[z] = 0$，这是因为 $z$ 服从的是均值为 $0$ 的正态分布 $z\\sim N(0,I)$。 此外我们还知道：\n\n$$\n\\begin{aligned}\nE[x] &= E[\\mu + \\Lambda z + \\epsilon] \\\\\n&= \\mu + \\Lambda E[z] + E[\\epsilon] \\\\\n&= \\mu\n\\end{aligned}\n$$\n\n综合以上这些条件，就得到了： \n\n$$\n\\mu_{zx} = \\begin{bmatrix}\n\\vec{0}\\\\ \\mu\n\\end{bmatrix}\n$$\n\n下一步就是要找出 $\\Sigma$，我们需要计算出 $\\Sigma_{zz} = E[(z - E[z])(z - E[z])^T]$（矩阵$\\Sigma$的左上部分（upper-left block）），$\\Sigma_{zx} = E[(z - E[z])(x - E[x])^T]$（右上部分(upper-right block)），以及$\\Sigma_{xx}=E[(x - E[x])(x - E[x])^T]$ （右下部分(lower-right block)）。 \n\n由于 $z$ 是一个正态分布 $z \\sim N (0, I)$，很容易就能知道 $\\Sigma_{zz} = Cov(z) = I$。另外：\n\n$$\n\\begin{aligned}\nE[(z - E[z])(x - E[x])^T] &= E[z(\\mu+\\Lambda z+\\epsilon-\\mu)^T] \\\\\n&= E[zz^T]\\Lambda^T+E[z\\epsilon^T] \\\\\n&= \\Lambda^T\n\\end{aligned}\n$$\n\n在上面的最后一步中，使用到了结论 $E[zz^T] = Cov(z)$（因为 $z$ 的均值为 $0$），而且 $E[z\\epsilon^T ] = E[z]E[\\epsilon^T ] = 0$）（因为 $z$ 和 $\\epsilon$ 相互独立，因此乘积（product）的期望（expectation）等于期望的乘积）。\n\n同样的方法，我们可以用下面的方法来找到 $\\Sigma_{xx}$：\n\n$$\n\\begin{aligned}\nE[(x - E[x])(x - E[x])^T] &= E[\\mu+\\Lambda z+\\epsilon-\\mu)(\\mu+\\Lambda z+\\epsilon-\\mu)^T] \\\\\n&= E[\\Lambda zz^T\\Lambda^T+\\epsilon z^T\\Lambda^T+\\Lambda z\\epsilon^T+\\epsilon\\epsilon^T] \\\\\n&= \\Lambda E[zz^T]\\Lambda^T+E[\\epsilon\\epsilon^T] \\\\\n&= \\Lambda\\Lambda^T+\\Psi\n\\end{aligned}\n$$\n\n把上面这些综合到一起，就得到了：\n\n$$\n\\begin{bmatrix}\nz\\\\x\n\\end{bmatrix}\\sim \n\\begin{pmatrix}\n\\begin{bmatrix}\n\\vec{0}\\\\ \\mu\n\\end{bmatrix},\\begin{bmatrix}\nI&\\Lambda^T\\\\ \\Lambda&\\Lambda\\Lambda^T+\\Psi\n\\end{bmatrix}\n\\end{pmatrix}\\qquad(3)\n$$\n\n因此，我们还能发现 $x$ 的边界分布（marginal distribution）为 $x \\sim N(\\mu,\\Lambda\\Lambda^T +\\Psi)$。所以，给定一个训练样本集合 $\\{x^{(i)}; i = 1, ..., m\\}$，参数（parameters）的最大似然估计函数的对数函数（log likelihood），就可以写为：\n\n$$\nl(\\mu,\\Lambda,\\Psi)=log\\prod_{i=1}^m\\frac{1}\n{(2\\pi)^{n/2}|\\Lambda\\Lambda^T+\\Psi|^{1/2}}\nexp(-\\frac 12(x^{(i)}-\\mu)^T(\\Lambda\\Lambda^T+\\Psi)^{-1}(x^{(i)}-\\mu))\n$$\n\n为了进行最大似然估计，我们就要最大化上面这个关于参数的函数。但确切地对上面这个方程式进行最大化，是很难的，不信你自己试试哈，而且我们都知道没有算法能够以封闭形式（closed-form）来实现这个最大化。所以，我们就改用期望最大化算法（EM algorithm）。下一节里面，咱们就来推导一下针对因子分析模型（factor analysis）的期望最大化算法（EM）。\n\n#### 4 针对因子分析模型（factor analysis）的期望最大化算法（EM） \n$E$ 步骤的推导很简单。只需要计算出来 $Q_i(z^{(i)}) = p(z^{(i)}|x^{(i)}; \\mu, \\Lambda, \\Psi)$。把等式$(3)$ 当中给出的分布代入到方程$(1-2)$，来找出一个高斯分布的条件分布，我们就能发现 $z^{(i)}|x^{(i)}; \\mu, \\Lambda, \\Psi \\sim N (\\mu_{z^{(i)}|x^{(i)}} , \\Sigma_{z^{(i)}|x^{(i)}} )$，其中：\n\n$$\n\\begin{aligned}\n\\mu_{z^{(i)}|x^{(i)}}&=\\Lambda^T(\\Lambda\\Lambda^T+\\Psi)^{-1}(x^{(i)}-\\mu) \\\\\n\\Sigma_{z^{(i)}|x^{(i)}}&=I-\\Lambda^T(\\Lambda\\Lambda^T+\\Psi)^{-1}\\Lambda\n\\end{aligned}\n$$\n\n所以，通过对 $\\mu_{z^{(i)}|x^{(i)}}$ 和 $\\Sigma_{z^{(i)}|x^{(i)}}$,进行这样的定义，就能得到：\n\n$$\nQ_i(z^{(i)})=\\frac{1}\n{(2\\pi)^{k/2}|\\Sigma_{z^{(i)}|x^{(i)}}|^{1/2}}\nexp(-\\frac 12(z^{(i)}-\\mu_{z^{(i)}|x^{(i)}})^T\\Sigma_{z^{(i)}|x^{(i)}}^{-1}(z^{(i)}-\\mu_{z^{(i)}|x^{(i)}}))\n$$\n\n接下来就是 $M$ 步骤了。这里需要去最大化下面这个关于参数 $\\mu, \\Lambda$, $\\Psi$ 的函数值：\n\n$$\n\\sum_{i=1}^m\\int_{z^{(i)}}Q_i(z^{(i)})log\\frac{p(x^{(i)},z^{(i)};\\mu,\\Lambda,\\Psi)}{Q_i(z^{(i)})}dz^{(i)}\\qquad(4)\n$$\n\n我们在本文中仅仅对 $\\Lambda$ 进行优化，关于 $\\mu$ 和 $\\Psi$ 的更新就作为练习留给读者自己进行推导了。\n把等式$(4)$ 简化成下面的形式：\n\n$$\n\\begin{aligned}\n\\sum_{i=1}^m&\\int_{z^{(i)}}Q_i(z^{(i)})[log p(x^{(i)}|z^{(i)};\\mu,\\Lambda,\\Psi)+log p(z^{(i)})-log Q_i(z^{(i)})]dz^{(i)} &(5)\\\\\n&=\\sum_{i=1}^m E_{z^{(i)}\\sim Q_i}[log p(x^{(i)}|z^{(i)};\\mu,\\Lambda,\\Psi)+log p(z^{(i)})-log Q_i(z^{(i)})] &(6)\n\\end{aligned}\n$$\n\n上面的等式中，\"$z^{(i)} \\sim Q_i$\" 这个下标（subscript），表示的意思是这个期望是关于从 $Q_i$ 中取得的 $z^{(i)}$ 的。在后续的推导过程中，如果没有歧义的情况下，我们就会把这个下标省略掉。删除掉这些不依赖参数的项目后，我们就发现只需要最大化：\n\n$$\n\\begin{aligned}\n\\sum_{i=1}^m&E[log p(x^{(i)}|z^{(i)};\\mu,\\Lambda,\\Psi)] \\\\\n&=\\sum_{i=1}^m E[log\\frac{1}{(2\\pi)^{n/2}|\\Psi|^{1/2}}\nexp(-\\frac 12(x^{(i)}-\\mu-\\Lambda z^{(i)})^T\\Psi^{-1}(x^{(i)}-\\mu-\\Lambda z^{(i)}))] \\\\\n&=\\sum_{i=1}^m E[-\\frac 12log|\\Psi|-\\frac n2log(2\\pi)-\\frac 12(x^{(i)}-\\mu-\\Lambda z^{(i)})^T\\Psi^{-1}(x^{(i)}-\\mu-\\Lambda z^{(i)})]\n\\end{aligned}\n$$\n\n\n我们先对上面的函数进行关于 $\\Lambda$ 的最大化。可见只有最后的一项依赖 $\\Lambda$。求导数，同时利用下面几个结论：$tr a = a (for\\quad a \\in R), tr AB = tr BA, \\nabla_A tr ABA^T C = CAB + C^T AB$，就能得到：\n\n$$\n\\begin{aligned}\n\\nabla_\\Lambda&\\sum_{i=1}^m -E[\\frac 12(x^{(i)}-\\mu-\\Lambda z^{(i)})^T\\Psi^{-1}(x^{(i)}-\\mu-\\Lambda z^{(i)})] \\\\\n&=\\sum_{i=1}^m \\nabla_\\Lambda E[-tr\\frac 12 z^{(i)T}\\Lambda^T\\Psi^{-1}\\Lambda z^{(i)}+tr z^{(i)T}\\Lambda^T\\Psi^{-1}(x^{(i)}-\\mu)] \\\\\n&=\\sum_{i=1}^m \\nabla_\\Lambda E[-tr\\frac 12 \\Lambda^T\\Psi^{-1}\\Lambda z^{(i)}z^{(i)T}+tr \\Lambda^T\\Psi^{-1}(x^{(i)}-\\mu)z^{(i)T}] \\\\\n&=\\sum_{i=1}^m E[-\\Psi^{-1}\\Lambda z^{(i)}z^{(i)T}+\\Psi^{-1}(x^{(i)}-\\mu)z^{(i)T}] \\\\\n\\end{aligned}\n$$\n\n设置导数为 $0$，然后简化，就能得到：\n\n$$\n\\sum_{i=1}^m\\Lambda E_{z^{(i)}\\sim Q_i}[z^{(i)}z^{(i)T}]=\n\\sum_{i=1}^m(x^{(i)}-\\mu)E_{z^{(i)}\\sim Q_i}[z^{(i)T}]\n$$\n\n接下来，求解 $\\Lambda$，就能得到： \n\n$$\n\\Lambda=(\\sum_{i=1}^m(x^{(i)}-\\mu)E_{z^{(i)}\\sim Q_i}[z^{(i)T}])(\\sum_{i=1}^m E_{z^{(i)}\\sim Q_i}[z^{(i)}z^{(i)T}])^{-1}\\qquad(7)\n$$\n\n有一个很有意思的地方需要注意，上面这个等式和用最小二乘线性回归（least squares regression）推出的正则方程（normal equation）有密切关系：\n\n$$\n\\theta^T=(y^TX)(X^TX)^{-1}\n$$\n\n与之类似，这里的 $x$ 是一个关于 $z$（以及噪音 noise）的线性方程。考虑在 $E$ 步骤中对 $z$ 已经给出了猜测，接下来就可以尝试来对与 $x$ 和 $z$ 相关的未知线性量（unknown linearity）$\\Lambda$ 进行估计。接下来不出意料，我们就会得到某种类似正则方程的结果。然而，这个还是和利用对 $z$ 的“最佳猜测（best guesses）” 进行最小二乘算法有一个很大的区别的；这一点我们很快就会看到了。\n\n为了完成 $M$ 步骤的更新，接下来我们要解出等式$(7)$ 当中的期望值（values of the expectations）。由于我们定义 $Q_i$ 是均值（mean）为 $\\mu_{z^{(i)}|x^{(i)}}$，协方差（covariance）为 $\\Sigma_{z^{(i)}|x^{(i)}}$ 的一个高斯分布，所以很容易能得到：\n\n$$\n\\begin{aligned}\nE_{z^{(i)}\\sim Q_i}[z^{(i)T}]&= \\mu_{z^{(i)}|x^{(i)}}^T \\\\\nE_{z^{(i)}\\sim Q_i}[z^{(i)}z^{(i)T}]&= \\mu_{z^{(i)}|x^{(i)}}\\mu_{z^{(i)}|x^{(i)}}^T+\\Sigma_{z^{(i)}|x^{(i)}}\n\\end{aligned}\n$$\n\n上面第二个等式的推导依赖于下面这个事实：对于一个随机变量 $Y$，协方差 $Cov(Y ) = E[Y Y^T ]-E[Y]E[Y]^T$ ，所以 $E[Y Y^T ] = E[Y ]E[Y ]^T +Cov(Y)$。把这个代入到等式$(7)$，就得到了 $M$ 步骤中 $\\Lambda$ 的更新规则：\n\n$$\n\\Lambda=(\\sum_{i=1}^m(x^{(i)}-\\mu)\\mu_{z^{(i)}|x^{(i)}}^T)(\\sum_{i=1}^m\\mu_{z^{(i)}|x^{(i)}} \\mu_{z^{(i)}|x^{(i)}}^T + \\Sigma_{z^{(i)}|x^{(i)}})^{-1}\\qquad(8)\n$$\n\n上面这个等式中，要特别注意等号右边这一侧的 $\\Sigma_{z^{(i)}|x^{(i)}}$。这是一个根据 $z^{(i)}$ 给出的 $x^{(i)}$ 后验分布（posterior distribution）$p(z^{(i)}|x^{(i)})$ 的协方差，而在 $M$ 步骤中必须要考虑到在这个后验分布中 $z^{(i)}$ 的不确定性（uncertainty）。推导 $EM$ 算法的一个常见错误就是在 $E$ 步骤进行假设，只需要算出潜在随机变量（latent random variable） $z$ 的期望 $E[z]$，然后把这个值放到 $M$ 步骤当中 $z$ 出现的每个地方来进行优化（optimization）。当然，这能解决简单问题，例如高斯混合模型（mixture of Gaussians），在因子模型的推导过程中，就同时需要 $E[zz^T ]$ 和 $E[z]$；而我们已经知道，$E[zz^T ]$ 和 $E[z]E[z]T$ 随着 $\\Sigma_{z|x}$ 而变化。因此，在 $M$ 步骤就必须要考虑到后验分布（posterior distribution）$p(z^{(i)}|x^{(i)})$中 $z$ 的协方差（covariance）。\n\n最后，我们还可以发现，在 $M$ 步骤对参数 $\\mu$ 和 $\\Psi$ 的优化。不难发现其中的 $\\mu$ 为：\n\n$$\n\\mu=\\frac 1m\\sum_{i=1}^m x^{(i)}\n$$\n\n由于这个值不随着参数的变换而改变（也就是说，和 $\\Lambda$ 的更新不同，这里等式右侧不依赖 $Q_i(z^{(i)}) = p(z^{(i)}|x^{(i)}; \\mu, \\Lambda, \\Psi)$，这个 $Qi(z^{(i)})$ 是依赖参数的），这个只需要计算一次就可以，在算法运行过程中，也不需要进一步更新。类似地，对角矩阵 $\\Psi$ 也可以通过计算下面这个式子来获得：\n\n$$\n\\Phi=\\frac 1m\\sum_{i=1}^m x^{(i)}x^{(i)T}-x^{(i)}\\mu_{z^{(i)}|x^{(i)}}^T\\Lambda^T  - \\Lambda\\mu_{z^{(i)}|x^{(i)}}x^{(i)T}+\\Lambda(\\mu_{z^{(i)}|x^{(i)}}\\mu_{z^{(i)}|x^{(i)}}^T+\\Sigma_{z^{(i)}|x^{(i)}})\\Lambda^T\n$$\n\n然后只需要设 $\\Psi_{ii} = \\Phi_{ii}$（也就是说，设 $\\Psi$ 为一个仅仅包含矩阵 $Φ$ 中对角线元素的对角矩阵）。\n\n\n### 参考","tags":["CS229"],"categories":["CS229"]},{"title":"[机器学习-CS229]期望最大化算法","url":"%2Fposts%2F5acf3829%2F","content":"# CS229 课程讲义中文翻译\n\n| 原作者 | 翻译 | 校对 |\n| --- | --- | --- |\n| [Andrew Ng  吴恩达](http://www.andrewng.org/) | [CycleUser](https://www.zhihu.com/people/cycleuser/columns) | [XiaoDong_Wang](https://github.com/Dongzhixiao) |\n\n\n|相关链接|\n|---|\n|[Github 地址](https://github.com/Kivy-CN/Stanford-CS-229-CN)|\n|[知乎专栏](https://zhuanlan.zhihu.com/MachineLearn)|\n|[斯坦福大学 CS229 课程网站](http://cs229.stanford.edu/)|\n|[网易公开课中文字幕视频](http://open.163.com/movie/2008/1/M/C/M6SGF6VB4_M6SGHFBMC.html)|\n\n# 第八章\n\n### 第九部分 期望最大化算法(EM algorithm)\n\n在前面的若干讲义中，我们已经讲过了期望最大化算法（EM algorithm），使用场景是对一个高斯混合模型进行拟合（fitting a mixture of Gaussians）。在本章里面，我们要给出期望最大化算法（EM algorithm）的更广泛应用，并且演示如何应用于一个大系列的具有潜在变量（latent variables）的估计问题（estimation problems）。我们的讨论从 **Jensen 不等式（Jensen’s inequality）** 开始，这是一个非常有用的结论。\n\n#### 1 Jensen 不等式（Jensen’s inequality）\n设 $f$ 为一个函数，其定义域（domain）为整个实数域（set of real numbers）。这里要回忆一下，如果函数 $f$ 的二阶导数 $f''(x) \\ge 0$ （其中的 $x \\in R$），则函数 $f$ 为一个凸函数（convex function）。如果输入的为向量变量，那么这个函数就泛化了，这时候该函数的海森矩阵（hessian） $H$ 就是一个半正定矩阵（positive semi-definite $H \\ge 0$）。如果对于所有的 $x$ ，都有二阶导数 $f''(x) > 0$，那么我们称这个函数 $f$ 是严格凸函数（对应向量值作为变量的情况，对应的条件就是海森矩阵必须为正定，写作 $H > 0$）。这样就可以用如下方式来表述 Jensen 不等式：\n\n**定理（Theorem）：** 设 $f$ 是一个凸函数，且设 $X$ 是一个随机变量（random variable）。然后则有：\n\n$$\nE[f(X)] \\ge f(EX).\n$$\n\n`译者注：函数的期望大于等于期望的函数值`\n\n此外，如果函数 $f$ 是严格凸函数，那么 $E[f(X)] = f(EX)$ 当且仅当 $X = E[X]$ 的概率（probability）为 $1$ 的时候成立（例如 $X$ 是一个常数）。\n\n还记得我们之前的约定（convention）吧，写期望（expectations）的时候可以偶尔去掉括号（parentheses），所以在上面的定理中， $f(EX) = f(E[X])$。\n\n为了容易理解这个定理，可以参考下面的图：\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note8f1.png)\n\n上图中，$f$ 是一个凸函数，在图中用实线表示。另外 $X$ 是一个随机变量，有 $0.5$ 的概率（chance）取值为 $a$，另外有 $0.5$ 的概率取值为 $b$（在图中 $x$ 轴上标出了）。这样，$X$ 的期望值就在图中所示的 $a$ 和 $b$ 的中点位置。\n\n图中在 $y$ 轴上也标出了 $f(a)$, $f(b)$ 和 $f(E[X])$。接下来函数的期望值 $E[f(X)]$ 在 $y$ 轴上就处于 $f(a)$ 和 $f(b)$ 之间的中点的位置。如图中所示，在这个例子中由于 $f$ 是凸函数，很明显 $E[f(X)] ≥ f(EX)$。\n\n顺便说一下，很多人都记不住不等式的方向，所以就不妨用画图来记住，这是很好的方法，还可以通过图像很快来找到答案。\n\n**备注。** 回想一下，当且仅当 $-f$ 是严格凸函数（[strictly] convex）的时候，$f$ 是严格凹函数（[strictly] concave）（例如，二阶导数 $f''(x)\\le 0$ 或者其海森矩阵 $H ≤ 0$）。Jensen 不等式也适用于凹函数（concave）$f$，但不等式的方向要反过来，也就是对于凹函数，$E[f(X)] \\le f(EX)$。\n\n#### 2 期望最大化算法（EM algorithm） \n假如我们有一个估计问题（estimation problem），其中由训练样本集 $\\{x^{(1)}, ..., x^{(m)}\\}$ 包含了 $m$ 个独立样本。我们用模型 $p(x, z)$ 对数据进行建模，拟合其参数（parameters），其中的似然函数（likelihood）如下所示：\n\n$$\n\\begin{aligned}\nl(\\theta) &= \\sum_{i=1}^m\\log p(x;\\theta) \\\\\n&= \\sum_{i=1}^m\\log\\sum_z p(x,z;\\theta)\n\\end{aligned}\n$$\n\n然而，确切地找到对参数 $\\theta$ 的最大似然估计（maximum likelihood estimates）可能会很难。此处的 $z^{(i)}$ 是一个潜在的随机变量（latent random variables）；通常情况下，如果 $z^{(i)}$ 事先得到了，然后再进行最大似然估计，就容易多了。\n\n这种环境下，使用期望最大化算法（EM algorithm）就能很有效地实现最大似然估计（maximum likelihood estimation）。明确地对似然函数$l(\\theta)$进行最大化可能是很困难的，所以我们的策略就是使用一种替代，在 $E$ 步骤 构建一个 $l$ 的下限（lower-bound），然后在 $M$ 步骤 对这个下限进行优化。\n\n对于每个 $i$，设 $Q_i$ 是某个对 $z$ 的分布（$\\sum_z Q_i(z) = 1, Q_i(z)\\ge 0$）。则有下列各式$^1$：\n\n$$\n\\begin{aligned}\n\\sum_i\\log p(x^{(i)};\\theta) &= \\sum_i\\log\\sum_{z^{(i)}}p(x^{(i)},z^{(i)};\\theta)&(1) \\\\\n&= \\sum_i\\log\\sum_{z^{(i)}}Q_i(z^{(i)})\\frac{p(x^{(i)},z^{(i)};\\theta)}{Q_i(z^{(i)})} &(2)\\\\\n&\\ge \\sum_i\\sum_{z^{(i)}}Q_i(z^{(i)})\\log\\frac{p(x^{(i)},z^{(i)};\\theta)}{Q_i(z^{(i)})}&(3) \n\\end{aligned}\n$$\n\n>1 如果 $z$ 是连续的，那么 $Q_i$ 就是一个密度函数（density），上面讨论中提到的对 $z$ 的求和（summations）就要用对 $z$ 的积分（integral）来替代。\n\n上面推导（derivation）的最后一步使用了 Jensen 不等式（Jensen’s inequality）。其中的 $f(x) = log x$ 是一个凹函数（concave function），因为其二阶导数 $f''(x) = -1/x^2 < 0$ 在整个定义域（domain） $x\\in R^+$ 上都成立。\n\n此外，上式的求和中的单项： \n\n$$\n\\sum_{z^{(i)}}Q_i(z^{(i)})[\\frac{p(x^{(i)},z^{(i)};\\theta)}{Q_i(z^{(i)})}]\n$$\n\n是变量（quantity）$[p(x^{(i)}, z^{(i)}; \\theta)/Q_i(z^{(i)})]$ 基于 $z^{(i)}$ 的期望，其中 $z^{(i)}$ 是根据 $Q_i$ 给定的分布确定。然后利用 Jensen 不等式（Jensen’s inequality），就得到了：\n\n$$\nf(E_{z^{(i)}\\sim Q_i}[\\frac{p(x^{(i)},z^{(i)};\\theta)}{Q_i(z^{(i)})}])\\ge\nE_{z^{(i)}\\sim Q_i}[f(\\frac{p(x^{(i)},z^{(i)};\\theta)}{Q_i(z^{(i)})})]\n$$\n\n其中上面的角标 $z^{(i)}\\sim Q_i$ 就表明这个期望是对于依据分布 $Q_i$ 来确定的 $z^{(i)}$ 的。这样就可以从等式 $(2)$ 推导出等式 $(3)$。\n\n接下来，对于任意的一个分布 $Q_i$，上面的等式 $(3)$ 就给出了似然函数 $l(\\theta)$ 的下限（lower-bound）。那么对于 $Q_i$ 有很多种选择。咱们该选哪个呢？如果我们对参数 $\\theta$ 有某种当前的估计，很自然就可以设置这个下限为 $\\theta$ 这个值。也就是，针对当前的 $\\theta$ 值，我们令上面的不等式中的符号为等号。（稍后我们能看到，这样就能证明，随着 $EM$迭代过程的进行，似然函数 $l(\\theta)$ 就会单调递增（increases monotonically）。）\n\n为了让上面的限定值（bound）与 $\\theta$ 特定值（particular value）联系紧密（tight），我们需要上面推导过程中的 Jensen 不等式这一步中等号成立。要让这个条件成立，我们只需确保是在对一个常数值随机变量（“constant”-valued random variable）求期望。也就是需要：\n\n$$\n\\frac{p(x^{(i)},z^{(i)};\\theta)}{Q_i(z^{(i)})}=c\n$$\n\n其中常数 $c$ 不依赖 $z^{(i)}$。要实现这一条件，只需满足：\n\n$$\nQ_i(z^{(i)})\\propto p(x^{(i)},z^{(i)};\\theta)\n$$\n\n实际上，由于我们已知 $\\sum_z Q_i(z^{(i)}) = 1$（因为这是一个分布），这就进一步表明：\n\n$$\n\\begin{aligned}\nQ_i(z^{(i)}) &= \\frac{p(x^{(i)},z^{(i)};\\theta)}{\\sum_z p(x^{(i)},z;\\theta)} \\\\\n&= \\frac{p(x^{(i)},z^{(i)};\\theta)}{p(x^{(i)};\\theta)} \\\\\n&= p(z^{(i)}|x^{(i)};\\theta)\n\\end{aligned}\n$$\n\n因此，在给定 $x^{(i)}$ 和参数 $\\theta$ 的设置下，我们可以简单地把 $Q_i$ 设置为 $z^{(i)}$ 的后验分布（posterior distribution）。\n\n接下来，对 $Q_i$ 的选择，等式 $(3)$ 就给出了似然函数对数（log likelihood）的一个下限，而似然函数（likelihood）正是我们要试图求最大值（maximize）的。这就是 $E$ 步骤。接下来在算法的 $M$ 步骤中，就最大化等式 $(3)$ 当中的方程，然后得到新的参数 $\\theta$。重复这两个步骤，就是完整的 $EM$ 算法，如下所示：\n\n&emsp;重复下列过程直到收敛（convergence）:  {\n\n&emsp;&emsp;（$E$ 步骤）对每个 $i$，设 \n　　\n$$\nQ_i(z^{(i)}):=p(z^{(i)}|x^{(i)};\\theta)\n$$\n\n&emsp;&emsp;（$M$ 步骤） 设 \n\n$$\n\\theta := arg\\max_\\theta\\sum_i\\sum_{z^{(i)}}Q_i(z^{(i)})log\\frac{p(x^{(i)},z^{(i)};\\theta)}{Q_i(z^{(i)})}\n$$\n\n} \n\n怎么才能知道这个算法是否会收敛（converge）呢？设 $\\theta^{(t)}$ 和 $\\theta^{(t+1)}$ 是上面 $EM$ 迭代过程中的某两个参数（parameters）。接下来我们就要证明一下 $l(\\theta^{(t)})\\le l(\\theta^{(t+1)})$，这就表明 $EM$ 迭代过程总是让似然函数对数（log-likelihood）单调递增（monotonically improves）。证明这个结论的关键就在于对 $Q_i$ 的选择中。在上面$EM$迭代中，参数的起点设为 $\\theta^{(t)}$，我们就可以选择 $Q_i^{(t)}(z^{(i)}): = p(z^{(i)}|x^{(i)};\\theta^{(t)})$。之前我们已经看到了，正如等式 $(3)$ 的推导过程中所示，这样选择能保证 Jensen 不等式的等号成立，因此：\n\n$$\nl(\\theta^{(t)})=\\sum_i\\sum_{z^{(i)}}Q_i^{(t)}(z^{(i)})log\\frac{p(x^{(i)},z^{(i)};\\theta^{(t)})}{Q_i^{(t)}(z^{(i)})}\n$$\n\n参数 $\\theta^{(t+1)}$  可以通过对上面等式中等号右侧进行最大化而得到。因此： \n\n$$\n\\begin{aligned}\nl(\\theta^{(t+1)}) &\\ge \\sum_i\\sum_{z^{(i)}}Q_i^{(t)}(z^{(i)})log\\frac{p(x^{(i)},z^{(i)};\\theta^{(t+1)})}{Q_i^{(t)}(z^{(i)})} &(4)\\\\\n&\\ge \\sum_i\\sum_{z^{(i)}}Q_i^{(t)}(z^{(i)})log\\frac{p(x^{(i)},z^{(i)};\\theta^{(t)})}{Q_i^{(t)}(z^{(i)})} &(5)\\\\\n&= l(\\theta^{(t)}) &(6)\n\\end{aligned}\n$$\n\n上面的第一个不等式推自：\n\n$$\nl(\\theta)\\ge \\sum_i\\sum_{z^{(i)}}Q_i(z^{(i)})log\\frac{p(x^{(i)},z^{(i)};\\theta)}{Q_i(z^{(i)})}\n$$\n\n上面这个不等式对于任意值的 $Q_i$ 和 $\\theta$ 都成立，尤其当 $Q_i = Q_i^{(t)}, \\theta = \\theta^{(t+1)}$。要得到等式 $(5)$，我们要利用 $\\theta^{(t+1)}$ 的选择能够保证：\n\n$$\narg\\max_\\theta \\sum_i\\sum_{z^{(i)}}Q_i(z^{(i)})log\\frac{p(x^{(i)},z^{(i)};\\theta)}{Q_i(z^{(i)})}\n$$\n\n这个式子对 $\\theta^{(t+1)}$ 得到的值必须大于等于 $\\theta^{(t)}$ 得到的值。最后，推导等式$(6)$ 的这一步，正如之前所示，因为在选择的时候我们选的 $Q_i^{(t)}$ 就是要保证 Jensen 不等式对 $\\theta^{(t)}$ 等号成立。\n\n因此，$EM$ 算法就能够导致似然函数（likelihood）的单调收敛。在我们推导 $EM$ 算法的过程中，我们要一直运行该算法到收敛。得到了上面的结果之后，可以使用一个合理的收敛检测（reasonable convergence test）来检查在成功的迭代（successive iterations）之间的 $l(\\theta)$ 的增长是否小于某些容忍参数（tolerance parameter），如果 $EM$ 算法对 $l(\\theta)$ 的增大速度很慢，就声明收敛（declare convergence）。\n\n**备注。** 如果我们定义\n\n$$\nJ(Q, \\theta)=\\sum_i\\sum_{z^{(i)}}Q_i(z^{(i)})log\\frac{p(x^{(i)},z^{(i)};\\theta)}{Q_i(z^{(i)})}\n$$\n\n通过我们之前的推导，就能知道 $l(\\theta) ≥ J(Q, \\theta)$。这样 $EM$ 算法也可看作是在 $J$ 上的坐标上升（coordinate ascent），其中 $E$ 步骤在 $Q$ 上对 $J$ 进行了最大化（自己检查哈），然后 $M$ 步骤则在 $\\theta$ 上对 $J$ 进行最大化。\n\n#### 3 高斯混合模型回顾（Mixture of Gaussians revisited ）\n\n有了对 $EM$ 算法的广义定义（general definition）之后，我们就可以回顾一下之前的高斯混合模型问题，其中要拟合的参数有 $\\phi, \\mu$ 和$\\Sigma$。为了避免啰嗦，这里就只给出在 $M$ 步骤中对$\\phi$ 和 $\\mu_j$ 进行更新的推导，关于 $\\Sigma_j$ 的更新推导就由读者当作练习自己来吧。\n\n$E$ 步骤很简单。还是按照上面的算法推导过程，只需要计算：\n\n$$\nw_j^{(i)}=Q_i(z^{(i)}=j)=P(z^{(i)}=j|x^{(i)};\\phi,\\mu,\\Sigma)\n$$\n\n这里面的 $Q_i(z^{(i)} = j)$ 表示的是在分布 $Q_i$上 $z^{(i)}$ 取值 $j$ 的概率。\n\n接下来在 $M$ 步骤，就要最大化关于参数 $\\phi,\\mu,\\Sigma$的值： \n\n$$\n\\begin{aligned}\n\\sum_{i=1}^m&\\sum_{z^{(i)}}Q_i(z^{(i)})log\\frac{p(x^{(i)},z^{(i)};\\phi,\\mu,\\Sigma)}{Q_i(z^{(i)})}\\\\\n&= \\sum_{i=1}^m\\sum_{j=1}^kQ_i(z^{(i)}=j)log\\frac{p(x^{(i)}|z^{(i)}=j;\\mu,\\Sigma)p(z^{(i)}=j;\\phi)}{Q_i(z^{(i)}=j)} \\\\\n&= \\sum_{i=1}^m\\sum_{j=1}^kw_j^{(i)}log\\frac{\\frac{1}{(2\\pi)^{n/2}|\\Sigma_j|^{1/2}}exp(-\\frac 12(x^{(i)}-\\mu_j)^T\\Sigma_j^{-1}(x^{(i)}-\\mu_j))\\cdot\\phi_j}{w_j^{(i)}}\n\\end{aligned}\n$$\n\n先关于 $\\mu_l$ 来进行最大化。如果去关于 $\\mu_l$ 的（偏）导数（derivative），得到：\n\n$$\n\\begin{aligned}\n\\nabla_{\\mu_l}&\\sum_{i=1}^m\\sum_{j=1}^kw_j^{(i)}log\\frac{\\frac{1}{(2\\pi)^{n/2}|\\Sigma_j|^{1/2}}exp(-\\frac 12(x^{(i)}-\\mu_j)^T\\Sigma_j^{-1}(x^{(i)}-\\mu_j))\\cdot\\phi_j}{w_j^{(i)}} \\\\\n&= -\\nabla_{\\mu_l}\\sum_{i=1}^m\\sum_{j=1}^kw_j^{(i)}\\frac 12(x^{(i)}-\\mu_j)^T\\Sigma_j^{-1}(x^{(i)}-\\mu_j) \\\\\n&= \\frac 12\\sum_{i=1}^m w_l^{(i)}\\nabla_{\\mu_l}2\\mu_l^T\\Sigma_l^{-1}x^{(i)}-\\mu_l^T\\Sigma_l^{-1}\\mu_l  \\\\\n&= \\sum_{i=1}^m w_l^{(i)}(\\Sigma_l^{-1}x^{(i)}-\\Sigma_l^{-1}\\mu_l)\n\\end{aligned}\n$$\n\n设上式为零，然后解出 $\\mu_l$ 就产生了更新规则（update rule）：\n\n$$\n\\mu_l := \\frac{\\sum_{i=1}^m w_l^{(i)}x^{(i)}}{\\sum_{i=1}^m w_l^{(i)}}\n$$\n\n这个结果咱们在之前的讲义中已经见到过了。\n\n咱们再举一个例子，推导在 $M$ 步骤中参数 $\\phi_j$ 的更新规则。把仅关于参数 $\\phi_j$ 的表达式结合起来，就能发现只需要最大化下面的表达式：\n\n$$\n\\sum_{i=1}^m\\sum_{j=1}^kw_j^{(i)}log\\phi_j\n$$\n\n然而，还有一个附加的约束，即 $\\phi_j$ 的和为$1$，因为其表示的是概率 $\\phi_j = p(z^{(i)} = j;\\phi)$。为了保证这个约束条件成立，即 $\\sum^k_{j=1}\\phi_j = 1$，我们构建一个拉格朗日函数（Lagrangian）：\n\n$$\n\\mathcal L(\\phi)=\\sum_{i=1}^m\\sum_{j=1}^kw_j^{(i)}log\\phi_j+\\beta(\\sum^k_{j=1}\\phi_j - 1)\n$$\n\n其中的 $\\beta$ 是 拉格朗日乘数（Lagrange multiplier）$^2$ 。求导，然后得到： \n\n$$\n\\frac{\\partial}{\\partial{\\phi_j}}\\mathcal L(\\phi)=\\sum_{i=1}^m\\frac{w_j^{(i)}}{\\phi_j}+1\n$$\n\n>2 这里我们不用在意约束条件 $\\phi_j \\ge 0$，因为很快就能发现，这里推导得到的解会自然满足这个条件的。\n\n设导数为零，然后解方程，就得到了：\n\n$$\n\\phi_j=\\frac{\\sum_{i=1}^m w_j^{(i)}}{-\\beta}\n$$\n\n也就是说，$\\phi_j\\propto \\sum_{i=1}^m w_j^{(i)}$。结合约束条件（constraint）$\\Sigma_j \\phi_j = 1$，可以很容易地发现 $-\\beta = \\sum_{i=1}^m\\sum_{j=1}^kw_j^{(i)} = \\sum_{i=1}^m 1 =m$. （这里用到了条件 $w_j^{(i)} =Q_i(z^{(i)} = j)$，而且因为所有概率之和等于$1$，即$\\sum_j w_j^{(i)}=1$）。这样我们就得到了在 $M$ 步骤中对参数 $\\phi_j$ 进行更新的规则了：\n\n$$\n\\phi_j := \\frac 1m \\sum_{i=1}^m w_j^{(i)}\n$$\n\n接下来对 $M$ 步骤中对 $\\Sigma_j$ 的更新规则的推导就很容易了。 \n \n\n### 参考","tags":["CS229"],"categories":["CS229"]},{"title":"[机器学习-CS229]混合高斯和期望最大化算法","url":"%2Fposts%2F4cf68548%2F","content":"# CS229 课程讲义中文翻译\n\n| 原作者 | 翻译 | 校对 |\n| --- | --- | --- |\n| [Andrew Ng  吴恩达](http://www.andrewng.org/) | [CycleUser](https://www.zhihu.com/people/cycleuser/columns) | [XiaoDong_Wang](https://github.com/Dongzhixiao) |\n\n\n|相关链接|\n|---|\n|[Github 地址](https://github.com/Kivy-CN/Stanford-CS-229-CN)|\n|[知乎专栏](https://zhuanlan.zhihu.com/MachineLearn)|\n|[斯坦福大学 CS229 课程网站](http://cs229.stanford.edu/)|\n|[网易公开课中文字幕视频](http://open.163.com/movie/2008/1/M/C/M6SGF6VB4_M6SGHFBMC.html)|\n\n\n# 第七章b\n\n#### 混合高斯 (Mixtures of Gaussians) 和期望最大化算法(the EM algorithm)\n\n在本章讲义中，我们要讲的是使用期望最大化算法（EM，Expectation-Maximization）来进行密度估计（density estimation）。\n\n一如既往，还是假设我们得到了某一个训练样本集$\\{x^{(1)},...,x^{(m)}\\}$。由于这次是非监督学习（unsupervised learning）环境，所以这些样本就没有什么分类标签了。\n\n我们希望能够获得一个联合分布 $p(x^{(i)},z^{(i)}) = p(x^{(i)}|z^{(i)})p(z^{(i)})$ 来对数据进行建模。其中的 $z^{(i)} \\sim Multinomial(\\phi)$ （即$z^{(i)}$ 是一个以 $\\phi$ 为参数的多项式分布，其中 $\\phi_j \\ge 0, \\sum_{j=1}^k \\phi_j=1$，而参数 $\\phi_j$ 给出了 $p(z^{(i)} = j)$），另外 $x^{(i)}|z^{(i)} = j \\sim N(μ_j,\\Sigma_j)$ **（译者注：$x^{(i)}|z^{(i)} = j$是一个以 $μ_j$ 和 $\\Sigma_j$ 为参数的正态分布）**。我们设 $k$ 来表示 $z^{(i)}$ 能取的值的个数。因此，我们这个模型就是在假设每个$x^{(i)}$ 都是从$\\{1, ..., k\\}$中随机选取$z^{(i)}$来生成的，然后 $x^{(i)}$ 就是服从$k$个高斯分布中的一个，而这$k$个高斯分布又取决于$z^{(i)}$。这就叫做一个**混合高斯模型（mixture of Gaussians model）。** 此外还要注意的就是这里的 $z^{(i)}$ 是**潜在的**随机变量（latent random variables），这就意味着其取值可能还是隐藏的或者未被观测到的。这就会增加这个估计问题（estimation problem）的难度。\n\n我们这个模型的参数也就是 $\\phi, \\mu$ 和 $\\Sigma$。要对这些值进行估计，我们可以写出数据的似然函数（likelihood）：\n\n$$\n\\begin{aligned}\nl(\\phi,\\mu,\\Sigma) &= \\sum_{i=1}^m \\log p(x^{(i)};\\phi,\\mu,\\Sigma) \\\\\n&= \\sum_{i=1}^m \\log \\sum_{z^{(i)}=1}^k p(x^{(i)}|z^{(i)};\\mu,\\Sigma)p(z^{(i)};\\phi)\n\\end{aligned}\n$$\n\n然而，如果我们用设上面方程的导数为零来尝试解各个参数，就会发现根本不可能以闭合形式（closed form）来找到这些参数的最大似然估计（maximum likelihood estimates）。（不信的话你自己试试咯。）\n\n随机变量 $z^{(i)}$表示着 $x^{(i)}$ 所属于的 $k$ 个高斯分布值。这里要注意，如果我们已知 $z^{(i)}$，这个最大似然估计问题就简单很多了。那么就可以把似然函数写成下面这种形式：\n\n$$\nl(\\phi,\\mu,\\Sigma)=\\sum_{i=1}^m \\log p(x^{(i)}|z^{(i)};\\mu,\\Sigma) + \\log p(z^{(i)};\\phi)\n$$\n\n对上面的函数进行最大化，就能得到对应的参数$\\phi, \\mu$ 和 $\\Sigma$：\n\n$$\n\\begin{aligned}\n&\\phi_j=\\frac 1m\\sum_{i=1}^m 1\\{z^{(i)}=j\\}, \\\\\n&\\mu_j=\\frac{\\sum_{i=1}^m 1\\{z^{(i)}=j\\}x^{(i)}}{\\sum_{i=1}^m 1\\{z^{(i)}=j\\}}, \\\\\n&\\Sigma_j=\\frac{\\sum_{i=1}^m 1\\{z^{(i)}=j\\}(x^{(i)}-\\mu_j)(x^{(i)}-\\mu_j)^T}{\\sum_{i=1}^m 1\\{z^{(i)}=j\\}}.\n\\end{aligned}\n$$\n\n事实上，我们已经看到了，如果 $z^{(i)}$ 是已知的，那么这个最大似然估计就几乎等同于之前用高斯判别分析模型（Gaussian discriminant analysis model）中对参数进行的估计，唯一不同在于这里的 $z^{(i)}$ 扮演了高斯判别分析当中的分类标签$^1$的角色。\n\n>1 这里的式子和之前在 PS1 中高斯判别分析的方程还有一些小的区别，这首先是因为在此处我们把 $z^{(i)}$ 泛化为多项式分布（multinomial），而不是伯努利分布（Bernoulli），其次是由于这里针对高斯分布中的每一项使用了一个不同的 $\\Sigma_j$。\n\n然而，在密度估计问题里面，$z^{(i)}$ 是不知道的。这要怎么办呢？\n期望最大化算法（EM，Expectation-Maximization）是一个迭代算法，有两个主要的步骤。针对我们这个问题，在 $E$ 这一步中，程序是试图去“猜测（guess）” $z^{(i)}$ 的值。然后在 $M$ 这一步，就根据上一步的猜测来对模型参数进行更新。由于在 $M$ 这一步当中我们假设（pretend）了上一步是对的，那么最大化的过程就简单了。下面是这个算法：\n\n\n&emsp;重复下列过程直到收敛（convergence）: {\n\n&emsp;&emsp;（$E$-步骤）对每个 $i, j$, 设 \n\n$$\nw_j^{(i)} := p(z^{(i)}=j|x^{(i)};\\phi,\\mu,\\Sigma)\n$$\n\n&emsp;&emsp;（$M$-步骤）更新参数：\n\n$$\n\\begin{aligned}\n&\\phi_j=\\frac 1m\\sum_{i=1}^m w_j^{(i)}, \\\\\n&\\mu_j=\\frac{\\sum_{i=1}^m w_j^{(i)}x^{(i)}}{\\sum_{i=1}^m w_j^{(i)}}, \\\\\n&\\Sigma_j=\\frac{\\sum_{i=1}^m w_j^{(i)}(x^{(i)}-\\mu_j)(x^{(i)}-\\mu_j)^T}{\\sum_{i=1}^m w_j^{(i)}}.\n\\end{aligned}\n$$\n\n} \n\n在 $E$ 步骤中，在给定 $x^{(i)}$ 以及使用当前参数设置（current setting of our parameters）情况下，我们计算出了参数 $z^{(i)}$ 的后验概率（posterior probability）。使用贝叶斯规则（Bayes rule），就得到下面的式子：\n\n$$\np(z^{(i)}=j|x^{(i)};\\phi,\\mu,\\Sigma)=\n\\frac{p(x^{(i)}|z^{(i)}=j;\\mu,\\Sigma)p(z^{(i)}=j;\\phi)}\n{\\sum_{l=1}^k p(x^{(i)}|z^{(i)}=l;\\mu,\\Sigma)p(z^{(i)}=l;\\phi)}\n$$\n\n上面的式子中，$p(z^{(i)}=j|x^{(i)};\\phi,\\mu,\\Sigma)$ 是通过评估一个高斯分布的密度得到的，这个高斯分布的均值为 $\\mu_i$，对$x^{(i)}$的协方差为$\\Sigma_j$；$p(z^{(i)} = j;\\phi)$ 是通过 $\\phi_j$ 得到，以此类推。在 $E$ 步骤中计算出来的 $w_j^{(i)}$ 代表了我们对 $z^{(i)}$ 这个值的“弱估计（soft guesses）”$^2$。\n\n>2 这里用的词汇“弱（soft）”是指我们对概率进行猜测，从 $[0, 1]$ 这样一个闭区间进行取值；而与之对应的“强（hard）”值得是单次最佳猜测，例如从集合 $\\{0,1\\}$ 或者 $\\{1, ..., k\\}$ 中取一个值。\n\n另外在 $M$ 步骤中进行的更新还要与 $z^{(i)}$ 已知之后的方程式进行对比。它们是相同的，不同之处只在于之前使用的指示函数（indicator functions），指示每个数据点所属的高斯分布，而这里换成了 $w_j^{(i)}$。\n\n$EM$ 算法也让人想起 $K$ 均值聚类算法，而在 $K$ 均值聚类算法中对聚类重心 $c(i)$ 进行了“强（hard）”赋值，而在 $EM$ 算法中，对$w_j^{(i)}$ 进行的是“弱（soft）”赋值。与 $K$ 均值算法类似，$EM$ 算法也容易导致局部最优，所以使用不同的初始参数（initial parameters）进行重新初始化（reinitializing），可能是个好办法。\n\n很明显，$EM$ 算法对 $z^{(i)}$ 进行重复的猜测，这种思路很自然；但这个算法是怎么产生的，以及我们能否确保这个算法的某些特性，例如收敛性之类的？在下一章的讲义中，我们会讲解一种对 $EM$ 算法更泛化的解读，这样我们就可以在其他的估计问题中轻松地使用 $EM$ 算法了，只要这些问题也具有潜在变量（latent variables），并且还能够保证收敛。\n\n\n### 参考","tags":["CS229"],"categories":["CS229"]},{"title":"[机器学习-CS229]k均值聚类算法","url":"%2Fposts%2F9cb7cfe3%2F","content":"# CS229 课程讲义中文翻译\n\n| 原作者 | 翻译 | 校对 |\n| --- | --- | --- |\n| [Andrew Ng  吴恩达](http://www.andrewng.org/) | [CycleUser](https://www.zhihu.com/people/cycleuser/columns) | [XiaoDong_Wang](https://github.com/Dongzhixiao) |\n\n\n|相关链接|\n|---|\n|[Github 地址](https://github.com/Kivy-CN/Stanford-CS-229-CN)|\n|[知乎专栏](https://zhuanlan.zhihu.com/MachineLearn)|\n|[斯坦福大学 CS229 课程网站](http://cs229.stanford.edu/)|\n|[网易公开课中文字幕视频](http://open.163.com/movie/2008/1/M/C/M6SGF6VB4_M6SGHFBMC.html)|\n\n\n# 第七章a\n\n#### $k$均值聚类算法(k-means clustering algorithm)\n\n在聚类的问题中，我们得到了一组训练样本集 $\\{x^{(1)},...,x^{(m)}\\}$，然后想要把这些样本划分成若干个相关的“类群（clusters）”。其中的 $x^{(i)}\\in R^n$，而并未给出分类标签 $y^{(i)}$ 。所以这就是一个无监督学习的问题了。\n$K$ 均值聚类算法如下所示：\n1. 随机初始化（initialize）**聚类重心（cluster centroids）** $\\mu_1, \\mu_2,..., \\mu_k\\in R^n$ 。 \n2. 重复下列过程直到收敛（convergence）: { \n对每个 $i$，设 \n\n$$\nc^{(i)}:=arg\\min_j||x^{(i)}-\\mu_j||^2\n$$\n\n对每个 $j$，设 \n\n$$\n\\mu_j:=\\frac{\\sum_{i=1}^m1\\{c^{(i)}=j\\}x^{(i)}}{\\sum_{i=1}^m1\\{c^{(i)}=j\\}}\n$$\n\n} \n\n在上面的算法中，$k$ 是我们这个算法的一个参数，也就是我们要分出来的群组个数（number of clusters）；而聚类重心 $\\mu_j$ 表示的是我们对各个聚类的中心位置的当前猜测。在上面算法的第一步当中，需要初始化（initialize）聚类重心（cluster centroids），可以这样实现：随机选择 $k$ 个训练样本，然后设置聚类重心等于这 $k$ 个样本 各自的值。（当然也还有其他的初始化方法。） \n\n算法的第二步当中，循环体内重复执行两个步骤：（i）将每个训练样本$x^{(i)}$ “分配（assigning）”给距离最近的聚类重心$\\mu_j$；（ii）把每个聚类重心$\\mu_j$ 移动到所分配的样本点的均值位置。下面的 图1 就展示了运行 $k$均值聚类算法的过程。\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note7af1.png)\n\n图1：$k$ 均值聚类算法。图中的圆形点表示的是训练样本，交叉符号表示的是聚类重心。(a) 原始训练样本数据集。 (b) 随机初始化的聚类重心（这里的初始化方法就跟我们上面说的不一样，并没有从训练样本中选择两个点）。(c-f) 运行 $k$ 均值聚类算法中的两步迭代的示意图。在每一次迭代中，我们把每个训练样本分配给距其最近的聚类重心（用同样颜色标识出），然后把聚类重心移动到所分配的样本的均值位置。（用颜色区分效果最好了。）图片引用自 Michael Jordan。\n\n$K$ 均值聚类算法能保证收敛性么？可以的，至少在一定意义上能这么说。尤其是我们可以定义一个下面这样的函数作为**失真函数（distortion function）：**\n\n$$\nJ(c,\\mu)=\\sum_{i=1}^m ||x^{(i)}-\\mu_{c^{(i)}}||^2\n$$\n\n这样就可以用 $J$ 来衡量每个样本 $x^{(i)}$ 和对应的聚类重心$\\mu_{c^{(i)}}$之间距离的平方和。很明显能看出 $k$ 均值聚类算法正好就是对 $J$ 的坐标下降过程。尤其是内部的循环体中，$k$ 均值聚类算法重复对 $J$ 进行最小化，当 $\\mu$ 固定的时候用 $c$ 来最小化 $J$，当 $c$ 固定的时候则用 $\\mu$ 最小化 $J$。这样就保证了 $J$ 是单调降低的（monotonically decrease），它的值也就必然收敛（converge）。（通常这也表明了 $c$ 和 $\\mu$ 也收敛。在理论上来讲，$k$均值 可能会在几种不同的聚类之间摆动(oscillate)，也就是说某些组不同值的 $c$ 和/或 $\\mu$ 对应有完全相同的 $J$ 值，不过在实践中这种情况几乎不会遇到。）\n\n失真函数 $J$，是一个非凸函数（non-convex function），所以对 $J$ 进行坐标下降（coordinate descent）并不一定能够收敛到全局最小值（global minimum）。也就是说，$k$ 均值聚类算法可能只是局部最优的（local optima）。通常除了这个问题之外，$k$ 均值聚类效果都不错，能给出很好的聚类。如果你担心陷入到某些比较差的局部最小值，通常可以多次运行 $k$ 均值距离（使用不同的随机值进行来对聚类重心 $\\mu_j$ 进行初始化）。然后从所有的不同聚类方案（clusterings）中，选择能提供最小失真（distortion） $J(c,\\mu)$ 的。\n\n\n### 参考","tags":["CS229"],"categories":["CS229"]},{"title":"[机器学习-CS229]感知机与大型边界分类器","url":"%2Fposts%2F917ab4a6%2F","content":"# CS229 课程讲义中文翻译\n\n\n| 原作者 | 翻译 | 校对 |\n| --- | --- | --- |\n| [Andrew Ng  吴恩达](http://www.andrewng.org/) | [CycleUser](https://www.zhihu.com/people/cycleuser/columns) | [XiaoDong_Wang](https://github.com/Dongzhixiao) |\n\n|相关链接|\n|---|\n|[Github 地址](https://github.com/Kivy-CN/Stanford-CS-229-CN)|\n|[知乎专栏](https://zhuanlan.zhihu.com/MachineLearn)|\n|[斯坦福大学 CS229 课程网站](http://cs229.stanford.edu/)|\n|[网易公开课中文字幕视频](http://open.163.com/movie/2008/1/M/C/M6SGF6VB4_M6SGHFBMC.html)|\n\n\n# 第六章\n\n#### 1 感知器（perceptron）和大型边界分类器（large margin classifiers）\n\n本章是讲义中关于学习理论的最后一部分，我们来介绍另外机器学习模式。在之前的内容中，我们考虑的都是**批量学习**的情况，即给了我们训练样本集合用于学习，然后用学习得到的假设 $h$ 来评估和判别测试数据。在本章，我们要讲一种新的机器学习模式：**在线学习**，这种情况下，我们的学习算法要在进行学习的同时给出预测。\n\n学习算法会获得一个样本序列，其中内容为有次序的学习样本，$(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), ...(x^{(m)},y^{(m)})$。最开始获得的就是$x^{(1)}$，然后需要预测$y^{(1)}$。在完成了这个预测之后，再把$y^{(1)}$的真实值告诉给算法（然后算法就利用这个信息来进行某种学习了）。接下来给算法提供$x^{(2)}$，再让算法对$y^{(2)}$进行预测，然后再把$y^{(2)}$ 的真实值告诉给算法，这样算法就又能学习到一些信息了。这样的过程一直持续到最末尾的样本$(x^{(m)},y^{(m)})$。在这种在线学习的背景下，我们关心的是算法在此过程中出错的总次数。因此，这适合需要一边学习一边给出预测的应用情景。\n\n接下来，我们将对感知器学习算法（perceptron algorithm）的在线学习误差给出一个约束。为了让后续的推导（subsequent derivations）更容易，我们就用正负号来表征分类标签，即设 $y =\\in \\{-1, 1\\}$。\n\n回忆一下感知器算法（在第二章中有讲到），其参数 $\\theta \\in R^{n+1}$，该算法据下面的方程来给出预测：\n\n$$\nh_\\theta(x)=g(\\theta^T x)\\qquad (1)\n$$\n\n其中：\n\n$$\ng(z)= \\begin{cases} 1 & if\\quad z\\ge 0 \\\\\n-1 & if\\quad z<0 \\end{cases}\n$$\n\n然后，给定一个训练样本 $(x, y)$，感知器学习规则（perceptron learning rule）就按照如下所示来进行更新。如果 $h_\\theta(x) = y$，那么不改变参数。若二者相等关系不成立，则进行更新$^1$:\n\n$$\n\\theta :=\\theta+yx\n$$\n\n>1 这和之前我们看到的更新规则（update rule）的写法稍微有一点点不一样，因为这里我们把分类标签（labels）改成了 $y \\in \\{-1, 1\\}$。另外学习速率参数（learning rate parameter） $\\alpha$ 也被省去了。这个速率参数的效果只是使用某些固定的常数来对参数 $\\theta$ 进行缩放，并不会影响生成器的行为效果。\n\n当感知器算法作为在线学习算法运行的时候，每次对样本给出错误判断的时候，则更新参数，下面的定理给出了这种情况下的在线学习误差的约束边界。要注意，下面的错误次数的约束边界与整个序列中样本的个数 $m$ 不具有特定的依赖关系（explicit dependence），和输入特征的维度 $n$ 也无关。\n\n**定理 (Block, 1962, and Novikoff, 1962)。** 设有一个样本序列：$(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), ...(x^{(m)},y^{(m)})$。假设对于所有的 $i$ ，都有 $||x^{(i)}|| \\le D$，更进一步存在一个单位长度向量 $u (||u||_2 = 1)$ 对序列中的所有样本都满足 $y(i) \\cdot (u^T x^{(i)}) \\ge \\gamma$（例如，如果$y^{(i)} = 1$，则$u^T x^{(i)} \\ge \\gamma$, 而如果 $y^{(i)} = -1$，则 $u^T x^{(i)} \\le -\\gamma$，以便 $u$ 就以一个宽度至少为 $\\gamma$ 的边界分开了样本数据）。而此感知器算法针对这个序列给出错误预测的总数的上限为 $(D/\\gamma)^2$ 。\n\n**证明:** 感知器算法每次只针对出错的样本进行权重更新。设 $\\theta(k)$ 为犯了第 $k$ 个错误（k-th mistake）的时候的权重。则 $\\theta^{(1)} = -\\theta$（因为初始权重为零），若第 $k$ 个错误发生在样本 $(x^{(i)},y^{(i)})$，则$g((x(i))^T \\theta^{(k)}) \\ne y^{(i)}$，也就意味着：\n\n$$\n(x^{(i)})^T\\theta^{(k)}y^{(i)}\\le 0\\qquad(2)\n$$\n\n另外根据感知器算法的定义，我们知道 $\\theta^{(k+1)} = \\theta^{(k)} + y^{(i)}x^{(i)}$\n然后就得到： \n\n$$\n\\begin{aligned}\n(\\theta^{(k+1)})^T u &= (\\theta^{(k)})^T u + y^{(i)}(x^{(i)})^T u\\\\\n&\\ge  (\\theta^{(k)})^T u + \\gamma\n\\end{aligned}\n$$\n\n利用一个简单的归纳法（straightforward inductive argument）得到：\n\n$$\n(\\theta^{(k+1)})^T u \\ge k\\gamma\\qquad (3)\n$$\n\n还是根据感知器算法的定义能得到：\n\n$$\n\\begin{aligned}\n||\\theta^{(k+1)}||^2 &= ||\\theta^{(k)} + y^{(i)}x^{(i)}||^2 \\\\\n&= ||\\theta^{(k)}||^2 + ||x^{(i)}||^2 + 2y^{(i)}(x^{(i)})^T\\theta^{(k)} \\\\\n&\\le ||\\theta^{(k)}||^2 + ||x^{(i)}||^2 \\\\\n&\\le ||\\theta^{(k)}||^2 + D\\qquad\\qquad(4)\n\\end{aligned}\n$$\n\n上面这个推导过程中，第三步用到了等式(2)。另外这里还要使用一次简单归纳法，上面的不等式(4) 表明：\n\n$$\n||\\theta^{(k+1)}||^2 \\le KD^2\n$$\n\n把上面的等式 (3) 和不等式 (4) 结合起来：\n\n$$\n\\begin{aligned}\n\\sqrt{k}D &\\ge ||\\theta^{(k+1)}|| \\\\\n          &\\ge (\\theta^{(k+1)})^T u \\\\\n          &\\ge k\\gamma\n\\end{aligned}\n$$\n\n上面第二个不等式是基于 $u$ 是一个单位长度向量（$z^T u = ||z||\\cdot ||u|| cos \\phi\\le ||z||\\cdot ||u||$，其中的$\\phi$是向量 $z$ 和向量 $u$ 的夹角）。结果则表明 $k\\le (D/\\gamma)^2$。因此，如果感知器犯了一个第 $k$ 个错误，则 $k\\le (D/\\gamma)^2$。\n\n\n\n\n\n### 参考","tags":["CS229"],"categories":["CS229"]},{"title":"[机器学习-CS229]正则化与模型选择","url":"%2Fposts%2Fb7fb7e1f%2F","content":"# CS229 课程讲义中文翻译\n\n| 原作者 | 翻译 | 校对 |\n| --- | --- | --- |\n| [Andrew Ng  吴恩达](http://www.andrewng.org/) | [CycleUser](https://www.zhihu.com/people/cycleuser/columns) | [LY2015](https://github.com/LIUYOU2015) [XiaoDong_Wang](https://github.com/Dongzhixiao) |\n\n|相关链接|\n|---|\n|[Github 地址](https://github.com/Kivy-CN/Stanford-CS-229-CN)|\n|[知乎专栏](https://zhuanlan.zhihu.com/MachineLearn)|\n|[斯坦福大学 CS229 课程网站](http://cs229.stanford.edu/)|\n|[网易公开课中文字幕视频](http://open.163.com/movie/2008/1/M/C/M6SGF6VB4_M6SGHFBMC.html)|\n\n# 第五章\n\n### 第七部分 正则化与模型选择\n\n设想一个机器学习的问题，我们要从一系列不同的模型中进行挑选。例如，我们可能是用一个多项式回归模型 (polynomial regression model) $h_\\theta (x)=g(\\theta_0+\\theta_1x+\\theta_2x^2+\\cdots+\\theta_kx^k)$ 想要判定这里的多项式次数 $k$ 应该是多少，$0, 1, …, 或者10$。那么我们怎么才能自动选择一个能够在偏差 (bias)/方差(variance)$^1$之间进行权衡的模型呢? 或者换一个说法，假如我们希望能够自动选出来一个带宽参数 (bandwidth parameter) $\\tau$ 来用于局部加权回归(locally weighted regression，所谓为 LWR，参考 note1的第2节)，或 者要自动选出一个参数 $C$ 用于拉格朗日正则化的支持向量机算法(l1-regularized SVM)。怎么来实现呢? \n\n>1 考虑到前面的讲义中我们已经提到过偏差(bias)/方差(variance)这两个概念很大区别，有的读者可能觉得是不是应该把它们叫做一对“孪生 (twin)”魔鬼(evils)。或许可以把它们俩当做是一对异卵双胞胎(non-identical twins)。理解概念差别就好了，怎么说什么的都不要紧的。\n\n为了具体一些，咱们这一系列讲义中都假设备选集合的模型个数有限 $M = \\{M_1,\\cdots,M_d\\}$。例如，在我们上面刚刚随便举的本章第一个例子中，$M_i$ 就是一个 $i$次多项式拟合模型(i-th order polynomial regression model)。(其实把 $M$ 扩展到无穷集合 也不难的。$^2$)换个说法就是，如果我们要从支持向量机算法 (SVM)、神经网络算法(neural network)、逻辑回归算法(logistic regression)当中三选一，那么这里的 $M$ 就应该都 包含了这些模型了。 \n\n>2 如果我们要从一个无穷的模型集合中进行选取一个，假如说要选取一个带宽参数 $\\tau\\in \\mathbb R^+$ (正实数)的某个可能的值，可以将 $\\tau$ 离散化，而只考虑 有限的一系列值。更广泛来说，我们要讲到的大部分算法都可以看做在模型空间(space of models) 中进行优化搜索 (performing optimization search)的 问题，这种搜索也可以在无穷模型类(infinite model classes)上进行。 \n\n#### 1 交叉验证（Cross Validation）\n\n假如我们得到了一个训练集 $S$。我们已经了解了经验风险最小化(empirical risk minimization，缩写为 ERM)，那么接下来就要通过使用 ERM 来进行模型选择来推导出一种新的算法: \n\n1. 对训练集 $S$ 中的每一个模型 (model) $M_i$ 进行训练，得到某假设类 (hypothesis) $h_i$\n2. 从这些假设中选取训练误差最小的假设 (hypothesis)\n\n上面这个算法是行不通的。比如考虑要选择多项式的阶(最高次项的次数)的情况。多项式的阶越高，对训练集 $S$ 的拟合程度就越好，训练误差自然也就更小。然而，这个方法选出来 的总是那种波动非常强 (high-variance) 的高次多项式模型 (high-degree polynomial model) ，这种情况我们之前就讲过了，通常都是很差的选择。 \n\n下面这个算法就更好一些。这个方法叫保留交叉验证 (hold-out cross validation)，也叫简单交叉验证 (simple cross validation)，步骤如下：\n\n1. 随机拆分训练集 $S$ 成 $S_{train}$ (例如，可以选择整体数据中 的 70% 用于训练) 和 $S_{cv}$ (训练集中剩余的 30%用于验 证)。这里的 $S_{cv}$ 就叫做保留交叉验证集(hold-out cross validation set)。 \n2. 只对集合 $S_{train}$ 中的每一个模型 $M_i$ 进行训练，然后得到假设类(hypothesis) $h_i$。 \n3. 筛选并输出对保留交叉验证集有最小误差 $\\hat\\epsilon_{S_{cv}}(h_i)$ 的假设$h_i$ 。(回忆一下，这里的 $\\hat\\epsilon_{S_{cv}}(h_i)$ 表示的是假设 $h$ 在保留交叉验证集 $S_{cv}$ 中的样本的经验误差(empirical error)。) \n\n这样通过在一部分未进行训练的样本集合 $S_{cv}$ 上进行测试， 我们对每个假设 $h_i$ 的真实泛化误差 (generalization error) 就能得到一个比上一个方法更好的估计，然后就能选择出来一个有最小估计泛化误差 (smallest estimated generalization error) 的假设了。通常可以选择 1/4 到 1/3 的数据样本用来作为保留交叉验证集(hold out cross validation set)，30% 是一个很典型的选择。\n\n还有另外一种备选方法，就是在第三步的时候，也可以换做选 择与最小估计经验误差 $\\hat\\epsilon_{S_{cv}}(h_i)$ 对应的模型 $M_i$ ，然后对整 个训练样本数据集 $S$ 使用 $M_i$ 来进行再次训练。(这个思路通常都不错，但有一种情景例外，就是学习算法对初始条件和数据的扰动(perturbations of the initial conditions and/or data) 非常敏感的情况。在这样的方法中，适用于 $S_{train}$ 的模型未必就能够同样适用于 $S_{cv}$，这样就最好还是放弃再训练的步骤 (forgo this retraining step)。) \n\n使用保留交叉验证集(hold out cross validation set)的一个弊端就是“浪费(waste)”了训练样本数据集的 30% 左右。甚至即便我们使用了备选的那个针对整个训练集使用模型进行 重新训练的步骤，也还不成，因为这无非是相当于我们只尝试在一个 $0.7m$ 规模的训练样本集上试图寻找一个好的模型来解决一个机器学习问题，而并不是使用了全部的 $m$ 个训练样 本，因为我们进行测试的都是每次在仅 $0.7m$ 规模样本上进 行训练而得到的模型。当然了，如果数据非常充足，或者是很廉价的话，也可以用这种方法，而**如果训练样本数据本身就很 稀缺的话（例如说只有 20 个样本），那就最好用其他方法了。** \n\n下面就是一种这样的方法，名字叫 **k-折交叉验证**（k-fold cross validation），这样每次的用于验证的保留数据规模都更小:  \n\n1. 随机将训练集 $S$ 切分成 $k$ 个不相交的子集。其中每一个子集的规模为 $m/k$ 个训练样本。这些子集为 $S_1,\\cdots,S_k$\n\n2. 对每个模型 $M_i$，我们都按照下面的步骤进行评估(evaluate):\n\n   对 $j=1,\\cdots,k$ \n\n   - 在 $S_1\\cup\\cdots\\cup S_{j-1}\\cup S_{j+1}\\cup\\cdots\\cup S_k$ (也就是除了 $S_j$ 之外的其他数据)，对模型 $M_i$ 得到假设 $h_{ij}$ 。接下来针对 $S_j$ 使用假设 $h_{ij}$ 进行测试，得到经验误差 $\\hat\\epsilon_{S_{cv}}(h_{ij})$ \n\n     对$\\hat\\epsilon_{S_{cv}}(h_{ij})$ 取平均值，计算得到的值就当作是模型 $M_i$ 的估计泛化误差（estimated generalization error）\n\n3. 选择具有最小估计泛化误差(lowest estimated generalization error)的模型 $M_i$ 的，然后在整个训练样本集 $S$ 上重新训练该模型。这样得到的假设 (hypothesis)就可以输出作为最终结果了。  \n\n通常这里进行折叠的次数 (number of folds) $k$ 一般是 10，即 $k = 10$。这样每次进行保留用于验证的数据块就只有 $1/k$ ，这 就比之前的 30% 要小多了，当然这样一来这个过程也要比简单的保留交叉验证方法消耗更多算力成本，因为现在需要对每个模型都进行 $k$ 次训练。 \n\n虽然通常选择都是设置 $k = 10$，不过如果一些问题中数据量 确实很匮乏，那有时候也可以走一点极端，设 $k = m$，这样是为了每次能够尽可能多地利用数据，尽可能少地排除数据。这 种情况下，我们需要在训练样本集 $S$ 中除了某一个样本外的其他所有样本上进行训练，然后在保留出来的单独样本上进行检验。然后把计算出来的 $m = k$ 个误差放到一起求平均值， 这样就得到了对一个模型的泛化误差的估计。这个方法有专门的名字，由于每次都保留了一个训练样本，所以这个方法就叫做**弃一法交叉验证**(leave-one-out cross validation)。\n\n最后总结一下，咱们讲了不同版本的交叉验证，在上文中是用来作为选择模型的方法，实际上也可以更单纯地用来对一个具体的模型或者算法进行评估。例如，如果你已经实现了某中学习算法，然后想要估计一下针对你的用途这个算法的性能表现 (或者是你创造了一种新的学习算法，然后希望在技术论文中 报告你的算法在不同测试集上的表现)，交叉验证都是个很好 的解决方法。 \n\n#### 2 特征选择（Feature Selection）\n\n模型选择(model selection)的一个非常重要的特殊情况就是特征选择(feature selection)。设想你面对一个监督学习问题 (supervised learning problem)，其中特征值的数量 $n$ 特别大 (甚至可能比训练样本集规模还大，即$n >> m$)，然而你怀疑可能只有一小部分的特征 (features) 是与学习任务“相关 (relevant)”的。甚至即便是针对 $n$ 个输入特征值使用一个简单的线性分类器 (linear classifier，例如感知器 perceptron)，你的假设类(hypothesis class)的 $VC$ 维(VC dimension) 也依然能达到 $O(n)$，因此有过拟合 (overfitting) 的潜在风险，除非训练样本集也足够巨大 (fairly large)。 \n\n在这样的一个背景下，你就可以使用一个特征选择算法，来降 低特征值的数目。假设有 $n$ 个特征，那么就有 $2^n$ 种可能的特征子集 (因为 $n$ 个特征中的任意一个都可以被某个特征子集(feature subsets)包含或者排除)，因此特征选择(feature selection)就可以看做是一个对 $2^n$ 种可能的模型进行选择 (model selection problem)的形式。对于特别大的 $n$，要是彻底枚举(enumerate)和对比全部 $2^n$ 种模型，成本就太高了， 所以通常的做法都是使用某些启发式的搜索过程(heuristic search procedure)来找到一个好的特征子集。下面的搜索过程叫做**向前搜索(forward search) ：**\n\n1. 初始化一个集合为空集 $\\mathcal F=\\emptyset$\n\n2. 循环下面的过程{\n\n   (a) 对于 $i=1,\\cdots,n$ 如果 $i\\notin \\mathcal F$，则令 $\\mathcal F_i=\\mathcal F\\cup \\{i\\}$，然后使用某种交叉验证来评估特征 $\\mathcal F_i$ \n\n   (b) 令 $\\mathcal F$ 为(a)中最佳特征子集\n\n   }\n\n3. 整个搜索过程中筛选出来了最佳特征子集(best feature subset)，将其输出。 \n\n算法的外层循环可以在 $\\mathcal F=\\{1,\\cdots,n\\}$ 达到全部特征规模时停止，也可以在 $|\\mathcal F|$ 超过某个预先设定的阈值时停止（阈值和你想要算法用到特征数量最大值有关）。\n\n这个算法描述的是对模型特征选择进行包装(**包装器特征选择，Wrapper feature selection** )的一个实例，此算法本身就是一个将学习算法进行“打包(wraps)”的过程，然后重复调用这个学习算法来评估(evaluate)此算法对不同的特征子集(feature subsets)的处理效果。除了向前搜索外，还可以使用其他的搜索过程。例如，可以**逆向搜索(backward search)**，从$\\mathcal F = \\{1, ..., n\\}$ ，即规模等同于全部特征开始，然后重复，每次删减一个特征，直到 $\\mathcal F$ 为空集时终止。 \n\n这种包装器特征选择算法(Wrapper feature selection algorithms)通常效果不错，不过对算力开销也很大，尤其是要对学习算法进行多次调用。实际上，完整的向前搜索 (forward search，也就是 $\\mathcal F$ 从空集开始，到最终达到整个样本集规模，即 $\\mathcal F =\\{1, ..., n\\}$ 终止)将要对学习算法调用约 $O(n^2)$ 次。 \n\n**过滤器特征选择(Filter feature selection methods)** 给出的特征子集选择方法更具有启发性(heuristic)，而且在算力上的开销成本也更低。这里的一个思路是，计算一个简单的分数 $S(i)$，用来衡量每个特征 $x_i$ 对分类标签(class labels) $y$ 所能体现的信息量。然后，只需根据需要选择最大分数 $S(i)$ 的 $k$ 个特征。 \n\n怎么去定义用于衡量信息量的分值 $S(i)$ 呢?一种思路是使用 $x_i$ 和 $y$ 之间的相关系数的值(或其绝对值)，这可以在训练 样本数据中算出。这样我们选出的就是与分类标签(class labels)的关系最密切的特征值(features)。实践中，通常（尤其当特征 $x_i$ 为离散值(discrete-valued features)）选择 $x_i$ 和 $y$ 的**互信息( mutual information, ${\\rm{MI}}(x_i, y)$ )** 来作为 $S(i)$ 。 \n\n$$\n{\\rm{MI}}(x_i, y)=\\sum_{x_i\\in\\{0, 1\\}}\\sum_{y\\in\\{0,1\\}}p(x_i,y)\\log\\frac{p(x_i,y)}{p(x_i)p(y)}\n$$\n\n(上面这个等式假设了 $x_i$ 和 $y$ 都是二值化；更广泛的情况下将会超过变量的范围 。)上式中的概率$p(x_i,y)$，$p(x_i)$ 和 $p(y)$ 都可以根据它们在训练集上的经验分布(empirical distributions)而推测(estimated)得到。 \n\n要对这个信息量分值的作用有一个更直观的印象，也可以将互信息(mutual information)表达成 $KL$ 散度(Kullback-Leibler divergence，也称 $KL$ 距离，常用来衡量两个概率分布的距离): \n\n$$\n{\\rm{MI}}(x_i,y)={\\rm KL}(p(x_i,y)\\,\\|\\,p(x_i)p(y))\n$$\n\n在下一节当中，你会与 $KL$ 散度进行更多的接触，这里比较通俗地说，这个概念对 $p(x_i,y)$ 和 $p(x_i)p(y)$ 的概率分布的差异程度给出一个衡量。如果 $x_i$ 和 $y$ 是两个独立的随机变量，那么必然有 $p(x_i, y) = p(x_i)p(y)$，而两个分布之间的 $KL$ 散度就应该是 $0$。这也符合下面这种很自然的认识：如果 $x_i$ 和 $y$ 相互独立，那么 $x_i$ 很明显对 $y$ 是“完全无信息量”(non-informative)，因此对应的信息量分值 $S(i)$ 就应该很小。与之相反地，如果 $x_i$ 对 $y$ “有很大的信息量 (informative)”，那么这两者的互信息 ${\\rm MI}(x_i,y)$ 就应该很大。  \n\n最后一个细节：现在你已经根据信息量分值 $S(i)$ 的高低来对特征组合(features)进行了排序，那么要如何选择特征个数 $k$ 呢?一个标准办法就是使用交叉验证(cross validation)来从可能的不同 $k$ 值中进行筛选。例如，在对文本分类(text classification)使用朴素贝叶斯方法(naive Bayes)，这个问题中的词汇规模(vocabulary size) $n$ 通常都会特别大，使用 交叉验证的方法来选择特征子集(feature subset)，一般都 提高分类器精度。 \n\n#### 3 贝叶斯统计(Bayesianstatistics)和正则化 (regularization) \n\n在本章，我们要讲一下另一种工具，用于我们对抗过拟合(overfitting)。 \n\n在本章的开头部分，我们谈到了使用最大似然(maximum likelihood，缩写为 ML)来进行参数拟合，然后根据下面的式子来选择参数: \n\n$$\n\\theta_{\\rm ML}=\\arg \\max_{\\theta}\\prod_{i=1}^{m}p(y^{(i)}|x^{(i)};\\theta) \n$$ \n\n在后续的讨论中，我们都是把 $\\theta$ 看作是一个未知参数 (unknown parameter)。在**频率**统计(frequentist statistics) 中，往往采用的观点是认为 $\\theta$ 是一个未知的常量(constant- valued)。在频率论(frequentist)的世界观中， $\\theta$ 只是碰巧未知，而不是随机的，而我们的任务就是要找出某种统计过程 (statistical procedures，例如最大似然法(maximum likelihood))，来对这些参数进行估计。 \n\n另外一种解决我们这个参数估计问题的方法是使用**贝叶斯**世界观，把 $\\theta$ 当做是未知的随机变量。在这个方法中，我们要先指定一个在 $\\theta$ 上的**先验分布(prior distribution)** $p(\\theta)$，这个 分布表达了我们关于参数的“预先判断(prior beliefs)”。给定一个训练集合 $S = \\{(x^{(i)},y^{(i)})\\}^m_{i=1}$，当我们被要求对一个新的 $x$ 的值进行预测的时候，我们可以计算在参数上的后验分布 (posterior distribution): \n\n$$\n\\begin{aligned}\np(\\theta|S)\n&=\\frac{p(S|\\theta)p(\\theta)}{p(S)}\\\\ \n&=\\frac{(\\prod_{i=1}^{m}p(y^{(i)}|x^{(i)},\\theta))p(\\theta)}{\\int_{\\theta} {\\left(\\prod_{i=1}^{m}p(y^{(i)}|x^{(i)},\\theta)p(\\theta)\\right)}d\\theta}\\qquad (1)\n\\end{aligned}\n$$\n\n在上面的等式中，$p(y(i)|x(i),\\theta)$ 来自你所用的机器学习问题中的模型。例如，如果你使用贝叶斯逻辑回归(Bayesian logistic regression)，你可能就会选择 $p(y^{(i)}|x^{(i)},\\theta)=h_\\theta(x^{(i)})^{y^{(i)}} (1-h_\\theta(x^{(i)}))^{(1-y^{(i)})}$ 其中，$h_\\theta(x^{(i)})=1/(1+\\exp(-\\theta^Tx^{(i)}))$.$^3$\n\n>3 由于我们在这里把 $\\theta$ 看作是一个随机变量了，就完全可以在其值上使用 条件判断，然后写成 “$p(y|x, \\theta)$” 来替代 “$p(y|x; \\theta)$”。  \n\n若有一个新的测试样本 $x$，然后要求我们对这个新样本进行预测，我们可以使用 $\\theta$ 上的后验分布(posterior distribution)来计算分类标签(class label)上的后验分布: \n\n$$\np(y|x,S)=\\int_\\theta p(y|x,\\theta)p(\\theta|S)d\\theta\\qquad (2)\n$$\n\n在上面这个等式中，$p(\\theta|S)$ 来自等式 (1)。例如，如果目标是要根据给定的 $x$ 来预测对应的 $y$ 的值，那就可以输出$^4$: \n\n>4 如果 $y$ 是一个离散值(discrete-valued)，那么此处的积分(integral)就用求和(summation)来替代。  \n\n$$\nE[y|x,S]=\\int_y y p(y|x,S)dy\n$$\n\n这里我们简单概述的这个过程，可认为是一种“完全贝叶斯 (fully Bayesian)”预测，其中我们的预测是通过计算相对于 $\\theta$ 上的后验概率 $p(\\theta|S)$ 的平均值而得出的。然而很不幸，这 个后验分布的计算通常是比较困难的。这是因为如等式 (1) 所示，这个计算需要对 $\\theta$ 进行积分(integral)，而 $\\theta$ 通常是高维度的(high-dimensional)，这通常是不能以闭合形式 (closed-form)来实现的。 \n\n因此在实际应用中，我们都是用一个与 $\\theta$ 的后验分布 (posterior distribution)近似的分布来替代。常用的一个近似是把对 $\\theta$ 的后验分布（正如等式$(2)$中所示）替换为一个单点估计(single point estimate)。对 $\\theta$ 的最大后验估计 (MAP，maximum a posteriori estimate)为: \n\n$$\n\\theta_{MAP}=\\arg \\max_\\theta \\prod_{i=1}^{m} p(y^{(i)}|x^{(i)})p(\\theta)\n$$\n\n注意到了么，这个式子基本和对 $\\theta$ 的最大似然估计(ML (maximum likelihood) estimate)是一样的方程，除了末尾多了 一个先验概率分布 $p(\\theta)$。 \n\n实际应用里面，对先验概率分布 $p(\\theta)$ 的常见选择是假设 $\\theta\\sim N(0 , \\tau ^2I)$。使用这样的一个先验概率分布，拟合出来的参数 $\\theta_{MAP}$ 将比最大似然得到的参数有更小的范数(norm)。 (更多细节参考习题集 #3。)在实践中，贝叶斯最大后验估计(Bayesian MAP estimate)比参数的最大似然估计 (ML estimate of the parameters)更易于避免过拟合。例如，贝叶斯逻辑回归(Bayesian logistic regression)就是一种非常有效率的文本分类(text classification)算法，即使文本分类中参数规模 $n$ 通常是远远大于样本规模 $m$ 的，即 $n\\gg m$。 \n\n\n### 参考","tags":["CS229"],"categories":["CS229"]},{"title":"[机器学习-CS229]学习理论","url":"%2Fposts%2Fbd524738%2F","content":"# CS229 课程讲义中文翻译\n\n| 原作者 | 翻译 | 校对 |\n| --- | --- | --- |\n| [Andrew Ng  吴恩达](http://www.andrewng.org/) | [CycleUser](https://www.zhihu.com/people/cycleuser/columns) | [LY2015](https://github.com/LIUYOU2015) [XiaoDong_Wang](https://github.com/Dongzhixiao) |\n\n|相关链接|\n|---|\n|[Github 地址](https://github.com/Kivy-CN/Stanford-CS-229-CN)|\n|[知乎专栏](https://zhuanlan.zhihu.com/MachineLearn)|\n|[斯坦福大学 CS229 课程网站](http://cs229.stanford.edu/)|\n|[网易公开课中文字幕视频](http://open.163.com/movie/2008/1/M/C/M6SGF6VB4_M6SGHFBMC.html)|\n\n\n\n# 第四章\n\n### 第六部分  学习理论（Learning Theory）\n\n#### 1 偏差/方差的权衡（Bias/variance tradeoff ）\n\n在讲线性回归的时候，我们讨论过这样的问题：拟合数据的时候，选择线性的“$y = \\theta_0 +\\theta_1x$”这样的“简单”模型，还是选择多项式的“$y= \\theta_0 + \\theta_1x+ ...+\\theta_5x^5$”这种“复杂”模型。如下图所示：\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note4f1.png)\n\n如最右侧图所示，用一个五次多项式来进行拟合，得到的并不是一个好模型。而且，虽然这个五次多项式对于训练集中的每一个 $x$（例如之前文中说的居住面积）都给出了非常好的预测的 $y$ 值（对应的就是房屋价格），但是我们也不能指望这个模型能够对训练集之外的点给出靠谱的预测。换句话说，用这种高次多项式来对训练集进行学习得到的模型根本不能扩展运用到其他房屋上面去。一个推测模型（hypothesis）的**泛化误差（generalization error）**（稍后再给出正式定义），正是那些不属于训练集的样本潜在的预期偏差（expected error on examples not necessarily in the training set）。\n\n上面图中最左边的线性拟合和最右边的高次多项式拟合都有非常大的泛化误差。然而，这两个模型各自出的问题是很不一样的。如果 $y$ 和 $x$ 之间的关系不是线性的，那么即便我们有一个非常大规模的训练集，然后用来进行线性拟合，得到的线性模型都还是不能够准确捕捉到数据的结构。我们粗略地将一个模型的**偏差（bias）** 定义为预期的泛化误差（expected generalization error），即便我们要去拟合的对象是一个非常大的甚至是无限的训练数据集。这样的话，对于上面三幅图中所展示的那个情况来看，最左边的那个线性模型就具有特别大的偏差（bias），可能是对数据欠拟合（也就是说，没有捕捉到数据所体现的结构特征）。\n\n除了这个偏差（bias）之外，还有另外一个构成泛化误差（generalization error）的因素，也就是模型拟合过程的**方差（variance）。** 例如在最右边的图中，使用了五次多项式进行了拟合，这样有很大的风险，很可能我们基于数据拟合出来的模型可能碰巧只适合于眼下这个小规模的有限的训练集，而并不能反映 $x$ 和 $y$ 之间更广泛的关系。例如，在实际中，可能我们选择的训练集中的房屋碰巧就是一些比平均价格要稍微贵一些的房屋，也可能有另外的一些比平均值要低一点的房屋，等等。通过对训练集拟合得到的这个“不太靠谱的（spurious）”的模式，我们得到的可能也就是一个有很大泛化误差（large generalization error）的模型。这样的话，我们就说这个模型的方差很大（large variance）$^1$。\n\n>1 在讲义里面，我们不准备给出对偏差（bias）和方差（variance）给出正式的定义，也就说道上面讨论这样的程度而已。当然了，这两者都有严格的正式定义，例如在线性回归里面，对于这两者的定义，有若干不同的观点，但是哪一个最权威最正确（right）呢？这个还有争议的。\n\n通常情况下，咱们需要在偏差（bias）和方差（variance）之间进行权衡妥协。如果我们的模型过于“简单（simple）”，而且参数非常少，那这样就可能会有很大的偏差（bias），而方差（variance）可能就很小；如果我们的模型过于“复杂（complex）”，有非常多的参数，那就可能反过来又特别大的方差（variance），而偏差（bias）就会小一些。在上面三种不同拟合的样例中，用二次函数来进行拟合得到的效果，明显是胜过一次线性拟合，也强于五次多项式拟合。\n\n#### 2 预先准备（Preliminaries）\n\n在这一部分的讲义中，我们要开始进入到机器学习的理论（learning theory）了。本章内容非常有趣，而且有启发性，还能帮助我们培养直觉，能够得到在不同背景下如何最佳应用学习算法的经验规则。此外，我们还会探究一些问题：首先，上文我们刚刚谈论到的偏差（bias）/方差（variance），能不能更正规地总结一下？这个问题还会引出关于模型选择的方法，这些方法可以在对一个训练集进行拟合的时候来帮助确定要用的多项式应该是几阶的。其次，在机器学习的过程中，我们真正关注的也就是泛化误差（generalization error），不过绝大部分的学习算法都是将训练集和模型结合的。那么针对训练集的表现好坏程度，为何就能告诉我们泛化误差的信息呢？例如，我们能将训练集的误差和泛化误差联系起来么？第三个，也是最后一点，是否存在某些条件，我们能否在这些条件下证明某些学习算法能够良好工作？\n\n我们先来给出两个很简单又很有用的引理（lemma）。\n\n引理1 (联合约束，The union bound)。设 $A_1, A_2, ..., A_k$ 是 $k$个不同事件（但不一定互相独立），则有：\n\n$$\nP(A_1\\cup...\\cup A_k)\\leq P(A_1)+...+P(A_k)\n$$\n\n在概率论中，联合约束通常被当做是公理（所以我们就不尝试证明了），实际上也很直观的： $k$ 个事件同时发生的概率最多是 $k$ 个不同的事件每个都发生的概率的总和。\n\n引理2 (Hoeffding 不等式) 。设 $Z_1,...,Z_m$ 是 $m$ 个独立的并且共同遵循伯努利分布（Bernoulli($\\phi$) distribution）的随机变量（independent and identically distributed (iid) random variables）。例如：$P(Z_i =1)=\\phi$ 而 $P(Z_i =0)= 1 - \\phi$. 设 $\\hat\\phi=(\\frac1m)\\sum^m_{i=1}Z_i$ 是这些随机变量的平均值，然后设任意的 $\\gamma \\geq 0$ 为某一固定值（fixed），则有：\n\n$$\nP(|\\phi-\\hat\\phi|>\\gamma)\\leq 2\\exp (-2\\gamma^2m)\n$$\n\n上面这个引理（在机器学习理论里面也称为 **切尔诺夫约束，Chernoff bound** ）表明，如果我们我们从一个伯努利分布的随机变量中选取平均值 $\\hat\\phi$ 来作为对 $\\phi$ 的估计值，那么只要 $m$ 足够大，我们偏移真实值很远的概率就比较小。另外一种表述方式是：如果你有一个有偏差的硬币（biased coin），抛起来落下人头朝上的概率是 $\\phi$，如果你抛了 $m$ 次，然后计算人头朝上的比例，若 $m$ 非常大，那么这个比例的值，就是一个对 $\\phi$ 的一个概率很高的很好的估计。\n\n基于上面这两个引理，我们就可以去证明在机器学习理论中一些很深刻和重要的结论了。\n\n为了简化表述，我们先集中关注一下二分法分类，其中的标签简化为 $y \\in \\{0, 1\\}$。然后我们即将讲到的所有内容也都会推广到其它问题中，例如回归问题以及多类别的分类问题等等。\n\n假设我们有一个给定的训练集 $S = \\{(x^{(i)},y^{(i)});i = 1,...,m\\}$，其样本规模为 $m$，集合中的训练样本 $(x^{(i)},y^{(i)})$ 是服从某概率分布 $D$ 的独立且同分布的随机变量。设一个假设（hypothesis）为$h$，我们则用如下的方法定义训练误差（也称为学习理论中的**经验风险 empirical risk** 或者**经验误差 empirical error）**：\n\n$$\n\\hat\\epsilon(h) =\\frac1m\\sum^m_{i=1}1\\{h(x^{(i)})\\neq y^{(i)}\\}\n$$\n\n这个值只是假设模型 $h$ 分类错误样本占据训练样本总数的分数。如果我们要特定指定对某个训练样本集合 $S$ 的经验误差 $\\hat\\epsilon(h)$，可以写作 $\\hat\\epsilon_S(h)$。然后我们就可以定义泛化误差（generalization error）为：\n\n$$\n\\epsilon(h) =P_{(x,y)\\sim D}(h(x)\\neq y)\n$$\n\n经验误差 $\\epsilon(h)$ 的这个定义实际上也就相当于，基于分布 $D$ 给出的一个新的样本 $(x, y)$ ，假设模型 $h$ 对该样本分类错误的概率。\n\n要注意，这里我们有一个预先假设，也就是训练集的数据与要用来检验假设用的数据都服从同一个分布 $D$（这一假设存在于对泛化误差的定义中）。这个假设通常也被认为是 PAC 假设之一$^2$。\n\n>2 PAC 是一个缩写，原型为“probably approximately correct”，这是一个框架和一系列假设的集合，在机器学习理论中的很多结构都是基于这些假设而证明得到的。这个系列假设中**最重要的两个，就是训练集与测试集服从同一分布，以及训练样本的独立性。**\n\n考虑线性分类的情况，假设 $h_\\theta (x) = 1\\{\\theta^T x \\geq 0\\}$。拟合参数 $\\theta$ 的合理方法是什么呢？一个思路就是可以使训练误差（training error）最小化，然后选择取最小值的时候的 $\\theta$ ：\n\n$$\n\\hat\\theta=arg\\min_\\theta\\hat\\epsilon(h_\\theta)\n$$\n\n我们把上面这个过程称之为**经验风险最小化**（empirical risk minimization，缩写为 ERM），而这种情况下通过学习算法得到的假设结果就是 $\\hat h = h_{\\hat\\theta}$ 。我们把 ERM 看做为最“基础（basic）”的学习算法，在这一系列的讲义中我们主要关注的就是这种算法。（其他的例如逻辑回归等等算法也可以看作是对 ERM 的某种近似（approximations）。）\n\n在咱们关于机器学习理论的研究中，有一种做法很有用处，就是把具体的参数化（specific parameterization）抽象出去，并且也把是否使用线性分选器（linear classifier）之类的问题也抽象出去。我们把通过学习算法所使用的**假设类（hypothesis class）$H$** 定义为所有分类器的集合（set of all classifiers）。对于线性分类问题来说，$H = \\{h_\\theta : h_\\theta(x) = 1\\{\\theta^T x \\geq 0\\}, \\theta \\in R^{n+1}\\}$，是一个对 $X$（输入特征） 进行分类的所有分类器的集合，其中所有分类边界为线性。更广泛来说，假设我们研究神经网络（neural networks），那么可以设 $H$ 为能表示某些神经网络结构的所有分类器的集合。\n\n现在就可以把 经验风险最小化（ERM）看作是对函数类 $H$ 的最小化，其中由学习算法来选择假设（hypothesis）：\n\n$$\n\\hat h=arg\\min_{h\\in H}\\hat\\epsilon(h)\n$$\n\n#### 3 有限个假设（finite H）的情况 \n\n我们首先来考虑一下假设类有限情况下的学习问题，其中假设类 $H = \\{h_1, ..., h_k\\}$，由 $k$ 个不同假设组成。因此，$H$ 实际上就是由 $k$ 个从输入特征 $X$ 映射到 $\\{0, 1\\}$ 的函数组成的集合，而经验风险最小化（ERM）就是从这样的 $k$ 个函数中选择训练误差最小（smallest training error）的作为 $\\hat h$。\n\n我们希望能够确保 $\\hat{h}$ 的泛化误差。这需要两个步骤：首先要表明 $\\hat\\epsilon(h)$ 是对所有 $h$ 的 $\\epsilon(h)$ 的一个可靠估计。其次就需要表明这个 $\\hat\\epsilon(h)$ 位于 $\\hat{h}$ 泛化误差的上界。\n\n任选一个固定的 $h_i \\in H$。假如有一个伯努利随机变量（Bernoulli random variable） $Z$，其分布入下面式中定义。 然后我们从 $D$ 中取样 $(x, y)$，并设 $Z=1\\{h_i(x)\\neq y\\}$。也就是说，我们会选择一个样本，然后令 $Z$ 指示 $h_i$ 是否对该样本进行了错误分类。类似地，我们还定义了一个 $Z_j=1\\{h_i(x^{(j)})\\neq y^{(j)}\\}$。由于我们的训练样本都是从 $D$ 中取来的独立随机变量（iid），所以在此基础上构建的 $Z$ 和 $Z_j$ 也都服从相同的分布。\n\n这样就能找到针对随机选取的训练样本进行错误分类的概率 — 也就是 $\\epsilon(h)$ — 正好就是 $Z$ (以及 $Z_j$) 的期望值（expected value）。然后，就可以把训练误差写成下面这种形式：\n\n$$\n\\hat\\epsilon(h_i)=\\frac 1m\\sum_{j=1}^mZ_j\n$$\n\n因此，$\\hat\\epsilon(h_i)$ 就正好是 $m$ 个随机变量 $Z_j$ 的平均值，而这个 $Z_j$ 是服从伯努利分布的独立随机变量（iid），其均值就是 $\\epsilon(h_i)$。接下来，就可以使用 Hoeffding 不等式，得到下面的式子：\n\n$$\nP(|\\epsilon(h_i)-\\hat\\epsilon(h_i)|>\\gamma)\\leq 2\\exp (-2\\gamma^2m)\n$$\n\n这就表明，对于我们给定的某个固定的 $h_i$，假如训练样本的规模 $m$ 规模很大的时候，训练误差有很接近泛化误差（generalization error）的概率是很高的。然而我们不仅仅满足于针对某一个特定的 $h_i$ 的时候能保证 $\\epsilon(h_i)$ 接近 $\\hat\\epsilon(h_i)$ 且接近的概率很高。我们还要证明同时针对所有的 $h \\in H$ 这个结论都成立。 为了证明这个结论，我们设 $A_i$ 来表示事件 $|\\epsilon(h_i) - \\hat\\epsilon(h_i)| > \\gamma$。我们已经证明了，对于任意给定的 $A_i$，都有 $P(A_i) \\le 2\\exp (-2\\gamma^2m)$ 成立。接下来，使用联合约束（union bound），就可以得出下面的关系：\n\n$$\n\\begin{aligned}\nP(\\exists h\\in H.|\\epsilon(h_i)-\\hat\\epsilon(h_i)|>\\gamma)& = P(A_1\\cup...\\cup A_k) \\\\\n                                                   & \\le \\sum_{i=1}^k P(A_i) \\\\\n                                                   & \\le \\sum_{i=1}^k 2\\exp (-2\\gamma^2m) \\\\\n                                                   & = 2k\\exp (-2\\gamma^2m)\n\\end{aligned}\n$$\n\n如果等式两边都用 1 来减去原始值（subtract both sides from 1），则不等关系改变为：\n\n$$\n\\begin{aligned}\nP(\\neg\\exists h\\in H.|\\epsilon(h_i)-\\hat\\epsilon(h_i)|>\\gamma)& \n= P(\\forall h\\in H.|\\epsilon(h_i)-\\hat\\epsilon(h_i)|\\le\\gamma) \\\\\n& \\ge 1-2k\\exp (-2\\gamma^2m)\n\\end{aligned}\n$$\n\n（$\\neg$ 这个符号的意思是 “非”。）如上所示，至少有 $1-2k exp(-2\\gamma^2 m)$ 的概率，我们能确保对于所有的 $h \\in H$，$\\epsilon(h)$ 在 $\\hat\\epsilon(h)$ 附近的 $\\gamma$ 范围内。这种结果就叫做一致收敛结果（uniform convergence result），因为这是一个针对所有的 $h \\in H$ 都同时成立的约束（与之相反的是只针对某一个 $h$ 才成立的情况）。\n\n在上面的讨论中，我们涉及到的是针对某些 $m$ 和 $\\gamma$ 的特定值，给定一个概率约束：对于某些 $h \\in H$, 都有 $|\\epsilon(h) - \\hat\\epsilon(h)| \\geq \\gamma$。这里我们感兴趣的变量（quantities of interest）有三个：$m$, $\\gamma$, 以及误差的概率（probability of error）；我们可以将其中的任意一个用另外两个来进行约束（bound either one in terms of the other two）。\n\n例如，我们可以提出下面这样的一个问题：给定一个 $\\gamma$ 以及某个 $\\delta \\geq 0$，那么如果要保证训练误差处于泛化误差附近 $\\gamma$ 的范围内的概率最小为 $1 – \\delta$，那么 $m$ 应该要多大呢？可以设 $\\delta = 2k exp(-2\\gamma^2 m)$ 然后解出来 $m$（自己给自己证明一下这样是对的吧！），然后我们就发现，如果有：\n\n$$\nm\\ge \\frac{1}{2\\gamma^2}log\\frac{2k}{\\delta}\n$$\n\n并且概率最小为 $1-\\delta$，就能保证对于所有的 $h \\in H$ 都有 $|\\epsilon(h) - \\hat\\epsilon(h)| ≤ \\gamma$ 。（反过来，这也表明，对于**某些** $h \\in H$， $|\\epsilon(h) - \\hat\\epsilon(h)| \\geq \\gamma$ 的概率最大为 $\\delta$。）这种联合约束也说明了需要多少数量的训练样本才能对结果有所保证。是某些特定的方法或者算法所需要训练集的规模 $m$ 来实现一定程度的性能（achieve a certain level of performance），这样的训练集规模 $m$ 也叫做此类算法的**样本复杂度** （the algorithm’s sample complexity）。\n\n上面这个约束的关键特性在于要保证结果，所需的训练样本数量只有 $k$ 的对数（only logarithmic in k），$k$ 即假设集合 $H$ 中的假设个数。这以特性稍后会很重要。\n\n同理，我们也可以将 $m$ 和 $\\delta$ 设置为固定值，然后通过上面的等式对 $\\gamma$ 进行求解，然后表明对于所有的 $h \\in H$ ，都有概率为 $1 –\\delta$（这里还是要你自己去证明了，不过你相信这个是对的就好了。）。\n\n$$\n|\\hat\\epsilon(h)-\\epsilon(h)|\\le \\sqrt{\\frac{1}{2m}log\\frac{2k}{\\delta}}\n$$\n\n现在，我们假设这个联合收敛成立（uniform convergence holds），也就是说，对于所有的 $h \\in H$，都有 $|ε(h)-\\hat\\epsilon(h)| ≤ \\gamma$。我们的学习算法选择了 $\\hat{h} = arg\\min_{h\\in H} \\hat\\epsilon(h)$，关于这种算法的泛化，我们能给出什么相关的证明呢？\n\n将 $h^∗ = arg \\min_{h\\in H} \\epsilon(h)$ 定义为 $H$ 中最佳可能假设（best possible hypothesis）。这里要注意此处的 $h^∗$ 是我们使用假设集合 $H$ 所能找出的最佳假设，所以很自然地，我们就能理解可以用这个 $h^∗$ 来进行性能对比了。则有：\n\n$$\n\\begin{aligned}\n\\epsilon(\\hat h) & \\le \\hat\\epsilon(\\hat h)+\\gamma \\\\\n                 & \\le \\hat\\epsilon(h^*)+\\gamma \\\\\n                 & \\le \\epsilon(h^*)+2\\gamma \n\\end{aligned}\n$$\n\n上面的第一行用到了定理 $| \\epsilon(\\hat h) - \\hat\\epsilon (\\hat h) | \\le \\gamma$（可以通过上面的联合收敛假设来推出）。第二行用到的定理是 $\\hat{h}$ 是选来用于得到最小 $\\hat\\epsilon(h)$ ，然后因此对于所有的 $h$ 都有 $\\hat\\epsilon(\\hat{h}) \\leq \\hat\\epsilon(h)$，也就自然能推出 $\\hat\\epsilon(\\hat{h}) \\le \\hat\\epsilon(h^∗)$ 。第三行再次用到了上面的联合收敛假设，此假设表明 $\\hat\\epsilon(h^∗) \\le \\epsilon(h^∗) + \\gamma$ 。所以，我们就能得出下面这样的结论：如果联合收敛成立，那么 $\\hat h$ 的泛化误差最多也就与 $H$ 中的最佳可能假设相差 $2\\gamma$。\n\n好了，咱们接下来就把上面这一大堆整理成一条定理（theorem）。\n\n**定理:** 设 $|H| = k$`译者注：即 H 集合中元素个数为 k`，然后设 $m$ 和 $\\delta$ 为任意的固定值。然后概率至少为 $1 - \\delta$，则有：\n\n$$\n\\epsilon(\\hat h)\\le (\\min_{h\\in H}\\epsilon(h))+2\\sqrt{\\frac{1}{2m}log\\frac{2k}{\\delta}}\n$$\n\n上面这个的证明，可以通过令 $\\gamma$ 等于平方根$\\sqrt{\\cdot}$的形式，然后利用我们之前得到的概率至少为 $1 – \\delta$ 的情况下联合收敛成立，接下来利用联合收敛能表明 $\\epsilon(h)$ 最多比 $\\epsilon(h^∗) = \\min_{h\\in H} \\epsilon(h)$ 多 $2\\gamma$（这个前面我们已经证明过了）。\n\n这也对我们之前提到过的在模型选择的过程中在偏差（bias）/方差（variance）之间的权衡给出了定量方式。例如，加入我们有某个假设类 $H$，然后考虑切换成某个更大规模的假设类 $H' \\supseteq H$。如果我们切换到了 $H'$ ，那么第一次的 $\\min_h \\epsilon(h)$ 只可能降低（因为我们这次在一个更大规模的函数集合里面来选取最小值了）。因此，使用一个更大规模的假设类来进行学习，我们的学习算法的“偏差（bias）”只会降低。然而，如果 $k$ 值增大了，那么第二项的那个二倍平方根项$2\\sqrt{\\cdot}$也会增大。这一项的增大就会导致我们使用一个更大规模的假设的时候，“方差（variance）”就会增大。\n\n通过保持 $\\gamma$ 和 $\\delta$ 为固定值，然后像上面一样求解 $m$，我们还能够得到下面的样本复杂度约束：\n\n**推论（Corollary）：** 设 $|H| = k$ ，然后令 $\\delta,\\gamma$ 为任意的固定值。对于满足概率最少为 $1 - \\delta$ 的 $\\epsilon(\\hat{h}) \\le min_{h\\in H} \\epsilon(h) + 2\\gamma$ ，下面等式关系成立：\n\n$$\n\\begin{aligned}\nm &\\ge \\frac{1}{2\\gamma^2}log\\frac{2k}{\\delta} \\\\\n  & = O(\\frac{1}{\\gamma^2}log\\frac{k}{\\delta})    \n\\end{aligned}\n$$\n\n#### 4 无限个假设（infinite H）的情况 \n\n我们已经针对有限个假设类的情况证明了一些有用的定理。然而有很多的假设类都包含有无限个函数，其中包括用实数参数化的类（比如线性分类问题）。那针对这种无限个假设的情况，我们能证明出类似的结论么？\n\n我们先从一些不太“准确”论证的内容开始（not the “right” argument）。当然也有更好的更通用的论证，但先从这种不太“准确”的内容除法，将有助于锻炼我们在此领域内的直觉（intuitions about the domain）。\n\n若我们有一个假设集合 $H$，使用 $d$ 个实数来进行参数化（parameterized by d real numbers）。由于我们使用计算机表述实数，而 IEEE 的双精度浮点数（ C 语言里面的 double 类型）使用了 64 bit 来表示一个浮点数（floating-point number,），这就意味着如果我们在学习算法中使用双精度浮点数（double- precision floating point），那我们的算法就由 64 d 个 bit 来进行参数化（parameterized by 64d bits）。这样我们的这个假设类实际上包含的不同假设的个数最多为 $k = 2^{64d}$ 。结合上一节的最后一段那个推论（Corollary），我们就能发现，要保证 $\\epsilon(\\hat{h}) \\leq \\epsilon(h^∗) + 2\\gamma$ ，同时还要保证概率至少为 $1 - \\delta$ ，则需要训练样本规模 $m$ 满足$m \\ge O(\\frac{1}{\\gamma^2}log\\frac{2^{64d}}{\\delta})=O(\\frac{d}{\\gamma^2}log\\frac{1}{\\delta})=O_{\\gamma,\\delta}(d)$（这里的 $\\gamma$，$\\delta$下标表示最后一个大$O$可能是一个依赖于$\\gamma$和$\\delta$的隐藏常数。）因此，所需的训练样本规模在模型参数中最多也就是线性的（the number of training examples needed is at most linear in the parameters of the model）。\n\nThe fact that we relied on 64-bit floating point makes this argument not entirely satisfying, but the conclusion is nonetheless roughly correct: If what we’re going to do is try to minimize training error, then in order to learn “well” using a hypothesis class that has d parameters, generally we’re going to need on the order of a linear number of training examples in d.\n\n由于我们要依赖 64 bit 浮点数，所以上面的论证还不能完全令人满意，但这个结论大致上是正确的：如果我们试图使训练误差（training error）最小化，那么为了使用具有 $d$ 个参数的假设类（hypothesis class）的学习效果“较好（well）”，通常就需要按照 $d$ 的线性数量来确定训练样本规模。（译者注：这句话的翻译肯定是错的，因为原文的语法我根本不能理解， 我第一次见到动词 need 后面用介词短语，而那个介词短语在此处我也不能确定具体意义，希望大家给提出指正，抱歉了。）\n\n（在这里要注意的是，对于使用经验风险最小化（empirical risk minimization ，ERM）的学习算法，上面这些结论已经被证明适用。因此，样本复杂度（sample complexity）对 $d$ 的线性依赖性通常适用于大多数分类识别学习算法（discriminative learning algorithms），但训练误差或者训练误差近似值的最小化，就未必适用于分类识别了。对很多的非 ERM 学习算法提供可靠的理论论证，仍然是目前很活跃的一个研究领域。）\n\n前面的论证还有另外一部分让人不太满意，就是依赖于对 $H$ 的参数化（parameterization）。根据直觉来看，这个参数化似乎应该不会有太大影响：我们已经把线性分类器（linear classifiers）写成了 $h_\\theta(x) = 1\\{\\theta_0 + \\theta_1x_1 + ···\\theta_n x_n \\geq 0\\}$ 的形式，其中有 $n+1$ 个参数 $\\theta_0,...,\\theta_n$ 。但也可以写成 $h_{u,v}(x) = 1\\{(u^2_0 - v_0^2) + (u^2_1 - v_1^2)x1 + ··· (u^2_n - v_n^2)x_n \\geq 0\\}$ 的形式，这样就有 $2n+2$ 个参数 $u_i, v_i$ 了。然而这两种形式都定义了同样的一个 $H：$ 一个 $n$ 维的线性分类器集合。\n\n要推导出更让人满意的论证结果，我们需要再额外定义一些概念。\n\n给定一个点的集合 $S = \\{x^{(i)}, ..., x^{(d)}\\}$（与训练样本集合无关），其中 $x(i) \\in X$，如果 $H$ 能够对 集合 $S$ 实现任意的标签化（can realize any labeling on S），则称 **$H$ 打散（shatter）** 了 $S$。例如，对于任意的标签集合 （set of labels）$\\{y^{(1)}, ..., y^{(d)}\\}$，都有 一些$h\\in H$ ，对于所有的$i = 1, ...d$，式子$h(x^{(i)}) = y^{(i)}$都成立。（译者注：关于 shattered set 的定义可以参考：[*https://en.wikipedia.org/wiki/Shattered\\_set*](https://en.wikipedia.org/wiki/Shattered_set) 更多关于 VC 维 的内容也可以参考：[*https://www.zhihu.com/question/38607822*](https://www.zhihu.com/question/38607822) ）\n\n给定一个假设类 $H$，我们定义其 **VC维度（Vapnik-Chervonenkis dimension），** 写作 $VC(H)$，这个值也就是能被 $H$ 打散（shatter）的最大的集合规模。（如果 $H$ 能打散任意大的集合（arbitrarily large sets），那么 $VC(H) = ∞$。）\n\n例如，若一个集合由下图所示的三个点组成：\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note4f2.png)\n\n那么二维线性分类器 $(h(x) = 1\\{\\theta_0 +\\theta_1 x_1 + \\theta_2 x_2 \\geq 0\\})$ 的集合 $H$ 能否将上图所示的这个集合打散呢？答案是能。具体来看则如下图所示，以下八种分类情况中的任意一个，我们都能找到一种用能够实现 “零训练误差（zero training error）” 的线性分类器（linear classifier）：\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note4f3.png)\n\n此外，这也有可能表明，这个假设类 $H$ 不能打散（shatter）4 个点构成的集合。因此，$H$ 可以打散（shatter）的最大集合规模为 3，也就是说 $VC（H）= 3$。\n\n这里要注意，$H$ 的 $VC$ 维 为3，即便有某些 3 个点的集合不能被 $H$ 打散。例如如果三个点都在一条直线上（如下图左侧的图所示），那就没办法能够用线性分类器来对这三个点的类别进行划分了（如下图右侧所示）。\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note4f4.png)\n\n换个方式来说，在 $VC$ 维 的定义之下，要保证 $VC(H)$ 至少为 $D$，只需要证明至少有一个规模为 $d$ 的集合能够被 $H$ 打散 就可以了。\n\n这样就能够给出下面的定理（theorem）了，该定理来自 Vapnik。（有不少人认为这是所有学习理论中最重要的一个定理。）\n\nTheorem. Let H be given, and let d = VC(H). Then with probability at least 1-δ, we have that for all h∈H,\n\n定理：给定 $H$，设 $d = VC(H)$。然后对于所有的 $h\\in H$，都有至少为 $1-\\delta$ 的概率使下面的关系成立：\n\n$$\n|\\epsilon(h)-\\hat\\epsilon(h)|\\le O(\\sqrt{\\frac{d}{m}log\\frac{d}{m}+\\frac 1mlog\\frac 1\\delta})\n$$\n\n此外，有至少为 $1-\\delta$ 的概率：\n\n$$\n\\hat\\epsilon(h)\\le \\epsilon(h^*)+O(\\sqrt{\\frac{d}{m}log\\frac{d}{m}+\\frac 1mlog\\frac 1\\delta})\n$$\n\n换句话说，如果一个假设类有有限的 $VC$ 维，那么只要训练样本规模 $m$ 增大，就能够保证联合收敛成立（uniform convergence occurs）。和之前一样，这就能够让我们以 $\\epsilon(h)$ 的形式来给 $\\epsilon(h^∗)$ 建立一个约束（bound）。此外还有下面的推论（corollary）：\n\nCorollary. For |ε(h) - \\hat\\epsilon(h)| ≤ \\gamma to hold for all h ∈ H (and hence $\\epsilon(hat{h}) ≤ \\epsilon(h^∗) + 2\\gamma$) with probability at least 1 - δ, it suffices that $m = O_{\\gamma,\\delta}(d)$.\n\n**推论（Corollary）：** 对于所有的 $h \\in H$ 成立的 $|\\epsilon(h) - \\hat\\epsilon(h)| \\le \\gamma$ （因此也有 $\\epsilon(\\hat h) ≤ \\epsilon(h^∗) + 2\\gamma$），则有至少为 $1 – \\delta$ 的概率，满足 $m = O_{\\gamma,\\delta}(d)$。\n\nIn other words, the number of training examples needed to learn “well” using H is linear in the VC dimension of H. It turns out that, for “most” hypothesis classes, the VC dimension (assuming a “reasonable” parameterization) is also roughly linear in the number of parameters. Putting these together, we conclude that (for an algorithm that tries to minimize training error) the number of training examples needed is usually roughly linear in the number of parameters of H.\n\n换个方式来说，要保证使用 假设集合 $H$ 的 机器学习的算法的学习效果“良好（well）”，那么训练集样本规模 $m$ 需要与 $H$ 的 $VC$ 维度 线性相关（linear in the VC dimension of H）。这也表明，对于“绝大多数（most）”假设类来说，（假设是“合理（reasonable）”参数化的）$VC$ 维度也大概会和参数的个数线性相关。把这些综合到一起，我们就能得出这样的一个结论：对于一个试图将训练误差最小化的学习算法来说：训练样本个数 通常都大概与假设类 $H$ 的参数个数 线性相关。\n\n\n### 参考","tags":["CS229"],"categories":["CS229"]},{"title":"[机器学习-CS229]支持向量机","url":"%2Fposts%2F7e4d61c4%2F","content":"# CS229 课程讲义中文翻译\n\n| 原作者 | 翻译 | 校对 |\n| --- | --- | --- |\n| [Andrew Ng  吴恩达](http://www.andrewng.org/) | [CycleUser](https://www.zhihu.com/people/cycleuser/columns) | [XiaoDong_Wang](https://github.com/Dongzhixiao) |\n\n\n|相关链接|\n|---|\n|[github 地址](https://github.com/Kivy-CN/Stanford-CS-229-CN)|\n|[知乎专栏](https://zhuanlan.zhihu.com/MachineLearn)|\n|[斯坦福大学 CS229 课程网站](http://cs229.stanford.edu/)|\n|[网易公开课中文字幕视频](http://open.163.com/movie/2008/1/M/C/M6SGF6VB4_M6SGHFBMC.html)|\n\n# 第三章\n\n### 第五部分 支持向量机(Support Vector Machines)\n\n本章的讲义主要讲述的是 支持向量机（Support Vector Machine ，缩写为 SVM） 学习算法。SVM 算得上是现有的最好的现成的(“off-the-shelf”)监督学习算法之一，很多人实际上认为这里没有“之一”这两个字的必要，认为 SVM 就是最好的现成的监督学习算法。讲这个 SVM 的来龙去脉之前，我们需要先讲一些关于边界的内容，以及对数据进行分割成大的区块（gap）的思路。接下来，我们要讲一下最优边界分类器(optimal margin classifier,)，其中还会引入一些关于拉格朗日对偶(Lagrange duality)的内容。然后我们还会接触到核(Kernels)，这提供了一种在非常高的维度(例如无穷维度)中进行 SVM 学习的高效率方法，最终本章结尾部分会讲 SMO 算法，也就是 SVM 算法的一个有效实现方法。\n\n#### 1 边界(Margins)：直觉(Intuition) \n\n咱们这回讲 SVM 学习算法，从边界(margins)开始说起。这一节我们会给出关于边界的一些直观展示(intuitions)，以及对于我们做出的预测的信心(confidence)；在本章的第三节中，会对这些概念进行更正式化的表述。\n\n考虑逻辑回归，其中的概率分布$p(y = 1|x;\\theta)$ 是基于 $h_\\theta(x) = g(\\theta^Tx)$ 而建立的模型。当且仅当 $h_\\theta(x) \\geq 0.5$ ，也就是 $\\theta^Tx \\geq 0$ 的时候，我们才会预测出“$1$”。假如有一个正向(Positive)的训练样本(positive tra_ining example)($y = 1$)。那么$\\theta^Tx$ 越大，$h_\\theta (x) = p(y = 1|x; w, b)$ 也就越大，我们对预测 Label 为 $1$ 的“信心(confidence)”也就越强。所以如果 $y = 1$ 且 $\\theta^T x \\gg 0$（远大于 $0$），那么我们就对这时候进行的预测非常有信心，当然这只是一种很不正式的粗略认识。与之类似，在逻辑回归中，如果有 $y = 0$ 且 $\\theta^T x \\ll 0$(远小于 0)，我们也对这时候给出的预测很有信心。所以还是以一种非常不正式的方式来说，对于一个给定的训练集，如果我们能找到一个 $\\theta$，满足当 $y^{(i)} = 1$ 的时候总有 $\\theta^T x^{(i)} \\gg 0$，而 $y^{(i)} = 0$ 的时候则 $\\theta^T x^{(i)} \\ll 0$，我们就说这个对训练数据的拟合很好，因为这就能对所有训练样本给出可靠（甚至正确）的分类。似乎这样就是咱们要实现的目标了，稍后我们就要使用**函数边界记号(notion of functional margins)** 来用正规的语言来表达该思路。\n\n还有另外一种的直观表示，例如下面这个图当中，画叉的点表示的是正向训练样本，而小圆圈的点表示的是负向训练样本，图中还画出了**分类边界(decision boundary)，** 这条线也就是通过等式 $\\theta^T x = 0$ 来确定的，也叫做**分类超平面(separating hyperplane)。** 图中还标出了三个点 $A,B$ 和 $C$。\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note3f1.png)\n\n可以发现 $A$ 点距离分界线很远。如果我们对 $A$ 点的 $y$ 值进行预测，估计我们会很有信心地认为在那个位置的 $y = 1$。与之相反的是 $C$，这个点距离边界线很近，而且虽然这个 $C$ 点也在预测值 $y = 1$ 的一侧，但看上去距离边界线的距离实在是很近的，所以也很可能会让我们对这个点的预测为 $y = 0$。因此，我们对 $A$ 点的预测要比对 $C$ 点的预测更有把握得多。$B$ 点正好在上面两种极端情况之间，更广泛地说，如果一个点距离**分类超平面(separating hyperplane)** 比较远，我们就可以对给出的预测很有信心。那么给定一个训练集，如果我们能够找到一个分类边界，利用这个边界我们可以对所有的训练样本给出正确并且有信心（也就是数据点距离分类边界要都很远）的预测，那这就是我们想要达到的状态了。当然上面这种说法还是很不正规，后面我们会使用**几何边界记号(notion of geometric margins)** 来更正规地来表达。\n\n#### 2 记号(Notation) \n\n在讨论 SVMs 的时候，出于简化的目的，我们先要引入一个新的记号，用来表示分类。假设我们要针对一个二值化分类的问题建立一个线性分类器，其中用来分类的标签(label)为 $y$，分类特征(feature)为 $x$。从此以后我们就用$y\\in\\{ {-1},1 \\}$（而不是之前的 $\\{0, 1\\}$） 来表示这个分类标签了。另外，以后咱们也不再使用向量 $\\theta$ 来表示咱们这个线性分类器的参数了，而是使用参数 $w$ 和 $b$，把分类器写成下面这样：\n\n$$\nh_{w,b}(x)=g(w^Tx+b)\n$$\n\n当 $z \\geq 0$，则 $g(z) = 1$；而反之若 $z \\lt 0$，则$g(z) = -1$。这里的这个 “$w, b$” 记号就可以让我们能把截距项(intercept term)$b$ 与其他的参数区别开。（此外我们也不用再像早些时候那样要去设定 $x_0 = 1$ 这样的一个额外的输入特征向量了。）所以，这里的这个参数 $b$ 扮演的角色就相当于之前的参数 $\\theta_0$ ，而参数 $w$ 则相当于 $[\\theta_1 \\dots \\theta_n]^T$。\n\n还要注意的是，从我们上面对函数 $g$ 的定义，可以发现我们的分类器给出的预测是 $1$ 或者 $-1$ （参考 感知器算法 perceptron algorithm），这样也就不需要先通过中间步骤(intermediate step)来估计 $y$ 为 $1$ （这里指逻辑回归中的步骤）的概率。\n\n#### 3 函数边界和几何边界(Functional and geometric margins) \n\n咱们来用正规语言来将函数边界记号和几何边界记号的概念进行正规化。给定一个训练集 $(x^{(i)}, y^{(i)})$，我们用下面的方法来定义对应该训练集的**函数边界** $(w, b)$：\n\n$$\n\\hat\\gamma^{(i)}=y^{(i)}(w^Tx^{(i)}+b)\n$$\n\n要注意，如果 $y^{(i)} = 1$，那么为了让函数边界很大（也就是说，我们的预测很可信并且很正确），我们就需要 $w^T x + b$ 是一个很大的正数。与此相对，如果 $y^{(i)} = -1$，那么为了让函数边界很大，我们就需要$w^T x + b$ 是一个（绝对值）很大的负数。而且，只要满足 $y^{(i)}(w^Tx^{(i)} + b) \\ge 0$，那我们针对这个样本的预测就是正确的。（自己检验证明吧。）因此，一个大的函数边界就表示了一个可信且正确的预测。\n\n对于一个线性分类器，选择上面给定的函数 $g$ （取值范围为$\\{-1, 1\\}$），函数边界的一个性质却使得这个分类器并不具有对置信度的良好量度。例如上面给定的这个函数 $g$，我们会发现，如果用 $2w$ 替换掉 $w$，然后用 $2b$ 替换 $b$，那么由于有 $g(w^Tx + b) = g(2w^Tx + 2b)$，所以这样改变也并不会影响 $h_{w,b}(x)$。也就是说，函数 $g$ 以及 $h_{w,b}(x)$ 只取决于 $w^T x + b$ 的正负符号(sign)，而不受其大小(magnitude)的影响。然而，把$(w, b)$ 翻倍成 $(2w,2b)$ 还会导致函数距离也被放大了 $2$ 倍。因此，这样看来就是只要随意去调整 $w$ 和 $b$ 的范围，我们就可以人为调整函数边界到足够大了，而不用去改变任何有实际意义的变量。直观地看，这就导致我们有必要引入某种归一化条件，例如使 $\\Vert w\\Vert_2 = 1$；也就是说，我们可以将 $(w, b)$ 替换成 $(\\frac{w}{\\Vert w\\Vert_2},\\frac{b}{\\Vert w\\Vert _2})$，然后考虑对应 $(\\frac{w}{\\Vert w\\Vert_2},\\frac{b}{\\Vert w\\Vert_2})$ 的函数边界。我们稍后再详细讨论这部分内容。\n\n给定一个训练集 $S = \\{(x^{(i)},y^{(i)}); i = 1, ..., m\\}$，我们将对应 $S$ 的函数边界 $(w, b)$ 定义为每个训练样本的函数边界的最小值。记作 $\\hat \\gamma$，可以写成：\n\n$$\n\\hat\\gamma= \\min_{i=1,...,m}\\hat\\gamma^{(i)}\n$$\n\n接下来，咱们要讲的是**几何边界(geometric margins)。** 例如下图所示：\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note3f2.png)\n\n图中给出了对应 $(w, b)$ 的分类边界，其倾斜方向（即法线方向）为向量 $w$ 的方向。这里的向量 $w$ 是与**分类超平面**垂直的（orthogonal，即夹角为 $90^\\circ$）。（你需要说服自己现实情况一定是这样的。）假设有图中所示的一个点 $A$，此点表示的是针对某训练样本的输入特征为 $x^{(i)}$ ，对应的标签(label)为 $y^{(i)} = 1$。然后这个点到分类边界的距离 $\\gamma^{(i)}$， 就通过 $AB$ 之间的线段能够获得。\n\n怎么找到的 $\\gamma^{(i)}$ 值呢？这样，$\\frac{w}{\\Vert w\\Vert}$ 是一个单位长度的向量，指向与 $w$ 相同的方向。因为这里 $A$ 点表示的是 $x^{(i)}$，所以就能找到一个点 $B$，其位置为 $x^{(i)} - \\gamma^{(i)} \\times \\frac{w}{\\Vert w\\Vert}$。这个 $B$ 点正好位于分类边界线上面，而这条线上的所有 $x$ 都满足等式 $w^T x + b = 0$ ，所以有：\n\n$$\nw^T(x^{(i)}-\\gamma^{(i)}\\frac{w}{\\Vert w\\Vert })+b=0\n$$\n\n通过上面的方程解出来的 $\\gamma^{(i)}$ 为：\n\n$$\n\\gamma^{(i)}=\\frac{w^Tx^{(i)}+b}{\\Vert w\\Vert }=(\\frac{w}{\\Vert w\\Vert })^Tx^{(i)}+\\frac{b}{\\Vert w\\Vert }\n$$\n\n这个解是针对图中 $A$ 处于训练样本中正向部分这种情况，这时候位于“正向(positive)”一侧就是很理想的情况。如果更泛化一下，就可以定义对应训练样本 $(x^{(i)}, y^{(i)})$ 的几何边界 $(w, b)$ 为：\n\n$$\n\\gamma^{(i)}=y^{(i)}((\\frac{w}{\\Vert w\\Vert })^Tx^{(i)}+\\frac{b}{\\Vert w\\Vert })\n$$\n\n这里要注意，如果 $\\Vert w\\Vert  = 1$，那么函数边界(functional margin)就等于几何边界(geometric margin)——我们可以用这种方法来将两个边界记号联系起来。此外，几何边界是不受参数缩放的影响的；也就是说，如果我们把 $w$ 改为 $2w$，$b$ 改为 $2b$，那么几何边界并不会改变。稍后这个性质就会派上用场了。特别要注意的是，由于这个与参数缩放的无关性，当试图对某个数据集的 $w$ 和 $b$ 进行拟合的时候，我们就可以倒入一个任意设置的缩放参数来约束 $w$，而不会改变什么重要项；例如，我们可以设置 $\\Vert w\\Vert  = 1$，或者 $|w_1| = 5$，或者 $|w_1 +b|+|w_2| = 2$，等等都可以，这些都只需要对 $w$ 和 $b$ 进行缩放就可以满足了。\n\n最后，给定一个训练集 $S = \\{(x^{(i)}, y^{(i)}); i = 1, ..., m\\}$，我们也可以将对应 $S$ 的几何边界 $(w, b)$ 定义为每个训练样本的几何边界的最小值：\n\n$$\n\\gamma=\\min_{i=1,...,m}\\gamma^{(i)}\n$$\n\n#### 4 最优边界分类器(optimal margin classifier) \n\n给定一个训练集，根据咱们前文的讨论，似乎很自然地第一要务就是要尝试着找出一个分类边界，使（几何）边界能够最大，因为这会反映出对训练集进行的一系列置信度很高的分类预测，也是对训练数据的一个良好“拟合(fit)”。这样生成的一个分类器，能够把正向和负向的训练样本分隔开，中间有一个“空白区(gap)”，也就是几何边界。\n\n到目前为止，我们都是假定给定的训练集是线性可分(linearly separable)的；也就是说，能够在正向和负向的样本之间用某种分类超平面来进行划分。那要怎样找到能够得到最大几何边界的那一组呢？我们可以提出下面的这样一个优化问题(optimization problem)：\n\n$$\n\\begin{aligned}\n\\max_{\\gamma,w,b} \\quad& \\gamma \\\\\ns.t. \\quad &y^{(i)}(w^Tx^{(i)}+b) \\geq \\gamma,\\quad i=1,...,m\\\\\n&\\Vert w\\Vert =1 \\\\\n\\end{aligned}\n$$\n\n也就是说，我们要让 $\\gamma$ 取最大值，使得每一个训练样本的函数边界都至少为 $\\gamma$(having functional margin at least $\\gamma$)。另外 $\\Vert w\\Vert  = 1$ 这个约束条件还能保证函数边界与几何边界相等，所以我们就还能够保证所有的几何边界都至少为 $\\gamma$。因此，对上面这个优化问题进行求解，就能得出对应训练集的最大可能几何边界(largest possible geometric margin)的 $(w, b)$。\n\n如果解出来上面的优化问题，那就全都搞定了。但 “$\\Vert w\\Vert  = 1$” 这个约束条件很讨厌，是非凸的(non-convex)，而且这个优化问题也明显不是那种我们随便扔给某些标准优化软件(standard optimization software)就能解决的。所以我们要把这个问题进行改善，让它更好解。例如：\n\n$$\n\\begin{aligned}\n\\max_{\\hat\\gamma,w,b} \\quad& \\frac{\\hat \\gamma}{\\Vert w\\Vert } \\\\\ns.t. \\quad &y^{(i)}(w^Tx^{(i)}+b) \\geq \\hat\\gamma,\\quad i=1,...,m\\\\\n\\end{aligned}\n$$\n\n这时候，我们要让 $\\frac{\\hat \\gamma}{\\Vert w\\Vert}$ 的取值最大，使得函数边界都至少为 $\\hat \\gamma$。由于几何边界和函数边界可以通过 $\\gamma = \\frac{\\hat \\gamma}{\\Vert w\\Vert}$ 来联系起来，所以这样就能得到我们想要的结果了。而且，这样还能摆脱掉 $\\Vert w\\Vert  = 1$ 这个讨厌的约束条件。然而，悲剧的是我们现在就有了一个很讨厌的（还是非凸的）目标函数 $\\frac{\\hat \\gamma}{\\Vert w\\Vert}$；而且，我们还是没有什么现成的软件(off-the-shelf software)能够解出来这样的一个优化问题。\n\n那接着看吧。还记得咱们之前讲过的可以对 $w$ 和 $b$ 设置任意的一个缩放约束参数，而不会改变任何实质性内容。咱们现在就要用到这个重要性质了。下面咱们就来引入一个缩放约束参数，这样针对训练集的函数边界 $w, b$ 的这个参数就可以设置为 $1$(scaling constraint that the functional margin of w, b with respect to the training set must be 1)：\n\n$$\n\\hat \\gamma =1\n$$\n\n对 $w$ 和 $b$ 使用某些常数来进行翻倍，结果就是函数边界也会以相同的常数进行加倍，这就确实是一个缩放约束了，而且只要对 $w$ 和 $b$ 进行缩放就可以满足。把这个性质用到咱们上面的优化问题中去，同时要注意到当 $\\frac{\\hat \\gamma}{\\Vert w\\Vert}  = \\frac{1}{\\Vert w\\Vert}$ 取得最大值的时候，$\\Vert w\\Vert ^2$ 取得最小值，所以就得到了下面的这个优化问题：\n\n$$\n\\begin{aligned}\n\\min_{\\gamma,w,b} \\quad& \\frac{1}{2}\\Vert w\\Vert ^2 \\\\\ns.t. \\quad &y^{(i)}(w^Tx^{(i)}+b) \\geq 1,\\quad i=1,...,m\\\\\n\\end{aligned}\n$$\n\n通过上面这样的转换，这个问题就变得容易解决了。上面的问题有一个凸二次对象(a convex quadratic objective)，且仅受线性约束(only linear constraints)。对这个问题进行求解，我们就能得到**最优边界分类器(optimal margin classifier)。** 这个优化问题的求解可以使用商业二次规划(commercial quadratic programming ，缩写QP)代码$^1$。\n\n>1 可能你更熟悉的是线性规划(linear programming)，这种方法适用的优化问题是有线性对象(linear objectives)和线性约束(linear constraints)。QP 软件的适用范围也很广泛，其中就包括这种凸二次对象(convex quadratic objectives)和线性约束的情况。\n\n这样，差不多就可以说问题已经得到了解决，接下来咱们就要岔开话题，聊一聊拉格朗日对偶性(Lagrange duality)。这样就会引出我们这个优化问题的对偶形式(dual form)，这种形式会在我们后续要使用核(kernels)的过程中扮演重要角色，核(kernels)可以有效地对极高维度空间中的数据建立最优边界分类器。通过这种对偶形式，我们还能够推出一种非常有效的算法，来解决上面这个优化问题，而且通常这个算法那还能比通用的 QP 软件更好用。\n\n#### 5 拉格朗日对偶性(Lagrange duality) \n\n咱们先把 SVMs 以及最大化边界分类器都放到一边，先来谈一下约束优化问题的求解。\n\n例如下面这样的一个问题：\n\n$$\n\\begin{aligned}\n\\min_w \\quad & f(w)& \\\\\ns.t. \\quad &h_i(w) =0,\\quad i=1,...,l\\\\\n\\end{aligned}\n$$\n\n可能有的同学还能想起来这个问题可以使用拉格朗日乘数法(method of Lagrange multipliers)来解决。（没见过也不要紧哈。）在这个方法中，我们定义了一个**拉格朗日函数(Lagrangian)** 为：\n\n$$\nL(w,\\beta)=f(w)+\\sum^l_{i=1}\\beta_i h_i(w)\n$$\n\n上面这个等式中，这个 $\\beta_i$ 就叫做**拉格朗日乘数(Lagrange multipliers)。** 然后接下来让 对 $L$ 取偏导数，使其为零：\n\n$$\n\\frac{\\partial L }{\\partial w_i} =0; \\quad \\frac{\\partial L }{\\partial \\beta_i} =0;\n$$\n\n然后就可以解出对应的 $w$ 和 $\\beta$ 了。在本节，我们对此进行一下泛化，扩展到约束优化(constra_ined optimization)的问题上，其中同时包含不等约束和等式约束。由于篇幅限制，我们在本课程$^2$不能讲清楚全部的拉格朗日对偶性(do the theory of Lagrange duality justice)，但还是会给出主要的思路和一些结论的，这些内容会用到我们稍后的最优边界分类器的优化问题(optimal margin classifier’s optimization problem)。\n\n>2 对拉格朗日对偶性该兴趣的读者如果想要了解更多，可以参考阅读 R. T. Rockefeller (1970) 所作的《凸分析》(Convex Analysis)，普林斯顿大学出版社(Princeton University Press)。\n\n下面这个，我们称之为**主** 最优化问题(primal optimization problem)：\n\n$$\n\\begin{aligned}\n\\min_w \\quad & f(w)& \\\\\ns.t. \\quad & g_i(w) \\le 0,\\quad i=1,...,k\\\\\n& h_i(w) =0,\\quad i=1,...,l\\\\\n\\end{aligned}\n$$\n\n要解决上面这样的问题，首先要定义一下**广义拉格朗日函数(generalized Lagrang_ian)：**\n\n$$\nL(w,\\alpha,\\beta)=f(w)+\\sum^k_{i=1}\\alpha_ig_i(w)+\\sum^l_{i=1}\\beta_ih_i(w)\n$$\n\n上面的式子中， $\\alpha_i$ 和 $\\beta_i$ 都是**拉格朗日乘数(Lagrange multipliers)**。设有下面这样一个量(quantity)：\n\n$$\n\\theta_{P}(w)=\\max_{\\alpha,\\beta:\\alpha_i \\geq 0}L(w,\\alpha,\\beta)\n$$\n\n上式中的 $P$ 是对 “primal” 的简写。设已经给定了某些 $w$。如果 $w$ 不能满足某些主要约束，（例如对于某些 $i$ 存在 $g_i(w) > 0$ 或者 $h_i(w) \\neq 0$），那么咱们就能够证明存在下面的等式关系：\n\n$$\n\\begin{aligned}\n\\theta_P(w)&=\\max_{\\alpha,\\beta:\\alpha_i \\geq 0} f(w)+\\sum^k_{i=1}\\alpha_ig_i(w)+\\sum^l_{i=1}\\beta_ih_i(w) &\\text{(1)}\\\\\n&= \\infty &\\text{(2)}\\\\\n\\end{aligned}\n$$\n\n与之相反，如果 $w$ 的某些特定值确实能满足约束条件，那么则有 $\\theta_P(w) = f(w)$。因此总结一下就是：\n\n$$\n\\theta_P(w)= \\begin{cases} f(w) & g_i(w)\\le{0}\\ \\rm{and}\\ h_i(w)=0\\quad\\text {if w satisfies primal constraints} \\\\\n\\infty & g_i(w)>0\\ \\rm{or}\\ h_i(w)\\ne{0}\\quad\\text{otherwise} \\end{cases}\n$$\n\n因此，如果 $w$ 的所有值都满足主要约束条件，那么$\\theta_P$的值就等于此优化问题的目标量(objective in our problem)，而如果约束条件不能被满足，那 $\\theta_P$的值就是正无穷了(positive infinity)。所以，进一步就可以引出下面这个最小化问题(minimization problem)：\n\n$$\n\\min_w \\theta_P(w)=\\min_w \\max_{\\alpha,\\beta:\\alpha_i\\geq0} L(w,\\alpha,\\beta)\n$$\n\n这个新提出的问题与之前主要约束问题有一样的解，所以还是同一个问题。为了后面的一些内容，我们要在这里定义一个目标量的最优值(optimal value of the objective)$p ^\\ast = \\displaystyle\\min_w \\theta_P (w)$；我们把这个称为 主要优化问题的**值** (value of the primal problem)。\n\n接下来咱们来看一个稍微不太一样的问题。我们定义下面这个 $\\theta_D$：\n\n$$\n\\theta_D(\\alpha,\\beta)=\\min_w L(w,\\alpha,\\beta)\n$$\n\n上面的式子中，$D$ 是 “dual” 的缩写。这里要注意，在对$\\theta_P$ 的定义中，之前是对 $\\alpha$, $\\beta$ 进行优化(找最大值)，这里则是找 $w$ 的最小值。\n\n现在我们就能给出这个**对偶**优化问题了：\n\n$$\n\\max_{\\alpha,\\beta:\\alpha_i\\geq 0} \\theta_D(\\alpha,\\beta)  = \\max_{\\alpha,\\beta:\\alpha_i\\geq 0} \\min_w L(w,\\alpha,\\beta)\n$$\n\n这个形式基本就和我们之前看到过的主要约束问题(primal problem)是一样的了，唯一不同是这里的“max” 和 “min” 互换了位置。我们也可以对这种对偶问题对象的最优值进行定义，即 $d^\\ast = \\displaystyle\\max_{\\alpha,\\beta:\\alpha_i\\geq 0}\\theta_D(w)$。\n\n主要约束问题和这里的对偶性问题是怎么联系起来的呢？通过下面的关系就很容易发现\n\n\n$$\nd^\\ast = \\max_{\\alpha,\\beta:\\alpha_i\\geq 0}\\min_w L(w,\\alpha,\\beta) \\leq \\min_w \\max_{\\alpha,\\beta:\\alpha_i\\geq 0}L(w,\\alpha,\\beta)  =p^\\ast\n$$\n\n（你应该自己证明一下，这里以及以后的一个函数的最大的最小值“max min”总是小于等于最小的最大值“min max”。）不过在某些特定的情况下，就会有二者相等的情况：\n\n$$\nd^\\ast =p^\\ast\n$$\n\n这样就可以解对偶问题来代替原来的主要约束问题了。接下来咱们就来看看导致上面二者相等的特定条件是什么。\n\n假设 $f$ 和 $g_i$ 都是凸的(convex$^3$)，$h_i$ 是仿射的(affine$^4$)。进一步设 $g$ 是严格可行的(strictly feasible)；这就意味着会存在某个 $w$，使得对所有的 $i$ 都有 $g_i(w) < 0$。\n\n>3 当 $f$ 有一个海森矩阵(Hessian matrix)的时候，那么当且仅当这个海森矩阵(Hessian matrix)是半正定矩阵，$f$ 才是凸的。例如，$f (w) = w^T w$ 就是凸的；类似的，所有的线性linear（以及仿射affine）函数也都是凸的。（不可微differentiable的函数 $f$ 也可以是凸的，不过在这里我们还不需要对凸性进行那么泛化的扩展定义。）\n\n>4 例如，存在 $a_i$和 $b_i$ 满足 $h_i(w) = a^T_i w + b_i$。仿射(Affine)的意思大概就跟线性 linear 差不多，不同的就是在矩阵进行了线性变换的基础上还增加了一个截距项(extra intercept term)$b_i$。\n\n基于上面的假设，可知必然存在 $w^\\ast$，$\\alpha^\\ast$， $\\beta^\\ast$ 满足$w^\\ast$ 为主要约束问题(primal problem)的解，而$\\alpha^\\ast$，$\\beta^\\ast$ 为对偶问题的解，此外存在一个 $p^\\ast = d^\\ast = L(w^\\ast,\\alpha^\\ast, \\beta^\\ast)$。另外，$w^\\ast$，$\\alpha^\\ast$， $\\beta^\\ast$这三个还会满足**卡罗需-库恩-塔克条件(Karush-Kuhn-Tucker conditions, 缩写为 KKT)，** 如下所示：\n\n$$\n\\begin{aligned}\n\\frac{\\partial}{\\partial w_i}L(w^\\ast,\\alpha^\\ast,\\beta^\\ast) &= 0,\\quad i=1,...,n & \\text{(3)}\\\\\n\\frac{\\partial}{\\partial \\beta_i}L(w^\\ast,\\alpha^\\ast,\\beta^\\ast)&= 0 ,\\quad i=1,...,l &  \\text{(4)}\\\\\n\\alpha_i^\\ast g_i(w^\\ast)&= 0,\\quad i=1,...,k & \\text{(5)}\\\\\ng_i(w^\\ast)&\\leq 0,\\quad i=1,...,k & \\text{(6)}\\\\\n\\alpha_i^\\ast &\\geq 0,\\quad i=1,...,k &\\text{(7)}\\\\\n\\end{aligned}\n$$\n\n反过来，如果某一组 $w^\\ast,\\alpha^\\ast,\\beta^\\ast$ 满足 KKT 条件，那么这一组值就也是主要约束问题(primal problem)和对偶问题的解。\n\n这里咱们要注意一下等式$(5)$，这个等式也叫做 **KKT 对偶互补** 条件(dual complementarity condition)。这个等式暗示，当$\\alpha_i^\\ast > 0$ 的时候，则有 $g_i(w^\\ast) = 0$。（也就是说，$g_i(w) \\leq 0$ 这个约束条件存在的话，则应该是相等关系，而不是不等关系。）后面的学习中，这个等式很重要，尤其对于表明 SVM 只有少数的“支持向量(Support Vectors)”；在学习 SMO 算法的时候，还可以用 KKT 对偶互补条件来进行收敛性检测(convergence test)。\n\n#### 6 最优边界分类器(Optimal margin classifiers )\n\n在前面的内容中，我们讲到了下面这种（主要约束）优化问题(optimization problem)，用于寻找最优边界分类器(optimal margin classifier)：\n\n$$\n\\begin{aligned}\n\\min_{\\gamma,w,b} \\quad & \\frac12 \\Vert w\\Vert ^2\\\\\n&\\\\\ns.t. \\quad &y^{(i)}(w^Tx^{(i)}+b)\\geq 1,\\quad i=1,...,m\\\\\n\\end{aligned}\n$$\n\n这里的约束条件(constra_ints)可以写成下面的形式：\n\n$$\ng_i(w)=-y^{(i)}(w^Tx^{(i)}+b)+1\\leq 0\n$$\n\n\n对于训练集中的每一个样本，都有这样的一个约束条件。要注意，通过 KKT 对偶互补条件可知，只有训练样本的函数边界确定为 $1$ 的情况下，才有 $\\alpha_i \\geq  0$ (这些样本对应的约束条件关系都是等式关系，也就是对应的 $g_i(w) = 0$)。如下图所示，其中用实线所表示的就是最大间隔分界超平面(maximum margin separating hyperplane)。\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note3f3.png)\n\n具有最小边界的样本点(points with the smallest margins)正好就是距离分类边界(decision boundary)最近的那些点；图中所示，一共有三个这样的点（一个是空心圆的负值，两个是叉号的正值），他们所处的位置与分类边界线(即实线)相平行的虚线上。因此，在这个优化问题中的最优解里面，只有这三个样本点所对应的 $\\alpha_i$ 是非零的。这三个点在此问题中被称作**支持向量(support vectors)。** 这种现象就是，支持向量的规模(number of support vectors)可以比整个训练集的规模(size of the tra_ining set)更小，这在稍后的内容中会用到。\n\n接着往下来。我们已经给出了问题的对偶形式，那么一个关键的思路就是，接下来我们需要把算法写成仅包含内积的形式 $<x^{(i)},x^{(j)}>$ （也可以理解为 $(x^{(i)})^Tx^{(j)}$），即输入特征空间中的点相乘得到的内积。当使用核技巧(kernel trick)的时候，把算法用内积的形式表达就非常重要了。\n\n在构建优化问题的拉格朗日函数(Lagrang_ian)的时候，有下面的等式：\n\n$$\nL(w,b,\\alpha )=\\frac{1}{2}\\Vert w\\Vert ^2 - \\sum^m_{i=1}\\alpha_i [y^{(i)}(w^Tx^{(i)}+b)-1]  \\qquad\\text{(8)}\n$$\n\n注意这里的拉格朗日乘数(Lagrange multipliers)中只有“$\\alpha_i$” 而没有 “$\\beta_i$”，因为这时候问题还只有不等式约束条件(inequality constra_ints)。\n\n接下来咱们找一下这个问题的对偶性形式。首先就要使 $L(w,b,\\alpha)$ 取最小值，调整 $w$ 和 $b$，而使 $\\alpha$ 固定，这样就能得到 $\\theta_D$，具体方法也就是令 $L$ 关于 $w$ 和 $b$ 的导数(derivatives)为零。接下来得到下面的等式：\n\n$$\n\\nabla_w L(w,b,\\alpha)=w-\\sum^m_{i=1}\\alpha_i y^{(i)}x^{(i)} =0\n$$\n\n改写一下形式也就得到了：\n\n$$\n\\begin{aligned}\nw=\\sum^m_{i=1}\\alpha_i y^{(i)}x^{(i)}\\qquad\\text{(9)}\n\\end{aligned}\n$$\n\n关于 $b$ 的导数为零，就有：\n\n$$\n\\begin{aligned}\n\\frac{\\partial}{\\partial b}L(w,b,\\alpha)=-\\sum^m_{i=1}\\alpha_i y^{(i)}=0 &   \\qquad  \\text{(10)}\n\\end{aligned}\n$$\n\n通过上面的等式 $(9)$取得 $w$ 的一种定义方式，然后把这个代入到拉格朗日函数，也就是等式$(8)$ 中，然后简化一下，就得到：\n\n$$L(w,b,\\alpha)=\\sum^m_{i=1}\\alpha_i-\\frac12 \\sum^m_{i,j=1} y^{(i)}y^{(j)}\\alpha_i\\alpha_j(x^{(i)})^Tx^{(j)}-b\\sum^m_{i=1}\\alpha_i y^{(i)}$$\n\n然后根据上面的等式 $(10)$ 就能知道最后一项必然是 $0$，所以可以进一步简化得到：\n\n$$L(w,b,\\alpha)=\\sum^m_{i=1}\\alpha_i-\\frac12 \\sum^m_{i,j=1} y^{(i)}y^{(j)}\\alpha_i\\alpha_j(x^{(i)})^Tx^{(j)}$$\n\n还记得之前我们得到的关于 $w$ 和 $b$ 来取 $L$ 最小值的等式吧。把这些与约束条件 $\\alpha_i \\geq 0$ （这个一直都成立的）结合起来，然后再结合等式 $(10)$，就得到了下面的对偶优化问题了：\n\n$$\n\\begin{aligned}\n\\max_\\alpha \\quad & W(\\alpha) =\\sum^m_{i=1}\\alpha_i-\\frac12\\sum^m_{i,j=1}y^{(i)}y^{(j)}\\alpha_i\\alpha_j\\langle\tx^{(i)} x^{(j)}\t\\rangle\\\\\ns.t. \\quad &  \\alpha_i \\geq 0,\\quad i=1,...,m\\\\\n& \\sum^m_{i=1} \\alpha_iy^{(i)}=0\n\\end{aligned}\n$$\n\n你应该能够证明我们这个优化问题的条件是满足 $p^\\ast = d^\\ast$ 和 KKT 条件的（等式 $(3-7)$）。这样，我们就可以通过解这个对偶问题来解决掉原来的主优化问题。具体来说，就是我们构建了一个以 $\\alpha_i$ 作为参数的取最大值问题(maximization problem)，如果能把这个问题解出来（也就是说找到 $\\alpha$ ，使得 $W(\\alpha)$ 取最大值，且满足约束条件）那么就可以利用等式 $(9)$ 来返回到问题中，找到最优的 $w$，作为一个关于 $\\alpha$ 的函数。得到了 $w^\\ast$ 之后，再考虑主优化问题，就能直接找到截距项 $b$ 的最优值了：\n\n$$\n\\begin{aligned}\nb^\\ast =- \\frac{\\max_{i:y^{(i)}=-1}(w^{\\ast})^ Tx^{(i)}+\\min_{i:y^{(i)}=1}(w^{\\ast})^Tx^{(i)}}{2}\\tag{11}\\\\\n\\end{aligned}\n$$\n\n（自己证明一下这个过程吧。）在继续之前，咱们再回头看一看等式 $(9)$，其中以 $\\alpha$ （最优值的）形式来表达了 $w$ 的最优值。加入咱们已经使用一个训练集对模型的参数进行了拟合，然后接下来要对一个新输入的 $x$ 的目标值进行预测。就要对 $w^T x + b$ 进行计算，如果得到的值大于 $0$，就预测 $y = 1$。通过利用等式 $(9)$，这个性质也可以写成：\n\n$$\n\\begin{aligned}\nw^Tx+b&= (\\sum^m_{i=1}\\alpha_i y^{(i)}x^{(i)})^Tx+b & \\text{(12)}\\\\\n&= \\sum^m_{i=1}\\alpha_i y^{(i)}\\langle x^{(i)},x \\rangle+b & \\text{(13)}\\\\\n\\end{aligned}\n$$\n\n这样的话，要进行一个预测，如果已经找到了 $\\alpha_i$，那就必须要计算一个值，该值只依赖新的输入特征 $x$ 与训练集中各个 点 的内积。另外，之前我们就已经发现除了支持向量(support vectors) $\\alpha_i$ 的值都是 $0$。这样上面的求和项目中的很多项就都是 $0$ 了，接下来咱们就只需要找到 $x$ 与支持向量(support vectors)的内积（这样实际用于计算的只有一小部分，而不是整个的训练集），然后就可以计算等式 $(13)$，得到结果，就可以用于预测了。\n\n通过检验优化问题的对偶形式，我们对要解决的问题的结构有了深入的了解，并且还根据输入特征向量之间的内积来编写整个算法。 在下一节中，我们将充分利用这些内容，然后对我们的分类问题使用核(kernels)方法。 最终得到的算法，就是支持向量机(support vector machines)算法，将能够在非常高的维度空间中有效地学习。\n\n#### 7 核(Kernels )\n\n咱们之前讲线性回归的时候，遇到过这样一个问题，其中输入的特征 $x$ 是一个房屋的居住面积，然后我们考虑使用特征 $x$，$x^2$ 以及 $x^3$ 来进行拟合得到一个立方函数(cubic function)。要区分出两组数据集，我们把“原始”的输入值称为一个问题的输入属性(input attributes)，例如住房面积 $x$。当这些值映射到另外的一些数据集来传递给学习算法的时候，这些新的数据值就称为输入特征(input features)。(很无奈，不同的作者经常用不同的名词来描述这两者，不过在咱们这个讲义里面，我们会争取保持所用术语的一致性。)然后我们还要用 $\\phi$ 来表示**特征映射(feature mapping)，** 这种特征映射也就是从输入的属性(input attributes)到传递给学习算法的输入特征(input features)之间的映射关系。例如，还说刚刚这个居住面积 $x$ 的例子，这里面的特征映射就可以表述为：\n\n$$\n\\phi(x)=\\left[ \\begin{aligned}& x\\\\ & x^2\\\\ & x^3 \\end{aligned}\\right]\n$$\n\n现在我们就不再简单直接地利用 SVM(支持向量机算法)来处理原始的输入属性 $x$ 了，而是可以尝试着利用映射产生的新的特征 $\\phi(x)$。那么，我们只需要简单回顾一下之前的算法，然后把所有的 $x$ 替换成 $\\phi(x)$。\n\n由于上面的算法可以整个用 $\\langle x,z\\rangle$ 的内积形式写出，这就意味着我们只需要把上面的所有内积都替换成 $\\langle\\phi(x),\\phi(z)\\rangle$ 的内积。更简洁地说，给定一个特征映射 $\\phi$，那么就可以定义一个对应的**核(Kernel)，** 如下所示：\n\n$$\nK(x,z)=\\phi(x)^T\\phi(z)\n$$\n\n然后，只需要把上面的算法里用到的 $\\langle x, z\\rangle$ 全部替换成 $K(x, z)$，这样就可以了，我们的算法就开始使用特征映射 $\\phi$ 来进行机器学习了。\n\n现在，给定一个特征映射 $\\phi$，很容易就可以找出 $\\phi(x)$ 和 $\\phi(z)$，然后取一下内积，就能计算 $K (x, z)$了。不过还不仅如此，更有意思的是，通常情况下对这个$K (x,z)$ 的计算往往都会非常容易(very inexpensive)，甚至即便 $\\phi(x)$ 本身很不好算的时候(可能是由于向量维度极高)也如此。在这样的背景下，给定一个特征映射 $\\phi$，我们就可以使用 SVM(支持向量机算法)来对高纬度特征空间进行机器学习，而可能根本不用麻烦地区解出来对应的向量 $\\phi(x)$。\n\n接下来就看一个例子吧。假设有 $x, z \\in R^n$，设有：\n\n$$\nK(x,z)= (x^Tz)^2\n$$\n\n这个也可以写成下面的形式：\n\n$$\n\\begin{aligned}\nK(x,z)&=(\\sum^n_{i=1}x_iz_i)(\\sum^n_{j=1}x_jz_j)\\\\\n&=\\sum^n_{i=1}\\sum^n_{j=1}x_ix_jz_iz_j\\\\\n&=\\sum^n_{i,j=1}(x_ix_j)(z_iz_j)\\\\\n\\end{aligned}\n$$\n\n因此，可见 $K (x, z) = \\phi(x)^T \\phi(z)$，其中特征映射 $\\phi$ 给出如下所示(这个例子中的 $n = 3$) ：\n\n$$\n\\phi(x)= \\left[ \\begin{aligned} &x_1x_1\\\\&x_1x_2\\\\&x_1x_3\\\\&x_2x_1\\\\&x_2x_2\\\\&x_2x_3\\\\ &x_3x_1\\\\&x_3x_2\\\\&x_3x_3 \\end{aligned}   \\right]\n$$\n\n到这里就会发现，计算高维度的 $\\phi(x)$ 需要的计算量是 $O(n^2)$ 级别的，而计算 $K (x, z)$ 则只需要 $O(n)$ 级的时间，也就是与输入属性的维度呈线性相关关系。\n\n与之相关的核(Kernel)可以设为：\n\n$$\n\\begin{aligned}\nK(x,z)&= (x^Tz+c)^2 \\\\\n&= \\sum^n_{i,j=1}(x_ix_j)(z_iz_j)+\\sum^n_{i=1}(\\sqrt{2c}x_i)(\\sqrt{2c}z_i)+c^2\\\\\n\\end{aligned}\n$$\n\n（自己检验。）对应的特征映射为（此处依然以 $n = 3$ 为例）：\n\n$$\n\\phi(x)= \\left[ \\begin{aligned} &x_1x_1\\\\&x_1x_2\\\\&x_1x_3\\\\&x_2x_1\\\\&x_2x_2\\\\&x_2x_3\\\\ &x_3x_1\\\\&x_3x_2\\\\&x_3x_3\\\\& \\sqrt{2c}x_1\\\\& \\sqrt{2c}x_2\\\\& \\sqrt{2c}x_3\\\\&c\\end{aligned}   \\right]\n$$\n\n其中 参数 $c$ 控制了第一组 $x_i$ 和第二组 $x_ix_j$ 的相对权重(relative weighting)。\n\n在此基础上进一步扩展，核(Kernel)$K (x, z) = (x^T z + c)^d$，就对应了一个$\\left(\\begin{aligned} n&+d \\\\ &d  \\end{aligned}\\right)$维度的特征空间的特征映射，对应所有从 $x_{i1},x_{i2}, ...,x_{ik}$ 一直到 $d$ 这种形式的所有多项式(monomials)。然而，即便是针对这种 $O(n^d)$ 维度的高维度空间，计算 $K (x, z)$ 让然只需要 $O(n)$ 级的时间。所以我们就不需要在这个非常高的维度特征空间中对特征向量进行具体的表示。\n\n现在，咱们来从另外一个角度来看一下核(Kernel)。凭直觉来看(这种直觉可能还有些错误，不过不要紧，先不用管)，如果 $\\phi(x)$ 和 $\\phi(z)$ 非常接近(close together)，那么就可能会认为 $K (x, z) = \\phi(x)^T\\phi(z)$ 就可能会很大。与之相反，如果 $\\phi(x)$ 和 $\\phi(z)$ 距离很远，比如近似正交(nearly orthogonal)，那么$K (x, z) = \\phi(x)^T\\phi(z)$ 就可能会很小。这样，我们就可以把核 $K (x, z)$ 理解成对$\\phi(x)$ 和 $\\phi(z)$ 的近似程度的一种度量手段，或者也可以说是对 $x$ 和 $z$ 的近似程度的一种度量手段。\n\n有了这种直观认识之后，假如你正在尝试某些学习算法，并且已经建立了某个函数 $K (x, z)$，然后你想着也许可以用这个函数来对 $x$ 和 $z$ 的近似程度进行衡量。例如，假如你就选择了下面这个函数：\n\n$$\nK(x,z)=\\exp (- \\frac{\\Vert x-z\\Vert ^2}{2\\sigma^2 })\n$$\n\n这个函数是对 $x$ 和 $z$ 的近似程度的一个很好的衡量，二者相近的时候函数值接近 $1$，而二者远离的时候函数值接近 $0$。那咱们能不能用这样定义的一个函数 $K$ 来作为一个 SVM(支持向量计算法)里面的核(Kernel)呢？在这个特定的样例里面，答案是可以的。(这个核(Kernel)也叫做 高斯核，对应的是一个无穷维度的特征映射 $\\phi$。)那么接下来进一步推广一下，给定某个函数 $K$，我们该怎样能够确定这个函数是不是一个有效的核(valid kernel)呢？例如，我们能否说如果存在着某一个特征映射 $\\phi$，则对于所有的 $x$ 和 $z$ 都有 $K (x, z) = \\phi(x)^T \\phi(z)$？\n\n现在暂时假设 $K$ 就是一个有效的核，对应着某种特征映射 $\\phi$。然后，考虑某个有 $m$ 个点的有限集合 $\\{x^{(1)},...,x^{(m)}\\}$(这个集合并不一定就必须是训练集)，然后设一个方形的 $m\\times m$ 矩阵 $K$，定义方式为矩阵的第 $(i, j)$ 个值 $K_{ij} = K (x^{(i)} , x^{(j)})$。这个矩阵就叫做核矩阵(Kernel matrix)。注意到没有，这里对符号 $K$ 进行了重复使用，既指代了$K(x,z)$ 这个核函数(kernel function)，也指代了核矩阵 $K$，这是由于这两者有非常明显的密切关系。\n\n如果 $K$ 是一个有效的核(Kernel)，那么就有$K_{ij} = K (x^{(i)}, x^{(j)}) = \\phi(x^{(i)})^T \\phi(x^{(j)}) = \\phi(x^{(j)})^T \\phi(x^{(i)}) = K(x^{(j)}, x^{(i)}) = K_{ji}$，这就说明 $K$ 是一个对称矩阵。此外，设 $\\phi_k(x)$ 表示向量 $\\phi(x)$ 的第 $k$ 个坐标值(k-th coordinate)，对于任意的向量 $z$，都有：\n\n$$\n\\begin{aligned}\nz^TKz&=   \\sum_i\\sum_j z_iK_{ij}z_j \\\\\n&=   \\sum_i\\sum_j z_i\\phi(x^{(i)})\\phi(x^{(j)})z_j\\\\\n&=   \\sum_i\\sum_jz_i\\sum_k \\phi_k(x^{(i)})\\phi_k(x^{(j)})z_j\\\\\n&=   \\sum_k\\sum_i\\sum_j z_i\\phi_k(x^{(i)})\\phi_k(x^{(j)})z_j\\\\\n&= \\sum_k(\\sum_i z_i\\phi_k(x^{(i)}))^2\\\\\n&\\geq 0 \\\\\n\\end{aligned}\n$$\n\n上面的推导中从第二步到最后一步都用到了习题集$1$第一题(Problem set 1 Q1)中的同样技巧。由于 $z$ 是任意的，这就表明了矩阵 $K$ 是半正定(positive semi-definite)的矩阵($K \\geq 0$)。\n\n这样，我们就证明了，如果 $K$ 是一个有效核函数(例如，假设该函数对应某种特征映射 $\\phi$ )，那么对应的核矩阵(Kernel Matrix)$K \\in R^{m\\times m}$ 就是一个对称半正定矩阵(symmetric positive semidefinite)。进一步扩展，这就不仅仅是一个 $K$ 是一个有效核函数的必要条件(necessary)，还成了充分条件(sufficient)，这个核函数也叫做默瑟核(Mercer kernel)。下面要展示的结果都是源自于 Mercer(due to Mercer)。$^5$\n\n>5 很多教材对默瑟定理(Mercer’s theorem)的描述都要更加复杂一些，里面牵涉到了 $L2$ 函数，但如果输入属性(input attributes)只在实数域 Rn 中取值，那么这里给出的这种表述也是等价的(equivalent.)。\n\n**默瑟定理(Mercer’s theorem)：** 设给定的 $K: R^n \\times  R^n \\rightarrow   R$。然后要使 $K$ 为一个有效的默瑟核(valid Mercer kernel)，其充分必要条件为：对任意的 $\\{x^{(1)},...,x^{(m)}\\, (m \\lt \\infty)$，都有对应的核矩阵(kernel matrix)为对称半正定矩阵(symmetric positive semi-definite)。\n\n对于一个给定的函数 $K$，除了之前的找出对应的特征映射 $\\phi$ 之外，上面的定理还给出了另外一种方法来验证这个函数是否为有效核函数(valid kernel)。在习题集2(problem set 2) 里面有很多联系，大家可以尝试一下。\n\n在课程中，我们也简要讲了核(Kernel)的若干样例。例如，手写数字识别问题当中，给定了一个手写数字$(0-9)$的图像（$16\\times 16$ 像素），目的是要认出写的是哪个数字。选用的要么是简单的多项式核(polynomial kernel) $K (x, z) = (x^T z)^d$ 或者 高斯核(Gaussian kernel)，SVM(支持向量机算法)在这个样例中表现出了非常出色的性能。这个结果挺让人吃惊的，因为输入属性 $x$ 是一个 $256$ 维的向量，也就是待识别图像的像素密度值向量，而系统之前并没有对于视觉判断的预备知识，甚至对像素彼此间是否相邻都不了解。课堂上的另一个案例中，用于分类的目标 $x$ 是字符串，例如$x$ 可以使一系列的氨基酸(amino acids)，然后连接在一起形成了蛋白质(protein)；要构建一个适用于大多数机器学习算法的合理又足够“小规模”的特征集，是很困难的，而字符串的长度还各自不同，这就更增加了难度。然而也有解决方案，设 $\\phi(x)$ 为一个特征向量，计算了 $x$ 当中每一个 $k$ 长度的字符串的出现次数(number of occurrences of each length-k substring in x)。如果考虑英语字母组成给的字符串，那么这样就存在 $26^k$ 个这样的字符。这样的话，$\\phi(x)$ 就是一个 $26^k$ 维的向量；这时候，即便 $k$ 的值并不算太大，这个值也会变得特别大，难以有效计算。(例如 $26^4 \\approx 460000$。)不过，如果使用(动态编程风格，dynamic programming-ish)的字符串匹配算法，就可以有效率地计算 $K(x,z ) = \\phi(x)^T\\phi(z)$，也就是说，我们就能够对 $26^k$ 维的特征空间进行隐式处理，而根本不用去特地计算这个空间中的特征向量。\n\n关于在支持向量机算法(support vector machines)中核(kernels)的使用，我们讲得已经够清楚了，所以就不在这里多做赘述了。不过有一个思路还是值得记住的，就是核(Kernel)的用法远远不仅限于 SVM 算法当中。具体来说，只要你的学习算法，能够写成仅用输入属性向量的内积来表达的形式，那么就可以通过引入 核K(Kernel)，替换成 $K(x,z)$，来对你的算法“强力”加速，使之能够在与 $K$ 对应的高维度特征空间中有效率地运行。核(Kernel)与感知器(Perceptron)相结合，还可以产生内核感知器算法(kernel perceptron algorithm)。后文我们要学到的很多算法，也都可以适用于这样的处理，这个方法也就成为“核技巧(kernel trick)”\n\n#### 8 正则化和不可区分的情况(Regularization and the non-separable case)\n\n到目前为止，咱们对 SVM(支持向量机算法)进行的推导，都是基于一个假设，也就是所有的数据都是线性可分的。在通过特征映射 $\\phi$ 来将数据映射到高维度特征空间的过程，通常会增加数据可分割的概率，但我们还是不能保证数据一直可以区分。而且，在某些案例中，查找一个分类超平面还不一定是我们的目的所在，因为也可能很容易就出现异常值。例如，如下图所示的是一个最优边界分类器(optimal margin classifier)，如果有一个单独的异常值投到了右上方的区域（如右图所示），这就会导致分界线出现显著的偏移，还会导致分类器的边界缩小了很多。\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note3f4.png)\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note3f5.png)\n\n要想让算法能够适用于非线性可分的数据集，并且使其对待异常值的敏感度降低一些，那就要把咱们的优化方法进行重构(reformulate)（使用 $l1$ **正则化** ），如下所示：\n\n$$\n\\begin{aligned}\n\\min_{\\gamma,w,b}& \\frac 12 \\Vert w\\Vert ^2+C\\sum^m_{}\\xi_i \\\\\ns.t.& y^{(i)}(w^Tx^{(i)}+b) \\geq1-\\xi_i,i=1,...,m\\\\\n& \\xi_i \\geq 0 ,i=1,...,m\n\\end{aligned}\n$$\n\n这样就允许数据集里面有（函数）边界小于$1$ 的情况了，然后如果一个样本的函数边界为 $1 - \\xi_i$ （其中 $\\xi \\geq  0$），这就需要我们给出 $C\\xi_i$ 作为**目标函数成本函数的降低值**(cost of the objective function being increased)。$C$ 是一个参数，用于控制相对权重，具体的控制需要在一对目标之间进行考量，一个是使得 $\\Vert w\\Vert ^2$ 取最小值（前面章节中的案例可以看到这样能够让边界最大化），另一个是确保绝大部分的样本都有至少为 $1$ 的函数边界。\n\n然后按照惯例，给出拉格朗日函数(Lagrangian)：\n\n$$\nL(w,b,\\xi,\\alpha,r)=\\frac12w^Tw+C\\sum^m_{i=1}\\xi_i-\\sum^m_{i=1}\\alpha_i[y^{(i)}(x^Tw+b)-1+\\xi_i]-\\sum^m_{i=1}r_i\\xi_i\n$$\n\n上面的式子中的 $\\alpha_i$ 和 $r_i$ 都是拉格朗日乘数（Lagrange multipliers 被限制为非负数，$\\geq 0$）。这次我们就不再对对偶形式进行详细推导了，不过如往常一样设 关于 $w$ 和 $b$ 的导数为零，然后再代回去进行简化，这样就能够得到下面的该问题的对偶形式：\n\n$$\n\\begin{aligned}\n\\max_\\alpha \\quad & W(\\alpha) =\\sum^m_{i=1}\\alpha_i-\\frac12\\sum^m_{i,j=1}y^{(i)}y^{(j)}\\alpha_i\\alpha_j \\langle  x^{(i)},x^{(j)} \\rangle \\\\\n s.t. \\quad & 0\\leq \\alpha_i \\leq C,i=1,...,m\\\\\n & \\sum^m_{i=1}\\alpha_iy^{(i)}=0  \\\\\n\\end{aligned}\n$$\n\n跟以前一样，我们还是可以把 $w$ 用 $\\alpha_i$ 来进行表述，如同等式 $(9)$ 所示，所以解完了对偶问题之后，我们就可以接下来使用等式 $(13)$ 来给出我们的预测。这里要注意，有一点很神奇，就是在加入了 $l1$ 正则化之后，对对偶问题的唯一改变只是约束从原来的 $0 \\leq \\alpha_i$ 现在变成了 $0 \\leq \\alpha_i \\leq C$。这里对 $b^\\ast$ 的计算也受到了影响而有所变动（等式 11 不再成立了(no longer valid)）；具体内容参考下一节，或者阅读 Platt 的论文。\n\n另外，KKT 对偶互补条件（dual complementarity condition，这个在下一节要用来测试 SMO 算法的收敛性）为：\n\n$$\n\\begin{aligned}\n\\alpha_i&= 0 &\\implies y^{(i)}(w^Tx^{(i)}+b)\\ge 1 \\qquad &\\text{(14)}\\\\\n\\alpha_i&= C &\\implies y^{(i)}(w^Tx^{(i)}+b)\\le 1 \\qquad &\\text{(15)}\\\\\n0\\lt \\alpha_i& \\lt C &\\implies y^{(i)}(w^Tx^{(i)}+b) = 1 \\qquad &\\text{(16)}\\\\\n\\end{aligned}\n$$\n\n现在，剩下的问题就只是给出一个算法来具体地解这个对偶问题了，我们下一节就来讲这个问题。\n\n#### 9 SMO 优化算法 \n\nSMO 优化算法是对 sequential minimal optimization（意为序列最小化优化）的缩写，此算法于 1998 年由 John Platt 在微软研究院提出，对于从 SVM（支持向量机算法）推导出的对偶问题，这一算法提供了一种有效的解法。然后我们要先讲一下坐标上升算法(coordinate ascent algorithm)，这个算法很有趣，而且也是用来推导出 SMO 优化算法的一步。\n\n##### 9.1 坐标上升算法(Coordinate ascent )\n\n假如要解决下面这样的无约束优化问题：\n\n$$\n\\max_\\alpha W(\\alpha_1,\\alpha_2,...,\\alpha_m)\n$$\n\n这里的 $W$ 就是关于参数 $\\alpha_i$ 的某种函数，此处暂时忽略掉这个问题和支持向量机算法(SVM)的任何关系。更早之前咱们就已经学过了两种优化算法了，梯度下降法和牛顿法。下面这个新的优化算法，就叫做坐标上升算法(coordinate ascent)：\n\n$$\n\\begin{aligned}\n&\\text{loop until converge}:\\{  \\\\\n&\\qquad For\\quad i=1,...,m, \\{ \\\\\n&\\qquad\\qquad\\alpha_i:= \\arg \\max_{\\hat \\alpha_i}W(\\alpha_1,...,\\alpha_{i-1},\\hat\\alpha_i,\\alpha_{i+1},...,\\alpha_m) \\\\\n&\\qquad\\} \\\\\n&\\}\n\\end{aligned}\n$$\n\n所以如上式中所示，算法最内层的循环(innermost loop)中，会对除了某些特定的 $\\alpha_i$ 之外的所有变量进行保存，然后重新优化 $W$ 来调整参数 $\\alpha_i$。这里给出的这个版本的算法那，最内层循环对变量重新优化的顺序是按照变量排列次序 $\\alpha_1, \\alpha_2, . . ., \\alpha_m, \\alpha_1, \\alpha_2, . . ..$ 进行的， 更复杂的版本可能还会选择其他的排列熟悉怒；例如，我们可以根据预测哪个变量可以使 $W(\\alpha)$ 增加最多，来选择下一个更新的变量(we may choose the next variable to update according to wh_ich one we expect to allow us to make the largest increase in $W(\\alpha)$)。\n\n如果在函数 $W$ 中，最内层循环中的 “$\\arg \\max$” 可以很有效地运行，那么坐标上升算法(coordinate ascent)就成了一个相当有效率的算法了。下面是一个坐标上升算法的示意图：\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note3f6.png)\n\n上图中的椭圆形就是我们要优化的二次函数的轮廓线。坐标上升算法的初始值设置为 $(2, -2)$，此外图中还标示了到全局最大值的路径。要注意，坐标上升法的每一步中，移动的方向都是平行于某个坐标轴的(Vert to one of the axes)，因为每次都只对一个变量进行了优化。\n\n##### 9.2 SMO 优化算法\n\n接下来，我们来简单推导一下 SMO 算法，作为 SVM（支持向量机算法）相关讨论的收尾。一些具体的细节就省略掉了，放到作业里面了，其他的一些内容可以参考课堂上发的纸质材料。\n\n下面就是一个（对偶）优化问题：\n\n$$\n\\begin{aligned}\n\\max_\\alpha \\quad & W(\\alpha)= \\sum^m_{i=1}\\alpha_i-\\frac12 \\sum^m_{i,j=1}y^{(i)}y^{(j)}\\alpha_i\\alpha_j\\langle x^{(i)},x^{(j)}\\rangle & \\text{(17)}\\\\\ns.t.\\quad & 0\\leq \\alpha_i \\leq C,i=1,...,m& \\text{(18)}\\\\\n& \\sum^m_{i=1}\\alpha_iy^{(i)}=0& \\text{(19)}\\\\\n\\end{aligned}\n$$\n\n我们假设有一系列满足约束条件 $(18-19)$ 的 $\\alpha_i$ 构成的集合。接下来，假设我们要保存固定的 $\\alpha_2, ..., \\alpha_m$ 的值，然后进行一步坐标上升，重新优化对应 $\\alpha_1$的目标值(re-optimize the objective with respect to $\\alpha_1$)。这样能解出来么？很不幸，不能，因为约束条件 $(19)$ 就意味着：\n\n$$\n\\alpha_1y^{(1)}=-\\sum^m_{i=2}\\alpha_iy^{(i)}\n$$\n\n或者，也可以对等号两侧同时乘以 $y^{(1)}$ ，然后会得到下面的等式，与上面的等式是等价的：\n\n$$\n\\alpha_1=-y^{(1)}\\sum^m_{i=2}\\alpha_iy^{(i)}\n$$\n\n（这一步用到了一个定理，即 $y^{(1)} \\in {-1, 1}$，所以$(y^{(1)})^2 = 1$）可见 $\\alpha_1$ 是由其他的 $\\alpha_i$ 决定的，这样如果我们保存固定的 $\\alpha_2, ..., \\alpha_m$ 的值，那就根本没办法对 $\\alpha_1$ 的值进行任何修改了，否则不能满足优化问题中的约束条件 $(19)$ 了。\n\n所以，如果我们要对 $\\alpha_i$ 当中的一些值进行更新的话，就必须至少同时更新两个，这样才能保证满足约束条件。基于这个情况就衍生出了 SMO 算法，简单来说内容如下所示：\n\n重复直到收敛 {\n\n1.  选择某一对的 $\\alpha_i$ 和 $\\alpha_j$ 以在下次迭代中进行更新 (这里需要选择那种能朝全局最大值方向最大程度靠近的一对值，using a heuristic that tries to pick the two that will allow us to make the b_iggest progress towards the global maximum)。 \n\n2.  使用对应的 $\\alpha_i$ 和 $\\alpha_j$ 来重新优化(Re-optimize) $W(\\alpha)$ ，而保持其他的 $\\alpha_k$ 值固定($k\\neq i,j$)。\n\n}\n\n我们可以检查在某些收敛公差参数 *tol* 范围内，KKT 对偶互补条件能否被满足，以此来检验这个算法的收敛性。这里的 *tol* 是收敛公差参数(convergence tolerance parameter)，通常都是设定到大概 $0.01$ 到 $0.001$。（更多细节内容参考文献以及伪代码。）\n\nSMO 算法有效的一个关键原因是对 $\\alpha_i, \\alpha_j$ 的更新计算起来很有效率。接下来咱们简要介绍一下推导高效率更新的大概思路。\n\n假设我们现在有某些 $\\alpha_i$ 满足约束条件 $(18-19)$，如果我们决定要保存固定的 $\\alpha_3, ..., \\alpha_m$ 值，然后要使用这组 $\\alpha_1$ 和 $\\alpha_2$ 来重新优化 $W (\\alpha_1, \\alpha_2, ..., \\alpha_m)$ ，这样成对更新也是为了满足约束条件。根据约束条件 $(19)$，可以得到：\n\n$$\n\\alpha_1y^{(1)} + \\alpha_2y^{(2)} = -\\sum^m_{i=3}\\alpha_iy^{(i)}\n$$\n\n等号右边的值是固定的，因为我们已经固定了$\\alpha_3, ..., \\alpha_m$ 的值，所以就可以把等号右边的项目简写成一个常数 $\\zeta$:\n\n$$\n\\alpha_1y^{(1)} + \\alpha_2y^{(2)} = \\zeta \\qquad \\text{(20)}\n$$\n\n然后我们就可以用下面的图来表述对 $\\alpha_1$ 和 $\\alpha_2$ 的约束条件：\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note3f7.png)\n\n根据约束条件$(18)$，可知 必须在图中 $\\alpha_1$和 $\\alpha_2$ 必须在 $[0, C] \\times  [0, C]$ 所构成的方框中。另外图中还有一条线 $\\alpha_1y^{(1)} +\\alpha_2y^{(2)}  = \\zeta$，而我们知道$\\alpha_1$和 $\\alpha_2$ 必须在这条线上。还需要注意的是，通过上面的约束条件，还能知道 $L \\leq \\alpha_2 \\leq H$；否则 ($\\alpha_1,\\alpha_2$) 就不能同时满足在方框内并位于直线上这两个约束条件。在上面这个例子中，$L = 0$。但考虑到直线 $\\alpha_1y^{(1)} + \\alpha_2y^{(2)}  = \\zeta$ 的形状方向，这个 $L = 0$ 还未必就是最佳的；不过通常来讲，保证$\\alpha_1, \\alpha_2$ 位于 $[0, C] \\times  [0, C]$ 方框内的 $\\alpha_2$ 可能的值，都会有一个下界 $L$ 和一个上界 $H$。\n\n利用等式$(20)$，我们还可以把 $\\alpha_1$ 写成 $\\alpha_2$ 的函数的形式：\n\n$$\n\\alpha_1=(\\zeta-\\alpha_2y^{(2)})y^{(1)}\n$$\n\n（自己检查一下这个推导过程吧；这里还是用到了定理：$y^{(1)} \\in {-1, 1}$，所以 $(y^{(1)})^2 = 1$。）所以目标函数 $W(\\alpha)$ 就可以写成：\n\n$$\nW(\\alpha_1,\\alpha_2,...,\\alpha_m)=W((\\zeta-\\alpha_2y^{(2)})y^{(1)},\\alpha_2,...,\\alpha_m)\n$$\n\n把 $\\alpha_3, ..., \\alpha_m$ 当做常量，你就能证明上面这个函数其实只是一个关于 $\\alpha_2$ 的二次函数。也就是说，其实也可以写成 $a\\alpha_2^2 + b\\alpha_2 + c$ 的形式，其中的 $a, b, c$ 参数。如果我们暂时忽略掉方框约束条件(18)(也就是说 $L \\leq \\alpha_2 \\leq H)$，那就很容易通过使导数为零来找出此二次函数的最大值，继而进行求解。我们设 $\\alpha_2^{new, unclipped}$ 表示为 $\\alpha$ 的结果值。你需要自己证明，如果我们要使关于 $\\alpha_2$ 的函数 $W$取最大值，而又受到方框约束条件的限制，那么就可以把 $\\alpha_2^{new, unclipped}$的值“粘贴”到$[L, H]$ 这个间隔内(taking $\\alpha_2^{new, unclipped}$ and “clipping” it to lie in the [L, H] interval)，这样来找到最优值结果，就得到了：\n\n$$\n\\alpha_2^{new}= \\begin{cases}  H & if \\quad\\alpha_2^{new, unclipped}> H\\\\ \\alpha_2^{new, unclipped} & if \\quad L\\leq \\alpha_2^{new, unclipped}\\leq H\\\\  L & if \\quad \\alpha_2^{new, unclipped} <L\\\\ \\end{cases} \n$$\n\n最终，找到了 $\\alpha_2^{new}$ 之后，就可以利用等式$(20)$来代回这个结果，就能得到 $\\alpha_1^{new}$ 的最优值。\n\n此外还有一些其他的细节，也都挺简单，不过这里就不讲了，你自己去读一下 Platt 的论文吧：一个是用于对后续用于更新的 $\\alpha_i, \\alpha_j$ 启发式选择; 另一个是如何在 SMO算法 运行的同时来对 $b$ 进行更新。\n\n\n# 参考","tags":["CS229"],"categories":["CS229"]},{"title":"[机器学习-CS229]生成学习算法","url":"%2Fposts%2Fe2308a48%2F","content":"# CS229 课程讲义中文翻译\n\n| 原作者 | 翻译 | 校对 |\n| --- | --- | --- |\n| [Andrew Ng  吴恩达](http://www.andrewng.org/) | [CycleUser](https://www.zhihu.com/people/cycleuser/columns) | [XiaoDong_Wang](https://github.com/Dongzhixiao) |\n\n\n|相关链接|\n|---|\n|[Github 地址](https://github.com/Kivy-CN/Stanford-CS-229-CN)|\n|[知乎专栏](https://zhuanlan.zhihu.com/MachineLearn)|\n|[斯坦福大学 CS229 课程网站](http://cs229.stanford.edu/)|\n|[网易公开课中文字幕视频](http://open.163.com/movie/2008/1/M/C/M6SGF6VB4_M6SGHFBMC.html)|\n\n\n# 第二章\n\n### 第四部分 生成学习算法(Generative Learning algorithms)\n\n目前为止，我们讲过的学习算法的模型都是$p (y|x;\\theta)$，也就是给定 $x$ 下 $y$ 的条件分布，以  $\\theta$  为参数。例如，逻辑回归中就是以 $h_\\theta(x) = g(\\theta^T x)$ 作为 $p (y|x;\\theta)$ 的模型，这里的 $g$ 是一个 $S$型函数（sigmoid function）。接下来，咱们要讲一下一种不同类型的学习算法。\n\n设想有这样一种分类问题，我们要学习基于一个动物的某个特征来辨别它是大象$(y=1)$还是小狗$(y=0)$。给定一个训练集，用逻辑回归或者基础版的**感知器算法（perceptron algorithm）** 这样的一个算法能找到一条直线，作为区分开大象和小狗的边界。接下来，要辨别一个新的动物是大象还是小狗，程序就要检查这个新动物的值落到了划分出来的哪个区域中，然后根据所落到的区域来给出预测。\n\n还有另外一种方法。首先，观察大象，然后我们针对大象的样子来进行建模。然后，再观察小狗，针对小狗的样子另外建立一个模型。最后要判断一种新动物归属哪一类，我们可以把新动物分别用大象和小狗的模型来进比对，看看新动物更接近哪个训练集中已有的模型。\n\n例如逻辑回归之类的直接试图建立 $p(y|x)$的算法，以及感知器算法（perceptron algorithm）等直接用投图（mappings directly）的思路来判断对应 $X$ 的值落到了 $\\{0, 1\\}$ 中哪个区域的算法，这些都叫**判别式学习算法（discriminative learning algorithms）。** 和之前的这些判别式算法不同，下面我们要讲的新算法是对 $p(x|y)$ 和 $p(y)$来进行建模。这类算法叫**做生成学习算法（generative learning algorithms）**。例如如果 $y$ 用来表示一个样例是  小狗 $(0)$ 或者  大象 $(1)$，那么$p(x|y = 0)$就是对小狗特征分布的建模，而$p(x|y = 1)$就是对大象特征分布的建模。\n\n对 $p(y)$ (通常称为**class priors**`译者注：这里没有找到合适的词进行翻译`) 和$p(x|y)$ 进行建模之后，我们的算法就是用**贝叶斯规则（Bayes rule）** 来推导对应给定 $x$ 下 $y$ 的**后验分布（posterior distribution）**：\n\n$$\np(y|x)=\\frac{p(x|y)p(y)}{p(x)}\n$$\n\n这里的**分母（denominator）** 为：$p(x) = p(x|y = 1)p(y = 1) + p(x|y = 0)p(y = 0)$（这个等式关系可以根据概率的标准性质来推导验证`译者注：其实就是条件概率`），这样接下来就可以把它表示成我们熟悉的 $p(x|y)$和 $p(y)$ 的形式了。实际上如果我们计算$p(y|x)$ 来进行预测，那就并不需要去计算这个分母，因为有下面的等式关系：\n\n$$\\begin{aligned}\n\\arg \\max_y p(y|x) & =\\arg \\max_y \\frac{p(x|y)p(y)}{p(x)}\\\\\n&= \\arg \\max_y p(x|y)p(y)\n\\end{aligned}$$\n\n#### 1 高斯判别分析（Gaussian discriminant analysis）\n\n咱们要学的第一个生成学习算法就是**高斯判别分析（Gaussian discriminant analysis** ，缩写为GDA。`译者注：高斯真棒！`）在这个模型里面，我们**假设 $p(x|y)$是一个多元正态分布。** 所以首先咱们简单讲一下多元正态分布的一些特点，然后再继续讲 GDA 高斯判别分析模型。\n\n##### 1.1 多元正态分布（multivariate normal distribution）\n\n$n$维多元正态分布，也叫做多变量高斯分布，参数为一个$n$维 **均值向量** $\\mu \\in  R^n $，以及一个 **协方差矩阵** $\\Sigma \\in  R^{n\\times n}$，其中$\\Sigma \\geq 0$ 是一个对称（symmetric）的半正定（positive semi-definite）矩阵。当然也可以写成\"$N (\\mu, \\Sigma)$\" 的分布形式，密度（density）函数为：\n\n$$\np(x;\\mu,\\Sigma)=\\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}} exp(-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu))\n$$\n\n在上面的等式中，\"$|\\Sigma|$\"的意思是矩阵$\\Sigma$的行列式（determinant）。对于一个在 $N(\\mu,\\Sigma)$分布中的随机变量 $X$ ，其平均值（跟正态分布里面差不多，所以并不意外）就是 $\\mu$ 了：\n\n$$\nE[X]=\\int_x xp(x;\\mu,\\Sigma)dx=\\mu\n$$\n\n随机变量$Z$是一个有值的向量（vector-valued random variable），$Z$ 的 **协方差（covariance）** 的定义是：$Cov(Z) = E[(Z-E[Z])(Z-E[Z])^T ]$。这是对实数随机变量的方差（variance）这一概念的泛化扩展。这个协方差还可以定义成$Cov(Z) = E[ZZ^T]-(E[Z])(E[Z])^T$（你可以自己证明一下这两个定义实际上是等价的。）如果 $X$ 是一个多变量正态分布，即 $X \\sim N (\\mu, \\Sigma)$，则有：\n\n$$\nCov(X)=\\Sigma\n$$\n\n下面这些样例是一些高斯分布的密度图，如下图所示：\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note2f1.png)\n\n最左边的图，展示的是一个均值为$0$（实际上是一个$2\\times 1$ 的零向量）的高斯分布，协方差矩阵就是$\\Sigma = I$ （一个 $2\\times 2$的单位矩阵，identity matrix）。这种均值为$0$ 并且协方差矩阵为单位矩阵的高斯分布也叫做**标准正态分布。** 中间的图中展示的是均值为$0$而协方差矩阵是$0.6I$ 的高斯分布的概率密度函数；最右边的展示的是协方差矩阵$\\Sigma = 2I$的高斯分布的概率密度函数。从这几个图可以看出，随着协方差矩阵$\\Sigma$变大，高斯分布的形态就变得更宽平（spread-out），而如果协方差矩阵$\\Sigma$变小，分布就会更加集中（compressed）。\n\n来看一下更多的样例：\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note2f2.png)\n\n上面这几个图展示的是均值为$0$，但协方差矩阵各不相同的高斯分布，其中的协方差矩阵依次如下所示：\n\n$$\n\\Sigma =\\begin{bmatrix} \n1 & 0 \\\\ 0 & 1 \\\\ \\end{bmatrix};\n\\Sigma =\\begin{bmatrix} \n1 & 0.5 \\\\ 0.5 & 1 \\\\ \n\\end{bmatrix};\n\\Sigma =\\begin{bmatrix} \n1 & 0.8 \\\\ 0.8 & 1 \\\\ \n\\end{bmatrix}\n$$\n\n第一幅图还跟之前的标准正态分布的样子很相似，然后我们发现随着增大协方差矩阵$\\Sigma$ 的反对角线（off-diagonal）的值，密度图像开始朝着  45° 方向 (也就是 $x_1 = x_2$ 所在的方向)逐渐压缩（compressed）。  看一下三个同样分布密度图的轮廓图（contours）能看得更明显：\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note2f3.png)\n\n下面的是另外一组样例，调整了协方差矩阵$\\Sigma$:\n\n$$\n\\Sigma =\\begin{bmatrix} \n1 & 0.5 \\\\ 0.5 & 1 \\\\ \n\\end{bmatrix};\n\\Sigma =\\begin{bmatrix} \n1 & 0.8 \\\\ 0.8 & 1 \\\\ \n\\end{bmatrix}\n\\Sigma =\\begin{bmatrix} \n3 & 0.8 \\\\ 0.8 & 1 \\\\ \\end{bmatrix};\n$$\n\n上面这三个图像对应的协方差矩阵分别如下所示：\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note2f4.png)\n\n从最左边的到中间`译者注：注意，左边和中间的这两个协方差矩阵中，右上和左下的元素都是负值！`很明显随着协方差矩阵中右上左下这个对角线方向元素的值的降低，图像还是又被压扁了（compressed），只是方向是反方向的。最后，随着我们修改参数，通常生成的轮廓图（contours）都是椭圆（最右边的图就是一个例子）。\n\n再举一些例子，固定协方差矩阵为单位矩阵，即$\\Sigma = I$，然后调整均值$\\mu$，我们就可以让密度图像随着均值而移动：\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note2f5.png)\n\n上面的图像中协方差矩阵都是单位矩阵，即 $\\Sigma = I$，对应的均值$\\mu$如下所示：\n\n$$\n\\mu =\\begin{bmatrix} \n1 \\\\ 0 \\\\ \n\\end{bmatrix};\n\\mu =\\begin{bmatrix} \n-0.5 \\\\ 0 \\\\ \n\\end{bmatrix};\n\\mu =\\begin{bmatrix} \n-1 \\\\ -1.5 \\\\ \n\\end{bmatrix};\n$$\n\n##### 1.2 高斯判别分析模型（Gaussian Discriminant Analysis model）\n\n假如我们有一个分类问题，其中输入特征 $x$ 是一系列的连续随机变量（continuous-valued random variables），那就可以使用高斯判别分析（Gaussian Discriminant Analysis ，缩写为 GDA）模型，其中对 $p(x|y)$用多元正态分布来进行建模。这个模型为：\n\n$$\n\\begin{aligned}\ny & \\sim Bernoulli(\\phi)\\\\\nx|y = 0 & \\sim N(\\mu_o,\\Sigma)\\\\\nx|y = 1 & \\sim N(\\mu_1,\\Sigma)\\\\\n\\end{aligned}\n$$\n\n分布写出来的具体形式如下：\n\n$$\n\\begin{aligned}\np(y) & =\\phi^y (1-\\phi)^{1-y}\\\\\np(x|y=0) & = \\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}} exp ( - \\frac{1}{2}(x-\\mu_0)^T\\Sigma^{-1}(x-\\mu_0)  )\\\\\np(x|y=1) & = \\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}} exp ( - \\frac{1}{2}(x-\\mu_1)^T\\Sigma^{-1}(x-\\mu_1)  )\\\\\n\\end{aligned}\n$$\n\n在上面的等式中，模型的参数包括$\\phi, \\Sigma, \\mu_0$ 和 $\\mu_1$。（要注意，虽然这里有两个不同方向的均值向量$\\mu_0$ 和 $\\mu_1$，针对这个模型还是一般只是用一个协方差矩阵$\\Sigma$。）取对数的似然函数（log-likelihood）如下所示：\n\n$$\n\\begin{aligned}\nl(\\phi,\\mu_0,\\mu_1,\\Sigma) &= \\log \\prod^m_{i=1}p(x^{(i)},y^{(i)};\\phi,\\mu_0,\\mu_1,\\Sigma)\\\\\n&= \\log \\prod^m_{i=1}p(x^{(i)}|y^{(i)};\\mu_0,\\mu_1,\\Sigma)p(y^{(i)};\\phi)\\\\\n\\end{aligned}\n$$\n\n通过使 $l$ 取得最大值，找到对应的参数组合，然后就能找到该参数组合对应的最大似然估计，如下所示（参考习题集1）：\n\n$$\n\\begin{aligned}\n\\phi & = \\frac {1}{m} \\sum^m_{i=1}1\\{y^{(i)}=1\\}\\quad\\text{caculate prob of y=1}\\\\\n\\mu_0 & = \\frac{\\sum^m_{i=1}1\\{y^{(i)}=0\\}x^{(i)}}{\\sum^m_{i=1}1\\{y^{(i)}=0\\}}\\quad \\text{take the average of sum of x(y=0)}\\\\\n\\mu_1 & = \\frac{\\sum^m_{i=1}1\\{y^{(i)}=1\\}x^{(i)}}{\\sum^m_{i=1}1\\{y^{(i)}=1\\}}\\quad\\text{take the average of sum of x(y=1)}\\\\\n\\Sigma & = \\frac{1}{m}\\sum^m_{i=1}(x^{(i)}-\\mu_{y^{(i)}})(x^{(i)}-\\mu_{y^{(i)}})^T\\\\\n\\end{aligned}\n$$\n\n用图形化的方式来表述，这个算法可以按照下面的图示所表示：\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note2f6.png)\n\n图中展示的点就是训练数据集，图中的两个高斯分布就是针对两类数据各自进行的拟合。要注意这两个高斯分布的轮廓图有同样的形状和拉伸方向，这是因为他们都有同样的协方差矩阵$\\Sigma$，但他们有不同的均值$\\mu_0$ 和 $\\mu_1$ 。此外，图中的直线给出了$p (y = 1|x) = 0.5$ 这条边界线。在这条边界的一侧，我们预测 $y = 1$是最可能的结果，而另一侧，就预测 $y = 0$是最可能的结果。\n\n##### 1.3 讨论：高斯判别分析（GDA）与逻辑回归（logistic regression）\n\n高斯判别分析模型与逻辑回归有很有趣的相关性。如果我们把变量（quantity）$p (y = 1|x; \\phi, \\mu_0, \\mu_1, \\Sigma)$ 作为一个 $x$ 的函数，就会发现可以用如下的形式来表达：\n\n$$\np(y=1|x;\\phi,\\Sigma,\\mu_0,\\mu_1)=\\frac 1 {1+exp(-\\theta^Tx)}\n$$\n\n其中的  $\\theta$  是对$\\phi$, $\\Sigma$, $\\mu_0$, $\\mu_1$的某种函数。这就是逻辑回归（也是一种判别分析算法）用来对$p (y = 1|x)$ 建模的形式。\n\n> 注：上面这里用到了一种转换，就是重新对$x^{(i)}$向量进行了定义，在右手侧（right-hand-side）增加了一个额外的坐标$x_0^{(i)} = 1$，然后使之成为了一个 $n+1$维的向量；具体内容参考习题集1。\n> {% asset_img 2019072916092529.png %}\n\n这两个模型中什么时候该选哪一个呢？一般来说，高斯判别分析（GDA）和逻辑回归，对同一个训练集，可能给出的判别曲线是不一样的。哪一个更好些呢？\n\n我们刚刚已经表明，如果$p(x|y)$是一个多变量的高斯分布（且具有一个共享的协方差矩阵$\\Sigma$），那么$p(y|x)$则必然符合一个逻辑函数（logistic function）。然而，反过来，这个命题是不成立的。例如假如$p(y|x)$是一个逻辑函数，这并不能保证$p(x|y)$一定是一个多变量的高斯分布。这就表明**高斯判别模型能比逻辑回归对数据进行更强的建模和假设（stronger modeling assumptions）。** 这也就意味着，**在这两种模型假设都可用的时候，高斯判别分析法去拟合数据是更好的，是一个更好的模型。** 尤其当$p(x|y)$已经确定是一个高斯分布（有共享的协方差矩阵$\\Sigma$），那么高斯判别分析是**渐进有效的（asymptotically efficient）。** 实际上，这也意味着，在面对非常大的训练集（训练样本规模 $m $特别大）的时候，严格来说，可能就没有什么别的算法能比高斯判别分析更好（比如考虑到对 $p(y|x)$估计的准确度等等）。所以在这种情况下就表明，高斯判别分析（GDA）是一个比逻辑回归更好的算法；再扩展一下，即便对于小规模的训练集，我们最终也会发现高斯判别分析（GDA）是更好的。\n\n奈何事有正反，由于逻辑回归做出的假设要明显更弱一些（significantly weaker），所以因此逻辑回归给出的判断鲁棒性（robust）也更强，同时也对错误的建模假设不那么敏感。有很多不同的假设集合都能够将$p(y|x)$引向逻辑回归函数。例如，如果$x|y = 0\\sim Poisson(\\lambda_0)$ 是一个泊松分布，而$x|y = 1\\sim Poisson(\\lambda_1)$也是一个泊松分布，那么$p(y|x)$也将是适合逻辑回归的（logistic）。逻辑回归也适用于这类的泊松分布的数据。但对这样的数据，如果我们强行使用高斯判别分析（GDA），然后用高斯分布来拟合这些非高斯数据，那么结果的可预测性就会降低，而且GDA这种方法也许可行，也有可能是不能用。\n\n总结一下也就是：高斯判别分析方法（GDA）能够建立更强的模型假设，并且在数据利用上更加有效（比如说，需要更少的训练集就能有\"还不错的\"效果），当然前提是模型假设争取或者至少接近正确。逻辑回归建立的假设更弱，因此对于偏离的模型假设来说更加鲁棒（robust）。然而，如果训练集数据的确是非高斯分布的（non-Gaussian），而且是有限的大规模数据（in the limit of large datasets），那么逻辑回归几乎总是比GDA要更好的。因此，在实际中，逻辑回归的使用频率要比GDA高得多。（关于判别和生成模型的对比的相关讨论也适用于我们下面要讲的朴素贝叶斯算法（Naive Bayes），但朴素贝叶斯算法还是被认为是一个非常优秀也非常流行的分类算法。）\n\n\n\n#### 2 朴素贝叶斯法（Naive Bayes）\n\n在高斯判别分析（GDA）方法中，特征向量 $x$ 是连续的，值为实数的向量。下面我们要讲的是当 $x_i$ 是离散值的时候来使用的另外一种学习算法。\n\n下面就来继续看一个之前见过的样例，来尝试建立一个邮件筛选器，使用机器学习的方法。这回咱们要来对邮件信息进行分类，来判断是否为商业广告邮件（就是垃圾邮件），还是非垃圾邮件。在学会了怎么实现之后，我们就可以让邮件阅读器能够自动对垃圾信息进行过滤，或者单独把这些垃圾邮件放进一个单独的文件夹中。对邮件进行分类是一个案例，属于文本分类这一更广泛问题集合。\n\n假设我们有了一个训练集（也就是一堆已经标好了是否为垃圾邮件的邮件）。要构建垃圾邮件分选器，咱们先要开始确定用来描述一封邮件的特征$x_i$有哪些。\n\n我们将用一个特征向量来表示一封邮件，这个向量的长度等于字典中单词的个数。如果邮件中包含了字典中的第 $i$ 个单词，那么就令 $x_i = 1$；反之则$x_i = 0$。例如下面这个向量：\n\n$$\nx=\\begin{bmatrix}1\\\\0\\\\0\\\\\\vdots \\\\1\\\\ \\vdots \\\\0\\end{bmatrix} \\begin{matrix}\\text{a}\\\\ \\text{aardvark}\\\\ \\text{aardwolf}\\\\ \\vdots\\\\ \\text{buy}\\\\ \\vdots\\\\ \\text{zygmurgy}\\\\ \\end{matrix}\n$$\n\n就用来表示一个邮件，其中包含了两个单词 \"a\" 和 \"buy\"，但没有单词 \"aardvark\"， \"aardwolf\" 或者 \"zymurgy\"  。这个单词集合编码整理成的特征向量也成为**词汇表（vocabulary,），** 所以特征向量 $x$ 的维度就等于词汇表的长度。\n\n> 注：实际应用中并不需要遍历整个英语词典来组成所有英语单词的列表，实践中更常用的方法是遍历一下训练集，然后把出现过一次以上的单词才编码成特征向量。这样做除了能够降低模型中单词表的长度之外，还能够降低运算量和空间占用，此外还有一个好处就是能够包含一些你的邮件中出现了而词典中没有的单词，比如本课程的缩写CS229。有时候（比如在作业里面），还要排除一些特别高频率的词汇，比如像冠词the，介词of 和and 等等；这些高频率但是没有具体意义的虚词也叫做stop words，因为很多文档中都要有这些词，用它们也基本不能用来判定一个邮件是否为垃圾邮件。\n\n选好了特征向量了，接下来就是建立一个生成模型（generative model）。所以我们必须对$p(x|y)$进行建模。但是，假如我们的单词有五万个词，则特征向量$x \\in  \\{0, 1\\}^{50000}$ （即 $x$是一个 $50000$ 维的向量，其值是$0$或者$1$），如果我们要对这样的 $x$进行多项式分布的建模，那么就可能有$2^{50000}$ 种可能的输出，然后就要用一个 $(2^{50000}-1)$维的参数向量。这样参数明显太多了。\n\n要给$p(x|y)$建模，先来做一个非常强的假设。我们**假设特征向量$x_i$ 对于给定的 $y$ 是独立的。** 这个假设也叫做**朴素贝叶斯假设（Naive Bayes ，NB assumption），** 基于此假设衍生的算法也就叫做**朴素贝叶斯分类器（Naive Bayes classifier）。** 例如，如果 $y = 1$ 意味着一个邮件是垃圾邮件；然后其中\"buy\" 是第$2087$个单词，而 \"price\"是第$39831$个单词；那么接下来我们就假设，如果我告诉你 $y = 1$，也就是说某一个特定的邮件是垃圾邮件，那么对于$x_{2087}$ （也就是单词 buy 是否出现在邮件里）的了解并不会影响你对$x_{39831}$ （单词price出现的位置）的采信值。更正规一点，可以写成 $p(x_{2087}|y) = p(x_{2087}|y, x_{39831})$。（要注意这个并不是说$x_{2087}$ 和 $x_{39831}$这两个特征是独立的，那样就变成了$p(x_{2087}) = p(x_{2087}|x_{39831})$，我们这里是说在给定了 $y$ 的这样一个条件下，二者才是有条件的独立。）\n\n然后我们就得到了等式：\n\n$$\n\\begin{aligned}\np(x_1, ..., x_{50000}|y) & = p(x_1|y)p(x_2|y,x_1)p(x_3|y,x_1,x_2) ... p(x_{50000}|y,x_1,x_2,...,x_{49999})\\\\\n& = p(x_1|y)p(x_2|y)p(x_3|y) ... p(x_{50000}|y)\\\\\n& = \\prod^n_{i=1}p(x_i|y)\\\\\n\\end{aligned}\n$$\n\n第一行的等式就是简单地来自概率的基本性质，第二个等式则使用了朴素贝叶斯假设。这里要注意，朴素贝叶斯假设也是一个很强的假设，产生的这个算法可以适用于很多种问题。\n\n我们这个模型的参数为 $\\phi_{i|y=1} = p (x_i = 1|y = 1), \\phi_{i|y=0} = p (x_i = 1|y = 0)$, 而 $\\phi_y = p (y = 1)$。和以往一样，给定一个训练集$\\{(x^{(i)},y^{(i)}); i = 1, ..., m\\}$，就可以写出下面的联合似然函数：\n\n$$\n\\mathcal{L}(\\phi_y,\\phi_{j|y=0},\\phi_{j|y=1})=\\prod^m_{i=1}p(x^{(i)},y^{(i)})\n$$\n\n找到使联合似然函数取得最大值的对应参数组合 $\\phi_y , \\phi_{i|y=0} 和 \\phi_{i|y=1}$ 就给出了最大似然估计：\n\n$$\n\\begin{aligned}\n\\phi_{j|y=1} &=\\frac{\\sum^m_{i=1}1\\{x_j^{(i)} =1 \\wedge y^{(i)} =1\\} }{\\sum^m_{i=1}1\\{y^{(i)} =1\\}} \\\\\n\\phi_{j|y=0} &= \\frac{\\sum^m_{i=1}1\\{x_j^{(i)} =1 \\wedge y^{(i)} =0\\} }{\\sum^m_{i=1}1\\{y^{(i)} =0\\}} \\\\\n\\phi_{y} &= \\frac{\\sum^m_{i=1}1\\{y^{(i)} =1\\}}{m}\\\\\n\\end{aligned}\n$$\n\n在上面的等式中，\"$\\wedge$(and)\"这个符号的意思是逻辑\"和\"。这些参数有一个非常自然的解释。例如 $\\phi_{j|y=1}$ 正是单词 $j$ 出现的邮件中垃圾邮件所占 $(y = 1)$ 的比例。\n\n拟合好了全部这些参数之后，要对一个新样本的特征向量 $x$ 进行预测，只要进行如下的简单地计算：\n\n$$\n\\begin{aligned}\np(y=1|x)&=  \\frac{p(x|y=1)p(y=1)}{p(x)}\\\\\n&= \\frac{(\\prod^n_{i=1}p(x_i|y=1))p(y=1)}{(\\prod^n_{i=1}p(x_i|y=1))p(y=1)+  (\\prod^n_{i=1}p(x_i|y=0))p(y=0)}  \\\\\n\\end{aligned}\n$$\n\n然后选择有最高后验概率的概率。\n\n最后我们要注意，刚刚我们对朴素贝叶斯算法的使用中，特征向量 $x_i$ 都是二值化的，其实特征向量也可以是多个离散值，比如$\\{1, 2, ..., k_i\\}$这样也都是可以的。这时候只需要把对$p(x_i|y)$ 的建模从伯努利分布改成多项式分布。实际上，即便一些原始的输入值是连续值（比如我们第一个案例中的房屋面积），也可以转换成一个小规模的离散值的集合，然后再使用朴素贝叶斯方法。例如，如果我们用特征向量 $x_i$ 来表示住房面积，那么就可以按照下面所示的方法来对这一变量进行离散化：\n\n|居住面积|$<400$|$400-800$|$800-1200$|$1200-1600$|$>1600$|\n|:-:|:-:|:-:|:-:|:-:|:-:|\n|离散值 $x_i$|$1$|$2$|$3$|$4$|$5$|\n\n这样，对于一个面积为 $890$ 平方英尺的房屋，就可以根据上面这个集合中对应的值来把特征向量的这一项的$x_i$值设置为$3$。然后就可以用朴素贝叶斯算法，并且将$p(x_i|y)$作为多项式分布来进行建模，就都跟前面讲过的内容一样了。当原生的连续值的属性不太容易用一个多元正态分布来进行建模的时候，将其特征向量离散化然后使用朴素贝叶斯法（NB）来替代高斯判别分析法（GDA），通常能形成一个更好的分类器。\n\n##### 2.1 拉普拉斯平滑（Laplace smoothing）\n\n刚刚讲过的朴素贝叶斯算法能够解决很多问题了，但还能对这种方法进行一点小调整来进一步提高效果，尤其是应对文本分类的情况。我们来简要讨论一下一个算法当前状态的一个问题，然后在讲一下如何解决这个问题。\n\n还是考虑垃圾邮件分类的过程，设想你学完了CS229的课程，然后做了很棒的研究项目，之后你决定在$2003$年$6$月`译者注：作者这讲义一定写得很早`把自己的作品投稿到NIPS会议，这个NIPS是机器学习领域的一个顶级会议，递交论文的截止日期一般是六月末到七月初。你通过邮件来对这个会议进行了讨论，然后你也开始收到带有 nips 四个字母的信息。但这个是你第一个NIPS论文，而在此之前，你从来没有接到过任何带有 nips 这个单词的邮件；尤其重要的是，nips 这个单词就从来都没有出现在你的垃圾/正常邮件训练集里面。加入这个 nips 是你字典中的第$35000$个单词那么你的朴素贝叶斯垃圾邮件筛选器就要对参数$\\phi_{35000|y}$ 进行最大似然估计，如下所示：\n\n$$\n\\begin{aligned}\n\\phi_{35000|y=1} &=  \\frac{\\sum^m_{i=1}1\\{x^{(i)}_{35000}=1 \\wedge y^{(i)}=1  \\}}{\\sum^m_{i=1}1\\{y^{(i)}=0\\}}  &=0 \\\\\n\\phi_{35000|y=0} &=  \\frac{\\sum^m_{i=1}1\\{x^{(i)}_{35000}=1 \\wedge y^{(i)}=0  \\}}{\\sum^m_{i=1}1\\{y^{(i)}=0\\}}  &=0 \\\\\n\\end{aligned}\n$$\n\n也就是说，因为之前程序从来没有在别的垃圾邮件或者正常邮件的训练样本中看到过 nips 这个词，所以它就认为看到这个词出现在这两种邮件中的概率都是$0$。因此当要决定一个包含 nips 这个单词的邮件是否为垃圾邮件的时候，他就检验这个类的后验概率，然后得到了：\n\n$$\n\\begin{aligned}\np(y=1|x) &= \\frac{ \\prod^n_{i=1} p(x_i|y=1)p(y=1) }   {\\prod^n_{i=1} p(x_i|y=1)p(y=1) +\\prod^n_{i=1} p(x_i|y=1)p(y=0)    }\\\\\n&= \\frac00\\\\\n\\end{aligned}\n$$\n\n这是因为对于\"  $\\prod^n_{i=1} p(x_i|y)$\"中包含了$p(x_{35000}|y) = 0$的都加了起来，也就还是$0$。所以我们的算法得到的就是 $\\frac00$，也就是不知道该做出怎么样的预测了。\n\n然后进一步拓展一下这个问题，统计学上来说，只因为你在自己以前的有限的训练数据集中没见到过一件事，就估计这个事件的概率为零，这明显不是个好主意。假设问题是估计一个多项式随机变量 $z$ ，其取值范围在$\\{1,..., k\\}$之内。接下来就可以用$\\phi_i = p (z = i)$ 来作为多项式参数。给定一个 $m$ 个独立观测$\\{z^{(1)}, ..., z^{(m)}\\}$ 组成的集合，然后最大似然估计的形式如下：\n\n$$\n\\phi_j=\\frac{\\sum^m_{i=1}1\\{z^{(i)}=j\\}}m\n$$\n\n正如咱们之前见到的，如果我们用这些最大似然估计，那么一些$\\phi_j$可能最终就是零了，这就是个问题了。要避免这个情况，咱们就可以引入**拉普拉斯平滑（Laplace smoothing），** 这种方法把上面的估计替换成：\n\n$$\n\\phi_j=\\frac{\\sum^m_{i=1}1\\{z^{(i)}=j\\}+1}{m+k}\n$$\n\n这里首先是对分子加$1$，然后对分母加$k$，要注意$\\sum^k_{j=1} \\phi_j = 1$依然成立（自己检验一下），这是一个必须有的性质，因为$\\phi_j$ 是对概率的估计，然后所有的概率加到一起必然等于$1$。另外对于所有的 $j$ 值，都有$\\phi_j \\neq 0$，这就解决了刚刚的概率估计为零的问题了。在某些特定的条件下（相当强的假设条件下，arguably quite strong），可以发现拉普拉斯平滑还真能给出对参数$\\phi_j$ 的最佳估计（optimal estimator）。\n\n回到我们的朴素贝叶斯分选器问题上，使用了拉普拉斯平滑之后，对参数的估计就写成了下面的形式：\n\n$$\n\\begin{aligned}\n\\phi_{j|y=1} & =\\frac{\\sum^m_{i=1}1\\{x_j^{(i)}=1\\wedge y ^{(i)}=1\\}+1}{\\sum^m_{i=1}1{\\{y^{(i)}=1\\}}+2}\\\\\n\\phi_{j|y=0} & =\\frac{\\sum^m_{i=1}1\\{x_j^{(i)}=1\\wedge y ^{(i)}=10\\}+1}{\\sum^m_{i=1}1{\\{y^{(i)}=0\\}}+2}\\\\\n\\end{aligned}\n$$\n\n（在实际应用中，通常是否对$\\phi_y$ 使用拉普拉斯并没有太大影响，因为通常我们会对每个垃圾邮件和非垃圾邮件都有一个合适的划分比例，所以$\\phi_y$ 会是对$p(y = 1)$ 的一个合理估计，无论如何都会与零点有一定距离。）\n\n##### 2.2 针对文本分类的事件模型（Event models for text classification）\n\n到这里就要给咱们关于生成学习算法的讨论进行一下收尾了，所以就接着讲一点关于文本分类方面的另一个模型。我们刚已经演示过的朴素贝叶斯方法能够解决很多分类问题了，不过还有另一个相关的算法，在针对文本的分类效果还要更好。\n\n在针对文本进行分类的特定背景下，咱们上面讲的朴素贝叶斯方法使用的是一种叫做**多元伯努利事件模型（Multi-Variate Bernoulli event model）。** 在这个模型里面，我们假设邮件发送的方式，是随机确定的（根据先验类*class priors*， $p(y)$），无论是不是垃圾邮件发送者，他是否给你发下一封邮件都是随机决定的。那么发件人就会遍历词典，决定在邮件中是否包含某个单词 $i$，各个单词之间互相独立，并且服从概率分布$p(x_i=1|y)=\\phi_{i|y}$。因此，一条消息的概率为：$p(y)\\prod^n_{i-1}p(x_i|y)$\n\n 然后还有另外一个模型，叫做**多项式事件模型（Multinomial event model）。** 要描述这个模型，我们需要使用一个不同的记号和特征集来表征各种邮件。设 $x_i$ 表示单词中的第$i$个单词。因此，$x_i$现在就是一个整数，取值范围为$\\{1,...,|V|\\}$，这里的$|V|$是词汇列表（即字典）的长度。这样一个有 $n$ 个单词的邮件就可以表征为一个长度为 $n$ 的向量$(x_1,x_2,...,x_n)$；这里要注意，不同的邮件内容，$n$ 的取值可以是不同的。例如，如果一个邮件的开头是\"A NIPS . . .\" ，那么$x_1 = 1$ (\"a\" 是词典中的第一个)，而$x_2 = 35000$ (这是假设 \"nips\"是词典中的第35000个)。\n\n在多项式事件模型中，我们假设邮件的生成是通过一个随机过程的，而是否为垃圾邮件是首先决定的（根据$p(y)$），这个和之前的模型假设一样。然后邮件的发送者写邮件首先是要生成  从对单词$(p(x_1|y))$ 的某种多项式分布中生成 $x_1$。然后第二步是独立于 $x_1$ 来生成 $x_2$，但也是从相同的多项式分布中来选取，然后是 $x_3$,$x_4$  等等，以此类推，直到生成了整个邮件中的所有的词。因此，一个邮件的总体概率就是$p(y)\\prod^n_{i=1}p(x_i|y)$。要注意这个方程看着和我们之前那个多元伯努利事件模型里面的邮件概率很相似，但实际上这里面的意义完全不同了。尤其是这里的$x_i|y$现在是一个多项式分布了，而不是伯努利分布了。\n\n我们新模型的参数还是$\\phi_y = p(y)$，这个跟以前一样，然后还有$\\phi_{k|y=1} = p(x_j =k|y=1)$ (对任何 $j$)以及 $\\phi_{i|y=0} =p(x_j =k|y=0)$。要注意这里我们已经假设了对于任何$j$ 的值，$p(x_j|y)$这个概率都是相等的，也就是意味着在这个哪个词汇生成的这个分布不依赖这个词在邮件中的位置$j$。\n\n如果给定一个训练集$\\{(x^{(i)},y^{(i)}); i = 1, ..., m\\}$，其中 $x^{(i)}  = ( x^{(i)}_{1} , x^{(i)}_{2} ,..., x^{(i)}_{n_i})$（这里的$n$是在第$i$个训练样本中的单词数目），那么这个数据的似然函数如下所示：\n\n$$\n\\begin{aligned}\n\\mathcal{L}(\\phi,\\phi_{k|y=0},\\phi_{k|y=1})& = \\prod^m_{i=1}p( x^{(i)},y^{(i)})\\\\\n& = \\prod^m_{i=1}(\\prod^{n_i}_{j=1}p(x_j^{(i)}|y;\\phi_{k|y=0},\\phi_{k|y=1}))p( y^{(i)};\\phi_y)\\\\\n\\end{aligned}\n$$\n\n让上面的这个函数最大化就可以产生对参数的最大似然估计：\n\n$$\n\\begin{aligned}\n\\phi_{k|y=1}&=  \\frac{\\sum^m_{i=1}\\sum^{n_i}_{j=1}1\\{x_j^{(i)}=k\\wedge y^{(i)}=1\\}}{\\sum^m_{i=1}1\\{y^{(i)}=1\\}n_i} \\\\\n\\phi_{k|y=0}&=  \\frac{\\sum^m_{i=1}\\sum^{n_i}_{j=1}1\\{x_j^{(i)}=k\\wedge y^{(i)}=0\\}}{\\sum^m_{i=1}1\\{y^{(i)}=0\\}n_i} \\\\\n\\phi_y&=   \\frac{\\sum^m_{i=1}1\\{y^{(i)}=1\\}}{m}\\\\\n\\end{aligned}\n$$\n\n如果使用拉普拉斯平滑（实践中会用这个方法来提高性能）来估计$\\phi_{k|y=0}$ 和 $\\phi_{k|y=1}$，就在分子上加1，然后分母上加$|V|$，就得到了下面的等式：\n\n$$\n\\begin{aligned}\n\\phi_{k|y=1}&=  \\frac{\\sum^m_{i=1}\\sum^{n_i}_{j=1}1\\{x_j^{(i)}=k\\wedge y^{(i)}=1\\}+1}{\\sum^m_{i=1}1\\{y^{(i)}=1\\}n_i+|V|} \\\\\n\\phi_{k|y=0}&=  \\frac{\\sum^m_{i=1}\\sum^{n_i}_{j=1}1\\{x_j^{(i)}=k\\wedge y^{(i)}=0\\}+1}{\\sum^m_{i=1}1\\{y^{(i)}=0\\}n_i+|V|} \\\\\n\\end{aligned}\n$$\n\n当然了，这个并不见得就是一个最好的分类算法，不过朴素贝叶斯分选器通常用起来还都出乎意料地那么好。所以这个方法就是一个很好的\"首发选择\"，因为它很简单又很好实现。\n\n\n### 参考","tags":["CS229"],"categories":["CS229"]},{"title":"[机器学习-CS229]线性回归,分类和逻辑回归和广义线性模型","url":"%2Fposts%2F62615419%2F","content":"# CS229 课程讲义中文翻译\n\n| 原作者 | 翻译 | 校对 |\n| --- | --- | --- |\n| [Andrew Ng  吴恩达](http://www.andrewng.org/) | [CycleUser](https://www.zhihu.com/people/cycleuser/columns) | [XiaoDong_Wang](https://github.com/Dongzhixiao) |\n\n\n|相关链接|\n|---|\n|[Github 地址](https://github.com/Kivy-CN/Stanford-CS-229-CN)|\n|[知乎专栏](https://zhuanlan.zhihu.com/MachineLearn)|\n|[斯坦福大学 CS229 课程网站](http://cs229.stanford.edu/)|\n|[网易公开课中文字幕视频](http://open.163.com/movie/2008/1/M/C/M6SGF6VB4_M6SGHFBMC.html)|\n\n\n# 第一章\n\n## 监督学习（Supervised learning）\n\n咱们先来聊几个使用监督学习来解决问题的实例。假如咱们有一个数据集，里面的数据是俄勒冈州波特兰市的 $47$ 套房屋的面积和价格：\n\n|居住面积（平方英尺）|价格（千美元）|\n|:-:|:-:|\n|$2104$|$400$|\n|$1600$|$330$|\n|$2400$|$369$|\n|$1416$|$232$|\n|$3000$|$540$|\n|$\\vdots$ |$\\vdots$ |\n\n用这些数据来投个图：\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note1f1.png)\n\n有了这样的数据，我们怎样才能学会预测波特兰其他房屋的价格，以及它们的居住面积？\n\n这里要先规范一下符号和含义，这些符号以后还要用到，咱们假设 $x^{(i)}$ 表示 “输入的” 变量值（在这个例子中就是房屋面积），也可以叫做**输入特征**；然后咱们用 $y^{(i)}$ 来表示“输出值”，或者称之为**目标变量**，这个例子里面就是房屋价格。这样的一对 $(x^{(i)},y^{(i)})$就称为一组训练样本，然后咱们用来让机器来学习的数据集，就是——一个长度为 $m$ 的训练样本的列表$\\{(x^{(i)},y^{(i)}); i = 1,\\dots ,m\\}$——也叫做一个**训练集**。另外一定注意，这里的上标$(i)$只是作为训练集的索引记号，和数学乘方没有任何关系，千万别误解了。另外我们还会用大写的$X$来表示 **输入值的空间**，大写的$Y$表示**输出值的空间**。在本节的这个例子中，输入输出的空间都是实数域，所以 $X = Y = R$。\n\n然后再用更加规范的方式来描述一下监督学习问题，我们的目标是，给定一个训练集，来让机器学习一个函数 $h: X → Y$，让 $h(x)$ 是一个与对应的真实 $y$ 值比较接近的评估值。由于一些历史上的原因，这个函数 $h$ 就被叫做**假设（hypothesis）**。用一个图来表示的话，这个过程大概就是下面这样：\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note1f2.png)\n\n如果我们要预测的目标变量是连续的，比如在咱们这个房屋价格-面积的案例中，这种学习问题就被称为**回归问题。** 如果$y$只能取一小部分的离散的值（比如给定房屋面积，咱们要来确定这个房子是一个住宅还是公寓），这样的问题就叫做**分类问题。**\n\n\n### 第一部分 线性回归\n\n为了让我们的房屋案例更有意思，咱们稍微对数据集进行一下补充，增加上每一个房屋的卧室数目：\n\n|居住面积（平方英尺）|卧室数目|价格（千美元）|\n|:-:|:-:|:-:|\n|$2104$|$3$|$400$|\n|$1600$|$3$|$330$|\n|$2400$|$3$|$369$|\n|$1416$|$2$|$232$|\n|$3000$|$4$|$540$|\n|$\\vdots$ |$\\vdots$ |$\\vdots$ |\n\n现在，输入特征 $x$ 就是在 $R^2$ 范围取值的一个二维向量了。例如 $x_1^{(i)}$ 就是训练集中第 $i$ 个房屋的面积，而 $x_2^{(i)}$  就是训练集中第 $i$ 个房屋的卧室数目。（通常来说，设计一个学习算法的时候，选择哪些输入特征都取决于你，所以如果你不在波特兰收集房屋信息数据，你也完全可以选择包含其他的特征，例如房屋是否有壁炉，卫生间的数量啊等等。关于特征筛选的内容会在后面的章节进行更详细的介绍，不过目前来说就暂时先用给定的这两个特征了。）\n\n要进行这个监督学习，咱们必须得确定好如何在计算机里面对这个**函数/假设** $h$ 进行表示。咱们现在刚刚开始，就来个简单点的，咱们把 $y$ 假设为一个以 $x$ 为变量的线性函数：\n\n$$ \nh_\\theta  (x) = \\theta_0 + \\theta_1 \\times x_1 + \\theta_2 \\times x_2\n$$\n\n这里的$\\theta_i$是**参数**（也可以叫做**权重**），是从 $x$ 到 $y$ 的线性函数映射的空间参数。在不至于引起混淆的情况下，咱们可以把$h_\\theta(x)$ 里面的 $\\theta$  省略掉，就简写成 $h(x)$。另外为了简化公式，咱们还设 $x_0 = 1$（这个为 **截距项 intercept term**）。这样简化之后就有了：\n\n$$ \nh(x) = \\sum^n_{i=0}  \\theta_i x_i = \\theta^T x\n$$\n\n等式最右边的 $\\theta$ 和 $x$ 都是向量，等式中的 $n$ 是输入变量的个数（不包括$x_0$）。\n\n现在，给定了一个训练集，咱们怎么来挑选/学习参数 $\\theta$ 呢？一个看上去比较合理的方法就是让 $h(x)$ 尽量逼近 $y$，至少对咱已有的训练样本能适用。用公式的方式来表示的话，就要定义一个函数，来衡量对于每个不同的 $\\theta$ 值，$h(x^{(i)})$ 与对应的 $y^{(i)}$ 的距离。这样用如下的方式定义了一个 **成本函数 （cost function**）:\n\n$$ \nJ(\\theta) = \\frac 12 \\sum^m_{i=1}(h_\\theta(x^{(i)})-y^{(i)})^2  \n$$\n\n如果之前你接触过线性回归，你会发现这个函数和**常规最小二乘法** 拟合模型中的最小二乘法成本函数非常相似。不管之前接触过没有，咱们都接着往下进行，以后就会发现这是一个更广泛的算法家族中的一个特例。\n\n#### 1 最小均方算法（LMS algorithm）\n\n我们希望选择一个能让 $J(\\theta)$ 最小的 $\\theta$ 值。怎么做呢，咱们先用一个搜索的算法，从某一个对 $\\theta$ 的“初始猜测值”，然后对 $\\theta$ 值不断进行调整，来让 $J(\\theta)$ 逐渐变小，最好是直到我们能够达到一个使 $J(\\theta)$ 最小的 $\\theta$。具体来说，咱们可以考虑使用梯度下降法（gradient descent algorithm），这个方法就是从某一个 $\\theta$ 的初始值开始，然后逐渐重复更新：$^1$\n\n$$ \n\\theta_j := \\theta_j - \\alpha \\frac \\partial {\\partial\\theta_j}J(\\theta)\n$$\n\n（上面的这个更新要同时对应从 $0$ 到 $n$ 的所有$j$ 值进行。）这里的 $\\alpha$ 也称为学习速率。这个算法是很自然的，逐步重复朝向 $J$ 降低最快的方向移动。\n\n>1 本文中 $:= $ 表示的是计算机程序中的一种赋值操作，是把等号右边的计算结果赋值给左边的变量，$a := b$ 就表示用 $b$ 的值覆盖原有的$a$值。要注意区分，如果写的是 $a == b$ 则表示的是判断二者相等的关系。（译者注：在 Python 中，单个等号 $=$ 就是赋值，两个等号 $==$  表示相等关系的判断。）\n\n要实现这个算法，咱们需要解决等号右边的导数项。首先来解决只有一组训练样本 $(x, y)$ 的情况，这样就可以忽略掉等号右边对 $J$ 的求和项目了。公式就简化下面这样：\n\n$$\n\\begin{aligned}\n\\frac \\partial {\\partial\\theta_j}J(\\theta) & = \\frac \\partial {\\partial\\theta_j} \\frac  12(h_\\theta(x)-y)^2\\\\\n& = 2 \\cdot\\frac 12(h_\\theta(x)-y)\\cdot \\frac \\partial {\\partial\\theta_j}  (h_\\theta(x)-y) \\\\\n& = (h_\\theta(x)-y)\\cdot \\frac \\partial {\\partial\\theta_j}(\\sum^n_{i=0} \\theta_ix_i-y) \\\\\n& = (h_\\theta(x)-y) x_j\n\\end{aligned}\n$$\n\n对单个训练样本，更新规则如下所示：\n\n$$ \n\\theta_j := \\theta_j + \\alpha (y^{(i)}-h_\\theta (x^{(i)}))x_j^{(i)}\n$$\n\n这个规则也叫 **LMS** 更新规则 （LMS 是 “least mean squares” 的缩写，意思是最小均方），也被称为 **Widrow-Hoff** 学习规则。这个规则有几个看上去就很自然直观的特性。例如，更新的大小与$(y^{(i)} − h_\\theta(x^{(i)}))$成正比；另外，当我们遇到训练样本的预测值与 $y^{(i)}$ 的真实值非常接近的情况下，就会发现基本没必要再对参数进行修改了；与此相反的情况是，如果我们的预测值 $h_\\theta(x^{(i)})$ 与 $y^{(i)}$ 的真实值有很大的误差（比如距离特别远），那就需要对参数进行更大地调整。\n\n当只有一个训练样本的时候，我们推导出了 LMS 规则。当一个训练集有超过一个训练样本的时候，有两种对这个规则的修改方法。第一种就是下面这个算法：\n\n$$\n\\begin{aligned}\n&\\qquad \\text{repeat untile converge} \\{ \\\\\n&\\qquad\\qquad\\theta_j := \\theta_j + \\alpha \\sum^m_{i=1}(y^{(i)}-h_\\theta (x^{(i)}))x_j^{(i)}\\quad(\\text{for every j}) \\\\\n&\\qquad\\}\n\\end{aligned}\n$$\n\n读者很容易能证明，在上面这个更新规则中求和项的值就是$\\frac {\\partial J(\\theta)}{\\partial \\theta_j}$ （这是因为对 $J$ 的原始定义）。所以这个更新规则实际上就是对原始的成本函数 $J $进行简单的梯度下降。这一方法在每一个步长内检查所有整个训练集中的所有样本，也叫做**批量梯度下降法（batch gradient descent**）。这里要注意，因为梯度下降法容易被局部最小值影响，而我们要解决的这个线性回归的优化问题只能有一个全局的而不是局部的最优解；因此，梯度下降法应该总是收敛到全局最小值（假设学习速率 $\\alpha$ 不设置的过大）。$J$ 很明确是一个凸二次函数。下面是一个样例，其中对一个二次函数使用了梯度下降法来找到最小值。\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note1f3.png)\n\n上图的椭圆就是一个二次函数的轮廓图。图中还有梯度下降法生成的规矩，初始点位置在$(48,30)$。图中的画的 $x$ （用直线连接起来了）标记了梯度下降法所经过的 $\\theta$ 的可用值。\n\n对咱们之前的房屋数据集进行批量梯度下降来拟合 $\\theta$ ，把房屋价格当作房屋面积的函数来进行预测，我们得到的结果是 $\\theta_0 = 71.27, \\theta_1 = 0.1345$。如果把 $h_{\\theta}(x)$ 作为一个定义域在 $x$ 上的函数来投影，同时也投上训练集中的已有数据点，会得到下面这幅图：\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note1f4.png)\n\n如果在数据集中添加上卧室数目作为输入特征，那么得到的结果就是 $\\theta_0 = 89.60, \\theta_1 = 0.1392, \\theta_2 = −8.738$\n\n这个结果就是用批量梯度下降法来获得的。此外还有另外一种方法能够替代批量梯度下降法，这种方法效果也不错。如下所示：\n\n$$\n\\begin{aligned}\n&\\qquad \\text{loop}：\\{ \\\\\n&\\qquad\\qquad \\text{i from 1 to m},\\{   \\\\\n&\\qquad\\qquad\\qquad\\theta_j := \\theta_j  +\\alpha(y^{(i)}-h_{\\theta}(x^{(i)}))x_j^{(i)} \\qquad(\\text{for every j}) \\\\\n&\\qquad\\qquad\\}  \\\\\n&\\qquad\\}\n\\end{aligned}\n$$\n\n在这个算法里，我们对整个训练集进行了循环遍历，每次遇到一个训练样本，根据每个单一训练样本的误差梯度来对参数进行更新。这个算法叫做**随机梯度下降法（stochastic gradient descent）**，或者叫**增量梯度下降法（incremental gradient descent）**。批量梯度下降法要在运行第一步之前先对整个训练集进行扫描遍历，当训练集的规模 $m$ 变得很大的时候，引起的性能开销就很不划算了；随机梯度下降法就没有这个问题，而是可以立即开始，对查询到的每个样本都进行运算。通常情况下，随机梯度下降法查找到足够接近最低值的 $\\theta$ 的速度要比批量梯度下降法更快一些。（也要注意，也有可能会一直无法收敛（converge）到最小值，这时候 $\\theta$ 会一直在 $J(\\theta)$ 最小值附近震荡；不过通常情况下在最小值附近的这些值大多数其实也足够逼近了，足以满足咱们的精度要求，所以也可以用。$^2$）由于这些原因，特别是在训练集很大的情况下，随机梯度下降往往比批量梯度下降更受青睐。\n\n>2 当然更常见的情况通常是我们事先对数据集已经有了描述，并且有了一个确定的学习速率$\\alpha$，然后来运行随机梯度下降，同时逐渐让学习速率 $\\alpha$ 随着算法的运行而逐渐趋于 $0$，这样也能保证我们最后得到的参数会收敛到最小值，而不是在最小值范围进行震荡。（译者注：由于以上种种原因，通常更推荐使用的都是随机梯度下降法，而不是批量梯度下降法，尤其是在训练用的数据集规模大的时候。）\n\n\n#### 2 法方程（The normal equations）\n\n上文中的梯度下降法是一种找出 $J$ 最小值的办法。然后咱们聊一聊另一种实现方法，这种方法寻找起来简单明了，而且不需要使用迭代算法。这种方法就是，我们直接利用找对应导数为 $0$ 位置的 $\\theta_j$，这样就能找到 $J$ 的最小值了。我们想实现这个目的，还不想写一大堆代数公式或者好几页的矩阵积分，所以就要介绍一些做矩阵积分的记号。\n\n\n##### 2.1 矩阵导数（Matrix derivatives）\n\n假如有一个函数 $f: R^{m\\times n} → R$ 从 $m\\times n$ 大小的矩阵映射到实数域，那么就可以定义当矩阵为 $A$ 的时候有导函数 $f$ 如下所示：\n\n$$ \n\\nabla_A f(A)=\\begin{bmatrix} \\frac {\\partial f}{\\partial A_{11}} & \\dots  & \\frac {\\partial f}{\\partial A_{1n}} \\\\ \\vdots  & \\ddots & \\vdots  \\\\ \\frac {\\partial f}{\\partial A_{m1}} & \\dots  & \\frac {\\partial f}{\\partial A_{mn}} \\\\ \\end{bmatrix}\n$$\n\n因此，这个梯度 $\\nabla_A f(A)$本身也是一个 $m\\times n$ 的矩阵，其中的第 $(i,j)$ 个元素是 $\\frac {\\partial f}{\\partial A_{ij}} $ 。\n假如 $ A =\\begin{bmatrix} A_{11} & A_{12} \\\\ A_{21} & A_{22} \\\\ \\end{bmatrix} $ 是一个 $2\\times 2$  矩阵，然后给定的函数 $f:R^{2\\times 2} → R$ 为:\n\n$$ \nf(A) = \\frac 32A_{11}+5A^2_{12}+A_{21}A_{22}\n$$\n\n这里面的 $A_{ij}$ 表示的意思是矩阵 $A$ 的第 $(i,j)$ 个元素。然后就有了梯度：\n\n$$ \n\\nabla _A f(A) =\\begin{bmatrix} \\frac  32 & 10A_{12} \\\\ A_{22} & A_{21} \\\\ \\end{bmatrix} \n$$\n\n然后咱们还要引入 **$trace$** 求迹运算，简写为 $tr$。对于一个给定的 $n\\times n$ 方形矩阵 $A$，它的迹定义为对角项和：\n\n$$ \ntrA = \\sum^n_{i=1} A_{ii}\n$$\n\n假如 $a$ 是一个实数，实际上 $a$ 就可以看做是一个 $1\\times 1$ 的矩阵，那么就有 $a$ 的迹 $tr a = a$。(如果你之前没有见到过这个“运算记号”，就可以把 $A$ 的迹看成是 $tr(A)$，或者理解成为一个对矩阵 $A$ 进行操作的 $trace$ 函数。不过通常情况都是写成不带括号的形式更多一些。) \n\n如果有两个矩阵 $A$ 和$B$，能够满足 $AB$ 为方阵，$trace$ 求迹运算就有一个特殊的性质： $trAB = trBA$ (自己想办法证明)。在此基础上进行推论，就能得到类似下面这样的等式关系：\n\n$$\ntrABC=trCAB=trBCA \\\\\ntrABCD=trDABC=trCDAB=trBCDA\n$$\n\n下面这些和求迹运算相关的等量关系也很容易证明。其中 $A$ 和 $B$ 都是方形矩阵，$a$ 是一个实数：\n\n$$ \ntrA=trA^T \\\\\ntr(A+B)=trA+trB \\\\\ntr a A=a trA\n$$\n\n接下来咱们就来在不进行证明的情况下提出一些矩阵导数（其中的一些直到本节末尾才用得上）。另外要注意等式$(4)$中的$A$ 必须是**非奇异方阵（non-singular square matrices**），而 $|A|$ 表示的是矩阵 $A$ 的行列式。那么我们就有下面这些等量关系：\n\n$$\n\\begin{aligned}\n   \\nabla_A tr AB & = B^T & \\text{(1)}\\\\\n   \\nabla_{A^T} f(A) & = (\\nabla_{A} f(A))^T &\\text{(2)}\\\\\n   \\nabla_A tr ABA^TC& = CAB+C^TAB^T &\\text{(3)}\\\\\n   \\nabla_A|A| & = |A|(A^{-1})^T &\\text{(4)}\\\\\n\\end{aligned}\n$$\n\n为了让咱们的矩阵运算记号更加具体，咱们就详细解释一下这些等式中的第一个。假如我们有一个确定的矩阵 $B \\in R^{n\\times m}$（注意顺序，是$n\\times m$，这里的意思也就是 $B$ 的元素都是实数，$B$ 的形状是 $n\\times m$ 的一个矩阵），那么接下来就可以定义一个函数$f: R^{m\\times n} → R$ ，对应这里的就是 $f(A) = trAB$。这里要注意，这个矩阵是有意义的，因为如果 $A \\in R^{m\\times n}$，那么 $AB$ 就是一个方阵，是方阵就可以应用 $trace$ 求迹运算；因此，实际上 $f$ 映射的是从 $R^{m\\times n}$ 到实数域 $R$。这样接下来就可以使用矩阵导数来找到 $\\nabla_Af(A)$ ，这个导函数本身也是一个 $m \\times n$的矩阵。上面的等式 $(1)$ 表明了这个导数矩阵的第 $(i,j)$个元素等同于 $B^T$ （$B$的转置）的第 $(i,j)$ 个元素，或者更直接表示成 $B_{ji}$。\n\n上面等式$(1-3)$ 都很简单，证明就都留给读者做练习了。等式$(4)$需要用逆矩阵的伴随矩阵来推导出。$^3$\n\n>3 假如咱们定义一个矩阵 $A'$，它的第 $(i,j)$ 个元素是$(−1)^{i+j}$ 与矩阵 $A$ 移除 第 $i$ 行 和 第 $j$ 列 之后的行列式的乘积，则可以证明有$A^{−1} = (A')^T /|A|$。（你可以检查一下，比如在 $A$ 是一个 $2\\times 2$ 矩阵的情况下看看 $A^{-1}$ 是什么样的，然后以此类推。如果你想看看对于这一类结果的证明，可以参考一本中级或者高级的线性代数教材，比如Charles Curtis, 1991, Linear Algebra, Springer。）这也就意味着 $A' = |A|(A^{−1})^T $。此外，一个矩阵 $A$ 的行列式也可以写成 $|A| = \\sum_j A_{ij}A'_{ij}$ 。因为 $(A')_{ij}$ 不依赖 $A_{ij}$ （通过定义也能看出来），这也就意味着$(\\frac  \\partial {\\partial A_{ij}})|A| = A'_{ij} $，综合起来也就得到上面的这个结果了。\n\n##### 2.2 最小二乘法回顾（Least squares revisited）\n\n通过刚才的内容，咱们大概掌握了矩阵导数这一工具，接下来咱们就继续用逼近模型（closed-form）来找到能让 $J(\\theta)$ 最小的 $\\theta$ 值。首先咱们把 $J$ 用矩阵-向量的记号来重新表述。\n\n给定一个训练集，把**设计矩阵（design matrix）** $x$ 设置为一个 $m\\times n$ 矩阵（实际上，如果考虑到截距项，也就是 $\\theta_0$ 那一项，就应该是 $m\\times (n+1)$ 矩阵），这个矩阵里面包含了训练样本的输入值作为每一行：\n\n$$ \nX =\\begin{bmatrix}\n-(x^{(1)}) ^T-\\\\\n-(x^{(2)}) ^T-\\\\\n\\vdots \\\\\n-(x^{(m)}) ^T-\\\\\n\\end{bmatrix} \n$$\n\n然后，咱们设 $\\vec{y}$ 是一个 $m$ 维向量（m-dimensional vector），其中包含了训练集中的所有目标值：\n\n$$ \ny =\\begin{bmatrix}\ny^{(1)}\\\\\ny^{(2)}\\\\\n\\vdots \\\\\ny^{(m)}\\\\\n\\end{bmatrix} \n$$\n\n因为 $h_\\theta (x^{(i)}) = (x^{(i)})^T\\theta$`译者注：这个怎么推出来的我目前还没尝试，目测不难`，所以可以证明存在下面这种等量关系：\n\n$$\n\\begin{aligned}\nX\\theta - \\vec{y}  &=\n\\begin{bmatrix}\n(x^{(1)})^T\\theta \\\\\n\\vdots \\\\\n(x^{(m)})^T\\theta\\\\\n\\end{bmatrix} -\n\\begin{bmatrix}\ny^{(1)}\\\\\n\\vdots \\\\\ny^{(m)}\\\\\n\\end{bmatrix}\\\\\n& =\n\\begin{bmatrix}\nh_\\theta (x^{1}) -y^{(1)}\\\\\n\\vdots \\\\\nh_\\theta (x^{m})-y^{(m)}\\\\\n\\end{bmatrix}\\\\\n\\end{aligned}\n$$\n\n对于向量 $\\vec{z}$ ，则有 $z^T z = \\sum_i z_i^2$ ，因此利用这个性质，可以推出:\n\n$$\n\\begin{aligned}\n\\frac 12(X\\theta - \\vec{y})^T (X\\theta - \\vec{y}) &=\\frac 12 \\sum^m_{i=1}(h_\\theta (x^{(i)})-y^{(i)})^2\\\\\n&= J(\\theta)\n\\end{aligned}\n$$\n\n最后，要让 $J$ 的值最小，就要找到函数对于$\\theta$导数。结合等式$(2)$和等式$(3)$，就能得到下面这个等式$(5)$：\n\n$$ \n\\nabla_{A^T} trABA^TC =B^TA^TC^T+BA^TC \\qquad \\text{(5)}\n$$\n\n因此就有：\n\n$$\n\\begin{aligned}\n\\nabla_\\theta J(\\theta) &= \\nabla_\\theta \\frac 12 (X\\theta - \\vec{y})^T (X\\theta - \\vec{y}) \\\\\n&= \\frac  12 \\nabla_\\theta (\\theta ^TX^TX\\theta -\\theta^T X^T \\vec{y} - \\vec{y} ^TX\\theta +\\vec{y}^T \\vec{y})\\\\\n&= \\frac  12 \\nabla_\\theta tr(\\theta ^TX^TX\\theta -\\theta^T X^T \\vec{y} - \\vec{y} ^TX\\theta +\\vec{y}^T \\vec{y})\\\\\n&= \\frac  12 \\nabla_\\theta (tr \\theta ^TX^TX\\theta - 2tr\\vec{y} ^T X\\theta)\\\\\n&= \\frac  12 (X^TX\\theta+X^TX\\theta-2X^T\\vec{y}) \\\\\n&= X^TX\\theta-X^T\\vec{y}\\\\\n\\end{aligned}\n$$\n\n在第三步，我们用到了一个定理，也就是一个实数的迹就是这个实数本身；第四步用到了 $trA = trA^T$ 这个定理；第五步用到了等式$(5)$，其中 $A^T =\\theta, B=B^T =X^TX, C=I$,还用到了等式 $(1)$。要让 $J$ 取得最小值，就设导数为 $0$ ，然后就得到了下面的**法线方程（normal equations）：**\n\n$$ \nX^TX\\theta =X^T\\vec{y}\n$$\n\n所以让 $J(\\theta)$ 取值最小的 $\\theta$ 就是\n\n$$\n\\theta = (X^TX)^{-1}X^T\\vec{y}\n$$\n\n#### 3 概率解释（Probabilistic interpretation）\n\n在面对回归问题的时候，可能有这样一些疑问，就是为什么选择线性回归，尤其是为什么选择最小二乘法作为成本函数 $J$ ？在本节里，我们会给出一系列的概率基本假设，基于这些假设，就可以推出最小二乘法回归是一种非常自然的算法。\n\n首先咱们假设目标变量和输入值存在下面这种等量关系：\n\n$$ \ny^{(i)}=\\theta^T x^{(i)}+ \\epsilon ^{(i)}\n$$\n\n上式中 $ \\epsilon ^{(i)}$ 是误差项，用于存放由于建模所忽略的变量导致的效果 （比如可能某些特征对于房价的影响很明显，但我们做回归的时候忽略掉了）或者随机的噪音信息（random noise）。进一步假设 $ \\epsilon ^{(i)}$   是独立同分布的 (IID ，independently and identically distributed) ，服从高斯分布（Gaussian distribution ，也叫正态分布 Normal distribution），其平均值为 $0$，方差（variance）为 $\\sigma ^2$。这样就可以把这个假设写成 $ \\epsilon ^{(i)} ∼ N (0, \\sigma ^2)$ 。然后 $ \\epsilon ^{(i)} $  的密度函数就是：\n\n$$ \np(\\epsilon ^{(i)} )= \\frac 1{\\sqrt{2\\pi}\\sigma} exp (- \\frac  {(\\epsilon ^{(i)} )^2}{2\\sigma^2})\n$$\n\n这意味着存在下面的等量关系：\n\n$$ \np(y ^{(i)} |x^{(i)}; \\theta)= \\frac 1{\\sqrt{2\\pi}\\sigma} exp (- \\frac  {(y^{(i)} -\\theta^T x ^{(i)} )^2}{2\\sigma^2})\n$$\n\n这里的记号 $p(y ^{(i)} |x^{(i)}; \\theta)$ 表示的是这是一个对于给定 $x^{(i)}$ 时 $y^{(i)}$ 的分布，用$\\theta$ 代表该分布的参数。 注意这里不能用 $\\theta(p(y ^{(i)} |x^{(i)},\\theta))$来当做条件，因为 $\\theta$ 并不是一个随机变量。这个 $y^{(i)}$  的分布还可以写成$y^{(i)} | x^{(i)}; \\theta ∼ N (\\theta ^T x^{(i)}, \\sigma^2)$。\n\n给定一个设计矩阵（design matrix）$X$，其包含了所有的$x^{(i)}$，然后再给定 $\\theta$，那么 $y^{(i)}$ 的分布是什么？数据的概率以$p (\\vec{y}|X;\\theta )$ 的形式给出。在$\\theta$取某个固定值的情况下，这个等式通常可以看做是一个 $\\vec{y}$ 的函数（也可以看成是 $X$ 的函数）。当我们要把它当做 $\\theta$ 的函数的时候，就称它为 **似然**函数（likelihood function)\n\n$$\nL(\\theta) =L(\\theta;X,\\vec{y})=p(\\vec{y}|X;\\theta)\n$$\n\n结合之前对 $\\epsilon^{(i)}$ 的独立性假设 （这里对$y^{(i)}$ 以及给定的 $x^{(i)}$ 也都做同样假设），就可以把上面这个等式改写成下面的形式：\n\n$$\n\\begin{aligned}\nL(\\theta) &=\\prod ^m _{i=1}p(y^{(i)}|x^{(i)};\\theta)\\\\\n&=\\prod ^m _{i=1} \\frac  1{\\sqrt{2\\pi}\\sigma} exp(- \\frac {(y^{(i)}-\\theta^T x^{(i)})^2}{2\\sigma^2})\\\\\n\\end{aligned}\n$$\n\n现在，给定了$y^{(i)}$ 和 $x^{(i)}$之间关系的概率模型了，用什么方法来选择咱们对参数 $\\theta$ 的最佳猜测呢？最大似然法（maximum likelihood）告诉我们要选择能让数据的似然函数尽可能大的 $\\theta$。也就是说，咱们要找的 $\\theta$ 能够让函数 $L(\\theta)$ 取到最大值。\n\n除了找到 $L(\\theta)$ 最大值，我们还以对任何严格递增的 $L(\\theta)$ 的函数求最大值。如果我们不直接使用 $L(\\theta)$，而是使用对数函数，来找**对数似然函数 $l(\\theta)$** 的最大值，那这样对于求导来说就简单了一些：\n\n$$\n\\begin{aligned}\nl(\\theta) &=\\log L(\\theta)\\\\\n&=\\log \\prod ^m _{i=1} \\frac  1{\\sqrt{2\\pi}\\sigma} exp(- \\frac {(y^{(i)}-\\theta^T x^{(i)})^2}{2\\sigma^2})\\\\\n&= \\sum ^m _{i=1}log \\frac  1{\\sqrt{2\\pi}\\sigma} exp(- \\frac {(y^{(i)}-\\theta^T x^{(i)})^2}{2\\sigma^2})\\\\\n&= m \\log \\frac  1{\\sqrt{2\\pi}\\sigma}- \\frac 1{\\sigma^2}\\cdot \\frac 12 \\sum^m_{i=1} (y^{(i)}-\\theta^Tx^{(i)})^2\\\\\n\\end{aligned}\n$$\n\n因此，对 $l(\\theta)$ 取得最大值也就意味着下面这个子式取到最小值：\n\n$$ \n\\frac 12 \\sum^m _{i=1} (y^{(i)}-\\theta^Tx^{(i)})^2\n$$\n\n到这里我们能发现这个子式实际上就是 $J(\\theta)$，也就是最原始的最小二乘成本函数（least-squares cost function）。\n\n总结一下也就是：在对数据进行概率假设的基础上，最小二乘回归得到的 $\\theta$ 和最大似然法估计的 $\\theta$ 是一致的。所以这是一系列的假设，其前提是认为最小二乘回归（least-squares regression）能够被判定为一种非常自然的方法，这种方法正好就进行了最大似然估计（maximum likelihood estimation）。（要注意，对于验证最小二乘法是否为一个良好并且合理的过程来说，这些概率假设并不是必须的，此外可能（也确实）有其他的自然假设能够用来评判最小二乘方法。）\n\n另外还要注意，在刚才的讨论中，**我们最终对 $\\theta$ 的选择并不依赖 $\\sigma^2$，而且也确实在不知道 $\\sigma^2$ 的情况下就已经找到了结果。** 稍后我们还要对这个情况加以利用，到时候我们会讨论指数族以及广义线性模型。\n\n#### 4 局部加权线性回归（Locally weighted linear regression）\n\n假如问题还是根据从实数域内取值的 $x\\in R$ 来预测 $y$ 。左下角的图显示了使用 $y = \\theta_0 + \\theta_1x$ 来对一个数据集进行拟合。我们明显能看出来这个数据的趋势并不是一条严格的直线，所以用直线进行的拟合就不是好的方法。\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note1f5.png)\n\n那么这次不用直线，而增加一个二次项，用$y = \\theta_0 + \\theta_1x +\\theta_2x^2$ 来拟合。（看中间的图） 很明显，我们对特征补充得越多，效果就越好。不过，增加太多特征也会造成危险的：最右边的图就是使用了五次多项式 $y = \\sum^5_{j=0} \\theta_jx^j$ 来进行拟合。看图就能发现，虽然这个拟合曲线完美地通过了所有当前数据集中的数据，但我们明显不能认为这个曲线是一个合适的预测工具，比如针对不同的居住面积 $x$ 来预测房屋价格 $y$。先不说这些特殊名词的正规定义，咱们就简单说，最左边的图像就是一个**欠拟合(under fitting)** 的例子，比如明显能看出拟合的模型漏掉了数据集中的结构信息；而最右边的图像就是一个**过拟合(over fitting)** 的例子。（在本课程的后续部分中，当我们讨论到关于学习理论的时候，会给出这些概念的标准定义，也会给出拟合程度对于一个猜测的好坏检验的意义。）\n\n正如前文谈到的，也正如上面这个例子展示的，一个学习算法要保证能良好运行，特征的选择是非常重要的。（等到我们讲模型选择的时候，还会看到一些算法能够自动来选择一个良好的特征集。）在本节，咱们就简要地讲一下局部加权线性回归（locally weighted linear regression ，缩写为LWR），这个方法是假设有足够多的训练数据，对不太重要的特征进行一些筛选。这部分内容会比较简略，因为在作业中要求学生自己去探索一下LWR 算法的各种性质了。\n\n在原始版本的线性回归算法中，要对一个查询点 $x$ 进行预测，比如要衡量$h(x)$，要经过下面的步骤：\n\n1. 使用参数 $\\theta$ 进行拟合，让数据集中的值与拟合算出的值的差值平方$\\sum_i(y^{(i)} − \\theta^T x^{(i)} )^2$最小(最小二乘法的思想)；\n2. 输出 $\\theta^T x$ 。\n\n相应地，在 LWR 局部加权线性回归方法中，步骤如下：\n\n1. 使用参数 $\\theta$ 进行拟合，让加权距离$\\sum_i w^{(i)}(y^{(i)} − \\theta^T x^{(i)} )^2$ 最小；\n2. 输出 $\\theta^T x$。\n\n\n上面式子中的 $w^{(i)}$ 是非负的权值。直观点说就是，如果对应某个$i$ 的权值 $w^{(i)}$ 特别大，那么在选择拟合参数 $\\theta$ 的时候，就要尽量让这一点的 $(y^{(i)} − \\theta^T x^{(i)} )^2$ 最小。而如果权值$w^{(i)}$  特别小，那么这一点对应的$(y^{(i)} − \\theta^T x^{(i)} )^2$ 就基本在拟合过程中忽略掉了。\n\n对于权值的选取可以使用下面这个比较标准的公式：$^4$\n\n$$\nw^{(i)} = exp(- \\frac {(x^{(i)}-x)^2}{2\\tau^2})\n$$\n\n>4 如果 $x$ 是有值的向量，那就要对上面的式子进行泛化，得到的是$w^{(i)} = exp(− \\frac {(x^{(i)}-x)^T(x^{(i)}-x)}{2\\tau^2})$，或者:$w^{(i)} = exp(− \\frac {(x^{(i)}-x)^T\\Sigma ^{-1}(x^{(i)}-x)}{2})$，这就看是选择用$\\tau$ 还是 $\\Sigma$。\n\n\n要注意的是，权值是依赖每个特定的点 $x$ 的，而这些点正是我们要去进行预测评估的点。此外，如果 $|x^{(i)} − x|$ 非常小，那么权值 $w^{(i)} $就接近 $1$；反之如果 $|x^{(i)} − x|$ 非常大，那么权值 $w^{(i)} $就变小。所以可以看出， $\\theta$ 的选择过程中，查询点 $x$ 附近的训练样本有更高得多的权值。（$\\theta$is chosen giving a much higher “weight” to the (errors on) training examples close to the query point x.）（还要注意，当权值的方程的形式跟高斯分布的密度函数比较接近的时候，权值和高斯分布并没有什么直接联系，尤其是当权值不是随机值，且呈现正态分布或者其他形式分布的时候。）随着点$x^{(i)} $ 到查询点 $x$ 的距离降低，训练样本的权值的也在降低，参数$\\tau$  控制了这个降低的速度；$\\tau$也叫做**带宽参数**，这个也是在你的作业中需要来体验和尝试的一个参数。\n\n局部加权线性回归是咱们接触的第一个**非参数** 算法。而更早之前咱们看到的无权重的线性回归算法就是一种**参数** 学习算法，因为有固定的有限个数的参数（也就是 $\\theta_i$ ），这些参数用来拟合数据。我们对 $\\theta_i$ 进行了拟合之后，就把它们存了起来，也就不需要再保留训练数据样本来进行更进一步的预测了。与之相反，如果用局部加权线性回归算法，我们就必须一直保留着整个训练集。这里的非参数算法中的 非参数“non-parametric” 是粗略地指：为了呈现出假设 $h$ 随着数据集规模的增长而线性增长，我们需要以一定顺序保存一些数据的规模。（The term “non-parametric” (roughly) refers to the fact that the amount of stuff we need to keep in order to represent the hypothesis h grows linearly with the size of the training set. ）\n\n### 第二部分 分类和逻辑回归（Classification and logistic regression）\n\n\n接下来咱们讲一下分类的问题。分类问题其实和回归问题很像，只不过我们现在要来预测的 $y$ 的值只局限于少数的若干个离散值。眼下咱们首先关注的是**二值化分类** 问题，也就是说咱们要判断的 $y$ 只有两个取值，$0$ 或者 $1$。（咱们这里谈到的大部分内容也都可以扩展到多种类的情况。）例如，假如要建立一个垃圾邮件筛选器，那么就可以用 $x^{(i)}$ 表示一个邮件中的若干特征，然后如果这个邮件是垃圾邮件，$y$ 就设为$1$，否则 $y$ 为 $0$。$0$ 也可以被称为**消极类别（negative class）**，而 $1$ 就成为**积极类别（positive class**），有的情况下也分别表示成“-” 和 “+”。对于给定的一个 $x^{(i)}$，对应的$y^{(i)}$也称为训练样本的**标签（label）**。\n\n\n#### 5 逻辑回归（Logistic regression）\n\n我们当然也可以还按照之前的线性回归的算法来根据给定的 $x$ 来预测 $y$，只要忽略掉 $y$ 是一个散列值就可以了。然而，这样构建的例子很容易遇到性能问题，这个方法运行效率会非常低，效果很差。而且从直观上来看，$h_\\theta(x)$ 的值如果大于$1$ 或者小于$0$ 就都没有意义了，因为咱们已经实现都确定了 $y \\in \\{0, 1\\}$，就是说 $y$ 必然应当是 $0$ 和 $1$ 这两个值当中的一个。\n\n所以咱们就改变一下假设函数$h_\\theta (x)$ 的形式，来解决这个问题。比如咱们可以选择下面这个函数：\n\n$$ \nh_\\theta(x) = g(\\theta^T x) = \\frac  1{1+e^{-\\theta^Tx}}\n$$\n\n其中有：\n\n$$ \ng(z)= \\frac 1 {1+e^{-z}}\n$$\n\n这个函数叫做**逻辑函数 （Logistic function）** ，或者也叫**双弯曲S型函数（sigmoid function**）。下图是 $g(z)$ 的函数图像：\n\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note1f6.png)\n\n注意到没有，当$z\\to +\\infty$  的时候 $g(z)$ 趋向于$1$，而当$z\\to -\\infty$ 时$g(z)$ 趋向于$0$。此外，这里的这个 $g(z)$ ，也就是 $h(x)$，是一直在 $0$ 和 $1$ 之间波动的。然后咱们依然像最开始那样来设置 $x_0 = 1$，这样就有了：$\\theta^T x =\\theta_0 +\\sum^n_{j=1}\\theta_jx_j$\n\n现在咱们就把 $g$ 作为选定的函数了。当然其他的从$0$到$1$之间光滑递增的函数也可以使用，不过后面我们会了解到选择 $g$ 的一些原因（到时候我们讲广义线性模型 GLMs，那时候还会讲生成学习算法，generative learning algorithms），对这个逻辑函数的选择是很自然的。再继续深入之前，下面是要讲解的关于这个 S 型函数的导数，也就是 $g'$ 的一些性质：\n\n$$\n\\begin{aligned}\ng'(z) & = \\frac d{dz}\\frac 1{1+e^{-z}}\\\\\n& = \\frac  1{(1+e^{-z})^2}(e^{-z})\\\\\n& = \\frac  1{(1+e^{-z})} \\cdot (1- \\frac 1{(1+e^{-z})})\\\\\n& = g(z)(1-g(z))\\\\\n\\end{aligned}\n$$\n\n那么，给定了逻辑回归模型了，咱们怎么去拟合一个合适的 $\\theta$ 呢？我们之前已经看到了在一系列假设的前提下，最小二乘法回归可以通过最大似然估计来推出，那么接下来就给我们的这个分类模型做一系列的统计学假设，然后用最大似然法来拟合参数吧。\n\n首先假设：\n\n$$\n\\begin{aligned}\nP(y=1|x;\\theta)&=h_{\\theta}(x)\\\\\nP(y=0|x;\\theta)&=1- h_{\\theta}(x)\\\\\n\\end{aligned}\n$$\n\n更简洁的写法是：\n\n$$ \np(y|x;\\theta)=(h_\\theta (x))^y(1- h_\\theta (x))^{1-y}\n$$\n\n假设 $m$ 个训练样本都是各自独立生成的，那么就可以按如下的方式来写参数的似然函数：\n\n$$\n\\begin{aligned}\nL(\\theta) &= p(\\vec{y}| X; \\theta)\\\\\n&= \\prod^m_{i=1}  p(y^{(i)}| x^{(i)}; \\theta)\\\\\n&= \\prod^m_{i=1} (h_\\theta (x^{(i)}))^{y^{(i)}}(1-h_\\theta (x^{(i)}))^{1-y^{(i)}} \\\\\n\\end{aligned}\n$$\n\n然后还是跟之前一样，取个对数就更容易计算最大值：\n\n$$\n\\begin{aligned}\nl(\\theta) &=\\log L(\\theta) \\\\\n&= \\sum^m_{i=1} y^{(i)} \\log h(x^{(i)})+(1-y^{(i)})\\log (1-h(x^{(i)}))\n\\end{aligned}\n$$\n\n怎么让似然函数最大？就跟之前咱们在线性回归的时候用了求导数的方法类似，咱们这次就是用**梯度上升法（gradient ascent）**。还是写成向量的形式，然后进行更新，也就是$ \\theta := \\theta +\\alpha \\nabla _\\theta l(\\theta)$ 。 `(注意更新方程中用的是加号而不是减号，因为我们现在是在找一个函数的最大值，而不是找最小值了。)` 还是先从只有一组训练样本$(x,y)$ 来开始，然后求导数来推出随机梯度上升规则：\n\n$$\n\\begin{aligned}\n\\frac  {\\partial}{\\partial \\theta_j} l(\\theta) &=(y\\frac  1 {g(\\theta ^T x)}  - (1-y)\\frac  1 {1- g(\\theta ^T x)}   )\\frac  {\\partial}{\\partial \\theta_j}g(\\theta ^Tx) \\\\\n&= (y\\frac  1 {g(\\theta ^T x)}  - (1-y)\\frac  1 {1- g(\\theta ^T x)}   )  g(\\theta^Tx)(1-g(\\theta^Tx)) \\frac  {\\partial}{\\partial \\theta_j}\\theta ^Tx \\\\\n&= (y(1-g(\\theta^Tx) ) -(1-y) g(\\theta^Tx)) x_j\\\\\n&= (y-h_\\theta(x))x_j\n\\end{aligned}\n$$\n\n上面的式子里，我们用到了对函数求导的定理 $ g'(z)= g(z)(1-g(z))$  。然后就得到了随机梯度上升规则：\n\n$$ \n\\theta_j := \\theta_j + \\alpha (y^{(i)}-h_\\theta (x^{(i)}))x_j^{(i)}\n$$\n\n如果跟之前的 LMS 更新规则相对比，就能发现看上去挺相似的；不过这并不是同一个算法，因为这里的$h_\\theta(x^{(i)})$现在定义成了一个 $\\theta^Tx^{(i)}$  的非线性函数。尽管如此，我们面对不同的学习问题使用了不同的算法，却得到了看上去一样的更新规则，这个还是有点让人吃惊。这是一个巧合么，还是背后有更深层次的原因呢？在我们学到了 GLM 广义线性模型的时候就会得到答案了。（另外也可以看一下 习题集1 里面 Q3 的附加题。）\n\n#### 6 题外话: 感知器学习算法（The perceptron learning algorithm）\n\n现在咱们来岔开一下话题，简要地聊一个算法，这个算法的历史很有趣，并且之后在我们讲学习理论的时候还要讲到它。设想一下，对逻辑回归方法修改一下，“强迫”它输出的值要么是 $0$ 要么是 $1$。要实现这个目的，很自然就应该把函数 $g$ 的定义修改一下，改成一个**阈值函数（threshold function）**：\n\n$$\ng(z)= \\begin{cases} 1 &  if\\quad z \\geq 0  \\\\\n0 &  if\\quad z < 0  \\end{cases}\n$$\n\n如果我们还像之前一样令 $h_\\theta(x) = g(\\theta^T x)$，但用刚刚上面的阈值函数作为 $g$ 的定义，然后如果我们用了下面的更新规则：\n\n$$ \n\\theta_j := \\theta_j +\\alpha(y^{(i)}-h_\\theta (x^{(i)}))x_j^{(i)}\n$$\n\n这样我们就得到了**感知器学习算法。**\n\n在 1960 年代，这个“感知器（perceptron）”被认为是对大脑中单个神经元工作方法的一个粗略建模。鉴于这个算法的简单程度，这个算法也是我们后续在本课程中讲学习理论的时候的起点。但一定要注意，虽然这个感知器学习算法可能看上去表面上跟我们之前讲的其他算法挺相似，但实际上这是一个和逻辑回归以及最小二乘线性回归等算法在种类上都完全不同的算法；尤其重要的是，很难对感知器的预测赋予有意义的概率解释，也很难作为一种最大似然估计算法来推出感知器学习算法。\n\n\n#### 7 让$l(\\theta)$ 取最大值的另外一个算法\n\n再回到用 S 型函数 $g(z)$ 来进行逻辑回归的情况，咱们来讲一个让 $l(\\theta)$ 取最大值的另一个算法。\n\n开始之前，咱们先想一下求一个方程零点的牛顿法。假如我们有一个从实数到实数的函数 $f:R \\to R$，然后要找到一个 $\\theta$ ，来满足 $f(\\theta)=0$，其中 $\\theta\\in R$ 是一个实数。牛顿法就是对 $\\theta$ 进行如下的更新：\n\n$$\n\\theta := \\theta - \\frac {f(\\theta)}{f'(\\theta)}\n$$\n\n这个方法可以通过一个很自然的解释，我们可以把它理解成用一个线性函数来对函数 $f$ 进行逼近，这条直线是 $f$ 的切线，而猜测值是 $\\theta$，解的方法就是找到线性方程等于零的点，把这一个零点作为 $\\theta$ 设置给下一次猜测，然后以此类推。\n\n下面是对牛顿法的图解：\n![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note1f7.png)\n\n在最左边的图里面，可以看到函数 $f$ 就是沿着 $y=0$ 的一条直线。这时候是想要找一个 $\\theta$ 来让 $f(\\theta)=0$。这时候发现这个 $\\theta$ 值大概在 $1.3$ 左右。加入咱们猜测的初始值设定为 $\\theta=4.5$。牛顿法就是在 $\\theta=4.5$ 这个位置画一条切线（中间的图）。这样就给出了下一个 $\\theta$ 猜测值的位置，也就是这个切线的零点，大概是$2.8$。最右面的图中的是再运行一次这个迭代产生的结果，这时候 $\\theta$ 大概是$1.8$。就这样几次迭代之后，很快就能接近 $\\theta=1.3$。\n\n牛顿法的给出的解决思路是让 $f(\\theta) = 0$ 。如果咱们要用它来让函数 $l$ 取得最大值能不能行呢？函数 $l$ 的最大值的点应该对应着是它的导数$l'(\\theta)$ 等于零的点。所以通过令$f(\\theta) = l'(\\theta)$，咱们就可以同样用牛顿法来找到 $l$ 的最大值，然后得到下面的更新规则：\n\n$$\n\\theta := \\theta - \\frac {l'(\\theta)}{l''(\\theta)}\n$$\n\n（扩展一下，额外再思考一下: 如果咱们要用牛顿法来求一个函数的最小值而不是最大值，该怎么修改？）`译者注：试试法线的零点`\n\n最后，在咱们的逻辑回归背景中，$\\theta$ 是一个有值的向量，所以我们要对牛顿法进行扩展来适应这个情况。牛顿法进行扩展到多维情况，也叫牛顿-拉普森法（Newton-Raphson method），如下所示：\n\n$$\n\\theta := \\theta - H^{-1}\\nabla_\\theta l(\\theta)\n$$\n\n上面这个式子中的 $\\nabla_\\theta l(\\theta)$和之前的样例中的类似，是关于 $\\theta_i$ 的 $l(\\theta)$ 的偏导数向量；而 $h$ 是一个 $n\\times n$ 矩阵 ,实际上如果包含截距项的话，应该是, $(n + 1)\\times (n + 1)$，也叫做 Hessian, 其详细定义是：\n\n$$\nH_{ij}= \\frac {\\partial^2 l(\\theta)}{\\partial \\theta_i \\partial \\theta_j}\n$$\n\n牛顿法通常都能比（批量）梯度下降法收敛得更快，而且达到最小值所需要的迭代次数也低很多。然而，牛顿法中的单次迭代往往要比梯度下降法的单步耗费更多的性能开销，因为要查找和转换一个  $n\\times n$的 Hessian 矩阵；不过只要这个 $n$ 不是太大，牛顿法通常就还是更快一些。当用牛顿法来在逻辑回归中求似然函数$l(\\theta)$ 的最大值的时候，得到这一结果的方法也叫做**Fisher评分（Fisher scoring）。**\n\n### 第三部分 广义线性模型 (Generalized Linear Models)$^5$\n\n>5 本节展示的内容受以下两份作品的启发：Michael I. Jordan, Learning in graphical models (unpublished book draft), 以及 McCullagh and Nelder, Generalized Linear Models (2nd ed.)。\n\n到目前为止，我们看过了回归的案例，也看了一个分类案例。在回归的案例中，我们得到的函数是 $y|x; \\theta ∼ N (\\mu, \\sigma^2)$；而分类的案例中，函数是 $y|x; \\theta ∼ Bernoulli(\\phi)$，这里面的$\\mu$ 和 $\\phi$ 分别是 $x$ 和 $\\theta$ 的某种函数。在本节，我们会发现这两种方法都是一个更广泛使用的模型的特例，这种更广泛使用的模型就叫做广义线性模型。我们还会讲一下广义线性模型中的其他模型是如何推出的，以及如何应用到其他的分类和回归问题上。\n\n#### 8 指数族 (The exponential family)\n\n在学习 GLMs 之前，我们要先定义一下指数组分布（exponential family distributions）。如果一个分布能用下面的方式来写出来，我们就说这类分布属于指数族：\n\n$$ \np(y;\\eta) =b(y)exp(\\eta^TT(y)-a(\\eta)) \\qquad \\text{(6)}\n$$\n\n上面的式子中，$\\eta$ 叫做此分布的**自然参数** （natural parameter，也叫**典范参数 canonical parameter**） ； $T(y)$ 叫做**充分统计量（sufficient statistic）** ，我们目前用的这些分布中通常 $T (y) = y$；而 $a(\\eta)$ 是一个**对数分割函数（log partition function）。** $e^{−a(\\eta)}$ 这个量本质上扮演了归一化常数（normalization constant）的角色，也就是确保 $p(y; \\eta)$ 的总和或者积分等于$1$。\n\n当给定 $T$, $a$ 和 $b$ 时，就定义了一个用 $\\eta$ 进行参数化的分布族（family，或者叫集 set）；通过改变 $\\eta$，我们就能得到这个分布族中的不同分布。\n\n现在咱们看到的伯努利（Bernoulli）分布和高斯（Gaussian）分布就都属于指数分布族。伯努利分布的均值是$\\phi$，也写作 $Bernoulli(\\phi)$，确定的分布是 $y \\in \\{0, 1\\}$，因此有 $p(y = 1; \\phi) = \\phi$; $p(y = 0;\\phi) = 1−\\phi$。这时候只要修改$\\phi$，就能得到一系列不同均值的伯努利分布了。现在我们展示的通过修改$\\phi$,而得到的这种伯努利分布，就属于指数分布族；也就是说，只要给定一组 $T$，$a$ 和 $b$，就可以用上面的等式$(6)$来确定一组特定的伯努利分布了。\n\n我们这样来写伯努利分布：\n\n$$\n\\begin{aligned}\np(y;\\phi) & = \\phi ^y(1-\\phi)^{1-y}\\\\\n& = exp(y \\log \\phi + (1-y)\\log(1-\\phi))\\\\\n& = exp( (log (\\frac {\\phi}{1-\\phi}))y+\\log (1-\\phi) )\\\\\n\\end{aligned}\n$$\n\n因此，自然参数（natural parameter）就给出了，即 $\\eta = log (\\frac   \\phi {1 − \\phi})$。 很有趣的是，如果我们翻转这个定义，用$\\eta$ 来解 $\\phi$ 就会得到 $\\phi = 1/ (1 + e^{−\\eta} )$。这正好就是之前我们刚刚见到过的 S型函数(sigmoid function)！在我们把逻辑回归作为一种广义线性模型（GLM）的时候还会得到：\n\n$$\n\\begin{aligned}\nT(y) &= y \\\\\na( \\eta) & = - \\log (1- \\phi) \\\\\n& = \\log {(1+ e^ \\eta)}\\\\\nb(y)&=1\n\\end{aligned}\n$$\n\n上面这组式子就表明了伯努利分布可以写成等式$(6)$的形式，使用一组合适的$T$, $a$ 和 $b$。\n\n接下来就看看高斯分布吧。还记得吧，在推导线性回归的时候，$\\sigma^2$ 的值对我们最终选择的 $\\theta$ 和 $h_\\theta(x)$ 都没有影响。所以我们可以给 $\\sigma^2$ 取一个任意值。为了简化推导过程，就令$\\sigma^2 = 1$。$^6$然后就有了下面的等式：\n\n$$\n\\begin{aligned}\np(y;\\mu) &= \\frac 1{\\sqrt{2\\pi}} exp (- \\frac  12 (y-\\mu)^2) \\\\\n& =  \\frac 1{\\sqrt{2\\pi}} exp (- \\frac  12 y^2) \\cdot exp (\\mu y -\\frac  12 \\mu^2) \\\\\n\\end{aligned}\n$$\n\n>6 如果我们把 $\\sigma^2$ 留作一个变量，高斯分布就也可以表达成指数分布的形式，其中 $\\eta \\in R^2$ 就是一个二维向量，同时依赖 $\\mu$ 和 $\\sigma$。然而，对于广义线性模型GLMs方面的用途， $\\sigma^2$ 参数也可以看成是对指数分布族的更泛化的定义： $p(y; \\eta, \\tau ) = b(a, \\tau ) exp((\\eta^T T (y) − a(\\eta))/c(\\tau))$。这里面的$\\tau$ 叫做**分散度参数（dispersion parameter）**，对于高斯分布， $c(\\tau) = \\sigma^2$ ；不过上文中我们已经进行了简化，所以针对我们要考虑的各种案例，就不需要再进行更加泛化的定义了。\n\n这样，我们就可以看出来高斯分布是属于指数分布族的，可以写成下面这样：\n\n$$\n\\begin{aligned}\n\\eta & = \\mu \\\\\nT(y) & = y \\\\\na(\\eta) & = \\mu ^2 /2\\\\\n& = \\eta ^2 /2\\\\\nb(y) & = (1/ \\sqrt {2\\pi })exp(-y^2/2)\n\\end{aligned}\n$$\n\n指数分布族里面还有很多其他的分布：\n- 例如多项式分布（multinomial），这个稍后我们会看到；\n- 泊松分布（Poisson），用于对计数类数据进行建模，后面再问题集里面也会看到；\n- 伽马和指数分布（the gamma and the exponential），这个用于对连续的、非负的随机变量进行建模，例如时间间隔；\n- 贝塔和狄利克雷分布（the beta and the Dirichlet），这个是用于概率的分布；\n- 还有很多，这里就不一一列举了。\n\n在下一节里面，我们就来讲一讲对于建模的一个更通用的“方案”，其中的$y$ （给定 $x$ 和 $\\theta$）可以是上面这些分布中的任意一种。\n\n#### 9 构建广义线性模型（Constructing GLMs）\n\n设想你要构建一个模型，来估计在给定的某个小时内来到你商店的顾客人数（或者是你的网站的页面访问次数），基于某些确定的特征 $x$ ，例如商店的促销、最近的广告、天气、今天周几啊等等。我们已经知道泊松分布（Poisson distribution）通常能适合用来对访客数目进行建模。知道了这个之后，怎么来建立一个模型来解决咱们这个具体问题呢？非常幸运的是，泊松分布是属于指数分布族的一个分布，所以我们可以对该问题使用广义线性模型（Generalized Linear Model，缩写为 GLM）。在本节，我们讲一种对刚刚这类问题构建广义线性模型的方法。\n\n进一步泛化，设想一个分类或者回归问题，要预测一些随机变量 $y$ 的值，作为 $x$ 的一个函数。要导出适用于这个问题的广义线性模型，就要对我们的模型、给定 $x$ 下 $y$ 的条件分布来做出以下三个假设：\n\n1.\t$y | x; \\theta ∼ Exponential Family(\\eta)$，即给定 $x$ 和 $\\theta, y$ 的分布属于指数分布族，是一个参数为 $\\eta$ 的指数分布。——假设1\n2.\t给定 $x$，目的是要预测对应这个给定 $x$ 的 $T(y)$ 的期望值。咱们的例子中绝大部分情况都是 $T(y) = y$，这也就意味着我们的学习假设 $h$ 输出的预测值 $h(x)$ 要满足 $h(x) = E[y|x]$。 （注意，这个假设通过对 $h_\\theta(x)$ 的选择而满足，在逻辑回归和线性回归中都是如此。例如在逻辑回归中， $h_\\theta (x) = [p (y = 1|x; \\theta)] =[ 0 \\cdot p (y = 0|x; \\theta)+1\\cdot p(y = 1|x;\\theta)] = E[y|x;\\theta]$。**译者注：这里的$E[y|x$]应该就是对给定$x$时的$y$值的期望的意思。**）——假设2\n3.\t自然参数 $\\eta$ 和输入值 $x$ 是线性相关的，$\\eta = \\theta^T x$，或者如果 $\\eta$ 是有值的向量，则有$\\eta_i = \\theta_i^T x$。——假设3\n\n上面的几个假设中，第三个可能看上去证明得最差，所以也更适合把这第三个假设看作是一个我们在设计广义线性模型时候的一种 **“设计选择 design choice”**，而不是一个假设。那么这三个假设/设计，就可以用来推导出一个非常合适的学习算法类别，也就是广义线性模型 GLMs，这个模型有很多特别友好又理想的性质，比如很容易学习。此外，这类模型对一些关于 $y$ 的分布的不同类型建模来说通常效率都很高；例如，我们下面就将要简单介绍一些逻辑回归以及普通最小二乘法这两者如何作为广义线性模型来推出。\n\n##### 9.1 普通最小二乘法（Ordinary Least Squares）\n\n\n我们这一节要讲的是普通最小二乘法实际上是广义线性模型中的一种特例，设想如下的背景设置：目标变量 $y$（在广义线性模型的术语也叫做**响应变量response variable**）是连续的，然后我们将给定 $x$ 的 $y$ 的分布以高斯分布 $N(\\mu, \\sigma^2)$ 来建模，其中 $\\mu$ 可以是依赖 $x$ 的一个函数。这样，我们就让上面的$ExponentialFamily(\\eta)$分布成为了一个高斯分布。在前面内容中我们提到过，在把高斯分布写成指数分布族的分布的时候，有$\\mu = \\eta$。所以就能得到下面的等式：\n\n$$\n\\begin{aligned}\nh_\\theta(x)& = E[y|x;\\theta] \\\\\n& = \\mu \\\\\n& = \\eta \\\\\n& = \\theta^Tx\\\\\n\\end{aligned}\n$$\n\n第一行的等式是基于假设2；第二个等式是基于定理当 $y|x; \\theta ∼ N (\\mu, \\sigma ^2)$，则 $y$ 的期望就是 $\\mu$ ；第三个等式是基于假设1，以及之前我们此前将高斯分布写成指数族分布的时候推导出来的性质 $\\mu = \\eta$；最后一个等式就是基于假设3。\n\n##### 9.2 逻辑回归（Logistic Regression）\n\n接下来咱们再来看看逻辑回归。这里咱们还是看看二值化分类问题，也就是 $y \\in \\{0, 1\\}$。给定了$y$ 是一个二选一的值，那么很自然就选择伯努利分布（Bernoulli distribution）来对给定 $x$ 的 $y$ 的分布进行建模了。在我们把伯努利分布写成一种指数族分布的时候，有 $\\phi = 1/ (1 + e^{−\\eta})$。另外还要注意的是，如果有 $y|x; \\theta ∼ Bernoulli(\\phi)$，那么 $E [y|x; \\theta] = \\phi$。所以就跟刚刚推导普通最小二乘法的过程类似，有以下等式：\n\n$$\n\\begin{aligned}\nh_\\theta(x)& = E[y|x;\\theta] \\\\\n& = \\phi \\\\\n& = 1/(1+ e^{-\\eta}) \\\\\n& = 1/(1+ e^{-\\theta^Tx})\\\\\n\\end{aligned}\n$$\n\n所以，上面的等式就给了给了假设函数的形式：$h_\\theta(x) = 1/ (1 + e^{−\\theta^T x})$。如果你之前好奇咱们是怎么想出来逻辑回归的函数为$1/ (1 + e^{−z} )$，这个就是一种解答：一旦我们假设以 $x$ 为条件的 $y$ 的分布是伯努利分布，那么根据广义线性模型和指数分布族的定义，就会得出这个式子。\n\n再解释一点术语，这里给出分布均值的函数 $g$ 是一个关于自然参数的函数，$g(\\eta) = E[T(y); \\eta]$，这个函数也叫做**规范响应函数（canonical response function），** 它的反函数 $g^{−1}$ 叫做**规范链接函数（canonical link function）。** 因此，对于高斯分布来说，它的规范响应函数正好就是识别函数（identify function）；而对于伯努利分布来说，它的规范响应函数则是逻辑函数（logistic function）。$^7$\n\n>7 很多教科书用 $g$ 表示链接函数，而用反函数$g^{−1}$ 来表示响应函数；但是咱们这里用的是反过来的，这是继承了早期的机器学习中的用法，我们这样使用和后续的其他课程能够更好地衔接起来。\n\n##### 9.3 Softmax 回归\n\n\n咱们再来看一个广义线性模型的例子吧。设想有这样的一个分类问题，其中响应变量 $y$ 的取值可以是 $k$ 个值当中的任意一个，也就是 $y \\in \\{1, 2, ..., k\\}$。例如，我们这次要进行的分类就比把邮件分成垃圾邮件和正常邮件两类这种二值化分类要更加复杂一些，比如可能是要分成三类，例如垃圾邮件、个人邮件、工作相关邮件。这样响应变量依然还是离散的，但取值就不只有两个了。因此咱们就用多项式分布（multinomial distribution）来进行建模。\n\n下面咱们就通过这种多项式分布来推出一个广义线性模型。要实现这一目的，首先还是要把多项式分布也用指数族分布来进行描述。\n\n要对一个可能有 $k$ 个不同输出值的多项式进行参数化，就可以用 $k$ 个参数 $\\phi_1,...,\\phi_ k$ 来对应各自输出值的概率。不过这么多参数可能太多了，形式上也太麻烦，他们也未必都是互相独立的（比如对于任意一个$\\phi_ i$中的值来说，只要知道其他的 $k-1$ 个值，就能知道这最后一个了，因为总和等于$1$，也就是$\\sum^k_{i=1} \\phi_i = 1$）。所以咱们就去掉一个参数，只用 $k-1$ 个：$\\phi_1,...,\\phi_ {k-1}$  来对多项式进行参数化，其中$\\phi_i = p (y = i; \\phi)，p (y = k; \\phi) = 1 −\\sum ^{k−1}_{i=1}\\phi_ i$。为了表述起来方便，我们还要设 $\\phi_k = 1 − \\sum_{i=1}^{k−1} \\phi_i$，但一定要注意，这个并不是一个参数，而是完全由其他的 $k-1$ 个参数来确定的。\n\n要把一个多项式表达成为指数组分布，还要按照下面的方式定义一个 $T (y) \\in R^{k−1}$:\n\n$$\nT(1)=\n    \\begin{bmatrix}\n      1\\\\\n      0\\\\\n\t  0\\\\\n\t  \\vdots \\\\\n\t  0\\\\\n    \\end{bmatrix},\nT(2)=\n    \\begin{bmatrix}\n      0\\\\\n      1\\\\\n\t  0\\\\\n\t  \\vdots \\\\\n\t  0\\\\\n    \\end{bmatrix},\nT(3)=\n    \\begin{bmatrix}\n      0\\\\\n      0\\\\\n\t  1\\\\\n\t  \\vdots \\\\\n\t  0\\\\\n    \\end{bmatrix},\nT(k-1)=\n    \\begin{bmatrix}\n      0\\\\\n      0\\\\\n\t  0\\\\\n\t  \\vdots \\\\\n\t  1\\\\\n    \\end{bmatrix},\nT(k)=\n    \\begin{bmatrix}\n      0\\\\\n      0\\\\\n\t  0\\\\\n\t  \\vdots \\\\\n\t  0\\\\\n    \\end{bmatrix},\n$$\n\n这次和之前的样例都不一样了，就是不再有 $T(y) = y$；然后，$T(y)$ 现在是一个 $k – 1$ 维的向量，而不是一个实数了。向量 $T(y)$ 中的第 $i$ 个元素写成$(T(y))_i$ 。\n\n现在介绍一种非常有用的记号。指示函数（indicator function）$1\\{\\cdot  \\}$，如果参数为真，则等于$1$；反之则等于$0$（$1\\{True\\} = 1, 1\\{False\\} = 0$）。例如$1\\{2 = 3\\} = 0$, 而$1\\{3 = 5 − 2\\} = 1$。所以我们可以把$T(y)$ 和 $y$ 的关系写成  $(T(y))_i = 1\\{y = i\\}$。（往下继续阅读之前，一定要确保你理解了这里的表达式为真！）在此基础上，就有了$E[(T(y))_i] = P (y = i) = \\phi_i$。\n\n现在一切就绪，可以把多项式写成指数族分布了。写出来如下所示：\n\n$$\n\\begin{aligned}\np(y;\\phi) &=\\phi_1^{1\\{y=1\\}}\\phi_2^{1\\{y=2\\}}\\dots \\phi_k^{1\\{y=k\\}} \\\\\n          &=\\phi_1^{1\\{y=1\\}}\\phi_2^{1\\{y=2\\}}\\dots \\phi_k^{1-\\sum_{i=1}^{k-1}1\\{y=i\\}} \\\\\n          &=\\phi_1^{(T(y))_1}\\phi_2^{(T(y))_2}\\dots \\phi_k^{1-\\sum_{i=1}^{k-1}(T(y))_i } \\\\\n          &=exp((T(y))_1 log(\\phi_1)+(T(y))_2 log(\\phi_2)+\\dots+(1-\\sum_{i=1}^{k-1}(T(y))_i)log(\\phi_k)) \\\\\n          &= exp((T(y))_1 log(\\frac{\\phi_1}{\\phi_k})+(T(y))_2 log(\\frac{\\phi_2}{\\phi_k})+\\dots+(T(y))_{k-1}log(\\frac{\\phi_{k-1}}{\\phi_k})+log(\\phi_k)) \\\\\n          &=b(y)exp(\\eta^T T(y)-a(\\eta))\n\\end{aligned}\n$$\n\n其中：\n\n$$\n\\begin{aligned}\n\\eta &= \n    \\begin{bmatrix}\n      \\log (\\phi _1/\\phi _k)\\\\\n      \\log (\\phi _2/\\phi _k)\\\\\n\t  \\vdots \\\\\n\t  \\log (\\phi _{k-1}/\\phi _k)\\\\\n    \\end{bmatrix}, \\\\\na(\\eta) &= -\\log (\\phi _k)\\\\\nb(y) &= 1\\\\\n\\end{aligned}\n$$\n\n这样咱们就把多项式方程作为一个指数族分布来写了出来。\n\n与 $i (for\\quad i = 1, ..., k)$对应的链接函数为：\n\n$$ \n\\eta_i =\\log \\frac  {\\phi_i}{\\phi_k}\n$$\n\n为了方便起见，我们再定义 $\\eta_k = \\log (\\phi_k/\\phi_k) = 0$。对链接函数取反函数然后推导出响应函数，就得到了下面的等式：\n\n$$\n\\begin{aligned}\ne^{\\eta_i} &= \\frac {\\phi_i}{\\phi_k}\\\\\n\\phi_k e^{\\eta_i} &= \\phi_i  \\qquad\\text{(7)}\\\\\n\\phi_k  \\sum^k_{i=1} e^{\\eta_i}&= \\sum^k_{i=1}\\phi_i= 1\\\\\n\\end{aligned}\n$$\n\n这就说明了$\\phi_k = \\frac  1 {\\sum^k_{i=1} e^{\\eta_i}}$，然后可以把这个关系代入回到等式$(7)$，这样就得到了响应函数：\n\n$$ \n\\phi_i = \\frac  { e^{\\eta_i} }{ \\sum^k_{j=1} e^{\\eta_j}}\n$$\n\n上面这个函数从$\\eta$ 映射到了$\\phi$，称为 **Softmax** 函数。\n\n要完成我们的建模，还要用到前文提到的假设3，也就是 $\\eta_i$ 是一个 $x$ 的线性函数。所以就有了 $\\eta_i= \\theta_i^Tx (for\\quad i = 1, ..., k − 1)$，其中的 $\\theta_1, ..., \\theta_{k−1} \\in R^{n+1}$ 就是我们建模的参数。为了表述方便，我们这里还是定义$\\theta_k = 0$，这样就有 $\\eta_k = \\theta_k^T x = 0$，跟前文提到的相符。因此，我们的模型假设了给定 $x$ 的 $y$ 的条件分布为：\n\n$$\n\\begin{aligned}\np(y=i|x;\\theta) &=  \\phi_i \\\\\n&= \\frac {e^{\\eta_i}}{\\sum^k_{j=1}e^{\\eta_j}}\\\\\n&=\\frac {e^{\\theta_i^Tx}}{\\sum^k_{j=1}e^{\\theta_j^Tx}}\\qquad\\text{(8)}\\\\\n\\end{aligned}\n$$\n\n这个适用于解决 $y \\in\\{1, ..., k\\}$ 的分类问题的模型，就叫做 **Softmax 回归。** 这种回归是对逻辑回归的一种扩展泛化。\n\n假设（hypothesis） $h$ 则如下所示:\n\n$$\n\\begin{aligned}\nh_\\theta (x) &= E[T(y)|x;\\theta]\\\\\n&= E \\left[\n    \\begin{array}{cc|c}\n      1(y=1)\\\\\n      1(y=2)\\\\\n\t  \\vdots \\\\\n\t  1(y=k-1)\\\\\n    \\end{array}x;\\theta\n\\right]\\\\\n&= E \\left[\n    \\begin{array}{c}\n      \\phi_1\\\\\n      \\phi_2\\\\\n\t  \\vdots \\\\\n\t  \\phi_{k-1}\\\\\n    \\end{array}\n\\right]\\\\\n&= E \\left[\n    \\begin{array}{ccc}\n      \\frac {exp(\\theta_1^Tx)}{\\sum^k_{j=1}exp(\\theta_j^Tx)} \\\\\n      \\frac {exp(\\theta_2^Tx)}{\\sum^k_{j=1}exp(\\theta_j^Tx)} \\\\\n\t  \\vdots \\\\\n\t  \\frac {exp(\\theta_{k-1}^Tx)}{\\sum^k_{j=1}exp(\\theta_j^Tx)} \\\\\n    \\end{array}\n\\right]\\\\\n\\end{aligned}\n$$\n\n也就是说，我们的假设函数会对每一个 $i = 1,...,k$ ，给出 $p (y = i|x; \\theta)$ 概率的估计值。（虽然咱们在前面假设的这个 $h_\\theta(x)$ 只有 $k-1$ 维，但很明显 $p (y = k|x; \\theta)$ 可以通过用 $1$ 减去其他所有项目概率的和来得到，即$1− \\sum^{k-1}_{i=1}\\phi_i$。）\n\n最后，咱们再来讲一下参数拟合。和我们之前对普通最小二乘线性回归和逻辑回归的原始推导类似，如果咱们有一个有 $m$ 个训练样本的训练集 $\\{(x^{(i)}, y^{(i)}); i = 1, ..., m\\}$，然后要研究这个模型的参数 $\\theta_i$ ，我们可以先写出其似然函数的对数：\n\n$$\n\\begin{aligned}\nl(\\theta)& =\\sum^m_{i=1} \\log p(y^{(i)}|x^{(i)};\\theta)\\\\\n&= \\sum^m_{i=1}log\\prod ^k_{l=1}(\\frac {e^{\\theta_l^Tx^{(i)}}}{\\sum^k_{j=1} e^{\\theta_j^T x^{(i)}}})^{1(y^{(i)}=l)}\\\\\n\\end{aligned}\n$$\n\n要得到上面等式的第二行，要用到等式$(8)$中的设定 $p(y|x; \\theta)$。现在就可以通过对 $l(\\theta)$ 取最大值得到的 $\\theta$ 而得到对参数的最大似然估计，使用的方法就可以用梯度上升法或者牛顿法了。\n\n","tags":["CS229"],"categories":["CS229"]},{"title":"[leetcode]5131.叶值的最小代价生成树","url":"%2Fposts%2F59f66146%2F","content":"{% asset_img 2019072210271214.png %}\n\n### 解题思路 \n```cpp\n// Memorize DP and Divide and Conquer\n// 计算叶节点[low,high]构成的树中,最小非叶结点之和\n// DP[i][j]用于保存从[i,j]构成的树中,最小非叶结点之和\nint solver(vector<int>& arr, int low, int high, vector<vector<int>>& dp)\n{\n    // 如果只有一个叶节点,则最小非叶结点和为0\n    if (low==high) return 0;\n    // 如果已经在前面计算中算出过并保存在了DP中,则直接返回结果\n    if (dp[low][high]!=-1) return dp[low][high];\n    int minSum=INT_MAX;\n    for(int i=low;i<high;++i)\n    {\n        // 获取左边[low,i]最大叶节点,右边[i+1,high]最大叶节点\n        int a=*max_element(arr.begin()+low,arr.begin()+i+1);\n        int b=*max_element(arr.begin()+i+1,arr.begin()+high+1);\n        // 获取非叶结点总和=左边[low,i]非叶最小和+右边[i+1,high]非叶最小和+左右最大叶节点乘积(构成根节点)\n        minSum=min(minSum,solver(arr,low,i,dp)+solver(arr,i+1,high,dp)+a*b);\n        if (dp[low][high]==-1)\n            dp[low][high]=minSum;\n        else\n            dp[low][high]=min(dp[low][high],minSum);\n    }\n    return dp[low][high];\n}\nint mctFromLeafValues(vector<int>& arr) {\n    int n=arr.size();\n    vector<vector<int>> dp(n+1,vector<int>(n+1,-1));\n    return solver(arr,0,n-1,dp);\n}\n```\n### 其他\n**2019/7/23**: 写\n```cpp\n    const int INF=1e8;\n    int dp[100][100];\n    int mctFromLeafValues(vector<int>& arr) {\n        int n=arr.size();\n        return solver(arr,0,n);\n    }\n    int solver(vector<int>& arr, int low, int high)\n    {\n        if (low>=high-1) return 0;\n        if (dp[low][high-1]!=0) return dp[low][high-1];\n        int minSum=INF;\n        for(int i=low+1;i<high;++i)\n        {\n            int a=*max_element(arr.begin()+low,arr.begin()+i);\n            int b=*max_element(arr.begin()+i,arr.begin()+high);\n            minSum=min(minSum,solver(arr,low,i)+solver(arr,i,high)+a*b);\n        }\n        dp[low][high-1]=minSum;\n        return minSum;\n    }\n```\n### 参考","tags":["分治法"],"categories":["OJ"]},{"title":"[leetcode]5132.颜色交替的最短路径","url":"%2Fposts%2Fcb089cf5%2F","content":"{% asset_img 2019072208441713.png %}\n\n### 解题思路 \n`v[x][k]`: 表示与`x`关联,出边色为`k`的边\n`d[x][k]`: 表示以`x`为终点,出边色为`k`的最短距离\n图的bfs遍历算法\n```cpp\nconst int INF = 1000000000;\nusing pii = pair<int, int>;\nvector<int> shortestAlternatingPaths(int n, vector<vector<int>>& red_edges, vector<vector<int>>& blue_edges) {\n    // 构建邻接表\n    vector<int> v[n][2];\n    for(auto e:red_edges)\n        v[e[0]][0].push_back(e[1]);\n    for(auto e:blue_edges)\n        v[e[0]][1].push_back(e[1]);\n    // 记录距离\n    vector<vector<int>> d(n,vector<int>(2));\n    for(int i=0;i<d.size();++i)\n        d[i][0]=d[i][1]=INF;\n    d[0][0]=d[0][1]=0;\n    // bfs\n    queue<pii> Q;\n    Q.push(pii(0,0));\n    Q.push(pii(0,1));\n    while(!Q.empty())\n    {\n        int x=0,k=0;\n        tie(x,k)=Q.front();\n        Q.pop();\n        for(auto y:v[x][k])\n        {\n            if (d[y][k^1]>d[x][k]+1)\n            {\n                d[y][k^1]=d[x][k]+1;\n                Q.push(pii(y,k^1));\n            }\n        }\n    }\n    // 获取到达每个点的最短距离\n    vector<int> res;\n    for(int i=0;i<d.size();++i)\n    {\n        int ret=min(d[i][0],d[i][1]);\n        if (ret==INF) ret=-1;\n        res.push_back(ret);                \n    }\n    return res;\n}\n```\n### 参考","tags":["bfs"],"categories":["OJ"]},{"title":"[leetcode]1033.移动石子直到连续","url":"%2Fposts%2F54dc590d%2F","content":"{% asset_img 2019072110180210.png %}\n\n### 解题思路 \n```cpp\nvector<int> numMovesStones(int a, int b, int c) {\n    if(a>b) swap(a,b);\n    if(b>c) swap(b,c);\n    if(a>b) swap(a,b);\n    vector<int> res(2,0);\n    res[1]=c-a-2; // 最大步数\n    while(!(b-a==1&&c-b==1))\n    {\n        if (b-a==1)\n            c=b+1;\n        else if (c-b==1)\n            a=b-1;\n        else if (b-a<=c-b)\n        {\n            c=a+1;\n            swap(b,c);\n        }\n        else\n        {\n            a=b+1;\n            swap(a,b);\n        }\n        res[0]++;\n    }\n    return res;\n}\n```\n### 参考","tags":["移动石子"],"categories":["OJ"]},{"title":"[leetcode]100.相同的树","url":"%2Fposts%2F7f69dba8%2F","content":"{% asset_img 201907210845019.png %}\n\n### 解题思路 \n值相等的条件下结构也相同则返回true.\n```cpp\n    bool isSameTree(TreeNode* p, TreeNode* q) {\n        if (!p&&!q) return true;\n        else if (!p||!q) return false;\n        else\n        {\n            if (p->val==q->val&&isSameTree(p->left,q->left)&&isSameTree(p->right,q->right)) return true;\n            return false;\n        }\n    }\n```\n### 参考","tags":["树"],"categories":["OJ"]},{"url":"%2Fposts%2F0%2F"},{"title":"[leetcode]67.二进制求和","url":"%2Fposts%2F15c0ccde%2F","content":"{% asset_img 201907210830187.png %}\n\n### 解题思路 \n与[[leetcode]43.字符串相乘](../86f13c57)相似\n```cpp\nstring addBinary(string a, string b) {\n    reverse(a.begin(),a.end());\n    reverse(b.begin(),b.end());\n    string res(a.size()+b.size()+2,'0');\n    int c=0,t=0,len=0,i=0;\n    for(i=0;i<max(a.size(),b.size());++i)\n    {\n        t=(i<a.size()?a[i]-'0':0)+(i<b.size()?b[i]-'0':0)+c;\n        c=t/2;\n        res[i]=t%2+'0';\n    }\n    if (c) // 是否有进位\n    {\n        res[i]=c+'0';\n        len=i+1;\n    } else len=i;\n    res.resize(len);\n    reverse(res.begin(),res.end());\n    return res;\n}\n```\n### 参考","tags":["大整数"],"categories":["OJ"]},{"title":"[leetcode]43.字符串相乘","url":"%2Fposts%2F86f13c57%2F","content":"{% asset_img 201907210756316.png %}\n\n### 解题思路 \n`大整数相乘`:列出来一步一步模拟竖式乘法.先将字符串逆置.\n对于`num2`的每个数乘一遍,得出结果放`t`,进位`c1`,每次算的结果与上次`res[i+j]`累加放入`t`中.\n关键记住:\n1) 数乘进位`c1`\n2) 累加进位`c2`\n3) 数乘或累加结果保留`t`\n```cpp\nstring multiply(string num1, string num2) {\n    if (num1==\"0\"||num2==\"0\") return \"0\";\n    string res(222,'0');\n    reverse(num1.begin(),num1.end());\n    reverse(num2.begin(),num2.end());\n    int c1=0,c2=0,t=0,i=0,j=0,len=0;\n    for(i=0;i<num1.size();++i)\n    {\n        c1=c2=0;\n        for(j=0;j<num2.size();++j)\n        {\n            t=(num2[j]-'0')*(num1[i]-'0')+c1; // 数乘结果\n            c1=t/10; // 数乘进位c1\n            t=res[i+j]-'0'+t%10+c2; // 累加结果\n            c2=t/10; // 累加进位c2\n            res[i+j]=t%10+'0';\n        }\n        if (c1||c2) res[i+j]=c1+c2+'0'; // (累加或数乘后)首位有进位\n    }\n    if (c1||c2) len=i+j; // 首位有进位\n    else len=i+j-1; // 首位无进位\n    res.resize(len);\n    reverse(res.begin(),res.end());\n    return res;\n}\n```\n### 参考","tags":["大整数"],"categories":["OJ"]},{"title":"[机器学习-CS229(notes)]第1部分 线性回归","url":"%2Fposts%2F1c04fb93%2F","content":"### 标记\n$m$: 表示训练样本数目\n$n$: 表示特征数目\n$\\mathbf{x}$: 表示输入变量/特征\n$\\mathbf{y}$: 表示输出变量/目标变量\n$\\mathbf{(x,y)}$: 表示训练样本\n$(\\mathbf{x}^{(i)},\\mathbf{y}^{(i)})$: 表示第$i$个训练样本\n### 代价函数\n要进行这个监督学习,必须确定好如何在计算机里面对这个**函数/假设**$h$进行表示\n$$\\begin{aligned}\n    h_\\theta(\\mathbf{x})=\\theta_0+\\theta_1x_1+\\theta_2x_2+\\cdots+\\theta_nx_n=\\sum_{i=0}^n\\theta_ix_i=\\mathbf{\\theta^Tx}\n\\end{aligned}$$\n$\\mathbf{x}=(x_0,x_1,\\cdots,x_n)^T,\\mathbf{\\theta}=(\\theta_0,\\theta_1,\\cdots,\\theta_n)^T$\n这里$\\theta_i$是**参数**(也可以成为**权重**),是从$x$到$y$的线性函数映射的空间参数.在不至于引起混淆的情况下,可以把$h_\\theta(\\mathbf{x})$里面的$\\theta$省略掉,简写为$h(\\mathbf{x})$.另外,为了化简公式,还设$x_0=1$(这个为**截距项**).\n**代价函数(cost function)**:\n$$\\begin{aligned}\n    J(\\mathbf{\\theta})=\\frac{1}{2}\\sum_{i=1}^m(h_\\theta(\\mathbf{x}^{(i)})-\\mathbf{y}^{(i)})^2\n\\end{aligned}$$\n为了让代价函数最小,有两种方法,一种是梯度下降法,还有一种是矩阵求导法\n### 梯度下降法\n我们希望能选择一个能让$J(\\mathbf{\\theta})$最小的$\\mathbf{\\theta}$值,怎么做呢,先用一个搜索的算法,从某一个对$\\mathbf{\\theta}$的\"初始猜测值\",然后对$\\mathbf{\\theta}$值不断进行调整,来让$J(\\mathbf{\\theta})$逐渐变小,最好是直到我们能够达到一个使$J(\\mathbf{\\theta})$最小的$\\mathbf{\\theta}$.\n#### 批量梯度下降法(Batch Gradient Descent,BGD)\n$$\\begin{aligned}\n    \\theta_j=\\theta_j-\\alpha\\frac{\\partial}{\\partial{\\theta_j}}J(\\mathbf{\\theta})\n\\end{aligned}$$\n上面这个更新要同时对应从$0$到$n$的所有$j$值进行.$\\alpha$称为学习速率.这个算法是很自然的,逐步重复朝向$J$降低最快的方向移动.\n我们来继续化简上式,已知\n$$\\begin{aligned}\n    J(\\mathbf{\\theta})=\\frac{1}{2}\\sum_{i=1}^m(h_\\theta(\\mathbf{x}^{(i)})-\\mathbf{y}^{(i)})^2\n\\end{aligned}$$\n那么\n$$\\begin{aligned}\n    \\frac{\\partial}{\\partial{\\theta_j}}J(\\mathbf{\\theta})&=\\frac{1}{2}\\cdot{2}\\cdot{\\sum_{i=1}^m(h_\\theta(\\mathbf{x}^{(i)})-\\mathbf{y}^{(i)})}\\cdot{\\frac{\\partial}{\\partial{\\theta_j}}}(h_\\theta(\\mathbf{x}^{(i)})-\\mathbf{y}^{(i)})\\\\\n    &=\\sum_{i=1}^m(h_\\theta(\\mathbf{x}^{(i)})-\\mathbf{y}^{(i)})\\cdot{\\frac{\\partial}{\\partial{\\theta_j}}}(\\sum_{k=0}^n\\theta_kx_k^{(i)}-\\mathbf{y}^{(i)})\\\\\n    &=\\sum_{i=1}^m(h_\\theta(\\mathbf{x}^{(i)})-\\mathbf{y}^{(i)})\\cdot{x_j^{(i)}}\n\\end{aligned}$$\n则\n$$\\begin{aligned}\n    \\theta_j=\\theta_j-\\alpha\\sum_{i=1}^m(h_\\theta(\\mathbf{x}^{(i)})-\\mathbf{y}^{(i)})\\cdot{x_j^{(i)}}\n\\end{aligned}$$\n上面这个更新要同时对应从$0$到$n$的所有$j$值进行.$\\alpha$称为学习速率.这个算法是很自然的,逐步重复朝向$J$降低最快的方向移动.\n#### 随机梯度下降法\n{% asset_img 201907202156275.png %}\n这个算法中,我们队整个训练集进行了循环遍历,每次遇到一个训练样本,根据每个单一训练样本的误差梯度来对参数进行更新.这个算法叫做**随机梯度下降(stochastic gradient descent)**,或者**叫增量梯度下降法(incremental gradient descent.**批量梯度下降法要在运行第一步之前先对整个训练集进行扫描遍历,当训练集的规模$m$变得很大的时候,引起的性能开销就很不划算了;随机梯度下降法就没有这个问题,而是可以立即开始,对查询到的每个样本都进行运算.通常,随机梯度下降法查找到足够接近最低值的$\\mathbf{\\theta}$的速度要比批量梯度下降法更快一些.(也要注意,也有可能会一直无法收敛(converge)到最小值,这时候$\\mathbf{\\theta}$会一直在$J(\\mathbf{\\theta})$最小值附近震荡),由于这些原因,特别是在训练集很大的情况下,随机梯度下降往往比批量梯度下降更受青睐.\n> 当然更常见的情况通常是我们事先对数据集已经有了描述，并且有了一个确定的学习速率$\\alpha$，然后来运行随机梯度下降，同时逐渐让学习速率 $\\alpha$随着算法的运行而逐渐趋于$0$，这样也能保证我们最后得到的参数会收敛到最小值，而不是在最小值范围进行震荡。（译者注：由于以上种种原因，通常更推荐使用的都是随机梯度下降法，而不是批量梯度下降法，尤其是在训练用的数据集规模大的时候。）\n\n### 矩阵求导法\n已知\n$$\\begin{aligned}\n    X\\mathbf{\\theta}-\\mathbf{y}&=\\begin{bmatrix}\n        \\mathbf{(x^{(1)})^T}\\\\\n        \\mathbf{(x^{(2)})^T}\\\\\n        \\cdot\\\\\n        \\cdot\\\\\n        \\cdot\\\\\n        \\mathbf{(x^{(m)})^T}\\\\\n    \\end{bmatrix}\\cdot{\\mathbf{\\theta}}-\\begin{bmatrix}\n        \\mathbf{y^{(1)}}\\\\\n        \\mathbf{y^{(2)}}\\\\\n        \\cdot\\\\\n        \\cdot\\\\\n        \\cdot\\\\\n        \\mathbf{y^{(m)}}\\\\\n    \\end{bmatrix}\\\\\n    &=\\begin{bmatrix}\n        \\mathbf{(x^{(1)})^T\\cdot\\theta-y^{(1)}}\\\\\n        \\mathbf{(x^{(2)})^T\\cdot\\theta-y^{(2)}}\\\\\n        \\cdot\\\\\n        \\cdot\\\\\n        \\cdot\\\\\n        \\mathbf{(x^{(m)})^T\\cdot\\theta-y^{(m)}}\\\\\n    \\end{bmatrix}=\\begin{bmatrix}\n        h_\\theta(\\mathbf{x^{(1)}})-\\mathbf{y}^{(1)}\\\\\n        h_\\theta(\\mathbf{x^{(2)}})-\\mathbf{y}^{(2)}\\\\\n        \\cdot\\\\\n        \\cdot\\\\\n        \\cdot\\\\\n        h_\\theta(\\mathbf{x^{(m)}})-\\mathbf{y}^{(m)}\\\\\n    \\end{bmatrix}\n\\end{aligned}$$\n那么\n$$\\begin{aligned}\n    J(\\mathbf{\\theta})=\\frac{1}{2}\\sum_{i=1}^m(h_\\theta(\\mathbf{x}^{(i)})-\\mathbf{y}^{(i)})^2=\\frac{1}{2}\\mathbf{(X\\theta-y)^T(X\\theta-y)}\n\\end{aligned}$$\n则\n$$\\begin{aligned}\n    \\nabla_\\theta J(\\mathbf{\\theta})&=\\nabla_\\theta \\left[\\frac{1}{2}\\mathbf{(X\\theta-y)^T(X\\theta-y)}\\right]\\\\\n    &=\\frac{1}{2}\\cdot{\\nabla_\\theta \\rm{tr}\\left(\\mathbf{\\theta^TX^TX\\theta-\\theta^TX^Ty-y^TX\\theta+y^Ty}\\right)}\\\\\n    &=\\frac{1}{2}\\left(\\mathbf{X^TX\\theta+X^TX\\theta-X^Ty-X^Ty}\\right)\\\\\n    &=\\mathbf{X^TX\\theta-X^Ty}\n\\end{aligned}$$\n令$\\nabla_\\theta J(\\mathbf{\\theta})=0$,则得到让$J(\\mathbf{\\theta})$取得最小值的$\\mathbf{\\theta}$就是\n$$\\begin{aligned}\n    \\mathbf{\\theta}=\\mathbf{(X^TX)^{-1}X^Ty}\n\\end{aligned}$$\n> 由于$J(\\mathbf{\\theta})\\in\\R$,因此$\\nabla_\\theta{J(\\theta)}=\\nabla_\\theta{\\rm{tr}J(\\theta)}$\n### 局部加权线性回归(Locally weighted linear regression,LWLR)\n局部加权线性回归（locally weighted linear regression ，缩写为LWR），这个方法是假设有足够多的训练数据，对不太重要的特征进行一些筛选。这部分内容会比较简略，因为在作业中要求学生自己去探索一下LWR 算法的各种性质了。\n权值选取标准公式:\n$$\\begin{aligned}\n    w(i,i)=\\exp\\left(-\\frac{(x^{(i)}-x)^2}{2k^2}\\right)=\\exp{\\left(-\\frac{(\\mathbf{x}^{(i)}-\\mathbf{x})^T(\\mathbf{x}^{(i)}-\\mathbf{x})}{2k^2}\\right)}\n\\end{aligned}$$\n#### 矩阵求导法\n线性回归的一个问题是有可能出现欠拟合现象，因为它求的是具有最小均方差的无偏估计。显而易见，如果模型欠拟合将不能取得最好的预测效果。所以有\n些方法允许在估计中引入一些偏差，从而降低预测的均方误差。\n一个方法是局部加权线性回归（Locally Weighted Linear Regression，LWLR）。在这个算法中，我们给预测点附近的每个点赋予一定的权重，然后与 线性\n回归 类似，在这个子集上基于最小均方误差来进行普通的回归。我们需要最小化的目标函数(代价函数)大致为:\n$$\\begin{aligned}\n    J(\\mathbf{\\theta})=\\frac{1}{2}\\sum_{i=1}^n\\mathbf{w}^{(i)}\\cdot(h_\\theta(\\mathbf{x}^{(i)})-\\mathbf{y}^{(i)})^2=\\frac{1}{2}(\\mathbf{X\\theta-y})^T\\mathbf{W}(\\mathbf{X\\theta-y})\n\\end{aligned}$$\n这里$W$是对角阵(只有主对角线有元素,其余为0,可以想象二次型$y=x^TAx$,则每个平方项前面有一个系数$w^{(i)}$),同样,为了使代价函数值最小,需要求出令$\\nabla_\\theta J(\\mathbf{\\theta})=0$时的$\\mathbf{\\theta}$,那么\n$$\\begin{aligned}\n    \\nabla_\\theta J(\\mathbf{\\theta})&=\\nabla_\\theta \\left[\\frac{1}{2}\\mathbf{(X\\theta-y)^TW(X\\theta-y)}\\right]\\\\\n    &=\\frac{1}{2}\\cdot{\\nabla_\\theta \\rm{tr}\\left(\\mathbf{\\theta^TX^TWX\\theta-\\theta^TX^TWy-y^TWX\\theta+y^TWy}\\right)}\\\\\n    &=\\frac{1}{2}\\left(\\mathbf{X^TWX\\theta+X^TW^TX\\theta-X^TWy-X^TW^Ty}\\right)\\\\\n    &=\\mathbf{X^TWX\\theta-X^TWy}\n\\end{aligned}$$\n令$\\nabla_\\theta J(\\mathbf{\\theta})=0$,则得到让$J(\\mathbf{\\theta})$取得最小值的$\\mathbf{\\theta}$就是\n$$\\begin{aligned}\n    \\mathbf{\\theta}=\\mathbf{(X^TWX)^{-1}X^TWy}\n\\end{aligned}$$\n\n### 参考","tags":["CS229"],"categories":["CS229"]},{"title":"[矩阵食谱]notes","url":"%2Fposts%2F468281cb%2F","content":"书籍下载:[The Matrix Cookbook](matrixcookbook.pdf)\n# 向量,矩阵,和标量求导\n$$\\begin{aligned}\n    &\\frac{\\partial{\\mathbf{AX}}}{\\partial{\\mathbf{X}}}=\\frac{\\partial{\\mathbf{XA}}}{\\partial{\\mathbf{X}}}=\\mathbf{A^T}, \\frac{\\partial{\\mathbf{AX^T}}}{\\partial{\\mathbf{X}}}=\\frac{\\mathbf{X^TA}}{\\partial{\\mathbf{X}}}=\\mathbf{A}\\\\\n    &\\frac{\\partial{\\mathbf{AXB}}}{\\partial{\\mathbf{X}}}=\\mathbf{A^TB^T}\\\\\n    &\\frac{\\partial{\\mathbf{AX^TB}}}{\\partial{\\mathbf{X}}}=\\mathbf{(A^TB^T)^T=BA}\\\\\n    &\\frac{\\partial{\\mathbf{AXX^TB}}}{\\partial{\\mathbf{X}}}=\\mathbf{A^T(X^TB)^T+((AX)^TB^T)^T}=\\mathbf{A^TB^TX+BAX}\\\\\n    &\\frac{\\partial{\\mathbf{(AX+B)}}}{\\partial{\\mathbf{X}}}=\\frac{\\partial{\\mathbf{AX}}}{\\partial{\\mathbf{X}}}=\\mathbf{A^T}\n\\end{aligned}$$\n同理:\n$$\\begin{aligned}\n    &\\frac{\\partial{\\mathbf{x^Ta}}}{\\partial{\\mathbf{x}}}=\\frac{\\partial{\\mathbf{a^Tx}}}{\\partial{\\mathbf{x}}}=\\mathbf{a}\\\\\n    &\\frac{\\partial{\\mathbf{x^TBx}}}{\\partial{\\mathbf{x}}}=\\mathbf{(B+B^T)x}\\\\\n    &\\frac{\\partial{\\mathbf{(Bx+b)^TC(Dx+d)}}}{\\partial{\\mathbf{x}}}=\\mathbf{B^TC(Dx+d)+D^TC^T(Bx+b)}\\\\\n    &\\frac{\\partial{\\mathbf{a^TXb}}}{\\partial{\\mathbf{X}}}=\\mathbf{ab^T}\\\\\n    &\\frac{\\partial{\\mathbf{a^TX^Tb}}}{\\partial{\\mathbf{X}}}=\\mathbf{ba^T}\\\\\n    &\\frac{\\partial{\\mathbf{a^TXa}}}{\\partial{\\mathbf{X}}}=\\frac{\\partial{\\mathbf{a^TX^Ta}}}{\\partial{\\mathbf{X}}}=\\mathbf{aa^T}\\\\\n    &\\frac{\\mathbf{b^TX^TXc}}{\\partial{\\mathbf{X}}}=\\mathbf{X(bc^T+cb^T)}\\\\\n    &\\frac{\\partial{\\mathbf{b^TX^TDXc}}}{\\partial{\\mathbf{X}}}=\\mathbf{D^TXbc^T+DXcb^T}\\\\\n    &\\frac{\\partial}{\\partial{\\mathbf{X}}}\\mathbf{(Xb+c)^TD(Xb+c)}=\\mathbf{(D+D^T)(Xb+c)b^T}\n\\end{aligned}$$\n# 迹(Trace)求导\n$$\\begin{aligned}\n    \\frac{\\partial{\\rm{Tr(F(\\mathbf{X}))}}}{\\partial{\\mathbf{X}}}=f(\\mathbf{X})^T\n\\end{aligned}$$\n$f(\\cdot)$是$F(\\cdot)$的标量导数(scalar derivative)\n同理\n$$\\begin{aligned}\n    &\\frac{\\partial}{\\partial{\\mathbf{X}}}\\rm{Tr(\\mathbf{X})}=\\mathbf{I}\\\\\n    &\\frac{\\partial}{\\partial{\\mathbf{X}}}\\rm{Tr(\\mathbf{XA})}=\\mathbf{A^T}\\\\\n    &\\frac{\\partial}{\\partial{\\mathbf{X}}}\\rm{Tr(\\mathbf{AXB})}=\\mathbf{A^TB^T}\\\\\n    &\\frac{\\partial}{\\partial{\\mathbf{X}}}\\rm{Tr(\\mathbf{AX^TB})}=\\mathbf{BA}\\\\\n    &\\frac{\\partial}{\\partial{\\mathbf{X}}}\\rm{Tr(\\mathbf{AX^T})}=\\frac{\\partial}{\\partial{\\mathbf{X}}}\\rm{Tr(\\mathbf{X^TA})}=\\mathbf{A}\\\\\n    &\\frac{\\partial}{\\partial{\\mathbf{X}}}\\rm{Tr(\\mathbf{X^2})}=2\\mathbf{X^2}\\\\\n    &\\frac{\\partial}{\\partial{\\mathbf{X}}}\\rm{Tr(\\mathbf{X^2B})}=\\frac{\\partial}{\\partial{\\mathbf{X}}}\\rm{Tr(\\mathbf{XXB})}=\\mathbf{B^TX^T+X^TB^T}\\\\\n    &\\frac{\\partial}{\\partial{\\mathbf{X}}}\\rm{Tr(\\mathbf{X^TBX})}=\\mathbf{BX+B^TX}=\\mathbf{(B+B^T)X}\\\\\n    &\\frac{\\partial}{\\partial{\\mathbf{X}}}\\rm{Tr(\\mathbf{XBX^T})}=\\mathbf{XB^T+XB}=\\mathbf{X(B^T+B)}\n\\end{aligned}$$\n# 梯度和Hessians\n$\\begin{array}{l}{\\text { Recall that a matrix } A \\in \\mathbb{R}^{n \\times n} \\text { is symmetric if } A^{T}=A, \\text { that is, } A_{i j}=A_{j i} \\text { for all } i, j . \\text { Also }} \\\\ {\\text { recall the gradient } \\nabla f(x) \\text { of a function } f : \\mathbb{R}^{n} \\rightarrow \\mathbb{R}, \\text { which is the } n \\text { -vector of partial derivatives }}\\end{array}$\n$$\\begin{aligned}\n    \\nabla f(x)=\\left[\\begin{array}{c}{\\frac{\\partial}{\\partial x_{1}} f(x)} \\\\ {\\vdots} \\\\ {\\frac{\\partial}{\\partial x_{n}} f(x)}\\end{array}\\right] \\text { where } x=\\left[\\begin{array}{c}{x_{1}} \\\\ {\\vdots} \\\\ {x_{n}}\\end{array}\\right]\n\\end{aligned}$$\n$\\begin{array}{l}{\\text { The hessian } \\nabla^{2} f(x) \\text { of a function } f : \\mathbb{R}^{n} \\rightarrow \\mathbb{R} \\text { is the } n \\times n \\text { symmetric matrix of twice partial }} \\\\ {\\text { derivatives, }}\\end{array}$\n$$\\begin{aligned}\n    \\nabla^{2} f(x)=\\left[\\begin{array}{cccc}{\\frac{\\partial^{2}}{\\partial x_{1}^{2}} f(x)} & {\\frac{\\partial^{2}}{\\partial x_{1} \\partial x_{2}} f(x)} & {\\cdots} & {\\frac{\\partial^{2}}{\\partial x_{1} \\partial x_{n}} f(x)} \\\\ {\\frac{\\partial^{2}}{\\partial x_{2} \\partial x_{1}} f(x)} & {\\frac{\\partial^{2}}{\\partial x_{2}^{2}} f(x)} & {\\cdots} & {\\frac{\\partial^{2}}{\\partial x_{2} \\partial x_{n}} f(x)} \\\\ {\\vdots} & {\\vdots} & {\\ddots} & {\\vdots} \\\\ {\\frac{\\partial^{2}}{\\partial x_{n} \\partial x_{1}} f(x)} & {\\frac{\\partial^{2}}{\\partial x_{n} \\partial x_{2}} f(x)} & {\\cdots} & {\\frac{\\partial^{2}}{\\partial x_{n}^{2}} f(x)}\\end{array}\\right]\n\\end{aligned}$$\n# 常见结论\n$$\\begin{aligned}\n    &(\\lambda I+B A)^{-1} B =B(\\lambda I+A B)^{-1}\n\\end{aligned}$$\n### 参考","tags":["矩阵食谱"],"categories":["矩阵食谱"]},{"title":"[pytorch]notes","url":"%2Fposts%2F9bc934eb%2F","content":"{% asset_img  %}\n\n### 解题思路 \n\n### 参考","categories":["OJ"]},{"title":"[paper]Domain Adaptation via Transfer Component Analysis","url":"%2Fposts%2F18787dff%2F","content":"\n### Abstract\n域自适应允许知识从一个域迁移到一个不同但相关的域.直观上,发现一个好的跨域的特征表示是关键的.在本文中，我们首先提出通过一种新的学习方法-迁移成分分析(TCA)来寻找这种表示。TCA试图在再生核Hilbert空间中使用maximum mean miscrepancy学习一些跨域的迁移成分.在这些被迁移成分跨越的子空间中，数据属性被保留，不同域的数据分布相互接近。因此，利用该子空间中的新表示，我们可以应用标准的机器学习方法在源域训练分类器或回归模型，以便在目标域中使用。 此外，为了揭示隐藏在来自源和目标域的数据标签之间的关系中的知识，我们在半监督学习环境中扩展TCA，它将标签信息编码到迁移成分学习中.我们称这种扩展为semi-supervised TCA。我们的工作的主要贡献在于，我们提出了一种新的降维框架，用于减小用于域适应的潜在空间中的域之间的距离。我们提出了两种无监督和半监督的特征提取方法，通过将数据投影到所学的迁移成分上，可以极大地减少域分布之间的距离。最后，我们的方法可以处理大型数据集，并自能够泛化到样本之外.\n通过在五个玩具数据集和两个实际应用程序上的实验，验证了该方法的有效性和效率:跨域室内WiFi定位和跨域文本分类。\n### INTRODUCTION\n域自适应旨在通过利用源域的训练数据(即使源域有不同的分布)解决目标域的学习问题.这是一个重要的学习问题，因为标记的数据常常难以获得，因此最好使用可用的任何相关数据。例如，在需要回归学习的室内WiFi定位中，标记的训练数据很难获得，而且代价很高。此外，一旦校准，这些数据可能很容易过时，因为WiFi信号强度可能是许多动态因素的函数，包括时间、设备和空间[2]。为了减少重新校准的工作量，我们可能需要调整一个在一个时间段(源域)中训练的本地化模型，以适应一个新的时间段(目标域),或者将在一个移动设备(源域)上训练的定位模型适应于一个新的移动设备(目标域)。\n域适配可以被认为是迁移学习的特殊设置，其目的在于跨不同但相关的任务或域迁移共享知识[3]-[5]。域自适应的一个主要计算问题是如何减小源域数据与目标域数据分布之间的差异。直观地，发现跨领域的良好特征表示是至关重要的[3]，[6]。良好的特征表示应该能够尽可能地减小域之间的分布的差异，同时保持原始数据的重要的性质(例如:几何特性、统计属性或侧信息[7])，特别是对于目标域数据尤为重要。最近，人们提出了几种学习领域自适应通用特征表示的方法[8]-[10]。DauméIII[8]设计了一个启发式核来增强特征，以解决自然语言处理中的一些特定领域适应问题。Blitzer等人[9]在[11]的激励下，提出了结构对应学习(SCL)算法，以诱导不同领域特征之间的对应关系。该方法依赖于两个领域中频繁出现的枢轴特征的启发式选择。虽然实验表明SCL可以减少基于A-距离测度的域间的差异[6]，但pivot feature selection的启发式准则可能对不同的应用很敏感.\n大多数先前基于特征的域适配方法没有明确地最小化域之间的分布的距离。最近，vonBrunau等人[12]提出了固定子空间分析（SSA），以匹配潜在空间中的分布。然而，SSA侧重于识别固定子空间，而不考虑保留子空间中的数据差异等属性。潘等人[10]提出了一种新的用于域自适应的降维方法-最大平均差嵌入法(MMDE)。MMDE的目的是学习在这些域下面的共享潜在空间，其中可以减少分布之间的距离，同时可以保留数据差异。然而，MMDE有两个主要限制：1)MMDE是直推式的，并不能泛化到样本之外的模式[13]；2)MMDE通过求解计算量大的半定程序(SDP)来学习隐空间(latent space)。\n本文提出了一种新的用于域自适应的特征提取方法-迁移成分分析(Transfer Component Analysis，TCA)。它试图学习在这两个域之下的一组公共的迁移成分,使得当投影到该子空间上时,不同域的数据分布的差异可以显著减小,并且可以保存数据属性。然后，可以在这个子空间中使用标准的机器学习方法来训练跨领域的分类或回归模型。更具体地说，如果两个域相互关联，它们可能存在几个公共成分(或潜在变量)。其中一些可能会导致域之间的数据分布不同，而另一些可能不会。同时，这些成分中的一些可以捕获原始数据下面的固有结构或区别信息，而其它成分则不能。因此，我们的目标是发现那些不会导致分布在域间发生很大变化的成分，并且能够很好地保留原始数据的结构或与任务相关的信息.\n我们的主要贡献是提出了一种新的降维方法，通过将数据投影到一个学习的迁移子空间来减小域间的距离。一旦找到子空间，就可以使用任何方法进行后续的分类、回归和聚类。此外，TCA及其半监督的扩展SSTCA比MMDE高效得多，可以处理样本外扩展问题[13]。本文的其余部分组织如下。在第二节中，我们首先介绍了域自适应问题和传统的降维方法，并描述了距离的Hilbert空间嵌入和分布之间的相关性测度.我们提出的域自适应特征提取方法见第III和IV节。在第五节中，我们对一些玩具数据集和两个实际应用问题进行了一系列实验，以验证所提方法的有效性和高效性。最后，我们在第六节结束我们的工作。\n### PREVIOUS WORKS AND PRELIMINARIES\n#### 域自适应\n我们认为域由两个主要成分组成：一个输入的特征空间$\\mathcal{X}$和一个输入的边缘概率分布$P(X)$, 其中$X=\\{x_1,...,x_n\\}\\in{\\mathcal{X}}$是一系列的学习样本.例如，如果我们的学习任务是文档分类，并且每个术语都被看作是一个二进制特征，那么$\\mathcal{X}$就是所有文档向量的空间。通常，如果两个域不同，它们可能具有不同的特征空间或不同的边际概率分布。在本文中，我们重点讨论只有一个源和一个目标域共享相同特征空间的情况。我们也假设源域中存在一些标记的数据$\\mathcal{D}_S$,目标域中只有未标记的数据$\\mathcal{D}_T$.更具体的说,将源域数据表示为$\\mathcal{D}_S=\\{(x_{S_1},y_{S_1}),...,(x_{S_{n_1}},y_{S_{n_1}})\\}$,其中$x_{S_i}\\in{\\mathcal{X}}$为输入,$y_{S_i}\\in{\\mathcal{Y}}$是对应的输出.同样,将目标域的数据表示为$\\mathcal{D}_T=\\{x_{T_1},...,x_{T_{n_2}}\\}$,其中输入$x_{T_i}$也在特征空间$\\mathcal{X}$中.\n令$\\mathcal{P}(X_S)$和$\\mathcal{Q}(X_T)$(或直接简写为$\\mathcal{P}$和$\\mathcal{Q}$)分别作为源域中的$X_S={x_{S_i}}$和目标域中的$X_T={x_{T_i}}$的边缘分布.总之,$\\mathcal{P}$与$\\mathcal{Q}$可以是不同的.我们的任务是预测与目标域输入$x_{T_iS}$相对应的标签$y_{T_iS}$.大多数域自适应方法的关键假设是认为$\\mathcal{P}\\neq{\\mathcal{Q}}$,但$P(Y_S|X_S)=P(Y_T|X_T)$.\ncovariate shift自适应问题也与区域适应有关。为了解决这个问题，重要性再加权是一项主要技术[14]-[18]。黄等人[14]提出了一种基于核的方法，称为核均值匹配(Kernal Mean Matching,KMM)。在再生内核Hilbert空间(RKHS)中重加权。Sugiyama等[15]提出了另一个重要的重加权算法，称为Kullback-Leibler重要性估计过程(Kullback-Leibler importance estimation procedure,KLIPEP),它与交叉验证相结合，自动执行模型选择。Bickel等人[16]建议将分配校正过程纳入核化Logistic回归。Kanamori等人[17]提出了一种无约束最小二乘重要性拟合(unconstrained least-squares importance fitting,uLSIF)方法，通过将直接重要性估计问题转化为最小二乘函数拟合问题。这些方法与我们提出的方法的主要区别在于，我们的目标是匹配潜在空间中域间的数据分布,其中可以保留数据属性，而不是在原始特征空间中匹配它们。最近，Sugiyama等[18]通过在非平稳子空间(nonstationary subspace)中估计重要性进一步扩展了uLSIF算法，即使在数据域的维数较高的情况下也表现良好。 然而，该方法侧重于估计潜在空间的重要性，而不是学习一个潜在空间用于自适应。注意，除了协变量移位自适应(covariate shift adaptation)，重要性估计技术也被应用于各种应用，例如独立成分分析(independent component analysis)[19]、outlier detection[20]和change-point detection[21]。\n除重加权方法外，von Bünau等人。[12]建议在潜在空间中匹配分布。更具体地说，他们从理论上研究了从多变量时间序列中可以识别平稳空间的条件。他们还提出了SSA方法，通过匹配不同时刻数据分布的前两个瞬间寻找stationary components。然而，SSA的重点在于如何识别静止子空间，而不考虑如何在潜在空间中保留数据属性。结果，SSA可以将数据映射到一些噪声因子，这些噪声因子在域上是固定的，但与目标监督任务完全无关。然后，对SSA学习的新表示进行训练的分类器可能无法获得良好的域自适应性能。我们将在第三节中展示一个激励人心的例子。\n#### Hilbert Space Embedding of Distributions\n1) 最大平均偏差(Maximum Mean Discrepancy,MMD)：给定两个分布的样本$X=\\{x_i\\}$和$Y=\\{y_i\\}$，有许多准则(如Kullback-Leibler(KL)散度)可用于评估他们的距离。然而，这些估计中有许多是参数估计，或者需要中间密度估计。最近，通过在RKHS[22]中嵌入分布设计了一种非参数距离估计。Greutton等[23]介绍了用于基于对应RKHS距离的比较分布的MMD。让内核诱导的特征映射为$\\phi$。$\\{x_1,...,x_{n_1}\\}$和$\\{y_1,...,y_{n_2}\\}$间的MMD的经验估计是$MMD(X,Y)=\\left\\Vert\\frac{1}{n_1}\\sum_{i=1}^{n_1}\\phi(x_i)-\\frac{1}{n_2}\\sum_{i=1}^{n_2}\\phi(y_i)\\right\\Vert_{\\mathcal{H}}^2$,其中$\\Vert\\cdot\\Vert_{\\mathcal{H}}$是RKHS norm.因此，两个分布之间的距离就是RKHS中两个平均元素之间的距离。[22]可以证明，当RKHS是通用的，MMD将渐近接近于零当且仅当这两个分布是相同的。\n2) Hilbert-Schmidt独立性准则(Hilbert–Schmidt Independence Criterion,HSIC)：与MMD相关，HSIC[24]是一种简单又强大的用于测量集合X和Y之间的相关性的非参数准则。顾名思义，它计算了RKHS中交叉协方差算子(cross-covariance operator)的Hilbert-Schmidt范数。一个(有偏的)经验估计可以很容易地从相应的核矩阵中得到,as $HSIC(X,Y)=\\frac{1}{(n-1)^2}tr(HKHK_{yy})$,其中$K,K_{yy}$是分别定义在$X,Y$上的核矩阵.$H=I-\\frac{1}{n}\\mathbf{11}^T$是centering matrix,$n$是在$X$和$Y$中的样本数量.与MMD类似,还可以证明，如果RKHS是通用的,当且仅当$X$和$Y$独立[25]，则HSIC渐近地接近零。相反，一个巨大的HSIC值表明它具有很强的相关性。\n#### Embedding Using HSIC(基于HSIC的嵌入)\n在嵌入或降维过程中，通常希望保留本地数据几何形状，同时最大限度地将嵌入与可用的边信息(side information)(例如标签)对齐。例如，在有色最大方差展开(有色MVU)[7],局部几何是以目标嵌入$K$上的局部距离约束的形式捕获的。以核矩阵$K_{yy}$表示的边信息对齐采用HSIC准则进行测量。从数学上讲，这将导致以下SDP：\n$$\\begin{aligned}\n    \\underset{K\\succeq{0}}{\\max{tr(HKHK_{yy})}}\\ \\text{subject to constraints on }K.\\tag{1}\n\\end{aligned}$$\n特别是，(1)当没有side information时(i.e.,$K_{yy}=I$),减为MVU[26]\n### TCA\n如Secion II-A所提到的,大多数域自适应方法假设$\\mathcal{P}\\neq{\\mathcal{Q}}$,但是$P(Y_S|X_S)=P(Y_T|X_T)$.然而,在许多实际应用中,由于观测数据背后的噪声或动态因素，条件概率$P(Y|X)$也可能在多个域之间发生变化。在本文中，我们使用了一个较弱的假设，即$\\mathcal{P}\\neq{\\mathcal{Q}}$,但存在一个转换$\\phi$使得$P(\\phi(X_S))\\approx{P(\\phi(X_T))}$和$P(Y_S|\\phi(X_S))\\approx{P(Y_T|\\phi(X_T))}$成立.然后，标准的监督学习方法可以应用于映射的源域数据$\\phi(X_S)$,连同相应的标签$Y_S$,训练模型以用于映射的目标域数据$\\phi(X_T)$.\n一个关键问题是如何找到此转换$\\phi$.因为我们在目标域中没有标记的数据,$\\phi$不能通过直接最小化$P(Y_S|\\phi(X_S))$和$P(Y_T|\\phi(X_T))$之间的距离来学习.这里,我们提出通过学习$\\phi$使得: 1)边缘分布$P(\\phi(X_S))$与$P(\\phi(X_T))$ 之间的距离很小,并且 2)$\\phi(X_S)$和$\\phi(X_T)$保留了$X_S$和$X_T$的重要属性.然后，我们假设这样的$\\phi$满足$P(Y_S|\\phi(X_S))\\approx{P(Y_T|\\phi(X_T))}$。我们认为，在这种假设下，域自适应更现实,虽然也更有挑战性。最后，利用$\\phi(X_S)$和$Y_S$训练的分类器$f$对$\\phi(X_T)$进行预测。\n#### $P(\\phi(X_S)$与$P(\\phi(X_T)$之间的最小距离\n假设$\\phi$是由通用内核(universal kernel)诱导的特征映射。像Section II-B.1所示,两个分布之间的距离$\\mathcal{P}$和$\\mathcal{Q}$可以用两个区域的经验平均值之间的距离来经验地度量$Dist(X_S',X_T')=\\Vert\\frac{1}{n_1}\\sum_{i=1}^{n_1}\\phi(x_{S_i})-\\frac{1}{n_2}\\sum_{i=1}^{n_2}\\phi(x_{T_i})\\Vert_\\mathcal{H}^2$.因此,通过最小化该数量可以找到期望的非线性映射$\\phi$.然而,$\\phi$通常是高度非线性的，直接优化最小化与$\\phi$有关的数量可能会陷入差的局部极小值。\n1) MMDE: 不是明确找到非线性变换$\\phi$,我们首先回顾了一种基于降维的域自适应方法MMDE[10]。它使用非线性映射$\\phi$将源域和目标域数据嵌入到共享的低维潜在空间中,然后通过求解SDP来学习相应的核矩阵K。具体来说，将在嵌入空间中定义在源域、目标域和跨域数据上的Gram矩阵分别定义为$K_{S,S},K_{T,T}$和$K_{S,T}$。关键思路是学习\n$$\\begin{aligned}\n    K=\\begin{bmatrix}\n        K_{S,S}&K_{S,T}\\\\\n        K_{T,S}&K_{T,T}\\\\\n    \\end{bmatrix}\\in{\\R^{(n_1+n_2)\\times{(n_1+n_2)}}}\\tag{2}\n\\end{aligned}$$\n比如,通过最小化距离(测量的关于MMD)在所有数据上定义的核矩阵。(MMD)在投影源和目标域数据之间，同时最大化嵌入的数据方差。通过利用核技巧,可以证明，Section III-A中的MMD距离可以写成$tr(KL)$,其中$K=[\\phi(x_i)^T\\phi(x_j)]$\n$$\\begin{aligned}\n    L_{ij}=\\begin{cases}\n        \\frac{1}{n_1^2}&\\rm{if}\\ x_i,x_j\\in{X_S}\\\\\n        \\frac{1}{n_2^2}&\\rm{if}\\ x_i,x_j\\in{X_T}\\\\\n        -\\frac{1}{n_1n_2}&\\rm{others}\n    \\end{cases}\n\\end{aligned}$$\n然后,MMDE的目标函数可以被写成\n$$\\begin{aligned}\n    \\underset{K\\succeq{0}}{\\max}\\ tr(KL)-\\lambda{tr(K)}\\ \\text{subject to constraints on }K\\tag{3}\n\\end{aligned}$$\n其中,目标中的第一项最小化了分布的距离,第二个项使特征空间的方差最大化，$\\lambda\\ge{0}$是折衷参数.\n然而,MMDE有几个局限性.首先,它是直推式的,并且不能泛化为不可见的模式。其次,由此产生的内核学习问题必须通过昂贵的SDP求解器来解决.最后，为了构造$X_S'$和$X_T'$的低维表示，必须用PCA[10]对得到的$K$进行进一步的后处理。这可能会丢弃$K$中潜在的有用信息。\n2) 未知模式的参数核映射(Parametric Kernel Map for Unseen Patterns):在这一部分中，我们提出了一个有效的基于核特征提取的框架来寻找非线性映射$\\phi$.它避免了SDP的使用，从而提高了计算量。此外，所学习的核可以推广到样本外模式.此外，我们还提出了一种统一的核学习方法，而不是像MMDE那样采用两步学习的方法，它使用了显式的低秩表示。\n首先，注意(2)中的核矩阵$K$可以分解为$K=(KK^{−\\frac{1}{2}})(K^{-\\frac{1}{2}}K)$,这通常被称为经验核映射(empirical kernel map)[27]。考虑使用矩阵$\\widetilde{W}\\in{\\R^{(n_1+n_2)\\times{m}}}$将经验核映射特征转换为$m$维空间(其中$m\\ll{n_1+n_2}$).然后生成核矩阵\n$$\\begin{aligned}\n    \\widetilde{K}=(KK^{-\\frac{1}{2}}\\widetilde{W})(\\widetilde{W}^TK^{-\\frac{1}{2}}K)=KWW^TK\\tag{4}\n\\end{aligned}$$\n其中$W=K^{-\\frac{1}{2}}\\widetilde{W}$.特别是，任意两种模式$x_i$和$x_j$之间的相应内核计算是$\\widetilde{k}(x_i,x_j)=k_{x_i}^TWW^Tk_{x_j}$,其中$k_x=[k(x_1,x),...,k(x_{n_1+n_2},x)]^T\\in{\\R^{n_1+n_2}}$.因此，这个核$\\widetilde{k}$为非样本内核评估提供了一种很容易的参数形式.\n### 参考","tags":["paper"],"categories":["paper"]},{"title":"[paper]A Survey on Transfer Learning","url":"%2Fposts%2F9c8580d1%2F","content":"\n### Overview\n#### Domain\n$$\\begin{aligned}\n    \\mathcal{D}&=\\{\\mathcal{X},P(X)\\}\\\\\n    \\mathcal{T}&=\\{\\mathcal{Y},f(\\cdot)\\}\n\\end{aligned}$$\n$\\mathcal{D}$ 是一个特定的`domain(域)`,由`feature space(特征空间)`$\\mathcal{X}$,`marginal probability distribution(边缘分布)`$P(X)$构成,$X=\\{x_1,...,x_n\\}\\in{\\mathcal{X}}$.\\\n$\\mathcal{T}$ 是`task(任务)`, 由标签空间$\\mathcal{Y}$,目标预测函数$f(\\cdot)$构成,它不是由观测得到的,但是可以由训练数据($\\{x_i,y_i\\}$)学习得到,从概率的观点看,$f(x)$可以写成$P(y|x)$.\n#### Transfer Learning\n**定义:** Given a source domain $\\mathcal{D}_S$ and learning task $\\mathcal{T}_S$,a target domain $\\mathcal{D}_T$ and learning task $\\mathcal{T}_T$, `transfer learning` aims to help improve the learning of the target predictive function $f_T(\\cdot)$ in $\\mathcal{D}_T$ using the knowledge in $\\mathcal{D}_S$ and $\\mathcal{T}_S$, where $\\mathcal{D}_S\\neq{\\mathcal{D}_T}$, or $\\mathcal{T}_S\\neq{\\mathcal{T}_T}$.\n- 源域和目标域不同,则表示(1)源域和目标域的特征空间不同($\\mathcal{X}_S\\neq{\\mathcal{X}_T}$,对应着文档的两个数据集用不同的语言来描述的),或者(2)特征空间相同,但边缘概率分布不同($P(X_S)\\neq{P(X_T)}$, where $X_{S_i}\\in{\\mathcal{X}_S},X_{T_i}\\in{\\mathcal{X}_T}$,对应着源域文档和目标域文档针对着不同的主题).\n- 对于给定的域$\\mathcal{D}_S,\\mathcal{D}_T$,学习任务不同($\\mathcal{T}_S\\neq{\\mathcal{T}_T}$)则表示(1)源域和目标域的标签空间不同($\\mathcal{Y}_S\\neq{\\mathcal{Y}_T}$,对应着比如:源域有2种文档类别,而目标域有10种文档类别),或者(2)源域和目标域的条件概率分布不同($P(Y_S|X_S)\\neq{P(Y_T|X_T)}$, where $Y_{S_i}\\in{\\mathcal{Y}_S, Y_{T_i}\\in{\\mathcal{Y}_T}}$, 对应着比如源域和目标域的文档的类别很不均衡)\n- 当$\\mathcal{D}_S=\\mathcal{D}_T$(源域与目标域相同),且$\\mathcal{T}_S=\\mathcal{T}_T$(源任务与目标任务相同),则学习问题变成了`传统的机器学习问题`.\n#### 迁移学习分类\n关于迁移学习,我们有以下3类问题需要解决:\n##### (1)迁移什么?\n在域和任务之间,哪部分知识可以用作迁移.有些知识只针对在特定域或任务,有些知识可以是不同域之间所共有的,那么它们或许可以提高目标域或目标任务的性能.在发现了哪部分知识可以被迁移之后,学习算法需要被转变为知识的迁移,这对应着\"怎样迁移\"的问题.\n##### (2)怎样迁移?\n##### (3)何时迁移?\n\"怎样迁移\"提出了在哪种情形下,迁移的方法可以起作用.我们同样也对在哪种情形下,知识应当不被迁移感兴趣.在一些情形中,源域和目标域是不相关的,暴力迁移可能不成功.最糟时,可以会损害目标域的学习性能,这种情形被称为`负迁移`.大多数当前的迁移学习的工作都集中在\"迁移什么\"与\"怎样迁移\",默认源域和目标域是彼此相关的.然而,这样避免负迁移是一个重要的公开问题.\n基于迁移学习的定义,我们将传统机器学习和不同的迁移学习子集进行的总结,我们将迁移学习划分为3个子集,基于不同的场景下的源域和目标域,源任务和目标任务,划分为`归纳迁移学习(inductive transfer learning)`,`直推式迁移学习(transductive transfer learning)`和`无监督迁移学习(unsupervised transfer learning)`\n![](201907151036071.png)\n1) 在`归纳迁移学习`的子集中,目标任务与源任务不同(无视源域和目标域是否相同).在这种情况下,一些在目标域标记的数据被需要来归纳出一个目标预测模型$f_T(\\cdot)$,用于目标域.另外,根据源域中存在标记和未标记的数据的不同情形,我们可以将归纳迁移学习进一步划分为2类:\n   1) 源域有大部分的标记的数据.这种情形下,归纳迁移学习子集与多任务学习子集是相似的.然而,归纳迁移学习的目标在于通过从源任务传递知识来让目标任务获取高的性能.而多任务学习在于试图让源任务与目标任务同时学习.\n   2) 源域中没有标记的数据.在这种情形下,归纳迁移学习子集与自学习子集是相似的.在自学习子集中,源域和目标域的标签空间可能是不同的,这表明源域不能被直接使用.因此,这等同于归纳学习子集中源域中标记的数据不可获得.\n2) 在`直推式迁移学习`子集中,源任务和目标任务是相同的(尽管源域和目标域是不同的).在这种情形下,目标域中没有可以使用的标记了的数据,在源域中却有许多可以使用的标记了的数据.另外,根据源域和目标域不同的情形,我们可以将直推式迁移学习进一步划分为2类:\n   1) 源域和目标域的特征空间不同($\\mathcal{X}_S\\neq{\\mathcal{X}_T}$)\n   2) 源域和目标域的特征空间相同,$\\mathcal{X}_S=\\mathcal{X}_T$,但是输入数据的边缘概率分布是不同的,$P(X_S)\\neq{P(X_T)}$.后面这种情况与知识迁移在文本分类,样本选择偏差(sample selection bias)或co-variate shift下的域自适应是相关的,它们的假设是相似的.\n3) 最后,在`无监督迁移学习`子集中,与直推式迁移学习子集相似,目标任务与源任务不同,但却相关.然而,无监督学习专注于解决在目标域下的无监督学习任务,比如聚类,降维和密度估计(density estimation).这种情形下,在训练模型时,源域和目标域下都没有可以使用的标记了的数据.\n迁移学习不同的子集和相关区域的关系都被总结在Table 2和Figure 2中.\n![](201907151037062.png)\n![](201907151037233.png)\n![](201907151100484.png)\n![](201907151101025.png)\n### Inductive transfer learning(归纳迁移学习)\n`归纳迁移学习`目标在于通过使用$\\mathcal{D}_S$与$\\mathcal{T}_S$($\\mathcal{T}_S\\neq{\\mathcal{T}_T}$)中的知识,帮助改善目标域$\\mathcal{D}_T$中的目标预测函数$f_T(\\cdot)$\n迁移学习的方法在以上三个不同设定可以基于\"迁移什么\"来划分成为4个类别.\n#### 实例的知识迁移\n归纳迁移学习子集的实例迁移方法是直观且吸引人的:尽管源域数据不能被直接重用,这里仍然存在特定部分的数据可以与目标域中标记了的数据一起重用.\nDai 提出的提升算法,TrAdaBoost,是算法AdaBoost的扩展版,用于解决归纳迁移学习问题.TrAdaBoost假设源域和目标域使用相同的特征和标签集合,但是数据在源域和目标域的分布是不同的.另外,TrAdaBoost假设,由于源域和目标域不同的数据分布,一些源域数据可能对于目标域的学习是有帮助的,但有一些数据可能不但没有帮助,反而有害.该算法通过迭代的重赋源域数据的权重来降低\"bad\"的效果并鼓励\"good\"源域数据来对目标域贡献更多.对于每一轮的迭代,TrAdaBoost根据有权重的源域和目标域的数据来训练分类器.错误率只根据目标数据来计算.此外,TrAdaBoost使用了与AdaBoost相同的策略来更新目标域中不正确的分类样本,同时使用一个与AdaBoost不同的策略来更新源域中不正确的分类样本.\n#### 特征表示的知识迁移\n归纳迁移学习问题中的特征表示的迁移方法目的在于找到\"good\"特征表示来最小化域分歧和类别或回归模型错误.对于不同类型的源域数据找到\"good\"特征表示的策略是不同的.如果源域中有许多标记数据可用,监督学习方法能被用来构建一个特征表示.这与多任务学习中的共同特征学习是相似的.如果源域中没有标记的数据,无监督学习方法被提出来构建特征的表示.\n##### 有监督的特征构建\n基于归纳迁移学习子集的有监督特征构建方法与多任务学习中使用的相似.基本的想法是学习一个相关类别共享的低维的表示.另外,所学的新的表示也能够减少每个任务的类别或回归模型的错误.Argyriou[40]针对多任务学习提出了一个系数特征学习方法.在归纳迁移学习子集中,通过解决下式的优化问题,公共特征能够被学习\n$$\\begin{aligned}\n    &\\underset{A,U}{\\argmin}\\sum_{t\\in\\{T,S\\}}\\sum_{i=1}^{n_t}L(y_{t_i},\\lang{a_t},U^Tx_{t_i}\\rang)+\\gamma||A||_{2,1}^2\\tag{1}\\\\\n    &s.t.\\quad U\\in{O^d}\n\\end{aligned}$$\n在这个式子中,$S$和$T$分别表示源域和目标域中的任务.$A=[a_S,a_T]\\in{R^{d\\times{2}}}$是一个参数矩阵.$U$是$d\\times{d}$的正交阵(映射函数)用于将原始的高维数据映射到低维数据的表示.$A$的$(r,p)$-norm(范数)被定义为$||A||_{r,p}:=(\\sum_{i=1}^d||a^i||_r^p)^{\\frac{1}{p}}$.优化问题(1)同时estimates了低维表示$U^TX_T,U^TX_S$和参数$A$.优化问题(1)可以被进一步转变成为一个等价的凸优化公式并被有效解决.在后序工作,Argyriou[41]提出了一个基于矩阵的spectral regularization框架用于多任务结构的学习.\nLee[42]从一个类似的相关预测任务中提出了一个凸优化算法来同时学习meta-priors和特征权重. meta-priors能够在不同的任务中迁移. Jebara[43]通过在多任务学习中使用SVMs来选择特征. Ruckert[54]设计了一个基于核的方法来进行归纳迁移,目的在于找到一个合适的核来用于目标数据.\n##### 无监督的特征构建\n在[22]中, Raina提议应用稀疏编码[55],它是一个无监督特征构建方法,用于学习迁移学习中高等级的特征.这个方法的基本主意由两步构成.第一步,高级别的基向量$b=\\{b_1,b_2,...,b_s\\}$在源域数据上可以被学习到,这可以通过解决下式的优化问题(2)\n$$\\begin{aligned}\n    &\\min_{a,b}\\sum_{i}||x_{S_i}-\\sum_j{a_{S_i}^jb_j}||_2^2+\\beta||a_{S_i}||_1\\tag{2}\\\\\n    &s.t.\\qquad ||b_j||_2\\leq{1},\\forall{j\\in{1,...,s}}\n\\end{aligned}$$\n在这个方程中,$a_{S_i}^j$是一个新的表示,由基$b_j$,输入$x_{S_i}$和$\\beta$是一个系数来平衡特征构建项和正则化项.在学习了基向量$b$以后,第二步,一个优化算法(3)被应用在目标域数据基于基向量$b$来学习高级别特征\n$$\\begin{aligned}\n    a_{T_i}^*=\\underset{a_{T_i}}{\\argmin}||x_{T_i}-\\sum_j{a_{T_i}^jb_j||_2^2+\\beta||a_{T_i}||_1}\\tag{3}\n\\end{aligned}$$\n最后,判别算法可以被应用在$\\{a_{T_i}^*\\}'s$与对应的标签来学习分类或回归模型在目标域使用.该方法的一个缺点是，在优化问题(2)中，在源域上学习的所谓高级基向量可能不适合在目标域中使用。\n近年来，多种学习方法被广泛应用于迁移学习。在[44]中，Wang和Mahadevan提出了一种基于流行对齐没有correspondences的方法Procrustes analysis，它可以通过对齐流形跨域传递知识。\n#### 参数知识迁移\n大多数归纳迁移学习设置的参数传递方法都假设相关任务的个别模型应该共享某些参数或超参数的先验分布。本节介绍的大多数方法，包括正则化框架和分层贝叶斯框架，旨在在多任务学习下工作。然而，它们可以很容易地修改为迁移学习。如前所述，多任务学习试图同时和完美地学习源任务和目标任务，然而迁移学习的目的仅仅是通过利用源域数据来提高目标域的性能。因此，在多任务学习中，源和目标数据的损失函数的权重相同。相反,在转移学习中，不同域的损失函数中的权重可以是不同的。直观上,我们可以给目标域的损失函数分配更大的权重，以确保我们能够在目标域中获得更好的性能。\nLawrence和Platt[45]提出了一种基于高斯过程(GP)的有效算法MT-IVM来处理多任务学习案例。MT-IVM试图通过共享相同的GP prior来学习多个任务上高斯过程的参数。Bonilla等人[46]还对GP环境下的多任务学习进行了研究。作者提出在任务上使用自由形式协方差矩阵来建模任务间依赖关系，其中GP prior用于诱导任务之间的相关性。Schwayghofer等人[47]提出了一种分层贝叶斯框架(HB)和GP相结合的多任务学习方法。\n一些学者除了对GP模型的先验进行传递外，还提出了在正则化框架下进行支持向量机参数传递的方法。Evgeniou和Pontil[48]将Hb的概念借用到支持向量机中，用于多任务学习。该方法假设每个任务的SVMS中的参数w可以分为两个项。一个是任务上的公共项，另一个是特定于任务的项.在归纳迁移学习中,\n$$\\begin{aligned}\n    w_S=w_0+v_S\\ and\\ w_T=w_0+v_T\n\\end{aligned}$$\n$w_S$和$w_T$是SVMs的参数,分别用于源任务和目标学习任务.$w_0$是公共参数,$v_S$和$v_T$是针对源任务和目标任务特点的参数.通过假设$f_t=w_t\\cdot{x}$作为任务$t$的超平面,一个扩展版适用于多任务学习的SVMs可以被写成如下形式\n$$\\begin{aligned}\n    \\underset{w_0,v_t,\\xi_{t_i}}{min}&J(w_0,v_t,\\xi_{t_i})\\tag{4}\\\\\n    =&\\sum_{t\\in\\{S,T\\}}\\sum_{i=1}^{n_t}\\xi_{t_i}+\\frac{\\lambda_1}{2}\\sum_{t\\in\\{S,T\\}}||v_t||^2+\\lambda_2||w_0||^2\\\\\n    s.t.\\qquad &y_{t_i}(w_0+v_t)\\cdot{x_{t_i}}\\ge{1-\\xi_{t_i}},\\\\\n    &\\xi_{t_i}\\ge{0},\\ i\\in\\{1,2,...,n_t\\}\\ and\\ t\\in\\{S,T\\}\n\\end{aligned}$$\n通过解决以上的优化问题,我们可以同时学习到参数$w_0,v_S$和$v_T$.\n一些研究人员进一步研究了参数传递方法。Gao等[49]提出了一种局部加权的总体学习框架，用于组合多个模型用于迁移学习，根据模型在目标域中的每个测试样例的预测强度来动态分配权重.\n#### 关系知识迁移\n与其他三种情况不同，关系知识迁移方法处理关系域中的迁移学习问题,在那里的数据是非独立同分布的(non-i.i.d.),可以用网络数据和社会网络数据等多种关系来表示。这种方法并不按照传统的假设来假设认为从每个域提取的数据是独立的和相同分布的(i.i.d.)。它试图将数据之间的关系从源域转移到目标域。在此背景下，提出了统计关系学习技术来解决这些问题。\nMihalkova等人[50]提出了一种跨关系域用马尔可夫逻辑网络(MLN)传递关系知识的算法Tamar。MLN[56]是一种强大的形式主义，它将一阶逻辑的紧凑表达性与概率灵活性相结合，用于统计关系学习。\n### 直推式迁移学习\nArnold等[58]首次提出了术语直推式迁移学习，要求源和目标任务相同，尽管域可能不同。除了这些条件,它们还要求目标域中的所有未标记数据在训练时都可用，但我们认为可以放松这种条件；相反，在我们定义的直推式迁移学习子集中，我们只要求在训练时看到部分未标记的目标数据，以获得边缘概率.\n请注意，单词“transtive”有几个意思。在传统的机器学习环境中，直推式迁移学习[59]是指要求在训练时看到所有测试数据，而所学习的模型不能被重用在未来的数据。因此,当一些新的测试数据到来时,它们必须与所有现有数据一起分类。在我们对迁移学习的分类中，相反,我们使用了\"transductive\"这个词来强调在这种类型的迁移学习中，任务必须是相同的，并且在目标域中必须有一些无标记数据。\n\n### 参考","tags":["paper"],"categories":["paper"]},{"title":"[迁移学习简明手册]第一章 迁移学习基本概念","url":"%2Fposts%2Fed6ecb6d%2F","content":"\n### 解题思路 \n\n### 参考","tags":["迁移学习"],"categories":["迁移学习"]},{"title":"[深度学习-CS231n]第二讲 图像分类","url":"%2Fposts%2Fbf3c7c1c%2F","content":"### Assignment\n{% asset_img 2019071416145145.png %}\n\n```python\nimport numpy as np\nfrom os import listdir\n\n# K近邻类\nclass NearestNeighbor:\n    def __init__(self):\n        pass\n\n    # 训练\n    def train(self, X, y):\n        X = np.array(X)\n        y = np.array(y)\n        self.Xtr = X\n        self.ytr = y\n\n    # 预测\n    def predict(self, X):\n        X = np.array(X)\n        num_test = X.shape[0]\n        Ypred = np.zeros(num_test, dtype = self.ytr.dtype)\n\n        for i in range(num_test):\n            # 当前点的各个维度值与所有点的各个维度值进行相减取平方和,找出最小值(即距离最近)\n            distances = np.sum(np.abs(self.Xtr - X[i,:]), axis=1)\n            min_index = np.argmin(distances)\n            Ypred[i] = self.ytr[min_index]\n\n        return Ypred\n\n# 文件内容转矩阵\ndef file2matrix(filename):\n    fr = open(filename)\n    arrayOfLines = fr.readlines()\n    numberOfLines = len(arrayOfLines)\n    returnMat = np.zeros((numberOfLines, 3))\n    classLabelVector = []\n    index = 0\n    for line in arrayOfLines:\n        line = line.strip()\n        listFromLine = line.split('\\t')\n        returnMat[index, :] = listFromLine[0:3]\n        classLabelVector.append(int(listFromLine[-1]))\n        index += 1\n    return returnMat, classLabelVector\n\n# 图片转vector\ndef img2vector(filename):\n    returnVect = np.zeros((1, 1024))\n    fr = open(filename)\n    for i in range(32):\n        lineStr = fr.readline()\n        for j in range(32):\n            returnVect[0, i * 32 + j] = int(lineStr[j])\n    return returnVect\n\n# 获取训练数据集\ntrainFiles = listdir('trainingDigits')\nlenTrainFiles = len(trainFiles)\ntrainDataSet = np.zeros((lenTrainFiles, 1024))\ntrainLabels = []\nfor i in range(lenTrainFiles):\n    fileFullNameStr = trainFiles[i]\n    fileNameStr = fileFullNameStr.split('.')[0]\n    classNumber = int(fileNameStr.split('_')[0])\n    trainLabels.append(classNumber)\n    trainDataSet[i,:] = img2vector('trainingDigits/{}'.format(trainFiles[i]))\n\n# 获取测试数据集\ntestFiles = listdir('testDigits')\nlenTestFiles = len(testFiles)\ntestDataSet = np.zeros((lenTestFiles, 1024))\ntestLabels = []\nfor i in range(lenTestFiles):\n    fileFullNameStr = testFiles[i]\n    fileNameStr = fileFullNameStr.split('.')[0]\n    classNumber = int(fileNameStr.split('_')[0])\n    testLabels.append(classNumber)\n    testDataSet[i, :] = img2vector('testDigits/{}'.format(fileFullNameStr))\n\n# k近邻预测\nnear = NearestNeighbor()\nnear.train(trainDataSet, trainLabels)\ny = near.predict(testDataSet)\nerrorCount = np.sum(y != testLabels)\nprint(\"error rate:{0:f}\".format(errorCount*1.0/lenTestFiles))\n```\n### 参考","tags":["CS231n"],"categories":["深度学习"]},{"title":"[深度学习入门-基于Python的理论与实现]第8章 深度学习","url":"%2Fposts%2F43d40337%2F","content":"\n### 解题思路 \n\n### 参考","tags":["深度学习入门"],"categories":["深度学习"]},{"title":"[深度学习入门-基于Python的理论与实现]第7章 卷积神经网络","url":"%2Fposts%2Fd52bdd13%2F","content":"\n### 介绍\n卷积神经网络(Convolutional Neural Network,CNN),CNN被用于图像识别,语音识别等各种场合,在图像识别的比赛中,基于深度学习的方法几乎都以CNN为基础.\n\n#### 整体结构\nCNN中新出现了`卷积层(Convolution层)`和`池化层(Pooling层)`,之前介绍的神经网络中,相邻层的所有神经元之间都有连接,这称为`全连接(fully-connected)`,我们用Affine层实现了全连接层.\n如图7-1,堆叠了4层\"Affine-ReLU\"组合,第5层是Affine层,最后由Softmax层输出最终结果(概率).\n{% asset_img 2019071110321712.png %}\nCNN结果如下\n{% asset_img 2019071110335813.png %}\n#### 全连接层存在的问题\n全连接神经网络中使用了全连接层(Affine层),全连接层中,相邻层的神经元全部连接在一起,输出的数量可以任意决定.\n全连接层会\"忽视\"数据的形状.比如,输入数据是图像时,图像通常是高,长,通道方向上的3维形状.但是,向全连接层输入时,需要将3维数据拉平为1维数据.而图像是3维形状,这个形状中应该含有重要的空间信息.`但是,全连接层会忽视形状,将全部的输入数据作为相同的神经元(同一维度的神经元)处理,所以无法利用与形状相关的信息.`\n`而卷积层可以保持形状不变.当输入数据是图像时,卷积层会以3维数据的形式接收输入数据,并同样以3维数据的形式输出至下一层.`\nCNN中,有时将卷积层的输入输出数据称为`特征图(feature map)`.其中,卷积层的输入数据称为`输入特征图(input feature map)`,输出数据称为`输出特征图(output feature map)`.\n#### 卷积运算\n卷积运算相当于图像处理中的\"滤波器运算\".\n{% asset_img 2019071111112514.png %}\n在本例中,输入大小是(4,4),滤波器大小是(3,3),输出大小是(2,2).另外,有的文献中也会用\"核\"这个词来表示这里所说的\"滤波器\".\n如图7-4所示,将各个位置上滤波器的元素和输入的对应元素相乘,然后再求和(有时将这个计算称为`乘积累加运算`).\n{% asset_img 2019071111142015.png %}\n{% asset_img 2019071111165316.png %}\n#### 填充\n在进行卷积层的处理之前,有时要向输入数据的周围填入固定的数据(比如0等),这称为`填充(padding)`,是卷积运算中经常会用到的处理.使用填充主要是为了调整输出的大小.\n{% asset_img 2019071113452117.png %}\n#### 步幅\n应用滤波器的位置间隔称为`步幅(stride)`.\n{% asset_img 2019071113481218.png %}\n增大步幅后,输出大小会变小.而增大填充后,输出大小会变大.\n假设输入大小$(H,W)$,滤波器大小为$(FH,FW)$,输出大小为$(OH,OW)$,填充为$P$,步幅为$S$.\n$$\\begin{aligned}\n    OH&=\\frac{H+2P-FH}{S}+1\\\\\n    OW&=\\frac{W+2P-FW}{S}+1\n\\end{aligned}$$\n#### 3维数据的卷积运算\n之前的卷积运算的例子都是以有高,长方向的2维形状为对象的.但是图像是3维数据,除了高,长方向之外,还需要处理通道方向.\n{% asset_img 2019071113532319.png %}\n{% asset_img 2019071113533920.png %}\n需要注意的是,在3维数据的卷积运算中,输入数据和滤波器的通道数要设为相同的值.\n#### 结合方块思考\n{% asset_img 2019071113552821.png %}\n{% asset_img 2019071113592622.png %}\n图7-11中,通过应用$FN$个滤波器,输出特征图也生成了$FN$个.如果将这$FN$个特征图汇集在一起,就得到了形状为$(FN,OH,OW)$的方块.将这个方块传给下一层,就是$CNN$的处理流.\n{% asset_img 2019071114032223.png %}\n#### 批处理\n具体地讲,就是按$(N,C,H,W)$的顺序保存数据.比如,将图7-12中的处理改成对$N$个数据进行批处理时,数据的形状如图7-13所示.\n网络间传递的是4维数据,对这$N$个数据进行了卷积运算.也就是说,批处理将$N$次的处理汇总成了1次进行.\n{% asset_img 2019071114062324.png %}\n### 池化层\n池化是缩小高,长方向上的空间的运算.\n{% asset_img 2019071114131625.png %}\n`一般来说,池化的窗口大小会和步幅设定成相同的值.`(比如,$3\\times{3}$的窗口的步幅会设为3)\n> 除了$Max$池化之外,还有$Average$池化等.相对于$Max$池化是从目标区域中取出最大值,$Average$池化则是计算目标区域的平均值.在图像识别领域,主要使用$Max$池化.本书说到的池化,一般指$Max$池化.\n\n#### 池化层的特征\n- **没有要学习的参数**\n  池化层和卷积层不同,没有要学习的参数.池化只是从目标区域中取最大值(或者平均值),所以不存在要学习的参数.\n- **通道数不发生变化**\n  经过池化运算,输入数据和输出数据的通道数不会发生变化.如图7-15所示,计算是按通道独立进行的.\n  {% asset_img 2019071114200426.png %}\n- **对微小的位置变化具有鲁棒性(健壮)**\n  输入数据发生微小偏差时,池化仍会返回相同的结果.\n  {% asset_img 2019071114224227.png %}\n### 卷积层和池化层的实现\n#### 基于im2col的展开\n$im2col$是一个函数,将输入数据展开以适合滤波器(权重).如图7-17所示,对3维的输入数据应用$im2col$后,数据转换为2维矩阵(正确地讲,是把包含批数量的4维数据转换成了2维数据).\n{% asset_img 2019071114264028.png %}\n如图7-18所示,对于输入数据,将应用滤波器的区域(3维方块)横向展开为1列.$im2col$会在所有应用滤波器的地方进行这展开处理.\n{% asset_img 2019071114282429.png %}\n在图7-18中,为了便于观察,将步幅设置得很大,以使滤波器的应用区域不重叠.而在实际的卷积运算中,滤波器的应用区域几乎都是重叠的,在这种情况下,使用$im2col$展开后,展开后的元素个数会多于原方块的元素个数.因此,使用$im2col$的实现存在比普通的实现消耗更多内存的缺点.\n使用$im2col$展开输入数据后,之后就只需将卷积层的滤波器(权重)纵向展开为1列,并计算2个矩阵的乘积即可(参照图7-19).这和全连接层的$Affine$层进行的处理基本相同.\n如图7-19所示,基于$im2col$方式的输出结果是2维矩阵.因为$CNN$中数据会保存为4维数组,所以要将2维输出数据转换为合适的形状.`以上就是卷积层的实现流程`\n{% asset_img 2019071114382830.png %}\n#### 卷积层的实现\n本书提供了$im2col$函数,并将它作为黑盒(不关心内部实现)使用.实际用来实现输入数据的展开.(图7-19的大方块展开成很多行,每行对应一个录波器的匹配,可以想象成滤波器嵌入到大方块中)\n```python\ndef im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n    \"\"\"\n    Parameters\n    ----------\n    input_data : 由(数据量, 通道, 高, 长)的4维数组构成的输入数据\n    filter_h : 滤波器的高\n    filter_w : 滤波器的长\n    stride : 步幅\n    pad : 填充\n\n    Returns\n    -------\n    col : 2维数组\n    \"\"\"\n    N, C, H, W = input_data.shape\n    out_h = (H + 2*pad - filter_h)//stride + 1\n    out_w = (W + 2*pad - filter_w)//stride + 1\n\n    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n\n    for y in range(filter_h):\n        y_max = y + stride*out_h\n        for x in range(filter_w):\n            x_max = x + stride*out_w\n            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n\n    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n    return col\n```\n另外,卷积层在进行反向传播时需要用到$col2im$,代码如下\n```python\ndef col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n    \"\"\"\n\n    Parameters\n    ----------\n    col :\n    input_shape : 输入数据的形状（例：(10, 1, 28, 28)）\n    filter_h :\n    filter_w\n    stride\n    pad\n\n    Returns\n    -------\n\n    \"\"\"\n    N, C, H, W = input_shape\n    out_h = (H + 2*pad - filter_h)//stride + 1\n    out_w = (W + 2*pad - filter_w)//stride + 1\n    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n\n    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n    for y in range(filter_h):\n        y_max = y + stride*out_h\n        for x in range(filter_w):\n            x_max = x + stride*out_w\n            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n\n    return img[:, :, pad:H + pad, pad:W + pad]\n```\n\n{% asset_img 2019071115392931.png %}\n现在使用$im2col$与$col2im$来实现卷积层.\n```python\nclass Convolution:\n    def __init__(self, W, b, stride=1, pad=0):\n        self.W = W\n        self.b = b\n        self.stride = stride\n        self.pad = pad\n        \n        # 中间数据（backward时使用）\n        self.x = None   \n        self.col = None\n        self.col_W = None\n        \n        # 权重和偏置参数的梯度\n        self.dW = None\n        self.db = None\n\n    def forward(self, x):\n        FN, C, FH, FW = self.W.shape\n        N, C, H, W = x.shape\n        out_h = 1 + int((H + 2*self.pad - FH) / self.stride) # 计算输出层的高\n        out_w = 1 + int((W + 2*self.pad - FW) / self.stride) # 计算输出层的宽\n\n        # 将输入数据展开以适合滤波器,每行数据匹配一个滤波器\n        col = im2col(x, FH, FW, self.stride, self.pad)\n        # 将滤波器展开为FN行后转置(每列为一个滤波器的权重展开)\n        col_W = self.W.reshape(FN, -1).T\n\n        # 矩阵乘法,则为一次计算完N个输入数据,FN个滤波器元素乘积累加和的结果(详细参考图7-19)\n        out = np.dot(col, col_W) + self.b\n        # 将数据转换为合适的形状\n        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n\n        # 记录原始数据(未被展开),展开后的数据\n        self.x = x\n        self.col = col # 输入数据(展开为行)\n        self.col_W = col_W # 滤波器数据(展开为列)\n\n        return out\n\n    def backward(self, dout):\n        FN, C, FH, FW = self.W.shape\n        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n\n        self.db = np.sum(dout, axis=0)\n        self.dW = np.dot(self.col.T, dout)\n        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n\n        dcol = np.dot(dout, self.col_W.T)\n        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n\n        return dx\n```\n#### 池化层的实现\n池化层的实现和卷积层相同,都使用$im2col$展开输入数据.`不过,池化的情况下,在通道方向上是独立的,这一点和卷积层不同.`\n{% asset_img 2019071115551232.png %}\n这样展开后,只需对展开的矩阵求各行的最大值,并转换为合适的形状即可(图7-22).\n{% asset_img 2019071115571633.png %}\n现在使用$im2col$与$col2im$来实现池化层\n```python\nclass Pooling:\n    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n        self.pool_h = pool_h\n        self.pool_w = pool_w\n        self.stride = stride\n        self.pad = pad\n        \n        self.x = None\n        self.arg_max = None\n\n    def forward(self, x):\n        N, C, H, W = x.shape\n        # 计算输出层的高\n        out_h = int(1 + (H - self.pool_h) / self.stride)\n        # 计算输出层的宽\n        out_w = int(1 + (W - self.pool_w) / self.stride)\n\n        # 将N个数据的输入层展开并拼成列\n        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n        # 将N个数据的输入层展开并拼成列\n        col = col.reshape(-1, self.pool_h*self.pool_w)\n        # 选取每行的最大值索引\n        arg_max = np.argmax(col, axis=1)\n        # 选出每行最大值\n        out = np.max(col, axis=1)\n        # 将数据转换为合适的形状\n        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n\n        # 保存原始输入数据\n        self.x = x\n        # 保存展开后每行最大值的索引\n        self.arg_max = arg_max\n\n        return out\n\n    def backward(self, dout):\n        dout = dout.transpose(0, 2, 3, 1)\n        \n        pool_size = self.pool_h * self.pool_w\n        dmax = np.zeros((dout.size, pool_size))\n        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n        dmax = dmax.reshape(dout.shape + (pool_size,)) \n        \n        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n        \n        return dx\n```\n### CNN的实现\n我们已经实现了卷积层和池化层,现在来组合这些层,搭建进行手写数字识别的$CNN$\n{% asset_img 2019071116285234.png %}\n```python\nfrom collections import OrderedDict\nimport numpy as np\nfrom dataset.mnist import load_mnist\nimport matplotlib.pyplot as plt\nimport pickle\n\n\"\"\"optimizers\"\"\"\n\nclass SGD:\n    \"\"\"随机梯度下降法（Stochastic Gradient Descent）\"\"\"\n\n    def __init__(self, lr=0.01):\n        self.lr = lr\n\n    def update(self, params, grads):\n        for key in params.keys():\n            params[key] -= self.lr * grads[key]\n\n\nclass Momentum:\n    \"\"\"Momentum SGD\"\"\"\n\n    def __init__(self, lr=0.01, momentum=0.9):\n        self.lr = lr\n        self.momentum = momentum\n        self.v = None\n\n    def update(self, params, grads):\n        if self.v is None:\n            self.v = {}\n            for key, val in params.items():\n                self.v[key] = np.zeros_like(val)\n\n        for key in params.keys():\n            self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n            params[key] += self.v[key]\n\n\nclass Nesterov:\n    \"\"\"Nesterov's Accelerated Gradient (http://arxiv.org/abs/1212.0901)\"\"\"\n\n    def __init__(self, lr=0.01, momentum=0.9):\n        self.lr = lr\n        self.momentum = momentum\n        self.v = None\n\n    def update(self, params, grads):\n        if self.v is None:\n            self.v = {}\n            for key, val in params.items():\n                self.v[key] = np.zeros_like(val)\n\n        for key in params.keys():\n            self.v[key] *= self.momentum\n            self.v[key] -= self.lr * grads[key]\n            params[key] += self.momentum * self.momentum * self.v[key]\n            params[key] -= (1 + self.momentum) * self.lr * grads[key]\n\n\nclass AdaGrad:\n    \"\"\"AdaGrad\"\"\"\n\n    def __init__(self, lr=0.01):\n        self.lr = lr\n        self.h = None\n\n    def update(self, params, grads):\n        if self.h is None:\n            self.h = {}\n            for key, val in params.items():\n                self.h[key] = np.zeros_like(val)\n\n        for key in params.keys():\n            self.h[key] += grads[key] * grads[key]\n            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n\n\nclass RMSprop:\n    \"\"\"RMSprop\"\"\"\n\n    def __init__(self, lr=0.01, decay_rate=0.99):\n        self.lr = lr\n        self.decay_rate = decay_rate\n        self.h = None\n\n    def update(self, params, grads):\n        if self.h is None:\n            self.h = {}\n            for key, val in params.items():\n                self.h[key] = np.zeros_like(val)\n\n        for key in params.keys():\n            self.h[key] *= self.decay_rate\n            self.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]\n            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n\n\nclass Adam:\n    \"\"\"Adam (http://arxiv.org/abs/1412.6980v8)\"\"\"\n\n    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n        self.lr = lr\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.iter = 0\n        self.m = None\n        self.v = None\n\n    def update(self, params, grads):\n        if self.m is None:\n            self.m, self.v = {}, {}\n            for key, val in params.items():\n                self.m[key] = np.zeros_like(val)\n                self.v[key] = np.zeros_like(val)\n\n        self.iter += 1\n        lr_t = self.lr * np.sqrt(1.0 - self.beta2 ** self.iter) / (1.0 - self.beta1 ** self.iter)\n\n        for key in params.keys():\n            # self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]\n            # self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)\n            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n            self.v[key] += (1 - self.beta2) * (grads[key] ** 2 - self.v[key])\n\n            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n\n            # unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias\n            # unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias\n            # params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)\n\n\n\"\"\"Utils\"\"\"\ndef im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n    \"\"\"\n\n    Parameters\n    ----------\n    input_data : 由(数据量, 通道, 高, 长)的4维数组构成的输入数据\n    filter_h : 滤波器的高\n    filter_w : 滤波器的长\n    stride : 步幅\n    pad : 填充\n\n    Returns\n    -------\n    col : 2维数组\n    \"\"\"\n    N, C, H, W = input_data.shape\n    out_h = (H + 2*pad - filter_h)//stride + 1\n    out_w = (W + 2*pad - filter_w)//stride + 1\n\n    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n\n    for y in range(filter_h):\n        y_max = y + stride*out_h\n        for x in range(filter_w):\n            x_max = x + stride*out_w\n            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n\n    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n    return col\n\n\ndef col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n    \"\"\"\n\n    Parameters\n    ----------\n    col :\n    input_shape : 输入数据的形状（例：(10, 1, 28, 28)）\n    filter_h :\n    filter_w\n    stride\n    pad\n\n    Returns\n    -------\n\n    \"\"\"\n    N, C, H, W = input_shape\n    out_h = (H + 2*pad - filter_h)//stride + 1\n    out_w = (W + 2*pad - filter_w)//stride + 1\n    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n\n    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n    for y in range(filter_h):\n        y_max = y + stride*out_h\n        for x in range(filter_w):\n            x_max = x + stride*out_w\n            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n\n    return img[:, :, pad:H + pad, pad:W + pad]\n\n\"\"\"functions\"\"\"\n# 批量softmax\ndef softmax(x):\n    if x.ndim == 2:\n        x = x.T\n        x = x - np.max(x, axis=0)\n        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n        return y.T\n    x = x - np.max(x)\n    return np.exp(x) / np.sum(np.exp(x))\n\ndef cross_entropy_error(y, t):\n    if y.ndim == 1:\n        t = t.reshape(1, t.size)\n        y = y.reshape(1, y.size)\n\n    # 数据t是one-hot-vector的情况下，转换为正确解标签的索引\n    if t.size == y.size:\n        t = t.argmax(axis=1)\n\n    batch_size = y.shape[0]\n    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n\n\n\"\"\"Layers\"\"\"\n\n\nclass Affine:\n    def __init__(self, W, b):\n        self.W = W\n        self.b = b\n\n        self.x = None\n        self.original_x_shape = None\n        # 权重和偏置参数的导数\n        self.dW = None\n        self.db = None\n\n    def forward(self, x):\n        # 对应张量\n        self.original_x_shape = x.shape\n        x = x.reshape(x.shape[0], -1)\n        self.x = x\n\n        out = np.dot(self.x, self.W) + self.b\n\n        return out\n\n    def backward(self, dout):\n        dx = np.dot(dout, self.W.T)\n        self.dW = np.dot(self.x.T, dout)\n        self.db = np.sum(dout, axis=0)\n\n        dx = dx.reshape(*self.original_x_shape)  # 还原输入数据的形状（对应张量）\n        return dx\n\nclass Relu:\n    def __init__(self):\n        self.mask = None\n\n    def forward(self, x):\n        self.mask = (x <= 0)\n        out = x.copy()\n        out[self.mask] = 0\n        return out\n\n    def backward(self, dout):\n        dout[self.mask] = 0\n        dx = dout\n        return dx\n\nclass Sigmoid:\n    def __init__(self):\n        self.out = None\n\n    def forward(self, x):\n        out = sigmoid(x)\n        self.out = out\n        return out\n\n    def backward(self, dout):\n        dx = dout * (1.0 - self.out) * self.out\n        return dx\n\nclass BatchNormalizaiton:\n    def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):\n        self.gamma = gamma\n        self.beta = beta\n        self.momentum = momentum\n        self.input_shape = None  # Conv层的情况下为4维，全连接层的情况下为2维\n\n        # 测试时使用的平均值和方差\n        self.running_mean = running_mean\n        self.running_var = running_var\n\n        # backward时使用的中间数据\n        self.batch_size = None\n        self.xc = None\n        self.std = None\n        self.dgamma = None\n        self.dbeta = None\n\n    def forward(self, x, train_flg=True):\n        self.input_shape = x.shape\n        if x.ndim != 2:\n            N, C, H, W = x.shape\n            x = x.reshape(N, -1)\n\n        out = self.__forward(x, train_flg)\n\n        return out.reshape(*self.input_shape)\n\n    def __forward(self, x, train_flg):\n        if self.running_mean is None:\n            N, D = x.shape\n            self.running_mean = np.zeros(D)\n            self.running_var = np.zeros(D)\n\n        if train_flg:\n            mu = x.mean(axis=0)\n            xc = x - mu\n            var = np.mean(xc ** 2, axis=0)\n            std = np.sqrt(var + 10e-7)\n            xn = xc / std\n\n            self.batch_size = x.shape[0]\n            self.xc = xc\n            self.xn = xn\n            self.std = std\n            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mu\n            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * var\n        else:\n            xc = x - self.running_mean\n            xn = xc / ((np.sqrt(self.running_var + 10e-7)))\n\n        out = self.gamma * xn + self.beta\n        return out\n\n    def backward(self, dout):\n        if dout.ndim != 2:\n            N, C, H, W = dout.shape\n            dout = dout.reshape(N, -1)\n\n        dx = self.__backward(dout)\n\n        dx = dx.reshape(*self.input_shape)\n        return dx\n\n    def __backward(self, dout):\n        dbeta = dout.sum(axis=0)\n        dgamma = np.sum(self.xn * dout, axis=0)\n        dxn = self.gamma * dout\n        dxc = dxn / self.std\n        dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis=0)\n        dvar = 0.5 * dstd / self.std\n        dxc += (2.0 / self.batch_size) * self.xc * dvar\n        dmu = np.sum(dxc, axis=0)\n        dx = dxc - dmu / self.batch_size\n\n        self.dgamma = dgamma\n        self.dbeta = dbeta\n\n        return dx\n\nclass Dropout:\n    def __init__(self, dropout_ratio=0.5):\n        self.dropout_ratio = dropout_ratio\n        self.mask = None\n\n    def forward(self, x, train_flg=True):\n        if train_flg:\n            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n            return x * self.mask\n        else:\n            return x * (1.0 - self.dropout_ratio)\n\n    def backward(self, dout):\n        return dout * self.mask\n\n\nclass SoftmaxWithLoss:\n    def __init__(self):\n        self.loss = None\n        self.y = None\n        self.t = None\n\n    def forward(self, x, t):\n        self.t = t\n        self.y = softmax(x)\n        self.loss = cross_entropy_error(self.y, self.t)\n        return self.loss\n\n    def backward(self, dout=1):\n        batch_size = self.t.shape[0]\n        if self.t.size == self.y.size:\n            dx = (self.y - self.t) / batch_size\n        else:\n            dx = self.y.copy()\n            dx[np.arange(batch_size), self.t] -= 1\n            dx = dx / batch_size\n        return dx\n\n\nclass Convolution:\n    def __init__(self, W, b, stride=1, pad=0):\n        self.W = W\n        self.b = b\n        self.stride = stride\n        self.pad = pad\n\n        # 中间数据（backward时使用）\n        self.x = None\n        self.col = None\n        self.col_W = None\n\n        # 权重和偏置参数的梯度\n        self.dW = None\n        self.db = None\n\n    def forward(self, x):\n        FN, C, FH, FW = self.W.shape\n        N, C, H, W = x.shape\n        out_h = 1 + int((H + 2 * self.pad - FH) / self.stride)\n        out_w = 1 + int((W + 2 * self.pad - FW) / self.stride)\n\n        col = im2col(x, FH, FW, self.stride, self.pad)\n        col_W = self.W.reshape(FN, -1).T\n\n        out = np.dot(col, col_W) + self.b\n        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n\n        self.x = x\n        self.col = col\n        self.col_W = col_W\n\n        return out\n\n    def backward(self, dout):\n        FN, C, FH, FW = self.W.shape\n        dout = dout.transpose(0, 2, 3, 1).reshape(-1, FN)\n\n        self.db = np.sum(dout, axis=0)\n        self.dW = np.dot(self.col.T, dout)\n        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n\n        dcol = np.dot(dout, self.col_W.T)\n        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n\n        return dx\n\n\nclass Pooling:\n    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n        self.pool_h = pool_h\n        self.pool_w = pool_w\n        self.stride = stride\n        self.pad = pad\n\n        self.x = None\n        self.arg_max = None\n\n    def forward(self, x):\n        N, C, H, W = x.shape\n        out_h = int(1 + (H - self.pool_h) / self.stride)\n        out_w = int(1 + (W - self.pool_w) / self.stride)\n\n        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n        col = col.reshape(-1, self.pool_h * self.pool_w)\n\n        arg_max = np.argmax(col, axis=1)\n        out = np.max(col, axis=1)\n        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n\n        self.x = x\n        self.arg_max = arg_max\n\n        return out\n\n    def backward(self, dout):\n        dout = dout.transpose(0, 2, 3, 1)\n\n        pool_size = self.pool_h * self.pool_w\n        dmax = np.zeros((dout.size, pool_size))\n        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n        dmax = dmax.reshape(dout.shape + (pool_size,))\n\n        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n\n        return dx\n\n\nclass SimpleConvNet:\n    \"\"\"简单的ConvNet\n\n    conv - relu - pool - affine - relu - affine - softmax\n\n    Parameters\n    ----------\n    input_size : 输入大小（MNIST的情况下为784）\n    hidden_size_list : 隐藏层的神经元数量的列表（e.g. [100, 100, 100]）\n    output_size : 输出大小（MNIST的情况下为10）\n    activation : 'relu' or 'sigmoid'\n    weight_init_std : 指定权重的标准差（e.g. 0.01）\n        指定'relu'或'he'的情况下设定“He的初始值”\n        指定'sigmoid'或'xavier'的情况下设定“Xavier的初始值”\n    \"\"\"\n    def __init__(self, input_dim=(1, 28, 28),\n                 conv_param={'filter_num' :30, 'filter_size' :5, 'pad' :0, 'stride' :1},\n                 hidden_size=100, output_size=10, weight_init_std=0.01):\n        filter_num = conv_param['filter_num']\n        filter_size = conv_param['filter_size']\n        filter_pad = conv_param['pad']\n        filter_stride = conv_param['stride']\n        input_size = input_dim[1]\n        conv_output_size = (input_size - filter_size + 2* filter_pad) / filter_stride + 1\n        pool_output_size = int(filter_num * (conv_output_size / 2) * (conv_output_size / 2))\n\n        # 初始化权重\n        self.params = {}\n        self.params['W1'] = weight_init_std * \\\n                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n        self.params['b1'] = np.zeros(filter_num)\n        self.params['W2'] = weight_init_std * \\\n                            np.random.randn(pool_output_size, hidden_size)\n        self.params['b2'] = np.zeros(hidden_size)\n        self.params['W3'] = weight_init_std * \\\n                            np.random.randn(hidden_size, output_size)\n        self.params['b3'] = np.zeros(output_size)\n\n        # 生成层\n        self.layers = OrderedDict()\n        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n                                           conv_param['stride'], conv_param['pad'])\n        self.layers['Relu1'] = Relu()\n        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n        self.layers['Relu2'] = Relu()\n        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n\n        self.last_layer = SoftmaxWithLoss()\n\n    def predict(self, x):\n        for layer in self.layers.values():\n            x = layer.forward(x)\n\n        return x\n\n    def loss(self, x, t):\n        \"\"\"求损失函数\n        参数x是输入数据、t是教师标签\n        \"\"\"\n        y = self.predict(x)\n        return self.last_layer.forward(y, t)\n\n    def accuracy(self, x, t, batch_size=100):\n        if t.ndim != 1: t = np.argmax(t, axis=1)\n\n        acc = 0.0\n\n        for i in range(int(x.shape[0] / batch_size)):\n            tx = x[i * batch_size:(i + 1) * batch_size]\n            tt = t[i * batch_size:(i + 1) * batch_size]\n            y = self.predict(tx)\n            y = np.argmax(y, axis=1)\n            acc += np.sum(y == tt)\n\n        return acc / x.shape[0]\n\n    def numerical_gradient(self, x, t):\n        \"\"\"求梯度（数值微分）\n\n        Parameters\n        ----------\n        x : 输入数据\n        t : 教师标签\n\n        Returns\n        -------\n        具有各层的梯度的字典变量\n            grads['W1']、grads['W2']、...是各层的权重\n            grads['b1']、grads['b2']、...是各层的偏置\n        \"\"\"\n        loss_w = lambda w: self.loss(x, t)\n\n        grads = {}\n        for idx in (1, 2, 3):\n            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n\n        return grads\n\n    def gradient(self, x, t):\n        \"\"\"求梯度（误差反向传播法）\n\n        Parameters\n        ----------\n        x : 输入数据\n        t : 教师标签\n\n        Returns\n        -------\n        具有各层的梯度的字典变量\n            grads['W1']、grads['W2']、...是各层的权重\n            grads['b1']、grads['b2']、...是各层的偏置\n        \"\"\"\n        # forward\n        self.loss(x, t)\n\n        # backward\n        dout = 1\n        dout = self.last_layer.backward(dout)\n\n        layers = list(self.layers.values())\n        layers.reverse()\n        for layer in layers:\n            dout = layer.backward(dout)\n\n        # 设定\n        grads = {}\n        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n\n        return grads\n\n    def save_params(self, file_name=\"params.pkl\"):\n        params = {}\n        for key, val in self.params.items():\n            params[key] = val\n        with open(file_name, 'wb') as f:\n            pickle.dump(params, f)\n\n    def load_params(self, file_name=\"params.pkl\"):\n        with open(file_name, 'rb') as f:\n            params = pickle.load(f)\n        for key, val in params.items():\n            self.params[key] = val\n\n        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n            self.layers[key].W = self.params['W' + str(i + 1)]\n            self.layers[key].b = self.params['b' + str(i + 1)]\n\n\n\"\"\"Optimizer\"\"\"\nclass Trainer:\n    \"\"\"进行神经网络的训练的类\n    \"\"\"\n\n    def __init__(self, network, x_train, t_train, x_test, t_test,\n                 epochs=20, mini_batch_size=100,\n                 optimizer='SGD', optimizer_param={'lr': 0.01},\n                 evaluate_sample_num_per_epoch=None, verbose=True):\n        self.network = network\n        self.verbose = verbose\n        self.x_train = x_train\n        self.t_train = t_train\n        self.x_test = x_test\n        self.t_test = t_test\n        self.epochs = epochs\n        self.batch_size = mini_batch_size\n        self.evaluate_sample_num_per_epoch = evaluate_sample_num_per_epoch\n\n        # optimzer\n        optimizer_class_dict = {'sgd': SGD, 'momentum': Momentum, 'nesterov': Nesterov,\n                                'adagrad': AdaGrad, 'rmsprpo': RMSprop, 'adam': Adam}\n        self.optimizer = optimizer_class_dict[optimizer.lower()](**optimizer_param)\n\n        self.train_size = x_train.shape[0]\n        self.iter_per_epoch = max(self.train_size / mini_batch_size, 1)\n        self.max_iter = int(epochs * self.iter_per_epoch)\n        self.current_iter = 0\n        self.current_epoch = 0\n\n        self.train_loss_list = []\n        self.train_acc_list = []\n        self.test_acc_list = []\n\n    def train_step(self):\n        batch_mask = np.random.choice(self.train_size, self.batch_size)\n        x_batch = self.x_train[batch_mask]\n        t_batch = self.t_train[batch_mask]\n\n        grads = self.network.gradient(x_batch, t_batch)\n        self.optimizer.update(self.network.params, grads)\n\n        loss = self.network.loss(x_batch, t_batch)\n        self.train_loss_list.append(loss)\n        if self.verbose: print(\"train loss:\" + str(loss))\n\n        if self.current_iter % self.iter_per_epoch == 0:\n            self.current_epoch += 1\n\n            x_train_sample, t_train_sample = self.x_train, self.t_train\n            x_test_sample, t_test_sample = self.x_test, self.t_test\n            if not self.evaluate_sample_num_per_epoch is None:\n                t = self.evaluate_sample_num_per_epoch\n                x_train_sample, t_train_sample = self.x_train[:t], self.t_train[:t]\n                x_test_sample, t_test_sample = self.x_test[:t], self.t_test[:t]\n\n            train_acc = self.network.accuracy(x_train_sample, t_train_sample)\n            test_acc = self.network.accuracy(x_test_sample, t_test_sample)\n            self.train_acc_list.append(train_acc)\n            self.test_acc_list.append(test_acc)\n\n            if self.verbose: print(\n                \"=== epoch:\" + str(self.current_epoch) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(\n                    test_acc) + \" ===\")\n        self.current_iter += 1\n\n    def train(self):\n        for i in range(self.max_iter):\n            self.train_step()\n\n        test_acc = self.network.accuracy(self.x_test, self.t_test)\n\n        if self.verbose:\n            print(\"=============== Final Test Accuracy ===============\")\n            print(\"test acc:\" + str(test_acc))\n\n\n# 读入数据\n(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n\n# 处理花费时间较长的情况下减少数据\nx_train, t_train = x_train[:5000], t_train[:5000]\nx_test, t_test = x_test[:1000], t_test[:1000]\n\nmax_epochs = 20\n\nnetwork = SimpleConvNet(input_dim=(1, 28, 28),\n                        conv_param={'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n                        hidden_size=100, output_size=10, weight_init_std=0.01)\n\ntrainer = Trainer(network, x_train, t_train, x_test, t_test,\n                  epochs=max_epochs, mini_batch_size=100,\n                  optimizer='Adam', optimizer_param={'lr': 0.001},\n                  evaluate_sample_num_per_epoch=1000)\ntrainer.train()\n\n# 保存参数\nnetwork.save_params(\"params.pkl\")\nprint(\"Saved Network Parameters!\")\n\n# 绘制图形\nmarkers = {'train': 'o', 'test': 's'}\nx = np.arange(max_epochs)\nplt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\nplt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\nplt.xlabel(\"epochs\")\nplt.ylabel(\"accuracy\")\nplt.ylim(0, 1.0)\nplt.legend(loc='lower right')\nplt.show()\n```\n运行结果如下\n{% asset_img 2019071120121335.png %}\n### CNN的可视化\n```python\nclass SimpleConvNet:\n    \"\"\"简单的ConvNet\n\n    conv - relu - pool - affine - relu - affine - softmax\n    \n    Parameters\n    ----------\n    input_size : 输入大小（MNIST的情况下为784）\n    hidden_size_list : 隐藏层的神经元数量的列表（e.g. [100, 100, 100]）\n    output_size : 输出大小（MNIST的情况下为10）\n    activation : 'relu' or 'sigmoid'\n    weight_init_std : 指定权重的标准差（e.g. 0.01）\n        指定'relu'或'he'的情况下设定“He的初始值”\n        指定'sigmoid'或'xavier'的情况下设定“Xavier的初始值”\n    \"\"\"\n    def __init__(self, input_dim=(1, 28, 28), \n                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n                 hidden_size=100, output_size=10, weight_init_std=0.01):\n        filter_num = conv_param['filter_num']\n        filter_size = conv_param['filter_size']\n        filter_pad = conv_param['pad']\n        filter_stride = conv_param['stride']\n        input_size = input_dim[1]\n        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n\n        # 初始化权重\n        self.params = {}\n        self.params['W1'] = weight_init_std * \\\n                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n        self.params['b1'] = np.zeros(filter_num)\n        self.params['W2'] = weight_init_std * \\\n                            np.random.randn(pool_output_size, hidden_size)\n        self.params['b2'] = np.zeros(hidden_size)\n        self.params['W3'] = weight_init_std * \\\n                            np.random.randn(hidden_size, output_size)\n        self.params['b3'] = np.zeros(output_size)\n\n        # 生成层\n        self.layers = OrderedDict()\n        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n                                           conv_param['stride'], conv_param['pad'])\n        self.layers['Relu1'] = Relu()\n        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n        self.layers['Relu2'] = Relu()\n        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n\n        self.last_layer = SoftmaxWithLoss()\n\n    def predict(self, x):\n        for layer in self.layers.values():\n            x = layer.forward(x)\n\n        return x\n\n    def loss(self, x, t):\n        \"\"\"求损失函数\n        参数x是输入数据、t是教师标签\n        \"\"\"\n        y = self.predict(x)\n        return self.last_layer.forward(y, t)\n\n    def accuracy(self, x, t, batch_size=100):\n        if t.ndim != 1 : t = np.argmax(t, axis=1)\n        \n        acc = 0.0\n        \n        for i in range(int(x.shape[0] / batch_size)):\n            tx = x[i*batch_size:(i+1)*batch_size]\n            tt = t[i*batch_size:(i+1)*batch_size]\n            y = self.predict(tx)\n            y = np.argmax(y, axis=1)\n            acc += np.sum(y == tt) \n        \n        return acc / x.shape[0]\n\n    def numerical_gradient(self, x, t):\n        \"\"\"求梯度（数值微分）\n\n        Parameters\n        ----------\n        x : 输入数据\n        t : 教师标签\n\n        Returns\n        -------\n        具有各层的梯度的字典变量\n            grads['W1']、grads['W2']、...是各层的权重\n            grads['b1']、grads['b2']、...是各层的偏置\n        \"\"\"\n        loss_w = lambda w: self.loss(x, t)\n\n        grads = {}\n        for idx in (1, 2, 3):\n            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n\n        return grads\n\n    def gradient(self, x, t):\n        \"\"\"求梯度（误差反向传播法）\n\n        Parameters\n        ----------\n        x : 输入数据\n        t : 教师标签\n\n        Returns\n        -------\n        具有各层的梯度的字典变量\n            grads['W1']、grads['W2']、...是各层的权重\n            grads['b1']、grads['b2']、...是各层的偏置\n        \"\"\"\n        # forward\n        self.loss(x, t)\n\n        # backward\n        dout = 1\n        dout = self.last_layer.backward(dout)\n\n        layers = list(self.layers.values())\n        layers.reverse()\n        for layer in layers:\n            dout = layer.backward(dout)\n\n        # 设定\n        grads = {}\n        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n\n        return grads\n        \n    def save_params(self, file_name=\"params.pkl\"):\n        params = {}\n        for key, val in self.params.items():\n            params[key] = val\n        with open(file_name, 'wb') as f:\n            pickle.dump(params, f)\n\n    def load_params(self, file_name=\"params.pkl\"):\n        with open(file_name, 'rb') as f:\n            params = pickle.load(f)\n        for key, val in params.items():\n            self.params[key] = val\n\n        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n            self.layers[key].W = self.params['W' + str(i+1)]\n            self.layers[key].b = self.params['b' + str(i+1)]\n\n\ndef filter_show(filters, nx=8, margin=3, scale=10):\n    \"\"\"\n    c.f. https://gist.github.com/aidiary/07d530d5e08011832b12#file-draw_weight-py\n    \"\"\"\n    FN, C, FH, FW = filters.shape\n    ny = int(np.ceil(FN / nx))\n\n    fig = plt.figure()\n    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n\n    for i in range(FN):\n        ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])\n        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n    plt.show()\n\n\nnetwork = SimpleConvNet()\n# 随机进行初始化后的权重\nfilter_show(network.params['W1'])\n\n# 学习后的权重\nnetwork.load_params(\"params.pkl\")\nfilter_show(network.params['W1'])\n```\n运行结果如下:\n{% asset_img 2019071208394136.png %}\n{% asset_img 2019071208405338.png %}\n#### 基于分层结构的信息提取\n{% asset_img 2019071208420339.png %}\n#### 具有代表性的CNN\n##### LeNet\n{% asset_img 2019071208423840.png %}\n##### AlexNet\n{% asset_img 2019071208430241.png %}\n### 参考\n1. [深度学习入门-基于Python的理论与实现](https://book.douban.com/subject/30270959/)\n","tags":["深度学习入门"],"categories":["深度学习"]},{"title":"[深度学习入门-基于Python的理论与实现]第6章 与学习相关的技巧","url":"%2Fposts%2Fe17ecfdd%2F","content":"### 参数更新\n#### 介绍\n神经网络的学习的目的是找到使损失函数的值尽可能小的参数.这是寻找最优化参数的问题,解决这个问题的过程称为`最优化(optimization)`.\n\n#### SGD(Stochastic Gradient Descent)\n公式如下\n$$\\begin{aligned}\n    W\\leftarrow{W-\\eta{\\frac{\\partial{L}}{\\partial{W}}}}\n\\end{aligned}$$\n代码实现\n```python\nclass SGD:\n    \"\"\"随机梯度下降法（Stochastic Gradient Descent）\"\"\"\n    def __init__(self, lr=0.01):\n        self.lr = lr\n        \n    def update(self, params, grads):\n        for key in params.keys():\n            params[key] -= self.lr * grads[key] \n```\n缺点: 如果函数的形状非均向(anisotropic),比如呈延伸状,搜索的路径就会非常低效.\n{% asset_img 201907081853164.png %}\n#### Momentum\n公式如下\n$$\\begin{aligned}\n    \\mathbf{v}&\\leftarrow{\\alpha\\mathbf{v}-\\eta\\frac{\\partial{L}}{\\partial{W}}}\\\\\n    W&\\leftarrow{W+\\mathbf{v}}\n\\end{aligned}$$\n代码实现\n```python\nclass Momentum:\n    \"\"\"Momentum SGD\"\"\"\n    def __init__(self, lr=0.01, momentum=0.9):\n        self.lr = lr\n        self.momentum = momentum\n        self.v = None\n        \n    def update(self, params, grads):\n        if self.v is None:\n            self.v = {}\n            for key, val in params.items():\n                self.v[key] = np.zeros_like(val)\n                \n        for key in params.keys():\n            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key] \n            params[key] += self.v[key]\n```\n和SGD时的情形相比,可以更快地朝$x$轴方向靠近,减弱\"之\"字形的变动程度.\n{% asset_img 201907081853575.png %}\n#### AdaGrad\n公式如下\n$$\\begin{aligned}\n    \\mathbf{h}&\\leftarrow{\\mathbf{h}+\\frac{\\partial{L}}{\\partial{W}}\\odot\\frac{\\partial{L}}{\\partial{W}}}\\\\\n    W&\\leftarrow{W-\\eta\\frac{1}{\\sqrt{\\mathbf{h}}}\\frac{\\partial{L}}{\\partial{W}}}\n\\end{aligned}$$\n$\\odot$表示对应矩阵元素的乘法\n代码实现\n```python\nclass AdaGrad:\n    \"\"\"AdaGrad\"\"\"\n    def __init__(self, lr=0.01):\n        self.lr = lr\n        self.h = None\n        \n    def update(self, params, grads):\n        if self.h is None:\n            self.h = {}\n            for key, val in params.items():\n                self.h[key] = np.zeros_like(val)\n            \n        for key in params.keys():\n            self.h[key] += grads[key] * grads[key]\n            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n```\n{% asset_img 201907081900196.png %}\n函数的取值高效地向着最小值移动.由于$y$轴方向上的梯度较大,因此刚开始变动较大,但是后面会根据这个较大的变动按比例进行调整,减小更新的步伐.因此,$y$轴方向上的更新程度被减弱,\"之\"字形的变动程度有所衰减.\n#### Adam\n将AdaGrad与Momentum融合在一起.\n代码实现\n```python\nclass Adam:\n    \"\"\"Adam (http://arxiv.org/abs/1412.6980v8)\"\"\"\n    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n        self.lr = lr\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.iter = 0\n        self.m = None\n        self.v = None\n        \n    def update(self, params, grads):\n        if self.m is None:\n            self.m, self.v = {}, {}\n            for key, val in params.items():\n                self.m[key] = np.zeros_like(val)\n                self.v[key] = np.zeros_like(val)\n        \n        self.iter += 1\n        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \n        \n        for key in params.keys():\n            #self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]\n            #self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)\n            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n            \n            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n            \n            #unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias\n            #unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias\n            #params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)\n```\n{% asset_img 201907081903557.png %}\nAdam是2015年提出的新方法.它的理论有些复杂,直观地讲,就是融合了Momentum和AdaGrad的方法,通过组合前面两个方法的优点,有望实现参数空间的高效搜索.此外,进行超参数的\"偏置校正\"也是Adam的特征.看图中,基于Adam的更新过程就像小球在碗中滚动一样.虽然Momentum也有类似的移动,但是相比之下,Adam的小球左右摇晃的程度有所减轻.这得益于学习的更新程度被适当地调整了.\n### 权重的初始值\n#### 为什么不能设为0值?\n因为在误差反向传播法中,所有的权重值都会进行相同的更新.\n#### 隐藏层的激活值的分布\n(待补充...)\n不明白,留待看完CS231n视频再来写\n### 总体代码\n```python\nfrom collections import OrderedDict\nimport numpy as np\nfrom dataset.mnist import load_mnist\n\n# 批量softmax\ndef softmax(x):\n    if x.ndim == 2:\n        x = x.T\n        x = x - np.max(x, axis=0)\n        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n        return y.T\n    x = x - np.max(x)\n    return np.exp(x) / np.sum(np.exp(x))\n\ndef cross_entropy_error(y, t):\n    if y.ndim == 1:\n        t = t.reshape(1, t.size)\n        y = y.reshape(1, y.size)\n\n    # 数据t是one-hot-vector的情况下，转换为正确解标签的索引\n    if t.size == y.size:\n        t = t.argmax(axis=1)\n\n    batch_size = y.shape[0]\n    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n\n\nclass Affine:\n    def __init__(self, W, b):\n        self.W = W\n        self.b = b\n        self.dW = None\n        self.db = None\n        self.x = None\n\n    def forward(self, x):\n        self.x = x\n        out = np.dot(self.x, self.W) + self.b\n        return out\n\n    def backward(self, dout):\n        dx = np.dot(dout,self.W.T)\n        self.dW = np.dot(self.x.T, dout)\n        self.db = np.sum(dout,axis=0)\n        return dx\n\nclass Relu:\n    def __init__(self):\n        self.mask = None\n\n    def forward(self, x):\n        self.mask = (x <= 0)\n        out = x.copy()\n        out[self.mask] = 0\n        return out\n\n    def backward(self, dout):\n        dout[self.mask] = 0\n        dx = dout\n        return dx\n\nclass Sigmoid:\n    def __init__(self, x):\n        self.out = None\n\n    def forward(self, x):\n        out = 1.0 / (1.0 + np.exp(-x))\n        self.out = out\n        return out\n\n    def backward(self, dout):\n        dx = dout * self.out * (1.0 - self.out)\n        return dx\n\nclass BatchNormalizaiton:\n    def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):\n        self.gamma = gamma\n        self.beta = beta\n        self.momentum = momentum\n        self.input_shape = None  # Conv层的情况下为4维，全连接层的情况下为2维\n\n        # 测试时使用的平均值和方差\n        self.running_mean = running_mean\n        self.running_var = running_var\n\n        # backward时使用的中间数据\n        self.batch_size = None\n        self.xc = None\n        self.std = None\n        self.dgamma = None\n        self.dbeta = None\n\n    def forward(self, x, train_flg=True):\n        self.input_shape = x.shape\n        if x.ndim != 2:\n            N, C, H, W = x.shape\n            x = x.reshape(N, -1)\n\n        out = self.__forward(x, train_flg)\n\n        return out.reshape(*self.input_shape)\n\n    def __forward(self, x, train_flg):\n        if self.running_mean is None:\n            N, D = x.shape\n            self.running_mean = np.zeros(D)\n            self.running_var = np.zeros(D)\n\n        if train_flg:\n            mu = x.mean(axis=0)\n            xc = x - mu\n            var = np.mean(xc ** 2, axis=0)\n            std = np.sqrt(var + 10e-7)\n            xn = xc / std\n\n            self.batch_size = x.shape[0]\n            self.xc = xc\n            self.xn = xn\n            self.std = std\n            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mu\n            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * var\n        else:\n            xc = x - self.running_mean\n            xn = xc / ((np.sqrt(self.running_var + 10e-7)))\n\n        out = self.gamma * xn + self.beta\n        return out\n\n    def backward(self, dout):\n        if dout.ndim != 2:\n            N, C, H, W = dout.shape\n            dout = dout.reshape(N, -1)\n\n        dx = self.__backward(dout)\n\n        dx = dx.reshape(*self.input_shape)\n        return dx\n\n    def __backward(self, dout):\n        dbeta = dout.sum(axis=0)\n        dgamma = np.sum(self.xn * dout, axis=0)\n        dxn = self.gamma * dout\n        dxc = dxn / self.std\n        dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis=0)\n        dvar = 0.5 * dstd / self.std\n        dxc += (2.0 / self.batch_size) * self.xc * dvar\n        dmu = np.sum(dxc, axis=0)\n        dx = dxc - dmu / self.batch_size\n\n        self.dgamma = dgamma\n        self.dbeta = dbeta\n\n        return dx\n\nclass Dropout:\n    def __init__(self, dropout_ratio=0.5):\n        self.dropout_ratio = dropout_ratio\n        self.mask = None\n\n    def forward(self, x, train_flg=True):\n        if train_flg:\n            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n            return x * self.mask\n        else:\n            return x * (1.0 - self.dropout_ratio)\n\n    def backward(self, dout):\n        return dout * self.mask\n\nclass SoftmaxWithLoss:\n    def __init__(self):\n        self.loss = None\n        self.y = None\n        self.t = None\n\n    def forward(self, x, t):\n        self.t = t\n        self.y = softmax(x)\n        self.loss = cross_entropy_error(self.y, self.t)\n        return self.loss\n\n    def backward(self, dout=1):\n        batch_size = self.t.shape[0]\n        if self.t.size == self.y.size:\n            dx = (self.y - self.t) / batch_size\n        else:\n            dx = self.y.copy()\n            dx[np.arange(batch_size), self.t] -= 1\n            dx = dx / batch_size\n        return dx\n\n\nclass MultiLayerNetExtend:\n    def __init__(self, input_size, hidden_size_list, output_size, activation='relu', weight_init_std='relu', weight_decay_lambda=0,\n                 use_dropout=False,dropout_ration=0.5,use_batchnorm=False):\n        self.input_size = input_size\n        self.output_size = output_size\n        self.hidden_size_list = hidden_size_list\n        self.hidden_layer_num = len(hidden_size_list)\n        self.use_dropout = use_dropout\n        self.weight_decay_lambda = weight_decay_lambda\n        self.use_batchnorm = use_batchnorm\n        self.params = {}\n\n        self.__init_weight(weight_init_std)\n\n        activation_layer = {'sigmoid':Sigmoid, 'relu': Relu}\n        self.layers = OrderedDict()\n        for idx in range(1, self.hidden_layer_num + 1):\n            self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)], self.params['b' + str(idx)])\n            if self.use_batchnorm:\n                self.params['gamma' + str(idx)] = np.ones(hidden_size_list[idx-1])\n                self.params['beta' + str(idx)] = np.zeros(hidden_size_list[idx-1])\n                self.layers['BatchNorm' + str(idx)] = BatchNormalizaiton(self.params['gamma' + str(idx)], self.params['beta' + str(idx)])\n\n            self.layers['Activation_function' + str(idx)] = activation_layer[activation]()\n\n            if self.use_dropout:\n                self.layers['Dropout' + str(idx)] = Dropout(dropout_ration)\n\n        idx = self.hidden_layer_num + 1\n        self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)], self.params['b' + str(idx)])\n        self.last_layer = SoftmaxWithLoss()\n\n    def __init_weight(self, weight_init_std):\n        all_size_list = [self.input_size] + self.hidden_size_list + [self.output_size]\n        for idx in range(1, len(all_size_list)):\n            scale = weight_init_std\n            if str(weight_init_std).lower() in ('relu','he'):\n                scale = np.sqrt(2.0 / all_size_list[idx - 1])\n            elif str(weight_init_std).lower() in ('sigmoid', 'xavier'):\n                scale = np.sqrt(1.0 / all_size_list[idx - 1])\n            self.params['W' + str(idx)] = scale * np.random.randn(all_size_list[idx-1], all_size_list[idx])\n            self.params['b' + str(idx)] = np.zeros(all_size_list[idx])\n\n    def predict(self, x, train_flag=False):\n        for key,layer in self.layers.items():\n            if \"Dropout\" in key or \"BatchNorm\" in key:\n                x = layer.forward(x, train_flag)\n            else:\n                x = layer.forward(x)\n        return x\n\n    def loss(self, x, t, train_flg=False):\n        y = self.predict(x, train_flg)\n        weight_decay = 0\n        for idx in range(1, self.hidden_layer_num + 2):\n            W = self.params['W' + str(idx)]\n            weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W**2)\n        return self.last_layer.forward(y, t) + weight_decay\n\n    def accuracy(self, X, T):\n        Y = self.predict(X, train_flag=False)\n        Y = np.argmax(Y, axis=1)\n        if T.ndim != 1 : T = np.argmax(T, axis=1)\n        accuracy = np.sum(Y == T) / float(X.shape[0])\n        return accuracy\n\n    def gradient(self, x, t):\n        # forward\n        self.loss(x, t, train_flg=True)\n\n        # backward\n        dout = 1\n        dout = self.last_layer.backward(dout)\n\n        layers = list(self.layers.values())\n        layers.reverse()\n        for layer in layers:\n            dout = layer.backward(dout)\n\n        # setting\n        grads = {}\n        for idx in range(1, self.hidden_layer_num + 2):\n            grads['W' + str(idx)] = self.layers['Affine' + str(idx)].dW + self.weight_decay_lambda * self.params['W' + str(idx)]\n            grads['b' + str(idx)] = self.layers['Affine' + str(idx)].db\n\n            if self.use_batchnorm and idx != self.hidden_layer_num+1:\n                grads['gamma' + str(idx)] = self.layers['BatchNorm' + str(idx)].dgamma\n                grads['beta' + str(idx)] = self.layers['BatchNorm' + str(idx)].dbeta\n\n        return grads\n\n# 读入数据\n(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n\nnetwork = MultiLayerNetExtend(input_size=784, hidden_size_list=[50], output_size=10,weight_decay_lambda=0.0,\n                              use_dropout=False, dropout_ration=0.0, use_batchnorm=True)\n\niters_num = 10000\ntrain_size = x_train.shape[0]\nbatch_size = 100\nlearning_rate = 0.1\n\ntrain_loss_list = []\ntrain_acc_list = []\ntest_acc_list = []\n\niter_per_epoch = max(train_size / batch_size, 1)\n\nfor i in range(iters_num):\n    batch_mask = np.random.choice(train_size, batch_size)\n    x_batch = x_train[batch_mask]\n    t_batch = t_train[batch_mask]\n\n    # 梯度\n    #grad = network.numerical_gradient(x_batch, t_batch)\n    grad = network.gradient(x_batch, t_batch)\n\n    # 更新\n    for key in ('W1', 'b1', 'W2', 'b2'):\n        network.params[key] -= learning_rate * grad[key]\n\n    loss = network.loss(x_batch, t_batch)\n    train_loss_list.append(loss)\n\n    if i % iter_per_epoch == 0:\n        train_acc = network.accuracy(x_train, t_train)\n        test_acc = network.accuracy(x_test, t_test)\n        train_acc_list.append(train_acc)\n        test_acc_list.append(test_acc)\n        print('{0:-10f} {1:-10f} {2:-10f}'.format(train_acc, test_acc, loss))\n```\n输出如下\n```\n  0.239300   0.245800   2.921222\n  0.937317   0.935500   0.169096\n  0.954217   0.947100   0.165541\n  0.961133   0.952900   0.085575\n  0.966467   0.958200   0.088835\n  0.970417   0.963000   0.084849\n  0.973833   0.964200   0.066727\n  0.975967   0.965700   0.070722\n  0.978567   0.966900   0.101742\n  0.979033   0.966500   0.032086\n  0.980717   0.969100   0.120274\n  0.981833   0.968300   0.031364\n  0.982750   0.969300   0.052079\n  0.984500   0.971800   0.031689\n  0.984850   0.971300   0.057065\n  0.985300   0.970100   0.093143\n  0.986933   0.971200   0.020156\n  ```\n  可以看到准确率不断升高,测试集最后的准确率为97%,已经非常不错了\n  上面的神经网络实现了`权值衰减`,`dropout`,`batch normalization`, 隐藏层的每层由(Affine层,batch norm层,Activation function层,dropout层)构成,隐藏层的最后一层只由单独的Affine层构成,然后就是输出层由SoftmaxWithLoss构成(softmax层+cross_entropy_error层),输出层输出的是损失值\n  上面实验我只使用了batch normalization\n\n### 参考\n1. [深度学习入门-基于Python的理论与实现](https://book.douban.com/subject/30270959/)\n","tags":["深度学习入门"],"categories":["深度学习"]},{"title":"[深度学习入门-基于Python的理论与实现]第5章 误差反向传播法","url":"%2Fposts%2F2a9e7b3d%2F","content":"### 计算图\n要正确理解误差反向传播法,有两种方法:一种是`基于数学式`,另一种是`基于计算图`.\n计算图通过节点和箭头表示计算过程.节点用$\\bigcirc$表示,$\\bigcirc$中是计算的内容,将计算的中间结果写在箭头的上方,表示各个节点的计算结果从左向右传递.\n**问题1: 太郎在超市买了2个100日元一个的苹果,消费税是10%,请计算支付金额.**\n{% asset_img 20190707094301147.png %}\n**问题2: 太郎在超市买了2个苹果,3个橘子.其中,苹果每个100日元,橘子每个150日元.消费税是10%,请计算支付金额.**\n{% asset_img 20190707094420148.png %}\n#### 局部计算\n计算图的特征是可以通过传递`局部计算`获得最终结果.`局部`这个词的意思是`与自己相关的某个小范围`.局部计算是指,无论全局发生了什么,都能只根据与自己相关的信息输出接下来的结果.\n{% asset_img 20190707094717149.png %}\n计算图可以集中精力于局部计算.无论全局的计算有多么复杂,各个步骤所要做的就是对象节点的局部计算.虽然局部计算非常简单,但是通过传递它的计算结果,可以获得全局的复杂计算的结果.\n#### 为何用计算图解题\n1. 无论全局是多么复杂的计算,都可以通过局部计算使各个节点致力于简单的计算,从而简化问题\n2. 利用计算图可以将中间的计算结果全部保存起来.\n3. **使用计算图最大的原因是,可以通过反向传播高效计算导数**\n\n假设我们想知道苹果价格的上涨会在多大程度上影响最终的支付金额,即\"支付金额关于苹果的价格的导数\".设苹果的价格为$x$,支付金额为$L$,则相当于求$\\frac{\\partial{L}}{\\partial{x}}$.这个导数的值表示当苹果的价格稍微上涨时,支付金额会增加多少.\n{% asset_img 20190707095447150.png %}\n`反向传播可以传播局部导数.`\n### 链式法则\n反向传播将局部导数向反方向(从右到左)传递,传递这个局部导数的原理,是基于`链式法则(chain rule)`的.\n链式法则即复合函数求导的原理,在此不过细讲解了.\n假设有: $z=(x+y)^2$,求$\\frac{\\partial{z}}{\\partial{x}},\\frac{\\partial{z}}{\\partial{y}}$\n我们将$z=(x+y)^2$可以看成由下面两式构成\n$$\\begin{aligned}\n    z&=t^2\\\\\n    t&=x+y\\tag{5.1}\n\\end{aligned}$$\n由于$\\frac{\\partial{z}}{\\partial{t}}=2t=2(x+y),\\frac{\\partial{t}}{\\partial{x}}=1,\\frac{\\partial{t}}{\\partial{y}}=1$\n得到\n$$\\begin{aligned}\n    \\frac{\\partial{z}}{\\partial{x}}=\\frac{\\partial{z}}{\\partial{t}}\\cdot{\\frac{\\partial{t}}{\\partial{x}}}=2(x+y)\\cdot{1}=2(x+y)\\\\\n    \\frac{\\partial{z}}{\\partial{y}}=\\frac{\\partial{z}}{\\partial{t}}\\cdot{\\frac{\\partial{t}}{\\partial{y}}}=2(x+y)\\cdot{1}=2(x+y)\\\\\n\\end{aligned}$$\n{% asset_img 20190707100644152.png %}\n{% asset_img 20190707100700153.png %}\n\n### 反向传播\n#### 加法节点的反向传播\n计算图如下\n{% asset_img 20190706141310128.png %}\n加法节点的反向传播只是将输入信号输出到下一个节点\n加法层的实现\n```python\n# 加法层\nclass AddLayer:\n    def __init__(self):\n        pass\n\n    # 正向传播\n    def forward(self, x, y):\n        out = x + y\n        return out\n\n    # 反向传播\n    def backward(self, dout):\n        dx = dout * 1\n        dy = dout * 1\n        return dx, dy\n```\n#### 乘法节点的反向传播\n计算图如下\n{% asset_img 20190706141353129.png %}\n乘法节点的反向传播会将上游的值乘以正向传播时的输入信号的\"翻转值\"后传递给下游,翻转值表示一种翻转关系.\n乘法层的实现\n```python\n# 乘法层\nclass MulLayer:\n    def __init__(self):\n        self.x = None\n        self.y = None\n\n    # 正向传播\n    def forward(self, x, y):\n        self.x = x\n        self.y = y\n        out = x * y\n        return out\n\n    # 反向传播\n    def backward(self, dout):\n        dx = dout * self.y\n        dy = dout * self.x\n        return dx, dy\n```\n#### 例子\n{% asset_img 20190706141453131.png %}\n以下是代码实现\n```python\nimport numpy as np\n\n# 乘法层\nclass MulLayer:\n    def __init__(self):\n        self.x = None\n        self.y = None\n\n    # 正向传播\n    def forward(self, x, y):\n        self.x = x\n        self.y = y\n        out = x * y\n        return out\n\n    # 反向传播\n    def backward(self, dout):\n        dx = dout * self.y\n        dy = dout * self.x\n        return dx, dy\n\n# 加法层\nclass AddLayer:\n    def __init__(self):\n        pass\n\n    # 正向传播\n    def forward(self, x, y):\n        out = x + y\n        return out\n\n    # 反向传播\n    def backward(self, dout):\n        dx = dout * 1\n        dy = dout * 1\n        return dx, dy\n\napple_num = 2\napple = 100\norange_num = 3\norange = 150\ntax = 1.1\nmulLayer1_1 = MulLayer()\nmulLayer2_1 = MulLayer()\naddLayer1_2 = AddLayer()\nmulLayer1_3 = MulLayer()\n\napple_price=mulLayer1_1.forward(apple_num,apple)\norige_price=mulLayer2_1.forward(orange_num,orange)\nsum_price=addLayer1_2.forward(apple_price,orige_price)\nfinal_price=mulLayer1_3.forward(sum_price,tax)\nprint(final_price)\ndsum_price,dtax = mulLayer1_3.backward(1)\nprint(dsum_price,dtax)\ndapple_price,dorange_price = addLayer1_2.backward(dsum_price)\nprint(dapple_price,dorange_price)\ndapple_num,dapple = mulLayer1_1.backward(dapple_price)\ndorange_num,dorange = mulLayer2_1.backward(dorange_price)\nprint(dapple_num,dapple,dorange,dorange_num)\n```\n输出结果: \n{% asset_img 20190706141731132.png %}\n### 激活函数层的实现\n#### ReLU层\n激活函数ReLU(Rectified Linear Unit)如下式\n$$\\begin{aligned}\n    y=\\begin{cases}\n        x&(x\\gt{0})\\\\\n        0&(x\\le{0})\\\\\n    \\end{cases}\n\\end{aligned}$$\n求关于$x$导后\n$$\\begin{aligned}\n    \\frac{\\partial{y}}{\\partial{x}}=\\begin{cases}\n        1&(x\\gt{0})\\\\\n        0&(x\\le{0})\\\\\n    \\end{cases}\n\\end{aligned}$$\n计算图如下\n{% asset_img 20190706143240134.png %}\n代码实现如下\n```python\nclass Relu:\n    def __init__(self):\n        self.mask = None\n\n    def forward(self, x):\n        self.mask = (x <= 0)\n        out = x.copy()\n        out[self.mask] = 0\n        return out\n\n    def backward(self, dout):\n        dout[self.mask] = 0\n        dx = dout\n        return dx\n```\n#### Sigmoid层\nsigmoid函数如下式\n$$\\begin{aligned}\n    y=\\frac{1}{1+\\exp(-x)}\n\\end{aligned}$$\n计算图如下\n{% asset_img 20190706143723136.png %}\n简洁版计算图如下\n{% asset_img 20190706143812137.png %}\n代码实现如下\n```python\nclass Sigmoid:\n    def __init__(self):\n        self.out = None\n\n    def forward(self, x):\n        out = 1.0 / (1.0 + np.exp(-x))\n        self.out = out\n        return out\n\n    def backward(self, dout):\n        dx = dout * self.out * (1.0 - self.out)\n        return dx\n```\n### Affine层\n公式如下\n{% asset_img 20190706144432138.png %}\n$$\\begin{aligned}\n    X\\cdot{W}+B=Y\n\\end{aligned}$$\n计算图如下\n{% asset_img 20190706144454139.png %}\n{% asset_img 20190706144827141.png %}\n代码实现如下\n```python\nclass Affine:\n    def __init__(self, W, b):\n        self.W = W\n        self.b = b\n        self.x = None\n        self.dw = None\n        self.db = None\n\n    def forward(self, x):\n        self.x = x\n        out = np.dot(x, self.W) + self.b\n        return out\n\n    def backward(self, dout):\n        dx = np.dot(dout, self.W.T)\n        self.dW = np.dot(self.x.T, dout)\n        self.db = np.sum(dout, axis=0)\n        return dx\n```\n### Softmax-with-Loss层\n{% asset_img 20190706150621142.png %}\n神经网络中进行的处理有`推理`和`学习`两个阶段.`神经网络的推理通常不使用Softmax层.`用图5-28的网络进行推理时,会将最后一个Affine层的输出作为识别结果.神经网络中未被正规化的输出结果(图5-28中Softmax层前面的Affine层的输出)有时被称为`得分`.也就是说,当神经网络的推理只需要给出一个答案的情况下,因为此时只对得分最大值感兴趣,所以不需要Softmax层.不过,`神经网络的学习阶段则需要Softmax层.`\n> 1.神经网络的推理通常不使用Softmax层\n> 2.神经网络的学习阶段则需要Softmax层\n\n下面为Softmax-with-Loss层的计算图\n{% asset_img 20190706151144143.png %}\n简易版的Softmax-with-Loss层的计算图\n{% asset_img 20190706151238144.png %}\nSoftmax-with-Loss层的代码实现如下\n```python\n# 批量交叉熵误差\ndef cross_entropy_error(y, t):\n    y = np.array(y,ndmin=2)\n    t = np.array(t,ndmin=2)\n    delta = 1e-7\n    batch_size = y.shape[0]\n    return -np.sum(t * np.log(y + delta)) / batch_size\n\n# Softmax函数\ndef softmax(x):\n    if x.ndim == 2:\n        x = x.T\n        x = x - np.max(x, axis=0)\n        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n        return y.T\n    x = x - np.max(x) # 溢出对策\n    return np.exp(x) / np.sum(np.exp(x))\n\n# Softmax-with-Loss层\nclass SoftmaxWithLoss:\n    def __init__(self):\n        self.loss = None # 损失\n        self.y = None    # softmax输出\n        self.t = None    # 监督数据(one-hot vector)\n\n    def forward(self, x, t):\n        self.t = t\n        self.y = softmax(x)\n        self.loss = cross_entropy_error(self.y, self.t)\n        return self.loss\n\n    def backward(self, dout=1):\n        batch_size = self.t.shape[0]\n        if self.t.size == self.y.size: # 监督数据是one-hot-vector的情况\n            dx = (self.y - self.t) / batch_size\n        else:\n            dx = self.y.copy()\n            dx[np.arange(batch_size), self.t] -= 1\n            dx = dx / batch_size\n        return dx\n```\n### 完整代码\n{% asset_img 20190706150621142.png %}\n附带包下载: [common.zip](common.zip),[dataset.zip](dataset.zip)\n```python\nimport numpy as np\nfrom dataset.mnist import load_mnist\nfrom collections import OrderedDict\n\ndef softmax(x):\n    if x.ndim == 2:\n        x = x.T\n        x = x - np.max(x, axis=0)\n        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n        return y.T \n    x = x - np.max(x) # 溢出对策\n    return np.exp(x) / np.sum(np.exp(x))\n\n\ndef cross_entropy_error(y, t):\n    if y.ndim == 1:\n        t = t.reshape(1, t.size)\n        y = y.reshape(1, y.size)\n\n    # 数据t是one-hot-vector的情况下，转换为正确解标签的索引\n    if t.size == y.size:\n        t = t.argmax(axis=1)\n\n    batch_size = y.shape[0]\n    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n\n# Affine层\nclass Affine:\n    def __init__(self, W, b):\n        self.W = W\n        self.b = b\n        self.dW = None\n        self.db = None\n\n    def forward(self, x):\n        self.x = x\n        out = np.dot(self.x, self.W) + self.b\n        return out\n\n    def backward(self, dout):\n        dx = np.dot(dout, self.W.T)\n        self.dW = np.dot(self.x.T, dout)\n        self.db = np.sum(dout, axis=0)\n        return dx\n\n# ReLU(Rectified Linear Unit)层\nclass Relu:\n    def __init__(self):\n        self.mask = None\n\n    def forward(self, x):\n        self.mask = (x <= 0)\n        out = x.copy()\n        out[self.mask] = 0\n        return out\n\n    def backward(self, dout):\n        dout[self.mask] = 0\n        dx = dout\n        return dx\n\n# Sigmoid层\nclass Sigmoid:\n    def __init__(self):\n        self.out = None\n\n    def forward(self, x):\n        out = 1.0 / (1.0 + np.exp(-x))\n        self.out = out\n        return out\n\n    def backward(self, dout):\n        dx = dout * self.out * (1.0 - self.out)\n        return dx\n\n# Softmax-With-Loss层\nclass SoftmaxWithLoss:\n\n    def __init__(self):\n        self.loss = None\n        self.y = None\n        self.t = None\n\n    def forward(self, x, t):\n        self.t = t\n        self.y = softmax(x)\n        self.loss = cross_entropy_error(self.y, self.t)\n        return self.loss\n\n    def backward(self, dout = 1):\n        batch_size = self.t.shape[0]\n        if self.t.size == self.y.size: # 监督数据是one-hot-vector的情况\n            dx = (self.y - self.t) / batch_size\n        else:\n            dx = self.y.copy()\n            dx[np.arange(batch_size), self.t] -= 1\n            dx = dx / batch_size\n        return dx\n\n\nclass TwoLayerNet:\n    def __init__(self, x_num, a1_num, a2_num, weight_init_std=0.01):\n        self.params = {}\n        self.params['W1'] = np.random.normal(0.0, pow(a1_num,-0.5), (x_num, a1_num))#weight_init_std * np.random.randn(x_num, a1_num)\n        self.params['b1'] = np.zeros(a1_num)\n        self.params['W2'] = np.random.normal(0.0, pow(a2_num,-0.5), (a1_num, a2_num))#weight_init_std * np.random.randn(a1_num, a2_num)\n        self.params['b2'] = np.zeros(a2_num)\n\n        self.layers = OrderedDict()\n        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n        self.layers['Relu1'] = Relu()\n        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n        self.lastLayer = SoftmaxWithLoss()\n\n    def predict(self, x):\n        for layer in self.layers.values():\n            x = layer.forward(x)\n        return x\n\n    def loss(self, x, t):\n        y = self.predict(x)\n        return self.lastLayer.forward(y, t)\n\n    def accuracy(self, x, t):\n        y = self.predict(x)\n        y = np.argmax(y, axis=1)\n        if t.ndim != 1:\n            t = np.argmax(t, axis=1)\n        accuracy = np.sum(y==t) / float(x.shape[0])\n        return accuracy\n\n    def gradient(self, x, y):\n        # forward\n        self.loss(x,y)\n\n        # backward\n        dout = 1\n        dout = self.lastLayer.backward(dout)\n\n        layers = list(self.layers.values())\n        layers.reverse()\n        for layer in layers:\n            dout = layer.backward(dout)\n\n        # update\n        grads = {}\n        grads['W1'] = self.layers['Affine1'].dW\n        grads['b1'] = self.layers['Affine1'].db\n        grads['W2'] = self.layers['Affine2'].dW\n        grads['b2'] = self.layers['Affine2'].db\n        return grads\n\n\n# 读入数据\n(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\nnetwork = TwoLayerNet(784, 50, 10)\n\niters_num = 10000 #\ntrain_size = x_train.shape[0] # 训练数量\nbatch_size = 100 # 批数据的大小\nlearning_rate = 0.1\n\ntrain_loss_list = []\ntrain_acc_list = []\ntest_acc_list = []\n\n# 循环iter_per_epoch次才能把所有数据遍历完一次\n# 一个epoch表示把所有数据轮询一遍\niter_per_epoch = max(train_size / batch_size, 1) # 批大小\n\nfor i in range(iters_num):\n    # 每次选取batch_size个数据\n    batch_mask = np.random.choice(train_size, batch_size)\n    x_batch = x_train[batch_mask]\n    t_batch = t_train[batch_mask]\n\n    # forward and backward\n    grad = network.gradient(x_batch, t_batch)\n\n    # update\n    for key in ['W1','W2','b1','b2']:\n        network.params[key] -= learning_rate * grad[key]\n\n    loss = network.loss(x_batch, t_batch)\n    train_loss_list.append(loss)\n\n    # 轮询一遍,然后记录准确率\n    if i % iter_per_epoch == 0:\n        train_acc = network.accuracy(x_train, t_train)\n        test_acc = network.accuracy(x_test, t_test)\n        train_acc_list.append(train_acc)\n        test_acc_list.append(test_acc)\n        print(train_acc, test_acc, loss)\n```\n### 参考\n1. [深度学习入门-基于Python的理论与实现](https://book.douban.com/subject/30270959/)\n","tags":["深度学习入门"],"categories":["深度学习"]},{"title":"[深度学习入门-基于Python的理论与实现]第4章 神经网络的学习","url":"%2Fposts%2F8d9d8d73%2F","content":"\n### 损失函数\n神经网络的学习中所用的指标称为`损失函数(loss function)`.\n#### 均方误差\n均方误差如下式所示(单个数据的损失函数)\n$$\\begin{aligned}\n    E=\\frac{1}{2}\\sum_k(y_k-t_k)^2\n\\end{aligned}$$\n这里,$y_k$是表示神经网络的输出,$t_k$表示监督数据,$k$表示数据的维数.\n**one-hot表示**: 将正确解标签表示为1,其他标签表示为0的表示方法称为`one-hot表示`.\n这里代码采用`one-hot表示`,代码如下\n```python\ndef mean_squared_error(y, t):\n    return 0.5 * np.sum((y-t)**2)\n```\n{% asset_img 20190705164752123.png %}\n#### 交叉熵误差\n首先给出通用交叉熵误差代码(适用于单个数据的交叉熵误差计算和mini-batch版交叉熵误差计算)\n```python\ndef cross_entropy_error(y, t):\n    # 若y只有1个样本(one-hot-vector)\n    if y.ndim == 1:\n        t = t.reshape(1, t.size)\n        y = y.reshape(1, y.size)\n\n    # 数据t是one-hot-vector的情况下，转换为正确解标签的索引\n    if t.size == y.size:\n        t = t.argmax(axis=1)\n\n    batch_size = y.shape[0]\n    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n```\n交叉熵误差如下式所示(单个数据的损失函数)\n$$\\begin{aligned}\n    E=-\\sum_k{t_k\\log{y_k}}\n\\end{aligned}$$\n这里,$log$表示以$e$为底数的自然对数$log_e$,$y_k$是神经网络的输出,$t_k$是正确解标签,采用`one-hot表示`.\n代码如下\n```python\ndef cross_entropy_error(y, t):\n    delta = 1e-7\n    return -np.sum(t * np.log(y + delta))\n```\n{% asset_img 20190705164602122.png %}\n### mini-batch学习\n选择全部数据的一部分,作为全部数据的\"近似\".神经网络的学习也是从训练数据中选出一批数据(称为mini-batch,小批量),然后对每个mini-batch进行学习.比如,从60000个训练数据中随机选择100笔,再用这100笔数据进行学习.这被称为`mini-batch学习`.\n#### mini-batch版交叉熵误差\n我们来实现一个可以同时处理单个数据和批量数据(数据作为batch集输入)两种情况的函数\n$$\\begin{aligned}\n    E=-\\frac{1}{N}\\sum_n\\sum_k{t_{nk}\\log{y_{nk}}}\n\\end{aligned}$$\n下面是代码(标签采用`one-hot表示`)\n```python\n# 批量交叉熵误差\ndef cross_entropy_error_mini_batch(y, t):\n    y = np.array(y,ndmin=2)\n    t = np.array(t,ndmin=2)\n    delta = 1e-7\n    batch_size = y.shape[0]\n    return -np.sum(t * np.log(y + delta)) / batch_size\n```\n由于是批量交叉熵误差,因此要计算单个数据的平均交叉熵误差.\n以下是`单个交叉熵误差,批量交叉熵误差,均方误差对比`的代码\n```python\nimport numpy as np\n\n# 单个交叉熵误差\ndef cross_entropy_error(y, t):\n    y = np.array(y)\n    t = np.array(t)\n    delta = 1e-7\n    return -np.sum(t * np.log(y + delta))\n\n# 均方误差(单个或批量)\ndef mean_squared_error(y, t):\n    y = np.array(y, ndmin=2)\n    t = np.array(t, ndmin=2)\n    return 0.5 * np.sum((y-t)**2)\n\n# 批量交叉熵误差\ndef cross_entropy_error_mini_batch(y, t):\n    y = np.array(y,ndmin=2)\n    t = np.array(t,ndmin=2)\n    delta = 1e-7\n    batch_size = y.shape[0]\n    return -np.sum(t * np.log(y + delta)) / batch_size\n\nt = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\ny = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])\nepoch = 5\nt_set=[]\ny_set=[]\n# 随机生成5个样本的相关信息\nfor i in range(epoch):\n    t_idx = np.arange(10)\n    y_idx = np.arange(10)\n    np.random.shuffle(t_idx)\n    np.random.shuffle(y_idx)\n    t_set.append(t[t_idx])\n    y_set.append(y[t_idx])\n\nsum_error = 0.0\nfor i in range(epoch):\n    sum_error+=cross_entropy_error(y_set[i],t_set[i])\nsum_error /= epoch\nprint('cross_entropy_error sum error: ',sum_error)\nsum_error = cross_entropy_error_mini_batch(y_set,t_set)\nprint('cross_entropy_error_mini_batch error: ',sum_error)\nsum_error = mean_squared_error(y_set,t_set)\nprint('mean_squared_error error: ',sum_error)\n```\n运行结果如下\n{% asset_img 20190705184648124.png %}\n### 数值微分\n#### 导数\n$$\\begin{aligned}\n    \\frac{df(x)}{dx}=\\lim_{h\\to{0}}\\frac{f(x+h)-f(x-h)}{2h}\n\\end{aligned}$$\n下面用代码实现求$f(x)=x^2$在$x=3$处的导数\n```python\ndef numerical_diff(f,x):\n    h = 1e-4\n    return (f(x+h)-f(x-h)) * 1.0 / (2.0*h)\n\ndef fun(x):\n    return x**2\n\nprint(numerical_diff(fun,3.0))\n```\n输出: `6.000000000012662`\n#### 偏导数\n求$z=f(x,y)$在点$(x,y)$处的偏导数\n$$\\begin{aligned}\n    \\frac{\\partial{f}}{\\partial{x}}&=\\lim_{h\\to{0}}\\frac{f(x+h,y)-f(x-h,y)}{2h}\\\\\n    \\frac{\\partial{f}}{\\partial{y}}&=\\lim_{h\\to{0}}\\frac{f(x,y+h)-f(x,y-h)}{2h}\\\\\n\\end{aligned}$$\n下面用代码实现求$f(x_0,x_1)=x_0^2+x_1^2$在$x_0=3,x_1=4$时,关于$x_0$的偏导数$\\frac{\\partial{f}}{\\partial{x_0}}$\n```python\ndef numerical_diff(f,x):\n    h = 1e-4\n    return (f(x+h)-f(x-h)) * 1.0 / (2.0*h)\n\ndef fun2(x0):\n    return x0**2+4.0**2\n\nprint(numerical_diff(fun2,3.0))\n```\n输出: `6.00000000000378`\n#### 梯度\n求$f(x,y)$在点$(x,y)$处的梯度\n$$\\begin{aligned}\n    \\mathbf{grad}{f(x,y)}&=\\nabla{f(x,y)}=(\\frac{\\partial{f}}{\\partial{x}},\\frac{\\partial{f}}{\\partial{y}})\\\\\n    &=(\\lim_{h\\to{0}}\\frac{f(x+h,y)-f(x-h,y)}{2h},\\lim_{h\\to{0}}\\frac{f(x,y+h)-f(x,y-h)}{2h})\n\\end{aligned}$$\n下面用代码实现求$f(x_0,x_1)=x_0^2+x_1^2$在$(3,4)$处的梯度\n```python\ndef numerical_gradient(f,x):\n    h = 1e-4\n    grad = np.zeros_like(x)\n    for idx in range(x.size):\n        tmp_val = x[idx]\n        \n        x[idx] = tmp_val + h # x+h\n        fxh1 = f(x) # f(x+h)\n        \n        x[idx] = tmp_val - h # x-h\n        fxh2 = f(x) # f(x-h)\n        \n        grad[idx] = (fxh1 - fxh2) / (2.0*h)\n        x[idx] = tmp_val\n    return grad\n\ndef fun2(x):\n    return x[0]**2+x[1]**2\n\nnumerical_gradient(fun2,np.array([3.0,4.0]))\n```\n输出: `array([6., 8.])`\n### 梯度法\n$$\\begin{aligned}\n    x_0=x_0-\\alpha\\frac{\\partial{f}}{\\partial{x_0}}\\\\\n    x_1=x_1-\\alpha\\frac{\\partial{f}}{\\partial{x_1}}\\\\\n\\end{aligned}$$\n$\\alpha$称为学习率(learning rate)\n我们用`数值微分的方法来实现梯度下降法`,来对函数$f(x_0,x_1)=x_0^2+x_1^2$在点$(-3,4)$处进行梯度下降,代码如下\n```python\ndef gradient_descent(f, x, lr=0.1, step_num=100):\n    for i in range(step_num):\n        grad = numerical_gradient(f,x)\n        x -= lr * grad\n    return x\n\nx=np.array([-3.0,4.0])\nprint(gradient_descent(fun2,x))\n```\n输出: `array([-6.11110793e-10,  8.14814391e-10])`,可以近似看作`(0,0)`\n### 2层神经网络的类(基于数值微分的方法)\n```python\n# coding: utf-8\nimport sys, os\nsys.path.append(os.pardir)  # 为了导入父目录的文件而进行的设定\nfrom common.functions import *\nfrom common.gradient import numerical_gradient\n\n\nclass TwoLayerNet:\n\n    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n        # 初始化权重\n        self.params = {}\n        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n        self.params['b1'] = np.zeros(hidden_size)\n        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n        self.params['b2'] = np.zeros(output_size)\n\n    def predict(self, x):\n        W1, W2 = self.params['W1'], self.params['W2']\n        b1, b2 = self.params['b1'], self.params['b2']\n    \n        a1 = np.dot(x, W1) + b1\n        z1 = sigmoid(a1)\n        a2 = np.dot(z1, W2) + b2\n        y = softmax(a2)\n        \n        return y\n        \n    # x:输入数据, t:监督数据\n    def loss(self, x, t):\n        y = self.predict(x)\n        \n        return cross_entropy_error(y, t)\n    \n    def accuracy(self, x, t):\n        y = self.predict(x)\n        y = np.argmax(y, axis=1)\n        t = np.argmax(t, axis=1)\n        \n        accuracy = np.sum(y == t) / float(x.shape[0])\n        return accuracy\n        \n    # x:输入数据, t:监督数据\n    def numerical_gradient(self, x, t):\n        loss_W = lambda W: self.loss(x, t)\n        \n        grads = {}\n        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n        \n        return grads\n        \n    def gradient(self, x, t):\n        W1, W2 = self.params['W1'], self.params['W2']\n        b1, b2 = self.params['b1'], self.params['b2']\n        grads = {}\n        \n        batch_num = x.shape[0]\n        \n        # forward\n        a1 = np.dot(x, W1) + b1\n        z1 = sigmoid(a1)\n        a2 = np.dot(z1, W2) + b2\n        y = softmax(a2)\n        \n        # backward\n        dy = (y - t) / batch_num\n        grads['W2'] = np.dot(z1.T, dy)\n        grads['b2'] = np.sum(dy, axis=0)\n        \n        da1 = np.dot(dy, W2.T)\n        dz1 = sigmoid_grad(a1) * da1\n        grads['W1'] = np.dot(x.T, dz1)\n        grads['b1'] = np.sum(dz1, axis=0)\n\n        return grads\n```\n不知道为啥,书中给的代码按数值微分的方法跑要跑非常久(反正我没有等到结果出来),后来我跑反向传播法结果却比较快出来\n### 参考\n1. [深度学习入门-基于Python的理论与实现](https://book.douban.com/subject/30270959/)","tags":["深度学习入门"],"categories":["深度学习"]},{"title":"[深度学习入门-基于Python的理论与实现]第3章 神经网络","url":"%2Fposts%2Fc2857c8f%2F","content":"\n### 激活函数\n`朴素感知机`是指单层网络,指的是激活函数使用了阶跃函数的模型.\n`多层感知机`是指神经网络,即使用sigmoid函数等平滑的激活函数的多层网络\n感知机使用了`阶跃函数 `作为激活函数,神经网络使用`sigmoid函数`或`ReLU函数`作为激活函数\n#### sigmoid函数\n$$\\begin{aligned}\n    h(x)=\\frac{1}{1+\\exp(-x)}\n\\end{aligned}$$\n代码实现\n```python\ndef sigmoid(x):\n    return 1.0 / (1+np.exp(-x))\n```\n实际上,上一章介绍的感知机和接下来要介绍的神经网络的主要区别就在于这个激活函数.\n#### 阶跃函数\n$$\\begin{aligned}\n    h(x)=\\begin{cases}\n        0&x\\le{0}\\\\\n        1&x\\gt{0}\\\\\n    \\end{cases}\n\\end{aligned}$$\n代码实现\n```python\ndef step_function(x):\n    return np.array(x > 0, dtype=np.int)\n```\n#### ReLU函数\n$$\\begin{aligned}\n    h(x)=\\begin{cases}\n        x&(x\\gt{0})\\\\\n        0&(x\\le{0})\\\\\n    \\end{cases}\n\\end{aligned}$$\n代码实现\n```python\ndef relu(x):\n    return np.maximum(0,x)\n```\n#### sigmoid函数和阶跃函数的比较\n**不同:** 感知机中神经元之间流动的是0或1的二元信号,而神经网络中流动的是连续的实数值信号.\n**相同:** 阶跃函数和sigmoid函数两者都是`非线性函数`,sigmoid函数是一条曲线,阶跃函数是一条像阶梯一样的折线.\n\n### 3层神经网络的实现\n#### 符号确认\n{% asset_img 20190704203839111.png %}\n#### 每一层的信号传递\n- **输入层到第1层的信号传递**\n{% asset_img 20190704203946112.png %}\n图3-17中增加了表示偏置的神经元\"1\".请注意,偏置的右下角的索引号只有一个.这是因为前一层的偏置神经元(神经元\"1\")只有一个.\n向量用小写加粗字母表示,矩阵用大写字母表示,标量用小写字母表示.\n$$\\begin{aligned}\n    \\mathbf a^{(1)}=\\mathbf xW^{(1)}+\\mathbf b^{(1)}\\\\\n\\end{aligned}$$\n其中\n$$\\begin{aligned}\n    \\mathbf a^{(1)}&=[a_1^{(1)},a_2^{(1)},a_3^{(1)}]\\\\\n    \\mathbf x&=[x_1,x_2]\\\\\n    \\mathbf b^{(1)}&=[b_1^{(1)},b_2^{(1)},b_3^{(1)}]\\\\\n    W^{(1)}&=\\begin{bmatrix}\n        w_{11}^{(1)}&w_{21}^{(1)}&w_{31}^{(1)}\\\\\n        w_{12}^{(1)}&w_{22}^{(1)}&w_{32}^{(1)}\\\\\n    \\end{bmatrix}\n\\end{aligned}$$\n{% asset_img 20190704204711113.png %}\n$$\\begin{aligned}\n    \\mathbf z^{(1)}&=sigmoid(\\mathbf a^{(1)})\n\\end{aligned}$$\n- **第1层到第2层的信号传递**\n{% asset_img 20190704204900114.png %}\n$$\\begin{aligned}\n    \\mathbf a^{(2)}&=\\mathbf z^{(1)}W^{(2)}+\\mathbf b^{(2)}\\\\\n    \\mathbf z^{(2)}&=sigmoid(\\mathbf a^{(2)})\\\\\n\\end{aligned}$$\n- **第2层到输出层(第3层)的信号传递**\n{% asset_img 20190704205158115.png %}\n$$\\begin{aligned}\n    \\mathbf a^{(3)}&=\\mathbf z^{(2)}W^{(3)}+\\mathbf b^{(3)}\\\\\n    \\mathbf y&=\\mathbf a^{(3)}\n\\end{aligned}$$\n代码如下\n```python\n    x = np.array([1.0, 0.5])\n    W1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\n    W2 = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])\n    W3 = np.array([[0.1, 0.3], [0.2, 0.4]])\n    b1 = np.array([0.1, 0.2, 0.3])\n    b2 = np.array([0.1, 0.2])\n    b3 = np.array([0.1, 0.2])\n    # 输入层到第1层的信号传递\n    a1 = x@W1 + b1\n    z1 = sigmoid(a1)\n    # 第1层到第2层的信号传递\n    a2 = z1@W2 + b2\n    z2 = sigmoid(a2)\n    # 第2层到输出层(第3层)的信号传递\n    a3 = z2@W3 + b3\n    y = a3\n```\n整理后\n```python\nimport numpy as np\n\ndef sigmoid(x):\n    return 1.0 / (1+np.exp(-x))\n\ndef init_network():\n    network = {}\n    network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\n    network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])\n    network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])\n    network['b1'] = np.array([0.1, 0.2, 0.3])\n    network['b2'] = np.array([0.1, 0.2])\n    network['b3'] = np.array([0.1, 0.2])\n    return network\n\ndef forward(network, x):\n    W1,W2,W3 = network['W1'],network['W2'],network['W3']\n    b1,b2,b3 = network['b1'],network['b2'],network['b3']\n\n    a1 = x@W1 + b1\n    z1 = sigmoid(a1)\n    a2 = z1@W2 + b2\n    z2 = sigmoid(a2)\n    a3 = z2@W3 + b3\n    y = a3\n    return y\n\nnetwork = init_network()\nx = np.array([1.0, 0.5])\ny = forward(network, x)\nprint(y)\n```\n输出结果为`[0.31682708 0.69627909]`\n\n#### 输出层的设计\n神经网络可以用在`分类问题`和`回归问题`上,不过需要根据情况改变输出层的激活函数.一般而言,`回归问题用恒等函数`,`分类问题用softmax函数`.\n##### 恒等函数(回归问题)\n{% asset_img 20190705091646117.png %}\n恒等函数会将输入按原样输出,对于输入的信息,不加以任何改动地直接输出.\n##### softmax函数(分类问题)\n{% asset_img 20190705091814118.png %}\n分类问题中使用的softmax函数可以用下面的式子表示\n$$\\begin{aligned}\n    y_k=\\frac{\\exp(a_k+C)}{\\sum_{i=1}^n{\\exp(a_i+C)}}\n\\end{aligned}$$\n$\\exp(x)$是表示$e^x$的指数函数($e=2.7182...$),式子中假设输出层共有$n$个神经元,计算第$k$个神经元的输出$y_k$.函数$softmax$的分子是输入信号$a_k$的指数函数,分母是所有输入信号的指数函数的和.\n为了防止溢出,我们在式子中加上常数$C$,一般在式子中$C=-max(a_i)$,这样能保证$\\exp(a_k-max(a_i))$不会溢出\n代码如下\n```python\n# 批量softmax\ndef softmax(x):\n    if x.ndim == 2:\n        x = x.T # 转置是为了将样本数放在最后1维,方便广播\n        x = x - np.max(x, axis=0) # ndarray的广播\n        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n        return y.T \n    x = x - np.max(x) # 溢出对策\n    return np.exp(x) / np.sum(np.exp(x))\n```\n**问: 解释一下为什么要判断`x.ndim==2`?**\n**答:** 因为数据最后在经过所有**隐藏层**后输出的数据应为`(样本数N,one-hot-vector打分)`,比如**MNIST数据集**,输出类别应分为**10类**,那么**one-hot-vector**打分即为`[0的分数,1的分数,...,9的分数]`,这里**softmax**的作用就是将分数进行`normal(归一化)`一下,使其值的范围落在`(0.0,1.0)`的区间.\n可以看一下效果\n{% asset_img 2019071310392743.png %}\n最后,判断类别可以直接如下所示,即选出分数占比最高的索引,即为预测的类别\n{% asset_img 2019071310452144.png %}\n可以看出,4个样本分别属于`7,8,6,2`号类别.\n**需要注意的是:** 即使使用了softmax函数,各个元素之间的大小关系也不会改变.这是因为指数函数($y=\\exp(x)$)是单调递增函数.即,$a$的各元素的大小关系和$y$的各元素的大小关系并没有改变.比如,$a$中最大的元素是第2个元素,$y$的最大值也是第2个元素.\n1. `一般而言,神经网络只把输出值最大的神经元所对应的类别作为识别的结果.`\n2. `在实际问题中,由于指数函数的运算需要一定的计算机运算量,因此输出层的softmax函数一般会被省略.`\n##### 输出层的神经元数量\n输出层的神经元数量需要根据待解决的问题来决定.`对于分类问题,输出层的神经元数量一般设定为类别的数量.`\n\n### 参考\n1. [深度学习入门-基于Python的理论与实现](https://book.douban.com/subject/30270959/)\n","tags":["深度学习入门"],"categories":["深度学习"]},{"title":"[深度学习入门-基于Python的理论与实现]第2章 感知机","url":"%2Fposts%2F5b01321f%2F","content":"\n### 感知机是什么\n{% asset_img 20190704191519100.png %}\n`图2-1`是一个接收两个输入信号的感知机的例子.$x_1,x_2$是`输入信号`,$y$是`输出信号`,$w_1,w_2$是权重($w$是weight的首字母).图中的$\\bigcirc$称为`神经元`或者`节点`.输入信号被送往神经元时,会被分别乘以固定的权重$(w_1x_1,w_2x_2)$.神经元会计算传送过来的信号的总和,只有当这个总和超过了某个界限时,才会输出1.这也称为`神经元被激活`.这里将这个界限值称为`阈值`,用符号$\\theta$表示.\n用数学形式表示感知机,就是如下\n$$\ny=\\begin{cases}\n    0&(w_1x_1+w_2x_2)\\le{\\theta}\\\\\n    1&(w_1x_1+w_2x_2)\\gt{\\theta}\\\\\n\\end{cases}\n$$\n感知机的多个输入信号都有各自固有的权重,这些权重发挥着控制各个信号的重要性的作用.也就是说,权重越大,对应该权重的信号的重要性就越高.\n### 导入权重和偏置\n我们将上式$\\theta$改为$-b$,于是变成\n$$\ny=\\begin{cases}\n    0&(b+w_1x_1+w_2x_2\\le{0})\\\\\n    1&(b+w_1x_1+w_2x_2\\gt{0})\\\\\n\\end{cases}\n$$\n表达的内容是完全相同的,$b$称为`偏置`,$w_1$和$w_2$称为`权重`.感知机会计算输入信号和权重的乘积,然后加上偏置,如果这个值大于0则输出1,否则输出0.\n### 简单逻辑电路\n#### 与门\n{% asset_img 20190704192834101.png %}\n代码实现\n```python\ndef AND(x1,x2):\n    x=np.array([x1,x2])\n    w=np.array([0.5,0.5])\n    b=-0.7\n    return w@x+b>=0\n```\n运行结果\n{% asset_img 20190704194215105.png %}\n#### 与非门\n{% asset_img 20190704192834102.png %}\n代码实现\n```python\ndef NAND(x1,x2):\n    x=np.array([x1,x2])\n    w=np.array([-0.5,-0.5])\n    b=0.7\n    return w@x+b>=0\n```\n运行结果\n{% asset_img 20190704194301106.png %}\n#### 或门\n{% asset_img 20190704193038103.png %}\n代码实现\n```python\ndef OR(x1,x2):\n    x=np.array([x1,x2])\n    w=np.array([0.5,0.5])\n    b=-0.2\n    return w@x+b>=0\n```\n运行结果\n{% asset_img 20190704194327107.png %}\n### 感知机的局限性\n#### 异或门\n{% asset_img 20190704194036104.png %}\n由于感知机的表达式只能表示由一条直线分割的空间,因此像异或门这样的实现需要用两条直线分割或曲线分割.\n由直线分割的空间称为`线性空间`,由曲线分割而成的空间称为`非线性空间`.\n2层神经网络(第0层开始输入层,隐藏层,输出层)可以表示线性空间,3层或以上的神经网络可以表示非线性空间\n因此异或门的实现需要感知机`叠加层`来实现,观察下图\n{% asset_img 20190704200128109.png %}\n实际等同于下面\n{% asset_img 20190704200153110.png %}\n逻辑电路中的异或门的实现,实际是`多层感知机`(这里是2层感知机)的一个特例\n以下为代码实现\n```python\ndef XOR(x1,x2):\n    s1=NAND(x1,x2)\n    s2=OR(x1,x2)\n    return AND(s1,s2)\n```\n运行结果\n{% asset_img 20190704194406108.png %}\n### 扩展\n#### 3层神经网络实现的异或门\n实际3层神经网络已经可以实现所有逻辑门了,可以表示所有线性,非线性空间了(只要每层结点数量,学习率$\\alpha$,世代次数(epoch)设定适当)\n```python\nimport numpy as np\n\nclass NeuralNetwork:\n    def __init__(self,x_num,a_num,y_num):\n        self.W1 = np.random.normal(0.0, pow(a_num, -0.5), (x_num,a_num))\n        self.W2 = np.random.normal(0.0, pow(y_num, -0.5), (a_num,y_num))\n\n    def acfun(self,x):\n        return 1.0 / (1 + np.exp(-x))\n\n    def train(self,x,y,alpha=0.1):\n        x = np.array(x,ndmin=2)\n        y = np.array(y,ndmin=2)\n\n        # forward\n        a1 = x@self.W1\n        z1 = self.acfun(a1)\n        a2 = z1@self.W2\n        z2 = self.acfun(a2)\n\n        # backward\n        e2 = y - z2 # 目标值-输出值\n        self.W2 += alpha * (z1.T@(e2*z2*(1-z2)))\n        e1 = e2@self.W2.T\n        self.W1 += alpha * (x.T@(e1*z1*(1-z1)))\n\n    def query(self,x):\n        x = np.array(x,ndmin=2)\n\n        # forward\n        a1 = x@self.W1\n        z1 = self.acfun(a1)\n        a2 = z1@self.W2\n        z2 = self.acfun(a2)\n        return z2\n\n\nx_num = 2\na_num = 30\ny_num = 2\nepoch = 5000\nx = [[0,0],[0,1],[1,0],[1,1]]\ny = [[1,0],[0,1],[0,1],[1,0]]\n\nneuralNetwork = NeuralNetwork(x_num,a_num,y_num)\nfor i in range(epoch):\n    neuralNetwork.train(x,y)\nans_set = []\nfor item in x:\n    ans = neuralNetwork.query(item)\n    print(item,ans)\n    ans_set.append(ans.argmax())\nprint(ans_set)\n```\n#### 4层神经网络实现的异或门\n```python\nimport numpy as np\n\nclass NeuralNetwork:\n    def __init__(self,x_num,a1_num,a2_num,y_num):\n        self.W1 = np.random.normal(0.0, pow(a1_num, -0.5), (x_num,a1_num))\n        self.W2 = np.random.normal(0.0, pow(a2_num, -0.5), (a1_num,a2_num))\n        self.W3 = np.random.normal(0.0, pow(y_num, -0.5), (a2_num,y_num))\n\n    def acfun(self,x):\n        return 1.0 / (1 + np.exp(-x))\n\n    def train(self,x,y,alpha=0.1):\n        x = np.array(x,ndmin=2)\n        y = np.array(y,ndmin=2)\n\n        # forward propagation\n        a1 = x@self.W1\n        z1 = self.acfun(a1)\n        a2 = z1@self.W2\n        z2 = self.acfun(a2)\n        a3 = z2@self.W3\n        z3 = self.acfun(a3)\n\n        # backward propagation\n        e3 = y - z3 # 目标值-输出值\n        self.W3 += alpha * (z2.T@(e3*z3*(1-z3)))\n        e2 = e3@self.W3.T\n        self.W2 += alpha * (z1.T@(e2*z2*(1-z2)))\n        e1 = e2@self.W2.T\n        self.W1 += alpha * (x.T@(e1*z1*(1-z1)))\n\n    def query(self,x):\n        x = np.array(x,ndmin=2)\n\n        # forward propagation\n        a1 = x@self.W1\n        z1 = self.acfun(a1)\n        a2 = z1@self.W2\n        z2 = self.acfun(a2)\n        a3 = z2@self.W3\n        z3 = self.acfun(a3)\n        return z3\n\n\nx_num = 2\na1_num = 5\na2_num = 5\ny_num = 2\nepoch = 5000\nx = [[0,0],[0,1],[1,0],[1,1]]\ny = [[1,0],[0,1],[0,1],[1,0]]\n\nneuralNetwork = NeuralNetwork(x_num,a1_num,a2_num,y_num)\nfor i in range(epoch):\n    neuralNetwork.train(x,y)\nans_set = []\nfor item in x:\n    ans = neuralNetwork.query(item)\n    print(item,ans)\n    ans_set.append(ans.argmax())\nprint(ans_set)\n```\n运行结果\n{% asset_img 20190705090929116.png %}\n# 扩展\n感知机的简单实现\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\n\ndef fit(X,y,alpha=0.01):\n    X=np.array(X)\n    y=np.array(y)\n    w=np.zeros(X.shape[1])\n    b=np.zeros(1)\n    while sum(y * (X @ w + b) <= 0):\n        mis_predict = y * (X @ w + b) <= 0\n        X_tmp = X[mis_predict]\n        y_tmp = y[mis_predict]\n        for i in range(sum(mis_predict)):\n            w=w+alpha*(X_tmp[i].T*y_tmp[i])\n            b=b+alpha*y_tmp[i]\n    return w,b\n\n\ndef auto_norm(X):\n    X = np.array(X)\n    minVals = X.min(0)\n    maxVals = X.max(0)\n    newVals = (X - minVals) / (maxVals - minVals)\n    return newVals\n\ndef main():\n    raw_x, raw_y = load_iris(return_X_y=True)\n    x = auto_norm(raw_x[:100, :2])\n    y = raw_y[:100] * 2 - 1\n    w, b = fit(x, y)\n    x0_min = x[:100, 0].min()\n    x0_max = x[:100, 0].max()\n    x1_min = -(w[0] * x0_min + b) / w[1]\n    x1_max = -(w[0] * x0_max + b) / w[1]\n    plt.title('perceptron')\n    plt.scatter(x[:50, 0], x[:50, 1], label='-1')\n    plt.scatter(x[-50:, 0], x[-50:, 1], label='1', marker='x')\n    plt.plot([x0_min, x0_max], [x1_min, x1_max],\n             label='fitting curve', color='r')\n    plt.legend()\n    plt.show()\n\n\nif __name__ == \"__main__\":\n    main()\n```\n程序运行结果如下:\n{% asset_img 2019072908425227.png %}\n显示每次迭代的结果\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\n\ndef fit(X,y,alpha=0.01):\n    X=np.array(X)\n    y=np.array(y)\n    w=np.ones(X.shape[1])\n    b=np.ones(1)\n    W=[]\n    B=[]\n    while sum(y * (X @ w + b) <= 0):\n        mis_predict = y * (X @ w + b) <= 0\n        X_tmp = X[mis_predict]\n        y_tmp = y[mis_predict]\n        for i in range(sum(mis_predict)):\n            w=w+alpha*(X_tmp[i].T*y_tmp[i])\n            b=b+alpha*y_tmp[i]\n            W=W+[w]\n            B=B+[b]\n    return np.array(W),np.array(B)\n\n\ndef auto_norm(X):\n    X = np.array(X)\n    minVals = X.min(0)\n    maxVals = X.max(0)\n    newVals = (X - minVals) / (maxVals - minVals)\n    return newVals\n\ndef main():\n    raw_x, raw_y = load_iris(return_X_y=True)\n    x = auto_norm(raw_x[:100, :2])\n    y = raw_y[:100] * 2 - 1\n    W, B = fit(x, y)\n    x0_min = x[:100, 0].min()\n    x0_max = x[:100, 0].max()\n    plt.title('perceptron')\n    plt.scatter(x[:50, 0], x[:50, 1], label='-1')\n    plt.scatter(x[-50:, 0], x[-50:, 1], label='1', marker='x')\n    for i in range(W.shape[0]):\n        w,b=W[i],B[i]\n        x1_min = -(w[0] * x0_min + b) / w[1]\n        x1_max = -(w[0] * x0_max + b) / w[1]\n        c='yellow'\n        if i==W.shape[0]-1:\n            c='red'\n        plt.plot([x0_min, x0_max], [x1_min, x1_max],\n                 label='fitting curve', color=c,alpha=float(i+1)/(W.shape[0]))\n    plt.xlim((x[:100, 0].min(), x[:100, 0].max()))\n    plt.ylim((x[:100, 1].min(), x[:100, 1].max()))\n    plt.show()\n\n\nif __name__ == \"__main__\":\n    main()\n```\n显示每次迭代后的结果如下图:\n{% asset_img 2019072909403428.png %}\n### 参考\n1. [深度学习入门-基于Python的理论与实现](https://book.douban.com/subject/30270959/)","tags":["深度学习入门"],"categories":["深度学习"]},{"title":"[Python神经网络编程]总结","url":"%2Fposts%2Fb861fb24%2F","content":"\n### 介绍\n误差值(E)=期望目标值-实际输出值\n#### 核心公式\n这里将书上的公式改为博主习惯的用法了,比如权重按照正常矩阵形式排列了,因此一些转置的地方没有进行转置,而没转置的地方可能转置了,公式会有些变化\n$$\\begin{aligned}\nW_{input\\_hidden}&=\\begin{bmatrix}\n        w_{1,1}&w_{1,2}&w_{1,3}\\\\\n        w_{2,1}&w_{2,2}&w_{2,3}\\\\\n        w_{3,1}&w_{3,2}&w_{3,3}\\\\\n    \\end{bmatrix}\n\\end{aligned}$$ \n$$\\begin{aligned}\n    \\frac{\\partial{E}}{\\partial{W_{i,j}}}&=-(e_j)\\cdot{sigmoid(\\Sigma_{i}W_{i,k}\\cdot{O_i})(1-sigmoid(\\Sigma_{i}W_{i,k}\\cdot{O_i}))}\\cdot{O_i}\\\\\n\\end{aligned}$$\n$$\\begin{aligned}\n    \\frac{\\partial{E}}{\\partial{W_{j,k}}}&=-(e_k)\\cdot{sigmoid(\\Sigma_{j}W_{j,k}\\cdot{O_j})}(1-sigmoid(\\Sigma_{j}W_{j,k}\\cdot{O_j}))\\cdot{O_j}\\\\\n\\end{aligned}$$\n$$\\begin{aligned}\n    newW_{j,k}&=oldW_{j,k}-\\alpha\\cdot{\\frac{\\partial{E}}{\\partial{W_{j,k}}}}\\\\\n    \\Delta{W_{j,k}}&=\\alpha*(O_j^T\\cdot{(E_k*{O_k*(1-O_k))}})\n\\end{aligned}$$\n注意:每个结点所连的链接权重对应于$W$的一列,即每列表示一个结点的链接权重\n#### trian算法\n- **预测输出**:\n  将输入带入,进行预测输出\n    $$\\begin{aligned}\n        H_{inputs}&=I\\cdot W_{input\\_hidden}\\\\\n        H_{outputs}&=sigmoid(H_{inputs})\\\\\n        O_{inputs}&=H_{outputs}\\cdot W_{hidden\\_output}\\\\\n        O_{outputs}&=sigmoid(O_{inputs})\\\\\n    \\end{aligned}$$\n- **反向传播**:\n  将期望目标值与实际输出值做差,反向传播误差,并按权重大小进行每一层的权重更新\n    $$\\begin{aligned}\n        E_{output\\_layer}&=T - O_{outpus}\\\\\n        W_{hidden\\_output}&+=\\alpha*(H_{outputs}^T\\cdot (E_{output\\_layer}*O_{outputs}*(1-O_{outputs})))\\\\\n        E_{hidden\\_layer}&=E_{output\\_layer}\\cdot W_{hidden\\_output}^T\\\\\n        W_{input\\_hidden}&+=\\alpha*(I^T\\cdot (E_{hidden\\_layer}*H_{outpus}*(1-H_{outpus})))\\\\\n    \\end{aligned}$$\n> 注:\n> $I,H,O$: 表示`输入层,隐藏层,输出层`\n> $T$: 表示`目标值`, $E$: 表示`误差值`, $W$: 表示`权重值`\n> $*$: `元素级乘法`, $\\cdot$: `点积(或矩阵乘法)`\n\n#### query算法\n- **预测输出**: 与train算法部分一样,然后返回输出结果\n\n**初始化权重:网络的核心**\n- **随机值(-0.5~0.5)**\n  {% asset_img 2019070409024091.png %}\n- **可选项:较复杂的权重(正态分布)**\n  {% asset_img 2019070409093792.png %}\n### 代码实现\n数据集下载:\n[mnist_train.csv(104MB)](mnist_train.csv)\n[mnist_test.csv(17.4MB)](mnist_test.csv)\n[mnist_train_100.csv(177KB)](mnist_train_100.csv)\n[mnist_test_10.csv(17.5KB)](mnist_test_10.csv)\n先用imshow来显示下图形\n{% asset_img 2019070415020895.png %}\n#### **3层神经网络**\n以下是自己写的,感觉原书公式有些不适合自己(改变了有些转置的地方),初始权重采用随机权重(-0.5~0.5)的3层神经网络\n```python\nimport numpy as np\n\nclass NeuralNetwork:\n    def __init__(self, I_num, H_num, O_num):\n        # 初始化输入层,隐藏层,输出层结点数量\n        self.I_num = I_num\n        self.H_num = H_num\n        self.O_num = O_num\n\n        # 初始化权重(采用正态分布)\n        self.W_ih=np.random.normal(0.0, pow(H_num,-0.5), (I_num,H_num))\n        self.W_ho=np.random.normal(0.0, pow(O_num,-0.5), (H_num,O_num))\n\n    # 激活函数采用sigmoid函数\n    def S(self, x):\n        return 1.0 / (1 + np.e ** (-x))\n\n    def train(self, inputs, targets, alpha=0.1):\n        inputs = np.array(inputs,ndmin=2)\n        targets = np.array(targets,ndmin=2)\n\n        H_i = inputs @ self.W_ih\n        H_o = self.S(H_i)\n        O_i = H_o @ self.W_ho\n        O_o = self.S(O_i)\n\n        E_o = targets - O_o\n        E_h = E_o @ self.W_ho.T\n        self.W_ho += alpha * (H_o.T @ (E_o * O_o * (1 - O_o)))\n        self.W_ih += alpha * (inputs.T @ (E_h * H_o * (1 - H_o)))\n\n    def query(self, inputs):\n        inputs = np.array(inputs, ndmin=2)\n\n        H_i = inputs @ self.W_ih\n        H_o = self.S(H_i)\n        O_i = H_o @ self.W_ho\n        O_o = self.S(O_i)\n        return O_o\n\n# 输入层节点数\ninput_nodes = 784\n# 隐藏层节点数\nhidden_nodes = 100\n# 输出层节点数\noutput_nodes = 10\n\n# 建立神经网络模型\nneuralNetwork = NeuralNetwork(input_nodes, hidden_nodes, output_nodes)\n\ndata_file = open('mnist_test.csv')\ndata_list = data_file.readlines()\ndata_file.close()\n\n# 交叉验证\nn = len(data_list)\ntrain_size = int(n * 0.9)\ntest_size = n - train_size\ntrain_list = data_list[:train_size]\ntest_list = data_list[-test_size:]\n\n# 5次迭代训练模型\nepoch = 5\nfor i in range(epoch):\n    for record in train_list:\n        all_values = record.split(',')\n        inputs = np.asfarray(all_values[1:]) / 255.0\n        targets = np.zeros(output_nodes)\n        targets[int(all_values[0])] = 1\n        neuralNetwork.train(inputs,targets)\n\n# 进行预测并计算正确率\ncorrect_labels = []\npredict_labels = []\nfor record in test_list:\n    all_values = record.split(',')\n    correct_label = int(all_values[0])\n    correct_labels.append(correct_label)\n    inputs = np.asfarray(all_values[1:]) / 255.0\n    outputs = neuralNetwork.query(inputs)\n    label = np.argmax(outputs)\n    predict_labels.append(label)\n\n# 计算预测正确率\npredict_labels = np.array(predict_labels)\ncorrect_labels = np.array(correct_labels)\nTrueCount = predict_labels==correct_labels\nscore = sum(TrueCount) * 1.0/len(TrueCount)\nprint(score)\n```\n以下为运行结果\n{% asset_img 2019070410100693.png %}\n结果仍然是93%,结果还是不错的\n#### **4层神经网络**\n```python\nimport numpy as np\n\nclass NeuralNetwork:\n    def __init__(self, I_num, H1_num, H2_num, O_num):\n        # initialize node num\n        self.I_num = I_num\n        self.H1_num = H1_num\n        self.H2_num = H2_num\n        self.O_num = O_num\n        # initialize weights\n        self.W1 = np.random.normal(0.0, pow(H1_num,-0.5), (I_num,H1_num))\n        self.W2 = np.random.normal(0.0, pow(H2_num,-0.5), (H1_num,H2_num))\n        self.W3 = np.random.normal(0.0, pow(O_num,-0.5), (H2_num,O_num))\n\n        #self.W1 = np.random.rand(I_num, H1_num) - 0.5\n        #self.W2 = np.random.rand(H1_num, H2_num) - 0.5\n        #self.W3 = np.random.rand(H2_num, O_num) - 0.5\n\n    def S(self, x):\n        return 1.0 / (1 + np.e ** (-x))\n\n    def train(self, inputs, targets, alpha=0.1):\n        inputs = np.array(inputs,ndmin=2)\n        targets = np.array(targets,ndmin=2)\n\n        H1_i = inputs @ self.W1\n        H1_o = self.S(H1_i)\n        H2_i = H1_o @ self.W2\n        H2_o = self.S(H2_i)\n        O_i = H2_o @ self.W3\n        O_o = self.S(O_i)\n\n        E3 = targets - O_o\n        E2 = E3 @ self.W3.T\n        E1 = E2 @ self.W2.T\n        self.W3 += alpha * (H2_o.T @ (E3 * O_o * (1 - O_o)))\n        self.W2 += alpha * (H1_o.T @ (E2 * H2_o * (1 - H2_o)))\n        self.W1 += alpha * (inputs.T @ (E1 * H1_o * (1 - H1_o)))\n\n    def query(self, inputs):\n        inputs = np.array(inputs, ndmin=2)\n\n        H1_i = inputs @ self.W1\n        H1_o = self.S(H1_i)\n        H2_i = H1_o @ self.W2\n        H2_o = self.S(H2_i)\n        O_i = H2_o @ self.W3\n        O_o = self.S(O_i)\n        return O_o\n\nI_num = 784\nH1_num = 100\nH2_num = 100\nO_num = 10\n\nneuralNetwork = NeuralNetwork(I_num, H1_num, H2_num, O_num)\n\ndata_file = open('mnist_test.csv')\ndata_list = data_file.readlines()\ndata_file.close()\n\nn = len(data_list)\ntrain_size = int(n * 0.9)\ntest_size = n - train_size\ntrain_list = data_list[:train_size]\ntest_list = data_list[-test_size:]\n\n\nepoch = 1\nfor i in range(epoch):\n    for record in train_list:\n        all_values = record.split(',')\n        inputs = np.asfarray(all_values[1:]) / 255.0\n        targets = np.zeros(O_num)\n        targets[int(all_values[0])] = 1\n        neuralNetwork.train(inputs,targets)\n\n\ncorrect_labels = []\npredict_labels = []\nfor record in test_list:\n    all_values = record.split(',')\n    correct_label = int(all_values[0])\n    correct_labels.append(correct_label)\n    inputs = np.asfarray(all_values[1:]) / 255.0\n    outputs = neuralNetwork.query(inputs)\n    label = np.argmax(outputs)\n    predict_labels.append(label)\npredict_labels = np.array(predict_labels)\ncorrect_labels = np.array(correct_labels)\nTrueCount = predict_labels==correct_labels\nscore = sum(TrueCount) * 1.0/len(TrueCount)\nprint(score)\n```\n{% asset_img 2019070416372896.png %}\n发现神经网络层数多了并不一定就很好\n### 扩展\n#### **3层神经网络实现异或(XOR)**\n```python\nimport numpy as np\n\nclass NeuralNetwork:\n    def __init__(self, I_num, H_num, O_num):\n        # initialize node num\n        self.I_num = I_num\n        self.H_num = H_num\n        self.O_num = O_num\n        # initialize weights\n        # self.W_ih=np.random.normal(0.0, pow(H_num,-0.5), (I_num,H_num))\n        # self.W_ho=np.random.normal(0.0, pow(O_num,-0.5), (H_num,O_num))\n        self.W_ih = np.random.rand(I_num, H_num) - 0.5\n        self.W_ho = np.random.rand(H_num, O_num) - 0.5\n\n    def S(self, x):\n        return 1.0 / (1 + np.e ** (-x))\n\n    def train(self, inputs, targets, alpha=0.1):\n        inputs = np.array(inputs,ndmin=2)\n        targets = np.array(targets,ndmin=2)\n\n        H_i = inputs @ self.W_ih\n        H_o = self.S(H_i)\n        O_i = H_o @ self.W_ho\n        O_o = self.S(O_i)\n\n        E_o = targets - O_o\n        E_h = E_o @ self.W_ho.T\n        self.W_ho += alpha * (H_o.T @ (E_o * O_o * (1 - O_o)))\n        self.W_ih += alpha * (inputs.T @ (E_h * H_o * (1 - H_o)))\n\n    def query(self, inputs):\n        inputs = np.array(inputs, ndmin=2)\n\n        H_i = inputs @ self.W_ih\n        H_o = self.S(H_i)\n        O_i = H_o @ self.W_ho\n        O_o = self.S(O_i)\n        return O_o\n\nI_num=2\nH_num=30\nO_num=2\n\ndatas=[[0,0],[1,0],[0,1],[1,1]]\ntargets=[[1,0],[0,1],[0,1],[1,0]]\nneuralNetwork = NeuralNetwork(I_num,H_num,O_num)\nepoch=5000\nfor i in range(epoch):\n    neuralNetwork.train(datas,targets,0.1)\n\noutpus=[]\nfor i in range(len(datas)):\n    ans=neuralNetwork.query(datas[i])\n    print(datas[i],ans)\n    label=ans.argmax()\n    outpus.append(label)\nprint(outpus)\n```\n{% asset_img 2019070416492598.png %}\n可以看到结果,正确实现了XOR的功能\n之前设置隐藏层2个结点,发现总是有错,后来将隐藏层设置为30个结点才,再把epoch设置比较大才将结果区分开来(epoch设置较小也会经常有错)\n实际3层神经网络已经近乎可以实现所有逻辑运算了(与,或,异或等等),只需要改变输入数据集和目标集\n博主试验了`与,或,与非,或非`都近乎完美预测\n**与:**\n```python\ndatas=[[0,0],[1,0],[0,1],[1,1]]\ntargets=[[1,0],[1,0],[1,0],[0,1]]\n```\n**或:**\n```python\ndatas=[[0,0],[1,0],[0,1],[1,1]]\ntargets=[[1,0],[0,1],[0,1],[0,1]]\n```\n**与非:**\n```python\ndatas=[[0,0],[1,0],[0,1],[1,1]]\ntargets=[[0,1],[0,1],[0,1],[1,0]]\n```\n**或非:**\n```python\ndatas=[[0,0],[1,0],[0,1],[1,1]]\ntargets=[[0,1],[1,0],[1,0],[1,0]]\n```\n### 参考\n1. [Python神经网络编程](https://book.douban.com/subject/30192800/)","tags":["Python神经网络编程"],"categories":["神经网络"]},{"title":"[机器学习实战]第14章 利用SVD来简化数据","url":"%2Fposts%2F587a303d%2F","content":"### SVD\n- **介绍**: 利用SVD实现,我们能够用小得多的数据集来表示原始数据集.这样做,实际上是去除了噪声和冗余信息.当我们试图节省空间时,去除信息就是很崇高的目标了,但是在这里我们则是从数据中抽取信息.基于这个视角,我们就可以把SVD看成是从噪声数据中抽取相关特征.\n- **矩阵分解**: 在很多情况下,数据中的一小段携带了数据集中的大部分信息,其他信息则要么是噪声,要么就是毫不相关的信息.在线性代数中还有很多矩阵分解技术.矩阵分解可以将原始矩阵表示成新的易于处理的形式,这种新形式是两个或多个矩阵的乘积.最常见的一种矩阵分解技术就是SVD. SVD将原始的数据集矩阵`Data`分解成三个矩阵$U,\\Sigma,V^T$.如果原始矩阵`Data`是`m`行`n`列,那么$U,\\Sigma,V^T$就分别是`m`行`m`列,`m`行`n`列和`n`行`n`列.上述过程可以写成如下一行:\n  $$\\begin{aligned}\n      Data_{m\\times{n}}=U_{m\\times{m}}\\Sigma_{m\\times{n}}V_{n\\times{n}}^T\\approx{U_{m\\times{k}}\\Sigma_{k\\times{k}}V_{k\\times{n}}^T}\n  \\end{aligned}$$\n  上述分解中会构建出一个矩阵$\\Sigma$,该矩阵只有对角元素,其他元素均为0(在numpy中,则直接返回对角元素).另一个惯例就是,$\\Sigma$的对角元素是从大到小排列的.这些对角元素称为`奇异值(Singular Value)`,它们对应了原始数据集矩阵`Data`的奇异值.回想上一章的PCA,我们得到的是矩阵的特征值,它们告诉我们数据集中的重要特征. $\\Sigma$中的奇异值也是如此. 奇异值和特征值是有关系的. 这里的奇异值就是矩阵$Data*Data^T$特征值的平方根.\n  前面提到过,矩阵$\\Sigma$只有从大到小排列的对角元素. 在科学和工程中,一直存在这样一个普遍事实:在某个奇异值的数目(r个)之后,其他的奇异值都置为0. 这就意味着数据集中仅有r个重要特征,而其余特征都是噪声或冗余特征.\n  - **SVD的应用代码**\n    {% asset_img 2019062909030576.png %}\n    返回的$U_{7\\times{7}},\\Sigma_{5\\times{5}},V^T_{5\\times{5}}$,numpy中是直接返回$\\Sigma$的对角元素的,因此我们应当默认把它看成一个矩阵\n    $$\\begin{aligned}\n        Data_{7\\times{5}}=U_{7\\times{3}}\\Sigma_{3\\times{3}}V^T_{3\\times{5}}\n    \\end{aligned}$$\n    因此,我们可以把$\\Sigma$的最后两个值去掉,然后我们来进行数据的还原\n    {% asset_img 2019062909144077.png %}\n    可以看到,结果很好的还原了原来数据,需要注意的是,只有在$\\Sigma$里值的差异较为巨大时,才能将较小的去掉,否则无法较好的还原原始的数据\n### 相似度计算\n- **欧几里得距离**\n  $$\\begin{aligned}\n      similarity=\\frac{1}{1+distance}\n  \\end{aligned}$$\n  相似度限制在0.0~1.0之间,当距离为0时,则相似度为1.0.距离非常大时,相似度也就趋近于0\n  下面用代码实现一下\n  {% asset_img 2019062910143882.png %}\n- **皮尔逊相关系数(Pearson correlation)**\n  $$\\begin{aligned}\n      &R_{ij}=\\frac{C_{ij}}{C_{ii}\\cdot{C_{jj}}}\n  \\end{aligned}$$\n  上式为皮尔逊相关系数的计算公式,C是协方差矩阵(Covariance matrix),numpy中的实现是`numpy.corrcoef()`,返回值为`-1.0~1.0`,为了限制其范围为`0.0~1.0`,则可作如下修改\n  $$\\begin{aligned}\n      similarity&=0.5+0.5R_{ij}\\\\\n                &=0.5+0.5\\frac{C_{ij}}{\\sqrt{C_{ii}\\cdot{C_{jj}}}}\n  \\end{aligned}$$\n  下面用代码实现一下\n  {% asset_img 2019062910082581.png %}\n  自己实现的pearson similariy和书上例子实现的是一样的结果\n- **余弦相似度(cosine similarity)**\n  $$\\begin{aligned}\n      similarity&=\\cos{\\theta}\\\\\n                &=\\frac{\\mathbf{xy}}{\\mathbf{\\Vert{x}\\Vert\\Vert{y}\\Vert}}\n  \\end{aligned}$$\n  下面用代码实现一下\n  {% asset_img 2019062910155883.png %}\n### 参考\n1. [<<机器学习实战>>第14章 利用SVD来简化数据](https://book.douban.com/subject/24703171/)","tags":["机器学习实战"],"categories":["机器学习"]},{"title":"[机器学习实战]第13章 利用PCA来简化数据","url":"%2Fposts%2Fd93c4c05%2F","content":"\n### 降维方法\n- **主成分分析(Principal Component Analysis,PCA)**: 数据从原来的坐标系转换到了新的坐标系,新坐标系的选择是由数据本身决定的.第一个新坐标轴选择的是原始数据中方差最大的方向,第二个新坐标轴的选择和第一个坐标轴正交且具有最大方差的方向.该过程一直重复,重复次数为原始数据中特征的数目.我们会发现,大部分方差都包含在最前面的几个新坐标轴中.因此,我们可以忽略余下的坐标轴,即对数据进行了降维处理.\n- **因子分析(Factor Analysis)**: 在因子分析中,我们假设在观察数据的生成中有一些观察不到的`隐变量(laten variable)`.假设观察数据是这些隐变量和某些噪声的线性组合.那么隐变量的数据可能比观察数据的数目少,也就是说通过找到隐变量就可以实现数据的降维.\n- **独立成分分析(Independent Component Analysis,ICA)**: ICA假设数据是从`N`个数据源生成的,这一点和因子分析有些类似.假设数据为多个数据源的混合观察结果,这些数据源之间在统计上是相互独立的,而在PCA中只假设数据是不相关的.同因子分析一样,如果数据源的数目少于观察数据的数目,则可以实现降维过程.\n### PCA\n- **理论**\n  $$\\begin{aligned}\n      X&=(X_1,X_2,...,X_n)\\\\\n      Y&=PX\\\\\n      S_x&=\\frac{1}{n}XX^T\\\\\n      S_y&=\\frac{1}{n}YY^T\\\\\n         &=\\frac{1}{n}P(XX^T)P^T\\\\\n        A&=XX^T\\to E^TAE=D\\\\\n        A&=EDE^T\\\\\n        P&=E^T\\\\\n        A&=P^TDP\\\\\n       S_y&=\\frac{1}{n}P(P^TDP)P^T\\\\\n       S_y&=\\frac{1}{n}D\n  \\end{aligned}$$\n  详细推导过程见:\n  1. [PCA-Tutorial-Intuition_jp.pdf](PCA-Tutorial-Intuition_jp.pdf)\n  2. [[numpy]notes](../fd60ea9b/#cov)\n- **伪代码**\n  将数据转换成前N个主成分的伪代码大致如下:\n  - 去除平均值\n  - 计算协方差矩阵\n  - 计算协方差矩阵的特征值和特征向量\n  - 将特征值从大到小排序\n  - 保留最前面N个特征值对应的特征向量\n  - 将数据转换到上述N个特征向量构建的新空间中\n- **代码实现**\n  数据集下载:[testSet.zip](testSet.zip)\n  ```python\n    import numpy as np\n    import matplotlib.pyplot as plt\n\n    def loadDataSet(fileName,delim='\\t'):\n        dataMat = []\n        fr = open(fileName)\n        for line in fr.readlines():\n            curLine = line.strip().split('\\t')\n            fltLine = list(map(float, curLine))\n            dataMat.append(fltLine)\n        return np.mat(dataMat)\n\n    def pca(dataMat,topNfeat=9999999):\n        # 取平均值\n        meanVals=np.mean(dataMat,axis=0)\n        # zero-mean零均值化\n        meanRemoved=dataMat-meanVals\n        # meanRemoved=[x_1,...,x_n],求Cov(meanRemoved,meanRemoved)协方差\n        covMat=np.cov(meanRemoved,rowvar=False)\n        # 求出协方差矩阵的特征值与特征向量\n        eigVals,eigVects=np.linalg.eig(np.mat(covMat))\n        # 将特征值排序\n        eigValInd=np.argsort(eigVals)\n        # 选择最大特征值对应的特征向量,将其构成矩阵\n        eigValInd=eigValInd[:-(topNfeat+1):-1]\n        redEigVects=eigVects[:,eigValInd]\n        # 降维\n        lowDDataMat=meanRemoved*redEigVects\n        reconMat=(lowDDataMat*redEigVects.T)+meanVals\n        return lowDDataMat,reconMat\n\n    dataMat = loadDataSet('testSet.txt')\n    lowDMat,reconMat=pca(dataMat,1)\n    plt.scatter(dataMat[:,0].A1, dataMat[:,1].A1,marker='o',s=50)\n    plt.scatter(reconMat[:,0].A1,reconMat[:,1].A1,marker='o',s=10,c='red')\n    plt.show()\n  ```\n  效果图如下\n  {% asset_img 2019062820362773.png %}\n\n### 小结\n降维技术使得数据变得更易使用,并且它们往往能够去除数据中的噪声,使得其他机器学习任务更加精确.降维往往作为预处理步骤,在数据应用到其他算法之前清洗数据.有很多技术可以用于数据降维,在这些技术中,独立成分分析,因子分析和主成分分析比较流行,其中又以主成分分析应用最广泛.\nPCA可以从数据中识别其主要特征,它是通过沿着数据最大方差方向旋转坐标轴来实现的.选择方差最大的方向作为第一条坐标轴,后续坐标轴则与前面的坐标轴正交.协方差矩阵上的特征值分析可以用一系列的正交坐标轴来获取.\n### 参考\n1. [PCA-Tutorial-Intuition_jp.pdf](PCA-Tutorial-Intuition_jp.pdf)\n2. [<<机器学习实战>>第13章 利用PCA来简化数据](https://book.douban.com/subject/24703171/)\n   ","tags":["机器学习实战"],"categories":["机器学习"]},{"title":"[wiki]notes","url":"%2Fposts%2F30cbbdf1%2F","content":"### Expectation Value\n- **intro**: The expectation value of a function $f(x)$ in a variable $x$ is denoted $\\lang{f(x)\\rang}$ or $E\\{f(x)\\}$.\n- **single discrete variable**:\n  $$\\begin{aligned}\n      \\lang{f(x)}\\rang=\\sum_{x}{f(x)P(x)}\n  \\end{aligned}$$\n  $P(x)$ is `probability density function(概率密度函数)`.\n- **single continuous variable**:\n  $$\\begin{aligned}\n      \\lang{f(x)}\\rang=\\int{f(x)P(x)dx}\n  \\end{aligned}$$\n- **multiple discrete variables**:\n  $$\\begin{aligned}\n      \\lang{f(x_1,...,x_n)}\\rang=\\sum_{x_1,...,x_n}{f(x_1,...,x_n)P(x_1,...,x_n)}\n  \\end{aligned}$$\n- **multiple continuous variables**:\n  $$\\begin{aligned}\n      \\lang{f(x_1,...,x_n)}\\rang=\\int{f(x_1,...,x_n)}P(x_1,...,x_n)d{x_1,...,x_n}\n  \\end{aligned}$$\n### Variance\n- **intro**: For a single variate $X$ having a distribution $P(x)$ with known `population mean(总体均值)` $\\mu$, the `population variance(总体方差)` $var(X)$, commonly also written $\\sigma^2$, is defined as\n  $$\\begin{aligned}\n      \\sigma^2\\equiv\\lang(X-\\mu)^2\\rang\n  \\end{aligned}$$\n  where $\\mu$ is the `population mean(总体均值)` and $\\lang{X}\\rang$ denotes the `expectation value(期望值)` of $X$.\n- **discrete distribution(离散分布)**:\n  $$\\begin{aligned}\n      \\sigma^2=\\sum_{i=1}^N{P(x_i)(x_i-\\mu)^2}\n  \\end{aligned}$$\n- **continuous distribution(连续分布)**:\n  $$\\begin{aligned}\n      \\sigma^2=\\int{P(x)(x-\\mu)^2}dx\n  \\end{aligned}$$\n  The variance is therefore equal to the second `central moment` $\\mu_2$.\n  If the underlying distribution is not known, then the `sample variance` may be computed as:\n  $$\\begin{aligned}\n      s_N^2\\equiv{\\frac{1}{N}\\sum_{i=1}^N(x_i-\\bar{x})^2}\n  \\end{aligned}$$\n  where $\\bar{x}$ is the `sample mean`.\n  Note that the `sample variance` $s_N^2$ defined above is not an `unbiased estimator` for the `population variance` $\\sigma^2$. In order to obtain an unbiased estimator for $\\sigma^2$, it is necessary to instead define a \"bias-corrected sample variance\"\n  $$\\begin{aligned}\n      s_{N-1}^2\\equiv{\\frac{1}{N-1}\\sum_{i=1}^N(x_i-\\bar{x})^2}\n  \\end{aligned}$$\n  The distinction between $s_N^2$ and $s_{N-1}^2$ is a common source of confusion, and extreme care should be exercised when consulting the literature to determine which convention is in use, especially since the uninformative notation $s$ is commonly used for both.\n  The square root of the variance is known as the `standard deviation`.\n\n### standard deviation\n- **intro**: a measure that is used to quantify the amount of variation or dispersion of a set of data values. A `low standard deviation` indicates that the data points tend to be close to the mean (also called the `expected value`) of the set, while a `high standard deviation` indicates that the data points are spread out over a wider range of values.\n  $\\sigma$: population standard deviation\n  $s$: sample standard deviation\n  - **sample standard deviation(样品标准偏差)**:\n    ${x_1,x_2,...,x_N}$ are the observed valued of the sample items,$\\bar{x}$ is the mean value of these observations, N is the number of observations in the sample.\n    $$\\begin{aligned}\n        s=\\sqrt{\\frac{1}{N-1}\\sum_{i=1}^N(x_i-\\bar{x})^2}\n    \\end{aligned}$$\n  - **population standard deviation(总体标准偏差)**:\n    $$\\begin{aligned}\n        &\\sigma=\\sqrt{\\frac{1}{N}\\sum_{i=1}^N(x_i-\\mu)^2}\\\\\n        &\\mu=\\frac{1}{N}\\sum_{i=1}^Nx_i\n    \\end{aligned}$$\n  - **unbiased and biased estimator** The simplest possible normalization is $\\frac{1}{n}$. However, this provides a `biased estimation of variance` particularly for small `n`. The proper normalization for an `unbiased estimator` is $\\frac{1}{n-1}$.\n  - **mean deviation(平均偏差)**:\n    $$\\begin{aligned}\n        Mean\\ Deviation=\\frac{1}{N}\\sum_{i=1}^N|x_i-\\mu|\n    \\end{aligned}$$\n  - **zero mean(零均值化)**:\n    $$\\begin{aligned}\n        Zero\\ Mean=x_i-\\bar{x}\n    \\end{aligned}$$\n    用处:\n    随机变量$A=\\{a_1,a_2,...,a_n\\},B=\\{b_1,b_2,...,b_n\\}$,都已进行`零均值化`.\n    则求`方差`,`协方差`\n    $$\\begin{aligned}\n        &\\sigma_A^2=\\lang{a_ia_i}\\rang_i=E\\{a_ia_i\\}\\\\\n        &\\sigma_{AB}^2=\\lang{a_ib_i}\\rang_i=E\\{a_ib_i\\}\\\\\n    \\end{aligned}$$\n    将变量$A,B$可以等价于行向量:\n    $$\\begin{aligned}\n        &\\mathbf{a}=(a_1,a_2,...,a_n)\\\\\n        &\\mathbf{b}=(b_1,b_2,...,b_n)\\\\\n    \\end{aligned}$$\n    则\n    $$\\begin{aligned}\n        &\\sigma_{\\mathbf{ab}}^2\\equiv{\\frac{1}{n-1}\\mathbf{ab}^T}\\qquad(1)\\\\\n        &\\sigma_{\\mathbf{ab}}^2\\equiv{\\frac{1}{n}\\mathbf{ab}^T}\\qquad(2)\\\\\n    \\end{aligned}$$\n    因此,当\n    $$\\begin{aligned}\n        &X=(X_1,...,X_m)^T\\\\\n        &X_1=(x_{11},x_{12},...,x_{1n})^T\\\\\n    \\end{aligned}$$\n    时,covariance matrix(协方差矩阵)$S_X$表示为\n    $$\\begin{aligned}\n        \\mathbf{S_X}&\\equiv{\\frac{1}{n}}XX^T\\\\\n        &=\n        \\begin{bmatrix}\n            \\lang{X_1X_1}\\rang&\\lang{X_1X_2}\\rang&...&\\lang{X_1X_m}\\rang\\\\\n            \\lang{X_2X_1}\\rang&\\lang{X_2X_2}\\rang&...&\\lang{X_2X_m}\\rang\\\\\n            .&.&...&.\\\\\n            .&.&...&.\\\\\n            \\lang{X_mX_1}\\rang&\\lang{X_mX_2}\\rang&...&\\lang{X_mX_m}\\rang\\\\\n        \\end{bmatrix}\\\\\n        &=\n        \\begin{bmatrix}\n            \\sigma_1^2&\\sigma_{12}^2&...&\\sigma_{1m}^2\\\\\n            \\sigma_{21}^2&\\sigma_{2}^2&...&\\sigma_{2m}^2\\\\\n            .&.&...&.\\\\\n            .&.&...&.\\\\\n            \\sigma_{m1}^2&\\sigma_{m2}^2&...&\\sigma_{m}^2\\\\\n        \\end{bmatrix}\n    \\end{aligned}$$\n    主对角线上的元素值为`方差`,非主对角线上的元素值为`协方差`\n    (1)为unbiased estimation of variance,(2)为biased estimation of variance\n\n### 参考\n1. [Wolfram MathWorld](http://mathworld.wolfram.com)","tags":["wiki"],"categories":["OJ"]},{"title":"[paper]TOC汇总","url":"%2Fposts%2Ff5b26e6e%2F","content":"\n- [ ] https://www.youtube.com/watch?v=qD6iD4TFsdQ\n- [ ] Kernel Mean Matching (KMM) [Smola, ICML-08]\n- [ ] 基于特征的迁移学习方法\n  - [ ] [Transfer component analysis (TCA) [Pan, TKDE-11]](tnn11.pdf)\n  - [ ] [Spectral Feature Alignment (SFA) [Pan, WWW-10]](p751.pdf)\n  - [ ] [Geodesic flow kernel (GFK) [Duan, CVPR-12]](subspace-cvpr2012.pdf)\n  - [ ] [Transfer kernel learning (TKL) [Long, TKDE-15]](06964812.pdf)\n- [ ] 《小王爱迁移》系列之一：迁移成分分析(TCA)方法简介\n  - [ ] 降维方法\n    - [ ] 机器学习: PCA,LLE,Laplacian eigen-map\n    - [ ] 迁移学习: TCA\n- [ ] 《小王爱迁移》系列之三：深度神经网络的可迁移性\n  - [ ] [How transferable are features in deep neural networks?](5347-how-transferable-are-features-in-deep-neural-networks.pdf)\n- [ ] 《小王爱迁移》系列之四：深度迁移（DaNN、DDC、DAN）\n  - [ ] [Domain Adaptive Neural Networks for Object Recognition](1409.6041.pdf)\n- 2019\n  - [07/14] [A Survey on Transfer Learning](tkde_transfer_learning.pdf)\n  - [] [Boosting for Transfer Learning](tradaboost.pdf)\n  - TCA\n    - [] [Domain Adaptation via Transfer Component Analysis](tnn11.pdf)\n      - **TCA的方法:** 它试图学习在这两个域之下的一组公共的迁移成分,使得当投影到该子空间上时,不同域的数据分布的差异可以显著减小,并且可以保存数据属性。然后，可以在这个子空间中使用标准的机器学习方法来训练跨领域的分类或回归模型。\n      - **TCA的目标:** 发现那些不会导致分布在域间发生很大变化的成分，并且能够很好地保留原始数据的结构或与任务相关的信息.\n      - **TCA的思路:** 提出了一种新的降维方法，通过将数据投影到一个学习的转移子空间来减小域间的距\n      - [The Derivation of TCA](The_Derivation_of_TCA.pdf)\n  - JDA\n    - [] [Transfer Feature Learning with Joint Distribution Adaptation](joint-distribution-adaptation-iccv13.pdf)\n离。一旦找到子空间，就可以使用任何方法进行后续的分类、回归和聚类。\n  - [] [Frustratingly Easy Domain Adaptation](P07-1033.pdf)\n  - [] [Transfer Learning via Dimensionality Reduction](AAAI08-108.pdf)\n    - MMDE方法提出,有两个主要限制:\n      - 1. MMDE是直推式的,并不能泛化到样本之外的模式\n      - 2. MMDE通过求解计算量大的半定程序(SDP)来学习潜在空间(latent space)\n  - [] [Correcting Sample Selection Bias by Unlabeled Data](CS-2006-44.pdf)\n    - 核均值匹配(Kernal Mean Matching,KMM)方法的提出\n  - [ ] 降维\n    - [ ] 综述 [Dimensionality Reduction: A Comparative Review](Dimensionality_Reduction_A_Comparative_Review.pdf)\n    - [ ] MVU方法 [An Introduction to Nonlinear Dimensionality Reduction by Maximum Variance Unfolding](nldr_aaai06.pdf)\n    - [ ] MMD方法 [A Kernel Two-Sample Test](10.1.1.369.1078.pdf)\n    - [ ] PCA tutorial [A tutorial on Principal Components Analysis](principal_components.pdf)\n  - [ ] 核\n    - [ ] Universal Kernels [Universal Kernels](micchelli06a.pdf)\n  - [07/24] HDA\n    - [ ] [Learning Cross-Domain Landmarks for Heterogeneous Domain Adaptation](Learning_Cross-Domain_Landmarks_for_Heterogeneous_Domain_Adaptation.pdf)    \n      - [ ] [Supplementary Material for Learning Cross-Domain Landmarks for Heterogeneous Domain Adaptation](Supplementary_Material_for_Learning_Cross-Domain_Landmarks_for_Heterogeneous_Domain_Adaptation.pdf)\n      - [ ] [A Kernel Method for the Two-Sample-Problem](GreBorRasSchSmo07.pdf)\n      - [ ] [Learning with Augmented Features for Heterogeneous Domain Adaptation](1206.4660.pdf)\n      - [ ] [Cross-view Action Recognition over Heterogeneous Feature Spaces](Wu_Cross-View_Action_Recognition_2013_ICCV_paper.pdf)\n      - [ ] [Feature Space Independent Semi-Supervised Domain Adaptation via Kernel Matching](06866177.pdf)\n      - [ ] [Semi-Supervised Subspace Co-Projection for Multi-Class Heterogeneous Domain Adaptation](ecml15.pdf)\n    - [ ] [Multiview Consensus Graph Clustering](16_TIP_08501973.pdf)\n  - [07/28] LiWen\n    - [ ] [Semi-Supervised Learning by Augmented Distribution Alignment](1905.08171.pdf)\n      - [ ] [Semi-Supervised Learning Literature Survey](ssl_survey.pdf)\n      - [ ] [Semi-Supervised Learning Tutorial](sslicml07.pdf)\n","tags":["paper"],"categories":["paper"]},{"title":"[leetcode]139.单词拆分","url":"%2Fposts%2F3b51cbfc%2F","content":"{% asset_img 2019062613441663.png %}\n\n### 解题思路 \n```cpp\nbool wordBreak(string s, vector<string>& wordDict) {\n    vector<int> dp(s.size()+1);\n    unordered_set<string> set(wordDict.begin(),wordDict.end());\n    dp[0]=1;\n    for(int i=1;i<=s.size();++i)\n    {\n        for(int j=0;j<i;++j)\n        {\n            if (dp[j]&&set.count(s.substr(j,i-j)))\n            {\n                dp[i]=1;\n                break;\n            }\n        }\n    }\n    return dp.back();\n}\n```\n### 参考","tags":["dp"],"categories":["OJ"]},{"title":"[机器学习实战]第10章 利用K-均值聚类算法对未标注数据分组","url":"%2Fposts%2Fd64acfa9%2F","content":"\n### 介绍\n- **聚类**: 聚类是一种无监督的学习,它将相似的对象归到同一个簇中.它有点像全自动分类.聚类方法几乎可以应用于所有对象,簇内的对象越相似,聚类的效果越好.\n- **K-均值聚类**: 之所以称之为K-均值是因为它可以发现k个不同的簇,且每个簇的中心采用簇中所含值的均值计算而成.\n- **簇识别**: 簇识别给出聚类结果的含义.假定有一些数据,现在将相似数据归到一起,簇识别会告诉我们这些簇到底都是些什么.\n- **聚类与分类的区别**: 分类的目标事先已知,而聚类则不一样.因为其产生的结果与分类相同,而只是类别没有预先定义,聚类有时也被称为无监督分类(unsupervised classification).聚类分析试图将相似对象归入同一簇,将不相似对象归到不同簇.相似这一概念取决于所选择的相似度计算方法.\n### K-均值聚类算法\n- **介绍**: K-均值是发现给定数据集的k个簇的算法.簇个数k是用户给定的,每一个簇通过其质心(centroid),即簇中所有点的中心来描述.\n- **K-均值聚类算法的伪代码**\n  - 创建k个点作为起始质心(经常是随机选择)\n  - 当任意一个点的簇分配结果发生改变时\n    - 对数据集中的每个数据点\n      - 对每个质心\n        - 计算质心与数据点之间的距离\n      - 将数据点分配到距离其最近的簇\n    - 对每一个簇,计算簇中所有点的均值并将均值作为质心\n\n- **K-均值聚类算法代码实现**\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef loadDataSet(fileName):\n    dataMat = []\n    fr = open(fileName)\n    for line in fr.readlines():\n        curLine = line.strip().split('\\t')\n        fltLine=list(map(float,curLine))\n        dataMat.append(fltLine)\n    return dataMat\n\n# 欧式距离\ndef distEclud(vecA,vecB):\n    return np.sqrt(np.sum(np.power(vecA-vecB,2)))\n\n# 为给定数据集构建一个包含k个随机质心的集合\n# 随机质心必须要在整个数据集的边界之内\ndef randCent(dataSet,k):\n    n = np.shape(dataSet)[1]\n    centroids = np.mat(np.zeros((k,n)))\n    # 遍历每个特征的值,并随机生成质心,质心应在每个特征值的范围内,不能越界\n    for j in range(n):\n        minJ=min(dataSet[:,j])\n        rangeJ=float(max(dataSet[:,j]) - minJ)\n        centroids[:,j] = minJ + rangeJ * np.random.rand(k,1)\n    return centroids\n\n# k-均值聚类\n# 初始化k个随机质心(代表k个簇),然后对于每个样本点找到与其最近的质心,则划分为该簇\n# 重新计算质心,然后再次找最近点并划分簇,不断重复该过程,直到样本点的簇分配结果不再改变为止\ndef kMeans(dataSet, k, distMeas=distEclud, createCent=randCent):\n    centroidsList=[]\n    m=np.shape(dataSet)[0]\n    # 存放[类别,欧式距离^2]\n    clusterAssment=np.mat(np.zeros((m,2)))\n    # 随机生成k个质心\n    centroids=createCent(dataSet,k)\n    # 用于标记样本点的簇分配结果是否改变了\n    clusterChanged=True\n    while clusterChanged:\n        clusterChanged = False\n        for i in range(m):\n            minDist=np.inf; minIndex=-1\n            for j in range(k):\n                # 计算样本点到每个质心的距离\n                distJI=distMeas(centroids[j,:],dataSet[i,:])\n                # 选择离样本点距离最近簇作为该样本点的簇\n                if distJI < minDist:\n                    minDist = distJI; minIndex = j\n            # 判断本次样本点的簇分配结果是否与之前不同\n            if clusterAssment[i,0] != minIndex:\n                clusterChanged = True\n            # 记录样本点的[簇,欧式距离^2]\n            clusterAssment[i,:]=minIndex,minDist**2\n        # 将每次迭代的结果放入centroidsList,保存起来\n        centroidsList.append(centroids.copy())\n        # 更新质心,即拿簇中所有点的横,纵坐标取平均值赋值给质心\n        for cent in range(k):\n            ptsInClust=dataSet[np.nonzero(clusterAssment[:,0].A==cent)[0]]\n            centroids[cent,:]=np.mean(ptsInClust,axis=0) # axis=0时,求列平均值\n    return centroids,clusterAssment,centroidsList\n\nmarkers=['o','v','s','p','P','*','x','D']\ncolors=['b', 'g', 'r', 'c', 'm', 'y', 'k', 'w']\ndatMat=np.mat(loadDataSet('testSet.txt'))\nk=5\nmyCentroids,clustAssing,centroidsList=kMeans(datMat,k)\ncentroidsList=np.array(centroidsList)\n# 显示坐标点\nfor classLabel in set(clustAssing[:,0].A1):\n    selectSamples=(clustAssing[:,0].A1.T==classLabel)\n    plt.scatter(datMat[selectSamples,0].A1,datMat[selectSamples,1].A1,marker=markers[int(classLabel)],c=colors[int(classLabel)], label=int(classLabel))\n# 显示质心收敛(迭代)的过程(0~迭代次数)\nfor iterTime in range(len(centroidsList)):\n    for classLabel in range(k):\n        plt.scatter(centroidsList[iterTime,int(classLabel),0],centroidsList[iterTime,int(classLabel),1],s=100,marker='$'+str(iterTime)+'$',c=colors[int(classLabel)])\nplt.legend()\nplt.show()\n```\n最终效果图\n{% asset_img 2019062610455462.png %}\n这里的数字为质心的位置,数字表示质心迭代的次数,比如0表示初始的随机质心的位置,1表示经过一次迭代后质心的位置,图中用**红色圆圈**圈起来的0表示每个簇初始质心的位置,在经过一系列迭代以后,质心的位置最终确定下来,每个簇最终质心的位置用**红色叉叉**表示,可以看到图中经常会有一团密集的数字,表明质心在该位置处不断收敛,最终确定下来\n\n### 二分K-均值算法\n- **介绍**: 为克服K-均值算法收敛于局部最小值的问题,有人提出了另一个称为二分K-均值(bisecting K-means)的算法.该算法首先将所有点作为一个簇,然后将该簇一分为二.之后选择其中一个簇继续进行划分,选择哪一个簇进行划分取决于对其划分是否可以最大程度降低SSE的值.上述基于SSE的划分过程不断重复,直到得到用户指定的簇数目为止.\n- **二分K-均值算法的伪代码**\n  - 将所有点看成一个簇\n  - 当簇数目小于k时\n  - 对于每一个簇\n    - 计算总误差(这里感觉不需要)\n    - 在给定的簇上面进行K-均值聚类(k=2)\n    - 计算将该簇一分为二之后的总误差\n  - 选择使得误差最小的那个簇进行划分操作\n- **二分K-均值算法**\n  二分K-均值算法流程图\n  {% asset_img 2019062622282466.png %}\n- **二分K-均值算法实现**\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef loadDataSet(fileName):\n    dataMat = []\n    fr = open(fileName)\n    for line in fr.readlines():\n        curLine = line.strip().split('\\t')\n        fltLine=list(map(float,curLine))\n        dataMat.append(fltLine)\n    return dataMat\n\n# 欧式距离\ndef distEclud(vecA,vecB):\n    return np.sqrt(np.sum(np.power(vecA-vecB,2)))\n\n# 为给定数据集构建一个包含k个随机质心的集合\n# 随机质心必须要在整个数据集的边界之内\ndef randCent(dataSet,k):\n    n = np.shape(dataSet)[1]\n    centroids = np.mat(np.zeros((k,n)))\n    # 遍历每个特征的值,并随机生成质心,质心应在每个特征值的范围内,不能越界\n    for j in range(n):\n        minJ=min(dataSet[:,j])\n        rangeJ=float(max(dataSet[:,j]) - minJ)\n        centroids[:,j] = minJ + rangeJ * np.random.rand(k,1)\n    return centroids\n\n# k-均值聚类\n# 初始化k个随机质心(代表k个簇),然后对于每个样本点找到与其最近的质心,则划分为该簇\n# 重新计算质心,然后再次找最近点并划分簇,不断重复该过程,直到样本点的簇分配结果不再改变为止\ndef kMeans(dataSet, k, distMeas=distEclud, createCent=randCent):\n    m=np.shape(dataSet)[0]\n    # 存放[类别,欧式距离^2]\n    clusterAssment=np.mat(np.zeros((m,2)))\n    # 随机生成k个质心\n    centroids=createCent(dataSet,k)\n    # 用于标记样本点的簇分配结果是否改变了\n    clusterChanged=True\n    while clusterChanged:\n        clusterChanged = False\n        for i in range(m):\n            minDist=np.inf; minIndex=-1\n            for j in range(k):\n                # 计算样本点到每个质心的距离\n                distJI=distMeas(centroids[j,:],dataSet[i,:])\n                # 选择离样本点距离最近簇作为该样本点的簇\n                if distJI < minDist:\n                    minDist = distJI; minIndex = j\n            # 判断本次样本点的簇分配结果是否与之前不同\n            if clusterAssment[i,0] != minIndex:\n                clusterChanged = True\n            # 记录样本点的[簇,欧式距离^2]\n            clusterAssment[i,:]=minIndex,minDist**2\n        # 更新质心,即拿簇中所有点的横,纵坐标取平均值赋值给质心\n        for cent in range(k):\n            ptsInClust=dataSet[np.nonzero(clusterAssment[:,0].A==cent)[0]]\n            centroids[cent,:]=np.mean(ptsInClust,axis=0) # axis=0时,求列平均值\n    return centroids,clusterAssment\n\n# 二分K-均值分类\ndef biKmeans(dataSet, k, distMeas=distEclud):\n    m = np.shape(dataSet)[0]\n    # 存放[类别,欧式距离^2(误差)]\n    clusterAssment = np.mat(np.zeros((m,2)))\n    # 初始化一个质心\n    centroid0 = np.mean(dataSet, axis=0).tolist()[0]\n    # 初始化质心列表(放入一个质心)\n    centList =[centroid0]\n    # 计算初始误差(这里为欧式距离^2)\n    for j in range(m):\n        clusterAssment[j,1] = distMeas(np.mat(centroid0), dataSet[j,:])**2\n    # 当簇的个数小于k时,不断将簇进行划分(K-均值聚类)\n    while (len(centList) < k):\n        lowestSSE = np.inf\n        for i in range(len(centList)):\n            # 获取当前属于i簇的样本点\n            ptsInCurrCluster = dataSet[np.nonzero(clusterAssment[:,0].A==i)[0],:]\n            # 将第i簇进行K-均值聚类(k=2)\n            centroidMat, splitClustAss = kMeans(ptsInCurrCluster, 2, distMeas)\n            # 一分为二之后的总误差\n            sseSplit = sum(splitClustAss[:,1])#compare the SSE to the currrent minimum\n            # 其余数据的总误差\n            sseNotSplit = sum(clusterAssment[np.nonzero(clusterAssment[:,0].A!=i)[0],1])\n            #print(\"sseSplit, and notSplit: \",sseSplit,sseNotSplit)\n            # 一分为二的总误差+其余数据的总误差 < lowestSSE\n            if (sseSplit + sseNotSplit) < lowestSSE:\n                # 最好的划分中心\n                bestCentToSplit = i\n                bestNewCents = centroidMat\n                bestClustAss = splitClustAss.copy()\n                lowestSSE = sseSplit + sseNotSplit\n        # 最好的划分重新编号,划分为0的簇编号为原来的编号,划分为1的簇编号为新的编号\n        bestClustAss[np.nonzero(bestClustAss[:,0].A == 1)[0],0] = len(centList)\n        bestClustAss[np.nonzero(bestClustAss[:,0].A == 0)[0],0] = bestCentToSplit\n        #print('the bestCentToSplit is: ',bestCentToSplit)\n        #print('the len of bestClustAss is: ', len(bestClustAss))\n        # 更新质心,原来编号的质心更新\n        centList[bestCentToSplit] = bestNewCents[0,:].tolist()[0]\n        # 划分后,增加一个新的质心\n        centList.append(bestNewCents[1,:].tolist()[0])\n        # 新的簇分配结果更新\n        clusterAssment[np.nonzero(clusterAssment[:,0].A == bestCentToSplit)[0],:]= bestClustAss\n    # 返回质心列表,簇分配结果\n    return np.mat(centList), clusterAssment\n\n# 标记列表\nmarkers=['o','v','s','p','P','*','x','D']\n# 颜色列表\ncolors=['b', 'g', 'r', 'c', 'm', 'y', 'k', 'w']\ndatMat=np.mat(loadDataSet('testSet2.txt'))\nk=5\nmyCentroids,clustAssing=biKmeans(datMat,k)\n\nfor classLabel in set(clustAssing[:,0].A1):\n    selectSamples=(clustAssing[:,0].A1.T==classLabel)\n    plt.scatter(datMat[selectSamples,0].A1,datMat[selectSamples,1].A1,marker=markers[int(classLabel)],c=colors[int(classLabel)], label=int(classLabel))\n    plt.scatter(myCentroids[int(classLabel),0],myCentroids[int(classLabel),1],marker='$'+str(int(classLabel))+'$',c=colors[int(classLabel)],s=100)\n\nplt.legend()\nplt.show()\n```\n最终效果图\n{% asset_img 2019062621585264.png %}\n用红圈圈起来的数字表示质心的位置,这次没有将每次迭代的代码写出来,有兴趣的读者可以自己进行添加,可以看出,二分K-均值算法聚类更加完美,克服了K-均值算法收敛于局部最小值的问题,始终求的是全局最小值\n### 参考\n1. [<<机器学习实战>>第10章 利用K-均值聚类算法对未标注数据分组](https://book.douban.com/subject/24703171/)","tags":["机器学习实战"],"categories":["机器学习"]},{"title":"[leetcode]16.最接近的三数之和","url":"%2Fposts%2Fad171662%2F","content":"{% asset_img 2019062609441361.png %}\n\n### 解题思路 \n`双指针法+去重`\n方法:前n-2项循环去重,最后2项双指针去重\n```cpp\nconst int INF=1e9;\nint threeSumClosest(vector<int>& nums, int target) {\n    sort(nums.begin(),nums.end());\n    int minDiff=INF,sum=0;\n    for(int i=0;i<nums.size()-2;++i)\n    {\n        if (i>0&&nums[i]==nums[i-1]) continue; // [记]标准去重\n        int l=i+1,r=nums.size()-1,remain=target-nums[i];\n        while(l<r)\n        {\n            int tmpDiff=abs(nums[l]+nums[r]-remain);\n            if (minDiff>tmpDiff)\n            {\n                minDiff=tmpDiff;\n                sum=nums[i]+nums[l]+nums[r];\n            } \n            // 去重\n            if (nums[l] + nums[r] < remain)\n            {\n                // 左指针去重\n                while (l < r&&nums[l + 1] == nums[l]) ++l;\n                ++l;\n            }\n            else\n            {\n                // 右指针去重\n                while (l < r&&nums[r - 1] == nums[r]) --r;\n                --r;\n            }\n        }\n    }\n    return sum;\n}\n```\n### 参考","tags":["双指针法"],"categories":["OJ"]},{"title":"[leetcode]443.压缩字符串","url":"%2Fposts%2F12ddbcae%2F","content":"{% asset_img 2019062514385954.png %}\n\n### 解题思路 \n`双指针法`\n关键能够理清楚题目的要点:\n1. 保留当前字母的指针i\n2. 遍历所有字母的指针j\n3. 用于进行填充字母和数字的指针cur\n数字转字符串用`to_string()`\n```cpp\nint compress(vector<char>& A) {\n    int n=A.size(),cur=0;\n    for(int i=0,j=0;j<n;i=j)\n    {\n        while(j<n&&A[i]==A[j]) ++j; // 跳过相同字符\n        A[cur++]=A[i]; // 记录字符\n        if (j-i==1) continue; // 相同字符长度为1\n        for(auto c:to_string(j-i))\n            A[cur++]=c;\n    }\n    return cur;\n}\n```\n### 参考","tags":["双指针法"],"categories":["OJ"]},{"title":"[leetcode]87.扰乱字符串","url":"%2Fposts%2F503713f7%2F","content":"{% asset_img 2019062513382851.png %}\n\n### 解题思路 \n题目的字符串操作主要是:`分割(无交换)+分割(有交换)`\n并且需要注意的是,两者对应分割或交换的长度应当是相同的,且所含字母个数应当一致\n剪枝:\n1. s1==s2\n2. 所含字母个数不同\n\n分割(无交换)比较:\n`isScramble(s1.substr(0,i),s2.substr(0,i))&&isScramble(s1.substr(i,s1.size()-i),s2.substr(i,s2.size()-i))`\n分割(有交换)比较:\n`isScramble(s1.substr(0,i),s2.substr(s2.size()-i))&&isScramble(s1.substr(i),s2.substr(0,s2.size()-1))`\n```cpp\nbool isScramble(string s1, string s2) {\n    if (s1==s2) return true;\n    int dict[26]={0};\n    for(int i=0;i<s1.size();++i)\n    {\n        dict[s1[i]-'a']++;\n        dict[s2[i]-'a']--;\n    }\n    for(auto item:dict)\n        if (item!=0) return false;\n    //只有两个以上的长度时才有继续递推下去的必要\n    for(int i=1;i<s1.size();++i)\n    {\n        //此种根节点划分情况下，根节点处的两个孩子结点没有交换\n        if (isScramble(s1.substr(0,i),s2.substr(0,i))\n            &&isScramble(s1.substr(i),s2.substr(i)))\n            return true;\n        //此种根节点划分情况下，根节点处的两个孩子结点发生了交换\n        if (isScramble(s1.substr(0,i),s2.substr(s2.size()-i))\n            &&isScramble(s1.substr(i),s2.substr(0,s2.size()-i)))\n            return true;\n    }\n    return false;\n}\n```\n### 参考\n1. [java 2ms recursive solution with explanation](https://leetcode.com/problems/scramble-string/discuss/285215/java-2ms-recursive-solution-with-explanation)","tags":["递归"],"categories":["OJ"]},{"title":"[机器学习实战]第9章 树回归","url":"%2Fposts%2F24b6890f%2F","content":"### ID3算法\n- **介绍**:ID3算法的核心是在决策树各个结点上**应用信息增益准则选择特征**,递归构建决策树.\n  **具体方法是**:从根节点(root node)开始,对结点计算所有可能的特征的信息增益,选择信息增益最大的特征作为结点的特征,由该特征的不同取值建立子节点;再对子节点递归地调用以上方法,构建决策树;直到所有特征的信息增益均很小或没有特征可以选择为止.最后得到一个决策树.ID3相当于用极大似然法进行概率模型的选择.\n- **定义**:\n  **输入**:训练数据集$D$,特征集$A$,阈值$\\varepsilon$;\n  **输出**:决策树$T$.\n  **(1)** 若$D$中所有实例属于同一类$C_k$,则$T$为单结点树,并将类$C_k$作为该结点的类标记,返回$T$;\n  **(2)** 若$A=\\empty$,则$T$为单结点树,并将$D$中实例数最大的类$C_k$作为该结点的类标记,返回$T$;\n  **(3)** 否则,按信息增益算法计算$A$中各特征对$D$的信息增益,选择信息增益最大的特征$A_g$;\n  **(4)** 如果$A_g$的信息增益小于阈值$\\varepsilon$,则置$T$为单结点树,并将$D$中实例数最大的类$C_k$作为该结点的类标记,返回$T$;\n  **(5)** 否则,对$A_g$的每一可能值$a_i$,依$A_g=a_i$将$D$分割为若干非空子集$D_i$,将$D_i$中实例数最大的类作为标记,构建子结点,由结点及其子结点构成树$T$,返回$T$;\n  **(6)** 对第$i$个子结点,以$D_i$为训练集,以$A-{A_g}$为特征集,递归地调用步(1)~步(5),得到子树$T_i$,返回$T_i$.\n  > 注:第3章使用的决策树代码是采用的ID3算法实现的,使用信息增益来选择特征进行数据集的划分,且ID3算法会按照该特征的所有可能取值来切分,如果有个特征有4种取值,那么数据将被切成4份.ID3不能直接处理连续型特征,只有事先将连续型特征转换成离散型,才能在ID3算法中使用.\n\n### CART(Classification And Regression Tree) \n**介绍**:分类与回归树(Classification And Regression Tree, CART)模型是应用广泛的**决策树学习方法**.CART同样由特征选择,树的生成及剪枝组成,**既可以用于分类也可以用于回归**.以下将用于分类与回归的树统称为决策树.\n  CART是在给定输入随机变量$X$条件下输出随机变量$Y$的条件概率分布的学习方法.CART假设决策树是二叉树,内部结点特征的取值为\"是\"和\"否\",左分支是取值为\"是\"的分支,右分支是取值为\"否\"的分支.这样的决策树等价于递归地二分每个特征,将输入空间即特征空间划分为有限个单元,并在这些单元上确定预测的概率分布,也就是在输入给定的条件下输出的条件概率分布.\n  CART算法由以下两步组成:\n  (1)决策树生成:基于训练数据集生成决策树,生成的决策树要尽量大;\n  (2)决策树剪枝:用验证数据集对已生成的树进行剪枝并选择最优子树,这时用损失函数最小作为剪枝的标准.\n**CART生成**:决策树的生成就是递归地构建二叉决策树的过程.对**回归树**用**平方误差最小化准则**,对**分类树**用**基尼指数(Gini index)最小化准则**,进行特征选择,生成二叉树.\n  1. **回归树的生成**\n        假设$X$与$Y$分别为输入和输出变量,并且$Y$是连续变量,给定训练数据集\n        $$\\begin{aligned}\n            D=\\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\\}\n        \\end{aligned}$$\n        考虑如何生成回归树.\n        一个回归树对应着输入空间(即特征空间)的一个划分以及在划分的单元的输出值.假设已将输入空间划分为$M$个单元$R_1,R_2,...,R_M$,并且在每个单元$R_m$上有一个固定的输出值$c_m$,于是回归树模型可表示为\n        $$\\begin{aligned}\n            f(x)=\\sum_{m=1}^M{c_mI(x\\in{R_m})}\\tag{5.16}\n        \\end{aligned}$$\n        当输入空间的划分确定时,可以用平方误差$\\sum_{x_i\\in{R_m}}(y_i-f(x_i))^2$来表示回归树对于训练数据的预测误差,用平方误差最小的准则求解每个单元上的最优输出值.易知,单元$R_m$上的$c_m$的最优值$\\hat{c}_m$是$R_m$上的所有输入实例$x_i$对应的输出$y_i$的均值,即\n        $$\\begin{aligned}\n            \\hat{c}_m=ave(y_i|x_i\\in{R_m})\\tag{5.17}\n        \\end{aligned}$$\n        问题是怎样对输入空间进行划分.这里采用启发式的方法,选择第$j$个变量$x^{(j)}$和它取的值$s$,作为切分变量(splitting variable)和切分点(splitting point),并定于两个区域:\n        $$\\begin{aligned}\n            R_1(j,s)=\\{x|x^{(j)}\\le{s}\\}\\ and\\ R_2(j,s)=\\{x|x^{(j)}\\gt{s}\\}\\tag{5.18}\n        \\end{aligned}$$\n        然后寻找最优切分变量$j$和最优切分点$s$.具体地,求解\n        $$\\begin{aligned}\n            \\min_{j,s}\\left[\\min_{c_1}\\sum_{x_i\\in{R_1(j,s)}}(y_i-c_1)^2+\\min_{c_2}\\sum_{x_i\\in{R_2(j,s)}}(y_i-c_2)^2\\right]\\tag{5.19}\n        \\end{aligned}$$\n        对固定输入变量$j$可以找到最优切分点$s$.\n        $$\\begin{aligned}\n            \\hat{c}_1=ave(y_i|x_i\\in{R_1(j,s)})\\ and\\ \\hat{c}_2=ave(y_i|x_i\\in{R_2(j,s)})\\tag{5.20}\n        \\end{aligned}$$\n        遍历所有输入变量,找到最优的切分变量$j$,构成一个对$(j,s)$.依此将输入空间划分为两个区域.接着,对每个区域重复上述划分过程,直到满足停止条件为止.这样就生成一棵回归树.这样的回归树通常称为最小二乘回归树(least squares regression tree),现将算法叙述如下:\n        - **最小二乘回归树生成算法**\n            **输入**:训练数据集$D$;\n            **输出**:回归树$f(x)$.\n            在训练数据集所在的输入空间中,递归地将每个区域划分为两个子区域并决定每个子区域上的输出值,构建二叉决策树:\n            (1)选择最优切分变量$j$与切分点$s$,求解\n            $$\\begin{aligned}\n                \\min_{j,s}\\left[\\min_{c_1}\\sum_{x_i\\in{R_1(j,s)}}(y_i-c_1)^2+\\min_{c_2}\\sum_{x_i\\in{R_2(j,s)}}(y_i-c_2)^2\\right]\\tag{5.21}\n            \\end{aligned}$$\n            遍历变量$j$,对固定的切分变量$j$扫描切分点$s$,选择使式(5.21)达到最小值的对$(j,s)$.\n            (2)用选定的对$(j,s)$划分区域并决定相应的输出值:\n            $$\\begin{aligned}\n                R_1(j,s)&=\\{x|x^{(j)}\\le{s}\\},\\ R_2(j,s)=\\{x|x^{(j)}\\gt{s}\\}\\\\\n                \\hat{c}_m&=\\frac{1}{N_m}\\sum_{x_i\\in{R_m(j,s)}}y_i,\\ x\\in{R_m},\\ m=1,2\n            \\end{aligned}$$\n            (3)继续对两个子区域调用步骤(1),(2),直至满足停止条件.\n            (4)将输入空间划分为$M$个区域$R_1,R_2,...,R_M$,生成决策树:\n            $$\\begin{aligned}\n                f(x)=\\sum_{m=1}^M{\\hat{c}_mI(x\\in{R_m})}\n            \\end{aligned}$$\n  2. **分类树的生成**\n   分类树用基尼指数选择最优特征,同时决定该特征的最优二值切分点.\n      - **基尼指数(定义)**: 分类问题中,假设有$K$个类,样本点属于第$k$类的概率为$p_k$,则概率分布的基尼指数定义为\n        $$\\begin{aligned}\n            Gini(p)=\\sum_{k=1}^K{p_k(1-p_k)}=1-\\sum_{k=1}^K{p_k^2}\\tag{5.22}\n        \\end{aligned}$$\n        对于二分类问题,若样本点属于第1个类的概率是$p$,则概率分布的基尼指数为\n        $$\\begin{aligned}\n            Gini(p)=2p(1-p)\\tag{5.23}\n        \\end{aligned}$$\n        对于给定的样本集合$D$,其基尼指数为\n        $$\\begin{aligned}\n            Gini(D)=1-\\sum_{k=1}^K\\left[\\frac{|C_k|}{|D|}\\right]^2\\tag{5.24}\n        \\end{aligned}$$\n        这里,$C_k$是$D$中属于第$k$类的样本子集,$K$是类的个数.\n        如果样本集合$D$根据特征$A$是否取某一可能值$a$被分割成$D_1$和$D_2$两部分,即\n        $$\\begin{aligned}\n            D_1=\\{(x,y)\\in{D}|A(x)=a\\},\\ D_2=D-D_1\n        \\end{aligned}$$\n        则在特征$A$的条件下,集合$D$的基尼指数定义为\n        $$\\begin{aligned}\n            Gini(D,A)=\\frac{|D_1|}{|D|}Gini(D_1)+\\frac{|D_2|}{|D|}Gini(D_2)\\tag{5.25}\n        \\end{aligned}$$\n        基尼指数Gini(D)表示集合$D$的不确定性,基尼指数Gini(D,A)表示经$A=a$分割后集合$D$的不确定性.基尼指数值越大,样本集合的不确定性也就越大,这一点与熵相似.\n      - **CART生成算法**:\n         **输入**:训练数据集$D$,停止计算的条件;\n         **输出**:CART决策树.\n         根据训练数据集,从根节点开始,递归地对每个结点进行以下操作,构建二叉决策树:\n         (1)设结点的训练数据集为$D$,计算现有特征对该数据集的基尼指数.此时,对每一个特征$A$,对其可能取的每个值$a$,根据样本点对$A=a$的测试为\"是\"或\"否\"将$D$分割成$D_1$和$D_2$两部分,利用式(5.25)计算$A=a$时的基尼指数.\n         (2)在所有可能的特征$A$以及它们所有可能的切分点$a$中,选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点.依最优特征与最优切分点,从现结点生成两个子结点,将训练数据集依特征分配到两个子结点中去.\n         (3)对两个子结点递归地调用(1),(2),直至满足停止条件.\n         (4)生成CART决策树.\n         算法停止计算的条件是结点中的样本个数小于预订阈值,或样本集的基尼指数小于预订阈值(样本基本属于同一类),或者没有更多的特征.\n**CART回归树生成实现**\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef loadDataSet(fileName):\n    dataMat = []\n    fr = open(fileName)\n    for line in fr.readlines():\n        curLine = line.strip().split('\\t')\n        fltLine = list(map(float,curLine)) #将数据转float类型\n        dataMat.append(fltLine)\n    return dataMat\n\n# 按照第i个特征的值将数据集划分为2类\ndef binSplitDataSet(dataSet, feature, value):\n    # 选取所有样本中第i个特征值>value的,构成一个样本矩阵\n    mat0=dataSet[np.nonzero(dataSet[:,feature]>value)[0],:]\n    # 选取所有样本中第i个特征值<=value的,构成一个样本矩阵\n    mat1=dataSet[np.nonzero(dataSet[:,feature]<=value)[0],:]\n    return mat0,mat1\n\n# 建立叶节点\ndef regLeaf(dataSet):\n    return np.mean(dataSet[:,-1])\n\n# 计算总方差,方差越小则分类越好(用来衡量错误率,可以理解为类似于决策树中的信息增益)\ndef regErr(dataSet):\n    return np.var(dataSet[:,-1])*np.shape(dataSet)[0]\n\n# 创建二叉树\n# 分叉结点[特征索引,特征值], 叶子结点[特征索引,均值]\ndef createTree(dataSet, leafType=regLeaf, errType=regErr, ops=(1,4)):\n    feat,val=chooseBestSplit(dataSet,leafType,errType,ops)\n    if feat == None: return val\n    # 字典存储子树\n    retTree = {}\n    # 存储特征索引\n    retTree['spInd']=feat\n    # 存储特征值\n    retTree['spVal']=val\n    # 根据特征索引与特征值进行数据集划分\n    lSet,rSet=binSplitDataSet(dataSet,feat,val)\n    # 递归创建左子树\n    retTree['left']=createTree(lSet,leafType,errType,ops)\n    # 递归创建右子树\n    retTree['right']=createTree(rSet,leafType,errType,ops)\n    return retTree\n\n# 找到最佳切分方式,即选择最好的特征进行划分\n# 1.如果特征值都相等,则直接创建叶节点\n# 2.如果切分数据集后效果提升不大,则直接创建叶节点\n# 3.如果两个切分后的子集中,有一个大小小于用户定义的tolN,那么直接创建叶节点\ndef chooseBestSplit(dataSet, leafType=regLeaf, errType=regErr, ops=(1,4)):\n    # tolS是容许的误差下降值,tolN是划分的最少样本数\n    tolS=ops[0]; tolN=ops[1]\n    # 停止条件,如果第i个特征的所有值相等,则停止,生成叶节点\n    if len(set(dataSet[:,-1].T.tolist()[0]))==1:\n        return None, leafType(dataSet)\n    m,n=np.shape(dataSet)\n    # 当前数据集的误差\n    S=errType(dataSet)\n    bestS=np.inf; bestIndex=0; bestValue=0\n    for featIndex in range(n-1):\n        for splitVal in set(dataSet[:,featIndex].A1):\n            mat0,mat1=binSplitDataSet(dataSet, featIndex, splitVal)\n            # 如果有一个少于划分的最少样本数tolN,则继续进行选择\n            if (np.shape(mat0)[0] < tolN) or (np.shape(mat1)[0] < tolN): continue\n            newS = errType(mat0) + errType(mat1)\n            # 选取误差值最小的,存储[特征索引,特征值,误差值]\n            if newS < bestS:\n                bestIndex = featIndex\n                bestValue = splitVal\n                bestS = newS\n    # 停止条件,当前误差与划分后的误差的差值较小(<tolS),即误差减少不大则停止,生成叶节点\n    if (S - bestS) < tolS:\n        return None, leafType(dataSet)\n    # 根据[特征索引,特征值]切分数据集\n    mat0, mat1 = binSplitDataSet(dataSet, bestIndex, bestValue)\n    # 如果切分出的数据集很小则停止切分,生成叶节点\n    if (np.shape(mat0)[0] < tolN) or (np.shape(mat1)[0] < tolN):\n        return None, leafType(dataSet)\n    return bestIndex,bestValue\n\nmyDat = loadDataSet('ex0.txt')\nmyMat = np.mat(myDat)[:,1:]\ntree = createTree(myMat)\nprint(tree)\nplt.scatter(myMat[:,0].A,myMat[:,1].A)\nplt.show()\n```\n代码运行结果如下\n{% asset_img 2019062517071456.png %}\n我们来将二叉树画出来,方块表示叶节点,椭圆表示分支结点\n{% asset_img 2019062518154357.png %}\n### 模型树(留待以后分解)\n### 参考\n1. [<<机器学习实战>>第9章 树回归](https://book.douban.com/subject/24703171/)\n2. [<<统计学习方法>>第5章 决策树](https://book.douban.com/subject/10590856/)","tags":["机器学习实战"],"categories":["机器学习"]},{"title":"[leetcode]523.连续的子数组和","url":"%2Fposts%2F8d014619%2F","content":"{% asset_img 2019062410410845.png %}\n\n### 解题思路 \n如果数组中存在连续的0(≥2个),那么直接返回true\nk==0时,sum%k会错误\n```cpp\nbool checkSubarraySum(vector<int>& nums, int k) {\n    for(int i=0;i<nums.size()-1;++i)\n    {\n        int sum=nums[i];\n        for(int j=i+1;j<nums.size();++j)\n        {\n            sum+=nums[j];\n            if (k==0&&sum==k)\n                return true;\n            else if (k&&sum%k==0)\n                return true;\n        }\n    }\n    return false;\n}\n```\n### 参考","tags":["暴力破解"],"categories":["OJ"]},{"title":"[leetcode]560.和为K的子数组","url":"%2Fposts%2Fec9fc6f%2F","content":"{% asset_img 2019062410183044.png %}\n\n### 解题思路 \n`法1:哈希表`\n$$\\begin{aligned}\n    nums[0:j]=sum[j]\\\\\n    nums[i:j]=sum[j]-sum[i]=k\\\\\n    => sum[i]=sum[j]-k\n\\end{aligned}$$\n因此,如果sum[i]=sum[j]-k存在,那么就可以计数加1\n```cpp\nint subarraySum(vector<int>& nums, int k) {\n    map<int,int> dict;\n    dict[0]=1;\n    int sum=0,cnt=0;\n    for(auto num:nums)\n    {\n        sum+=num;           // 求累积和nums[0:j]=sum[j]\n        cnt+=dict[sum-k];   // 如果sum[i]存在,那么由sum[j]-sum[i]=k可以推出,sum[i]=sum[j]-k\n        dict[sum]++;        // 累积和记录\n    }\n    return cnt;\n}\n```\n时间复杂度: $O(n)$\n空间复杂度: $O(1)$\n\n`法2:累加和`\n```cpp\nint subarraySum(vector<int>& nums, int k) {\n    int res=0;\n    for(int i=0;i<nums.size();++i)\n    {\n        int sum=0;\n        for(int j=i;j<nums.size();++j)\n        {\n            sum+=nums[j];\n            res+=sum==k?1:0;\n        }\n    }\n    return res;\n}\n```\n时间复杂度: $O(n^2)$\n\n### 参考\n1. [zintrulcre](https://leetcode-cn.com/problems/two-sum/solution/he-wei-kde-zi-shu-zu-by-zintrulcre/)","tags":["哈希表"],"categories":["OJ"]},{"title":"[leetcode]18.四数之和","url":"%2Fposts%2Fc6c8e404%2F","content":"{% asset_img 2019062409300043.png %}\n\n### 解题思路 \n`双指针法`:可用于解n项和\n方法:前n-2项循环去重,最后2项双指针去重\n```cpp\nvector<vector<int>> fourSum(vector<int>& nums, int target) {\n    vector<vector<int>> res;\n    if (nums.size()<=3) return res;\n    sort(nums.begin(),nums.end());\n    for(int i=0;i<nums.size()-3;++i)\n    {\n        if (i>0&&nums[i]==nums[i-1]) continue;  // 去重\n        for(int j=i+1;j<nums.size()-2;++j)\n        {\n            if (j>i+1&&nums[j]==nums[j-1]) continue;    // 去重\n            int l=j+1,r=nums.size()-1,sum=target-(nums[i]+nums[j]);\n            while(l<r)\n            {\n                if (nums[l]+nums[r]==sum)\n                {\n                    res.push_back(vector<int>{nums[i],nums[j],nums[l],nums[r]});\n                    // 左右指针去重\n                    while(l<r&&nums[l]==nums[l+1]) ++l;\n                    while(l<r&&nums[r]==nums[r-1]) --r;\n                    ++l,--r;\n                }\n                else if (nums[l]+nums[r]<sum)\n                    ++l;\n                else\n                    --r;                \n            }\n        }\n    }\n    return res;\n}\n```\n\n`回溯法`\n```cpp\nvector<vector<int>> res;\nvoid backTrace(vector<int>&nums, vector<int> tmp, int from, int remain, int k)\n{\n    if (k<0) return ;\n    if (k==0&&remain==0)\n        res.push_back(tmp);\n    else\n    {\n        for(int i=from;i<nums.size();++i)\n        {\n            if (i>from&&nums[i]==nums[i-1]) continue;   // 去重\n            tmp.push_back(nums[i]);\n            backTrace(nums,tmp,i+1,remain-nums[i],k-1);\n            tmp.pop_back();\n        }\n    }\n}\nvector<vector<int>> fourSum(vector<int>& nums, int target) {\n    if (nums.size()<=3) return res;\n    sort(nums.begin(),nums.end());\n    backTrace(nums,vector<int>(),0,target,4);\n    return res;\n}\n```\n超时了\n### 参考","tags":["双指针法"],"categories":["OJ"]},{"title":"[leetcode]15.三数之和","url":"%2Fposts%2Fd6b26033%2F","content":"{% asset_img 2019062408542942.png %}\n\n### 解题思路 \n`双指针法`:可用于解n项和\n方法:前n-2项循环去重,最后2项双指针去重\n```cpp\nvector<vector<int>> threeSum(vector<int>& nums) {\n    vector<vector<int>> res;\n    if (nums.size()<=2) return res;\n    sort(nums.begin(),nums.end());\n    for(int i=0;i<nums.size()-2;++i)\n    {\n        if(i>0&&nums[i]==nums[i-1]) continue;   // 去重\n        int l=i+1,r=nums.size()-1,sum=-nums[i];\n        while(l<r)\n        {\n            if (nums[l]+nums[r]==sum)\n            {\n                res.push_back(vector<int>{nums[i],nums[l],nums[r]});\n                // 左右指针去重\n                while(l<r&&nums[l]==nums[l+1]) ++l;\n                while(l<r&&nums[r]==nums[r-1]) --r;\n                ++l,--r;\n            }\n            else if (nums[l]+nums[r]<sum)\n            {\n                // 左指针去重\n                while (l < r&&nums[l + 1] == nums[l]) ++l;\n                ++l;\n            }\n            else\n            {\n                // 右指针去重\n                while (l < r&&nums[r - 1] == nums[r]) --r;\n                --r;\n            }\n        }\n    }\n    return res;\n}\n```\n\n`回溯法`\n```cpp\nvector<vector<int>> res;\nvoid backTrace(vector<int>& nums,vector<int> tmp, int from, int remain, int k)\n{\n    if (k<0) return;\n    if (remain==0&&k==0)\n        res.push_back(tmp);\n    for(int i=from;i<nums.size();++i)\n    {\n        if (i>from&&nums[i]==nums[i-1]) continue;   // 去重\n        tmp.push_back(nums[i]);\n        backTrace(nums,tmp,i+1,remain-nums[i],k-1);\n        tmp.pop_back();\n    }\n}\nvector<vector<int>> threeSum(vector<int>& nums) {\n    sort(nums.begin(),nums.end());\n    backTrace(nums,vector<int>(),0,0,3);\n    return res;\n}\n```\n效率有些低,超时\n### 参考","tags":["双指针法"],"categories":["OJ"]},{"title":"[leetcode]1094.拼车","url":"%2Fposts%2F3a32a6cb%2F","content":"{% asset_img 2019062315112039.png %}\n\n### 解题思路 \n考虑每个路段都有可能有上车或下车的人,于是采用一个字典存放对应路段上车人数,另一个字典存放对应路段下车人数\n需要注意的是\n1. `先下后上`这个次序很重要,决定了应当先判断下车人数再判断上车人数\n2. 可能一个路段有好几个人上车(终点不同),也可能一个路段有好几个人下车(起点不同),因此开始时需要累加\n```cpp\nconst int INF=1e9;\nbool carPooling(vector<vector<int>>& trips, int capacity) {\n    map<int,int> dict_up,dict_down;\n    int sum=0,from=INF,to=0;\n    for(auto row:trips)\n    {\n        dict_up[row[1]]+=row[0];    // 记录每个路段上车人数\n        dict_down[row[2]]+=row[0];  // 记录每个路段下车人数\n        from=min(from,row[1]);  // 起始路段位置\n        to=max(to,row[2]);      // 终止路段位置\n    }\n    for(int i=from;i<to;++i)\n    {\n        if (dict_down[i])   // 先下\n            sum-=dict_down[i];\n        if (dict_up[i])     // 后上\n            sum+=dict_up[i];\n        if (sum>capacity)   // 判断是否超载\n            return false;\n    }\n    return true;\n}\n```\n### 参考","tags":["数学方法"],"categories":["OJ"]},{"title":"[leetcode]1093.大样本统计","url":"%2Fposts%2F999304e8%2F","content":"{% asset_img 2019062314363838.png %}\n\n### 解题思路 \n题目有些不好理解,开始时理解题目意思理解了半天,实际是求\n[min(idx),max(idx),mean(value),median(value),mode(idx)]\n统计的数是0~255(在题目中是作为下标存在的),而每个数k(0~255)统计的个数为count[k],\nmin(idx):求的是统计中的最小的数(即最小的下标)\nmax(idx):求的是统计中的最大的数(即最大的下标)\nmean(value):求的是统计的均值(即总和除以总统计个数)\nmedian(value):求的是中位数(即统计个数中的中间数(若奇数,则为中间1个数,若偶数,则为中间2个数平均值))\nmode(idx):求的是众数(即统计个数count[k]最多的那个数k)\n只有median(value)最难求,因为若总统计数为奇数,则为中间那个数,总统计数为偶数,则为中间两个数取平均值\n```cpp\nvector<double> sampleStats(vector<int>& count) {\n    vector<double> res{255,0,0,0,0};\n    int len=count.size();\n    int cnts=accumulate(count.begin(),count.end(),0);\n    int m1=cnts/2,m2=m1+(cnts%2?0:1);\n    for(int i=0,cnt=0,flag=1;i<len;cnt+=count[i],++i)\n    {\n        if (count[i])\n        {\n            // min(idx)\n            if (flag) \n            {\n                flag=0;\n                res[0]=i;\n            }\n            // max(idx)\n            res[1]=i; \n            // mean(value)\n            res[2]+=(double)i*count[i]/cnts; \n            // median(value)\n            if (cnt<m1&&cnt+count[i]>=m1) res[3]+=(double)i/2;\n            if (cnt<m2&&cnt+count[i]>=m2) res[3]+=(double)i/2;\n            // mode(idx)\n            res[4]=count[res[4]]>count[i]?res[4]:i;\n        }\n    }\n    return res;\n}\n```\n### 参考","tags":["数学方法"],"categories":["OJ"]},{"title":"[leetcode]12.整数转罗马数字","url":"%2Fposts%2F6755b54e%2F","content":"{% asset_img 2019062214582331.png %}\n\n### 解题思路 \n整数转罗马数字\n```cpp\nstring intToRoman(int num) {\n    int v[]={1,4,5,9,10,40,50,90,100,400,500,900,1000};\n    string s[]={\"I\",\"IV\",\"V\",\"IX\",\"X\",\"XL\",\"L\",\"XC\",\"C\",\"CD\",\"D\",\"CM\",\"M\"};\n    string res=\"\";\n    for(int i=12;i>=0;)\n        if (num>=v[i])\n            res+=s[i],num-=v[i];\n        else\n            --i;\n    return res;\n}\n```\n### 参考","tags":["数学方法"],"categories":["OJ"]},{"title":"[leetcode]516.最长回文子序列","url":"%2Fposts%2F87373ebd%2F","content":"{% asset_img 2019062210303129.png %}\n\n### 解题思路 \n- 状态定义: $dp[i][j]$表示$s[i...j]$这个子串的最长回文序列\n- 状态转移方程:\n  - 当$s[i]==s[j]$时:\n    $dp[i][j]=dp[i+1][j-1]+2$\n  - 当$s[i]!=s[j]$时:\n    $dp[i][j]=max(dp[i+1][j],dp[i][j-1])$,其中$dp[i+1][j]$表示选择$s[j]$,$dp[i][j-1]$表示选择$s[i]$\n再就是注意循环的次序,应当从后往前进行推导,如下图所示\n{% asset_img 2019062210391630.png %}\n```cpp\nint longestPalindromeSubseq(string s) {\n    int m=s.size();\n    if (s.empty()) return 0;\n    vector<vector<int>> dp(m+1,vector<int>(m+1));\n    for(int i=0;i<=m;++i)\n        dp[i][i]=1;\n    for(int i=m;i>=1;--i)\n    {\n        for(int j=i+1;j<=m;++j)\n        {\n            if (s[i-1]==s[j-1])\n                dp[i][j]=dp[i+1][j-1]+2;\n            else\n                dp[i][j]=max(dp[i+1][j],dp[i][j-1]);\n        }\n    }\n    return dp[1][m];\n}\n```\n时间复杂度: $O(n^2)$\n### 参考\n- [liang001](https://leetcode-cn.com/problems/two-sum/solution/golangdong-tai-gui-hua-suan-fa-by-liang001/)","categories":["OJ"]},{"title":"[概率与统计]第4章 随机变量的数字特征","url":"%2Fposts%2F318ec0d4%2F","content":"\n### 数学期望\n- 定义 (1)离散型随机变量的数学期望\n  设随机变量$X$的概率分布为\n  $$\\begin{aligned}\n      P\\{X=x_k\\}=p_k,k=1,2,...\n  \\end{aligned}$$\n  如果级数$\\sum_{k=1}^{+\\infin}{x_kp_k}$绝对收敛,则称此级数为随机变量$X$的数学期望或均值,记作$E(X)$,即$E(X)=\\sum_{k=1}^{+\\infin}{x_kp_k}$\n  (2)连续型随机变量的数学期望\n  设随机变量$X$的概率密度为$f(x)$,如果积分$\\int_{-\\infin}^{+\\infin}xf(x)dx$绝对收敛,则称此积分为随机变量$X$的数学期望或均值,记作$E(X)$,即\n  $$\\begin{aligned}\n      E(x)=\\int_{-\\infin}^{+\\infin}{xf(x)}dx\n  \\end{aligned}$$\n### 数学期望的性质\n- 设$C$是常数,则有$E(C)=C$\n- 设$X$是随机变量,$C$是常数,则有\n  $$\\begin{aligned}\n  E(CX)=CE(X)\n  \\end{aligned}$$\n- 设$X$和$Y$是任意两个随机变量,则有\n  $$\\begin{aligned}\n      E(X\\pm{Y})=E(X)\\pm{E(Y)}\n  \\end{aligned}$$\n- 设随机变量$X$和$Y$相互独立,则有\n  $$\\begin{aligned}\n      E(XY)=E(X)E(Y)\n  \\end{aligned}$$\n  > 注: 性质(4)要求$X$和$Y$的相互独立,可以减弱为$X$和$Y$不相关就有$E(XY)=E(X)E(Y)$.事实上$E(XY)=E(X)E(Y)$成立的充要条件是$X$和$Y$不相关\n### 随机变量$X$的函数$Y=g(X)$的数学期望\n- 设随机变量$X$的概率分布为\n  $$\\begin{aligned}\n      P\\{X=x_k\\}=p_k,k=1,2,...\n  \\end{aligned}$$\n  如果级数$\\sum_{k=1}^{+\\infin}{g(x_k)p_k}$绝对收敛,则随机变量$Y=g(X)$的数学期望为\n  $$\\begin{aligned}\n      E(Y)=E[g(X)]=\\sum_{k=1}^{+\\infin}{g(x_k)p_k}\n  \\end{aligned}$$\n- 设随机变量$X$的概率密度为$f(x)$,如果积分$\\int_{-\\infin}^{+\\infin}{g(x)f(x)}dx$绝对收敛,则随机变量$Y=g(X)$的数学期望为\n  $$\\begin{aligned}\n      E(Y)=E[g(X)]=\\int_{-\\infin}^{+\\infin}{g(x)f(x)}dx\n  \\end{aligned}$$\n### 随机变量$(X,Y)$的函数$Z=g(X,Y)$的数学期望\n- 设随机变量$(X,Y)$的概率分布为\n  $$\\begin{aligned}\n      P\\{X=x_i,Y=y_j\\}=p_{ij},i,j=1,2,...\n  \\end{aligned}$$\n  如果级数$\\sum_{i=1}^{+\\infin}\\sum_{j=1}^{+\\infin}g(x_i,y_j)p_{ij}$绝对收敛,则随机变量$Z=g(X,Y)$的数学期望为\n  $$\\begin{aligned}\n      E(Z)=E[g(X,Y)]=\\sum_{i=1}^{+\\infin}\\sum_{j=1}^{+\\infin}g(x_i,y_j)p_{ij}\n  \\end{aligned}$$\n- 设随机变量$(X,Y)$的概率密度为$f(x,y)$,如果积分$\\int_{-\\infin}^{+\\infin}\\int_{-\\infin}^{+\\infin}{g(x,y)f(x,y)}dxdy$绝对收敛,则随机变量$Z=g(X,Y)$的数学期望为\n  $$\\begin{aligned}\n      E(Z)=E[g(X,Y)]=\\int_{-\\infin}^{+\\infin}\\int_{-\\infin}^{+\\infin}{g(x,y)f(x,y)}dxdy\n  \\end{aligned}$$\n### 方差\n- 定义: 设$X$是随机变量,如果数学期望$E\\{[X-E(x)]^2\\}$存在,则称之为$X$的方差,记作$D(X)$,即\n  $$\\begin{aligned}\n      D(X)=E\\{[X-E(X)]^2\\}\n  \\end{aligned}$$\n  称$\\sqrt{D(X)}$为随机变量$X$的标准差或均方差,记作$\\sigma(X)$,即$\\sigma(X)=\\sqrt{D(X)}$\n### 方差计算公式\n$$\\begin{aligned}\n    D(X)=E(X^2)-[E(X)]^2\n\\end{aligned}$$\n> 证明:\n> $$\\begin{aligned}\n>    D(X)&=E\\{[X-E(X)]^2\\}=E\\{X^2-2XE(X)+[E(X)]^2\\}\\\\\n>        &=E(X^2)-2E(X)E(X)+[E(X)]^2=E(X^2)-[E(X)]^2\n>   \\end{aligned}$$\n> 由于对任何随机变量$X,D(X)\\ge{0}$,故恒有\n> $$\\begin{aligned}\n>   E(X^2)\\ge{[E(X)]^2}   \n> \\end{aligned}$$\n> 有时在已知$X$的数学期望与方差时,还用此公式求$E(X^2)$\n### 方差的性质\n- 设$C$是常数,则$D(C)=0$,反之,从$D(X)=0$中不能得出$X$为常数的结论\n- 设$X$是随机变量,$a$和$b$是常数,则有\n  $$\\begin{aligned}\n      D(aX+b)=a^2D(X)\n  \\end{aligned}$$\n- 设随机变量$X$和$Y$相互独立,则有\n  $$\\begin{aligned}\n      D(X\\pm{Y})=D(X)+D(Y)\n  \\end{aligned}$$\n> 注: 性质(3)要求$X$和$Y$相互独立,可以减弱为$X$和$Y$不相关就有$D(X\\pm{Y})=D(X)+D(Y)$.事实上$D(X\\pm{Y})=D(X)+D(Y)$成立的充要条件是$X$和$Y$不相关\n### 常用随机变量的数学期望和方差\n- 0-1分布\n  $$\\begin{aligned}\n      E(X)=p,\\ D(X)=p(1-p)\n  \\end{aligned}$$\n- 二项分布,$X\\sim B(n,p)$\n  $$\\begin{aligned}\n      EX=np,\\ D(X)=np(1-p)\n  \\end{aligned}$$\n- 泊松分布,$X\\sim P(\\lambda)$\n  $$\\begin{aligned}\n      E(X)=\\lambda,\\ D(X)=\\lambda\n  \\end{aligned}$$\n- 几何分布,$P\\{X=k\\}=p(1-p)^{k-1},\\ k=1,2,...,0<p<1$\n  $$\\begin{aligned}\n      E(X)=\\frac{1}{p},\\ D(X)=\\frac{1-p}{p^2}\n  \\end{aligned}$$\n- 均匀分布,$X\\sim U(a,b)$\n  $$\\begin{aligned}\n      E(X)=\\frac{a+b}{2},\\ D(X)=\\frac{(b-a)^2}{12}\n  \\end{aligned}$$\n- 指数分布,$X\\sim E(\\lambda)$\n  $$\\begin{aligned}\n      E(X)=\\frac{1}{\\lambda},\\ D(X)=\\frac{1}{\\lambda^2}\n  \\end{aligned}$$\n- 正态分布,$X\\sim N(\\mu,\\sigma^2)$\n  $$\\begin{aligned}\n      E(X)=\\mu,\\ D(X)=\\sigma^2\n  \\end{aligned}$$\n### 矩,协方差和相关系数\n- (1)设$X$是随机变量,如果\n  $$\\begin{aligned}\n      E(X^k),\\ k=1,2,...\n  \\end{aligned}$$\n  存在,则称之为$X$的$k$阶原点矩.\n- (2)设$X$是随机变量,如果\n  $$\\begin{aligned}\n      E\\{[X-E(X)]^k\\},\\ k=1,2,...\n  \\end{aligned}$$\n  存在,则称之为$X$的$k$阶中心矩.\n- (3)设$X$和$Y$是两个随机变量,如果\n  $$\\begin{aligned}\n      E(X^kY^l),\\ k,l=1,2,...\n  \\end{aligned}$$\n  存在,则称之为$X$和$Y$的$k+l$阶混合矩.\n- (4)设$X$和$Y$是两个随机变量,如果\n  $$\\begin{aligned}\n      E\\{[X-E(X)]^k[Y-E(Y)]^l\\},\\ k,l=1,2,...\n  \\end{aligned}$$\n  存在,则称之为$X$和$Y$的$k+l$阶混合中心矩.\n### 协方差\n- 定义: 对于随机变量$X$和$Y$,如果$E\\{[X-E(X)][Y-E(Y)]\\}$存在,则称之为$X$和$Y$的协方差,记作$Cov(X,Y)$,即\n  $$\\begin{aligned}\n      Cov(X,Y)=E\\{[X-E(X)][Y-E(Y)]\\}\n  \\end{aligned}$$\n### 相关系数\n- 定义: 对于随机变量$X$和$Y$,如果$D(X)D(Y)\\neq{0}$,则称$\\frac{Cov(X,Y)}{\\sqrt{D(X)}\\sqrt{D(Y)}}$为$X$和$Y$的相关系数,记为$\\rho_{XY}$,即\n  $$\\begin{aligned}\n      \\rho_{XY}=\\frac{Cov(X,Y)}{\\sqrt{D(X)}\\sqrt{D(Y)}}\n  \\end{aligned}$$\n  如果$D(X)D(Y)=0$,则$\\rho_{XY}=0$.\n### 不相关\n- 定义: 如果随机变量$X$和$Y$的相关系数$\\rho_{XY}=0$,则称$X$和$Y$不相关.\n### 协方差的公式和性质\n- (1) $Cov(X,Y)=E(XY)-E(X)E(Y)$.\n- (2) $D(X\\pm{Y})=D(X)+D(Y)\\pm{2Cov(X,Y)}$.\n- (3) 协方差性质\n  - (A) $Cov(X,Y)=Cov(Y,X)$\n  - (B) $Cov(aX,bY)=abCov(X,Y)$,其中$a,b$是常数\n  - (C) $Cov(X_1+X_2,Y)=Cov(X_1,Y)+Cov(X_2,Y)$\n### 相关系数性质\n- (1) $|\\rho_{XY}|\\le{1}$\n- (2) $|\\rho_{XY}|=1$的充分必要条件是存在常数$a$和$b$,其中$a\\neq{0}$,使得\n  $$\\begin{aligned}\n      P{Y=aX+b}=1.\n  \\end{aligned}$$\n### 独立于不相关\n- (1) 如果随机变量$X$和$Y$相互独立,则$X$和$Y$必不相关;反之,$X$和$Y$不相关时,$X$和$Y$却不一定相互独立.\n- (2) 对二维正态随机变量$(X,Y),X$和$Y$相互独立的充分必要条件是$\\rho=0$.\n- (3) 对二维正态随机变量$(X,Y),X$和$Y$相互独立与$X$和$Y$不相关时等价的.\n### 参考\n1. [复习全书](https://book.douban.com/subject/6101482/)\n","tags":["概率与统计"],"categories":["概率与统计"]},{"title":"[leetcode]59.螺旋矩阵II","url":"%2Fposts%2F19a82a9b%2F","content":"{% asset_img 2019062114300318.png %}\n\n### 解题思路 \n蛇形填数\n```cpp\nvector<vector<int>> generateMatrix(int n) {\n    vector<vector<int>> M(n,vector<int>(n));\n    int tot=0,i=0,j=-1;\n    while(tot<n*n)\n    {\n        while(j+1<n&&!M[i][j+1]) M[i][++j]=++tot;\n        while(i+1<n&&!M[i+1][j]) M[++i][j]=++tot;\n        while(j-1>=0&&!M[i][j-1]) M[i][--j]=++tot;\n        while(i-1>=0&&!M[i-1][j]) M[--i][j]=++tot;\n    }\n    return M;\n}\n```\n### 参考","tags":["蛇形填数"],"categories":["OJ"]},{"title":"[leetcode]54.螺旋矩阵","url":"%2Fposts%2Fafb97584%2F","content":"{% asset_img 2019062114111217.png %}\n\n### 解题思路 \n该题类似于[蛇形填数](../19a82a9b),但需要注意,是给定了矩阵M,那么关键在于三点:\n(1)蛇形访问\n(2)访问过的不再进行访问\n(3)结束的标志\n蛇形访问可以通过`while语句实现`\n访问过的不再进行访问可以通过设置一个visited来记录下标是否访问过\n结束的标志,即访问完所有元素则结束\n```cpp\nvector<int> spiralOrder(vector<vector<int>>& M) {\n    vector<int> res;\n    if (M.empty()) return res;\n    int tot=0,m=M.size(),n=M[0].size(),i=0,j=-1;\n    vector<vector<int>> vis(m,vector<int>(n));\n    while(res.size()<m*n)\n    {\n        while(j+1<n&&!vis[i][j+1]) res.push_back(M[i][++j]),vis[i][j]=1;           \n        while(i+1<m&&!vis[i+1][j]) res.push_back(M[++i][j]),vis[i][j]=1;\n        while(j-1>=0&&!vis[i][j-1]) res.push_back(M[i][--j]),vis[i][j]=1;\n        while(i-1>=0&&!vis[i-1][j]) res.push_back(M[--i][j]),vis[i][j]=1;\n    }\n    return res;\n}\n```\n### 参考","tags":["蛇形填数"],"categories":["OJ"]},{"title":"[机器学习实战]第8章 预测数值型数据:回归","url":"%2Fposts%2Fc8a272e3%2F","content":"### 回归系数求解公式\n- 标准回归\n  - 损失函数\n  $$\\begin{aligned}\n    J(\\theta)&=\\frac{1}{2}\\sum_{i=1}^m(\\theta^Tx^{(i)}-y^{(i)})^2\\\\\n    &=\\frac{1}{2}(X\\theta-\\vec{y})^T(X\\theta-\\vec{y})\n  \\end{aligned}$$\n  - 求使损失函数最小的$\\theta$\n  最小化损失函数(目标函数)$J(\\theta)$,即\n  $$\\begin{aligned}\n      \\nabla_\\theta{J(\\theta)}&=\\frac{1}{2}\\nabla_\\theta(\\theta^TX^TX\\theta-\\theta^TX^T\\vec{y}-\\vec{y}^TX\\theta+\\vec{y}^T\\vec{y})\\\\\n      &=\\frac{1}{2}(2X^TX\\theta-2X^T\\vec{y})\\\\\n      &=X^TX\\theta-X^T\\vec{y}\n  \\end{aligned}$$\n  令$\\nabla_\\theta{J(\\theta)}=0$,则\n  $$\\begin{aligned}\n      \\theta=(X^TX)^{-1}X^T\\vec{y}\n  \\end{aligned}$$\n- 局部加权线性回归\n  - 损失函数\n    $$\\begin{aligned}\n        J(\\theta)&=\\frac{1}{2}\\sum_{i=1}^mw^{(i)}(\\theta^Tx^{(i)}-y^{(i)})^2\\\\\n        &=\\frac{1}{2}(X\\theta-\\vec{y})^TW(X\\theta-\\vec{y})\\\\\n        \\theta&=(X^TWX)^{-1}X^TW\\vec{y}\n    \\end{aligned}$$\n    注意:$W_{ii}=w^{(i)}=\\exp(-\\frac{\\Vert x^{(i)}-x\\Vert^2}{2\\tau^2})$,为**radial basis function kernel(RBF kernel)**, $W$为对角阵,对角线上为权重.\n    > **Radial basis function kernel(RBF kernel)**\n    $$\\begin{aligned}\n        K\\left(\\mathbf{x}, \\mathbf{x}^{\\prime}\\right)=\\exp \\left(-\\frac{\\left\\|\\mathbf{x}-\\mathbf{x}^{\\prime}\\right\\|^{2}}{2 \\tau^{2}}\\right)\n    \\end{aligned}$$\n  - 求使损失函数最小的$\\theta$\n  最小化损失函数(目标函数)$J(\\theta)$,即\n  $$\\begin{aligned}\n      \\nabla_\\theta{J(\\theta)}&=\\frac{1}{2}\\nabla_\\theta(\\theta^TX^TWX\\theta-\\theta^TX^TW\\vec{y}-\\vec{y}^TWX\\theta+\\vec{y}^TW\\vec{y})\\\\\n      &=\\frac{1}{2}(2X^TWX\\theta-2X^TW\\vec{y})\\\\\n      &=X^TWX\\theta-X^TW\\vec{y}\n  \\end{aligned}$$\n  令$\\nabla_\\theta{J(\\theta)}=0$,则\n  $$\\begin{aligned}\n      \\theta=(X^TWX)^{-1}X^TW\\vec{y}\n  \\end{aligned}$$\n- 岭回归\n  - 损失函数\n  $$\\begin{aligned}\n    J(\\theta)&=\\frac{1}{2}\\sum_{i=1}^m(\\theta^Tx^{(i)}-y^{(i)})^2+\\frac{\\lambda}{2}\\Vert\\theta\\Vert^2\\\\\n    &=\\frac{1}{2}(X\\theta-\\vec{y})^T(X\\theta-\\vec{y})+\\frac{\\lambda}{2}\\theta^T\\theta\\\\\n  \\end{aligned}$$\n  - 求使损失函数最小的$\\theta$\n  最小化损失函数(目标函数)$J(\\theta)$,即\n  $$\\begin{aligned}\n      \\nabla_\\theta{J(\\theta)}&=\\frac{1}{2}\\nabla_\\theta(\\theta^TX^TX\\theta-\\theta^TX^T\\vec{y}-\\vec{y}^TX\\theta+\\vec{y}^T\\vec{y}+\\lambda\\theta^T\\theta)\\\\\n      &=\\frac{1}{2}(2X^TX\\theta-2X^T\\vec{y}+2\\lambda\\theta)\\\\\n      &=X^TX\\theta-X^T\\vec{y}+\\lambda\\theta\n  \\end{aligned}$$\n  令$\\nabla_\\theta{J(\\theta)}=0$,则\n  $$\\begin{aligned}\n      \\theta=(X^TX+\\lambda I)^{-1}X^T\\vec{y}\n  \\end{aligned}$$\n- Lasso回归\n  - 损失函数\n  $$\\begin{aligned}\n    J(\\theta)&=\\frac{1}{2}\\sum_{i=1}^m(\\theta^Tx^{(i)}-y^{(i)})^2+\\lambda\\Vert\\theta\\Vert_1\\\\\n    &=\\frac{1}{2}(X\\theta-\\vec{y})^T(X\\theta-\\vec{y})+\\lambda\\Vert\\theta\\Vert_1\\\\\n  \\end{aligned}$$\n  - 求使损失函数最小的$\\theta$\n  最小化损失函数(目标函数)$J(\\theta)$,即\n  $$\\begin{aligned}\n      \\nabla_\\theta{J(\\theta)}&=\\frac{1}{2}\\nabla_\\theta(\\theta^TX^TX\\theta-\\theta^TX^T\\vec{y}-\\vec{y}^TX\\theta+\\vec{y}^T\\vec{y})+\\lambda\\begin{bmatrix}\n          sign(\\theta_1)\\\\\n          sign(\\theta_2)\\\\\n          .\\\\\n          .\\\\\n          .\\\\\n          sign(\\theta_n)\\\\\n      \\end{bmatrix}\\\\\n      &=X^TX\\theta-X^T\\vec{y}+\\lambda\\begin{bmatrix}\n          sign(\\theta_1)\\\\\n          sign(\\theta_2)\\\\\n          .\\\\\n          .\\\\\n          .\\\\\n          sign(\\theta_n)\\\\\n      \\end{bmatrix}\\\\\n  \\end{aligned}$$\n  其中\n  $$sign(\\theta_i)=\\begin{cases}\n      -1&\\theta_i\\lt 0\\\\\n      0&\\theta_i=0\\\\\n      1&\\theta_i\\gt 0\n  \\end{cases}$$\n  似乎不太好解,后续再进行补充...\n### 介绍\n回归(一般都指线性回归)的目的是预测数值型的目标值.最直接的办法是依据输入写出一个目标值的计算公式.这就是所谓的回归方程,而这个方程的系数称为回归系数,求这些回归系数的过程就是回归.\n假定输入数据存放在矩阵$X$中,而回归系数存放在向量$\\theta$中,那么对于给定的数据$X_1$,预测结果将会通过$Y_1=\\theta^TX_1$给出.现在的问题是,手里有一些$X$和对应的$y$,怎样才能找到$w$呢?一个常用的方法是找出使误差最小的$w$.这里的误差是指预测$y$值和真实$y$值之间的差值,使用该误差的简单累加将使得正差值和负差值相互抵消,所以我们采用平方误差.\n- 平方误差\n  $$\\begin{aligned}\n      J(\\theta)&=\\frac{1}{2}\\sum_{i=1}^m(\\theta^Tx^{(i)}-y^{(i)})^2\\\\\n      &=\\frac{1}{2}(X\\theta-\\vec{y})^T(X\\theta-\\vec{y})\n  \\end{aligned}$$\n  如果对$\\nabla_\\theta{J(\\theta)}=0$,解出$\\theta$如下:\n  $$\\begin{aligned}\n      \\theta=(X^TX)^{-1}X^T\\vec{y}\n  \\end{aligned}$$\n  $w$上方的小标记表示,这是当前可以估计出的$w$的最优解.\n  上述公式中包含$(X^TX)^{-1}$,也就是需要对矩阵求逆,因此这个方程只在逆矩阵存在的时候适用.然而,矩阵的逆可能并不存在,因此必须要在代码中对此作出判断\n  上述求$\\theta$的方法称作OLS(ordinary least squares),意思是\"普通最小二乘法\"\n\n### 利用标准回归找出最佳拟合直线\n- 标准回归求解公式\n  $$\\begin{aligned}\n      \\theta=(X^TX)^{-1}X^Ty\n  \\end{aligned}$$\n  效果图\n  {% asset_img 2019062217201432.png %}\n  代码如下\n  ```python\n    import numpy as np\n    import matplotlib.pyplot as plt\n\n    # 文本数据处理\n    def loadDataSet(fileName):\n        # 获取特征个数\n        numFeat = len(open(fileName).readline().split('\\t'))-1\n        # 数据集矩阵,标签值矩阵\n        dataMat = []; labelMat = []\n        fr = open(fileName)\n        for line in fr.readlines():\n            lineArr=[]\n            curLine=line.strip().split('\\t')\n            for i in range(numFeat):\n                lineArr.append(float(curLine[i]))\n            dataMat.append(lineArr)\n            labelMat.append(float(curLine[-1]))\n        # 返回数据集矩阵,标签集矩阵\n        return dataMat,labelMat\n\n    # 求取线性回归系数\n    def standRegres(xArr,yArr):\n        xMat=np.mat(xArr); yMat=np.mat(yArr).T\n        xTx=xMat.T*xMat\n        if np.linalg.det(xTx) == 0.0:\n            print(\"This matrix is singular, cannot do inverse\")\n            return\n        ws = xTx.I * (xMat.T*yMat)\n        return ws\n\n    # 在用内积来预测y的时候,第一维将乘以前面的常数x0,第二维将乘以输入变量x1,\n    # 因假设x0=1,最终会得到y=ws[0]+ws[1]*x1,yHat为预测值,用于与真实y值区分开来\n    xArr,yArr=loadDataSet('ex0.txt') # 加载数据集和标签集\n    ws=standRegres(xArr,yArr) # 求取回归系数\n    xMat=np.mat(xArr); yMat=np.mat(yArr)\n    plt.scatter(xMat[:,1].A1,yMat.A1) # 画点\n    xCopy=xMat.copy()\n    xCopy.sort(0) # 将x坐标递增排序\n    yHat=xCopy*ws # 计算预测y坐标\n    plt.plot(xCopy[:,1],yHat,color='r')\n    plt.show()\n  ```\n### 利用局部加权线性回归找出最佳拟合曲线\n- 局部加权线性回归求解公式\n  线性回归的一个问题是有可能出现欠拟合现象,因为它求的是具有最小均方误差的无偏估计.显而易见,如果模型欠拟合将不能取得最好的预测效果,所以有些方法允许在估计中引入一些偏差,从而降低预测的均方误差\n  其中的一个方法是局部加权线性回归(Locally Weighted Linear Regression,LWLR),在该算法中,我们给待预测点附近的每个点赋予一定权重;在这个子集上基于最小均方差来进行普通的回归.与kNN一样,这种算法每次预测均需要事先选取出对应的数据子集.\n  该算法解出回归系数w的形式如下:\n  $$\\begin{aligned}\n      \\theta=(X^TWX)^{-1}X^TW\\vec{y}\n  \\end{aligned}$$\n  其中$W$是一个矩阵,用来给每个数据点赋予权重.\n  LWLR使用\"核\"(与SVM中的核类似)来对附近的点赋予更高的权重.最常用的核是高斯核,高斯核对应的权重如下\n  $$\\begin{aligned}\n      w(i,i)=\\exp\\left(-\\frac{\\Vert x^{(i)}-x\\Vert^2}{2k^2}\\right)\n  \\end{aligned}$$\n  这样就构建了一个只含对角元素的权重矩阵$W$,并且点$x$与$x^{(i)}$越近,$w(i,i)$将会越大.上述公式包含一个需要用户指定的参数$k$,它决定了对附近的点赋予多大的权重.这也是使用LWLR时唯一需要考虑的参数\n  以下是当$k$取不同的值时的效果图\n  {% asset_img 2019062218402034.png %}\n  $k=0.01$(完美拟合)时的代码如下\n  ```python\n    import numpy as np\n    import matplotlib.pyplot as plt\n\n    # 数据集提取\n    def loadDataSet(fileName):\n    numFeat = len(open(fileName).readline().split('\\t'))-1\n    dataMat = []; labelMat = []\n    fr = open(fileName)\n    for line in fr.readlines():\n        lineArr=[]\n        curLine=line.strip().split('\\t')\n        for i in range(numFeat):\n            lineArr.append(float(curLine[i]))\n        dataMat.append(lineArr)\n        labelMat.append(float(curLine[-1]))\n    return dataMat,labelMat\n\n    # lwlr=Locally Weighted Linear Regression\n    def lwlr(testPoint,xArr,yArr,k=1.0):\n        xMat=np.mat(xArr); yMat=np.mat(yArr).T\n        m=np.shape(xMat)[0]\n        weights=np.mat(np.eye(m))\n        for j in range(m): # 高斯核函数计算权重\n            diffMat=testPoint-xMat[j,:]\n            diffMat=diffMat[:,1]\n            weights[j,j]=np.exp(diffMat.T*diffMat/(-2.0*k**2))\n        xTx=xMat.T*(weights*xMat)\n        if np.linalg.det(xTx)==0.0:\n            print('This matrix is singular, cannot do inverse')\n            return\n        ws =xTx.I*(xMat.T*(weights*yMat))\n        return testPoint*ws\n\n    def lwlrTest(testArr,xArr,yArr,k=1.0):\n        m=np.shape(testArr)[0]\n        yHat=np.zeros(m)\n        # 每个点x(i),都将其与所有点进行比较,计算权重\n        for i in range(m):\n            yHat[i]=lwlr(testArr[i],xArr,yArr,k)\n        return yHat\n\n    # 局部加权线性回归\n    xArr,yArr=loadDataSet('ex0.txt')\n    xArr,yArr=np.mat(xArr),np.mat(yArr)\n    plt.scatter(xArr[:,1].A,yArr.A,s=10)\n    xCopy=xArr.copy()\n    xCopy.sort(0)\n    yHat=lwlrTest(xCopy,xArr,yArr,k=0.01)\n    plt.plot(xCopy[:,1],yHat,color='r')\n    plt.show()\n  ```\n  简单线性回归达到了与局部加权线性回归类似的效果.这也表明一点,必须在未知数据上比较效果才能选取到最佳模型.\n  本例展示了如何使用局部加权线性回归来构建模型,可以得到比普通线性回归更好的效果,局部加权线性回归的问题在于,每次必须在整个数据集上运行.也即是说为了做出预测,必须保存所有的训练数据.\n### 利用岭回归找出最佳拟合曲线\n- 岭回归求解公式\n  $$\\begin{aligned}\n      \\theta=(X^TX+\\lambda{I})^{-1}X^T\\vec{y}      \n  \\end{aligned}$$\n  书上未有给出详细的利用岭回归进行曲线拟合的代码实现,后序再进行\n- 公式推导\n  只是在最小二乘法的基础上加了个正则化项($\\frac{\\lambda}{2}\\Vert\\theta\\Vert^2$)\n  $$\\begin{aligned}\n      J(\\theta)&=\\frac{1}{2}\\sum_{i=1}^m(\\theta^Tx^{(i)}-y^{(i)})^2+\\frac{\\lambda}{2}\\Vert\\theta\\Vert^2\\\\\n      &=\\frac{1}{2}(X\\theta-\\vec{y})^T(X\\theta-\\vec{y})+\\frac{\\lambda}{2}\\theta^T\\theta\\\\\n      &=\\frac{1}{2}\\left[(X\\theta-\\vec{y})^T(X\\theta-\\vec{y})+\\lambda\\theta^T\\theta\\right]\n  \\end{aligned}$$\n  最小化损失函数(目标函数)$J(\\theta)$,即\n  $$\\begin{aligned}\n      \\nabla_\\theta{J(\\theta)}&=\\frac{1}{2}\\nabla_\\theta(\\theta^TX^TX\\theta-\\theta^TX^T\\vec{y}-\\vec{y}^TX\\theta+\\vec{y}^T\\vec{y}+\\lambda\\theta^T\\theta)\\\\\n      &=\\frac{1}{2}(2X^TX\\theta-2X^T\\vec{y}+2\\lambda\\theta)\\\\\n      &=X^TX\\theta-X^T\\vec{y}+\\lambda\\theta\n  \\end{aligned}$$\n  令$\\nabla_\\theta{J(\\theta)}=0$,则\n  $$\\begin{aligned}\n      \\theta=(X^TX+\\lambda I)^{-1}X^T\\vec{y}\n  \\end{aligned}$$\n### 其他\n#### 2019/07/21: 写的LWLR代码\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef LoadData(fileName):\n    featNum = len(open(fileName).readline().split('\\t'))-1\n    fr = open(fileName)\n    data=[]\n    for oneRow in fr.readlines():\n        oneRowData = oneRow.split('\\t')\n        data=data+[[float(var) for var in oneRowData]]\n    data=np.array(data)\n    dataSet = data[:,:-1]\n    labels = data[:,-1]\n    return dataSet,labels\n\ndef LWLR(x_test,X,y,k):\n    m=X.shape[0]\n    x_test=np.mat(x_test)\n    X=np.mat(X)\n    y=np.mat(y)\n    y=y.reshape(y.size,1)\n    W=np.mat(np.eye(m))\n    for i in range(m):\n        diff=X[i]-x_test\n        diff=diff[:,1]\n        W[i,i]=np.exp(-diff.T*diff/(2.0*k**2))\n    XTWX=X.T*W*X\n    if np.linalg.det(XTWX)==0.0:\n        print('This matrix is singular, cannot do inverse')\n        return\n    theta=XTWX.I*X.T*W*y\n    return x_test*theta\n\ndef LWLRTest(X_test,X,y,k=0.1):\n    m = X_test.shape[0]\n    y_pred = np.zeros(m)\n    for i in range(m):\n        y_pred[i]=LWLR(X_test[i],X,y,k)\n    return y_pred\n\ndataSet,labels = LoadData('ex0.txt')\nplt.scatter(dataSet[:,1],labels)\nX=np.mat(dataSet)\ny=np.mat(labels)\nX_test=X.copy()\nX_test.sort(0)\ny_pred=LWLRTest(X_test,X,y,k=0.01)\nplt.plot(X_test[:,1].A,y_pred,color='r')\nplt.show()\n```\n#### 2019/07/22: 写的LWLR代码\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef LoadData(fileName):\n    n = len(open(fileName).readline().split('\\t'))\n    fr = open(fileName)\n    dataSet = []\n    for oneRowData in fr.readlines():\n        oneRow = oneRowData.split('\\t')\n        dataSet = dataSet + [[float(var) for var in oneRow]]\n    dataSet = np.array(dataSet)\n    X, y = dataSet[:,:-1], dataSet[:,-1]\n    return X, y\n\ndef LWLR(x_test, X_train, y_train, k):\n    X_train = np.mat(X_train)\n    y_train = np.mat(y_train)\n    y_train = y_train.reshape(y_train.size, 1)\n    m = X_train.shape[0]\n    W = np.mat(np.eye(m))\n    for i in range(m):\n        diff = x_test - X_train[i]\n        dist = np.sum(diff.T*diff)\n        W[i,i] = np.exp(-dist/(2.0*k**2))\n    XTWX=X_train.T*W*X_train\n    if (np.linalg.det(XTWX) == 0.0):\n        print('This matrix is singular')\n        return\n    theta = XTWX.I*X_train.T*W*y_train\n    return x_test*theta\n\ndef LWLRTest(X_test, X_train, y_train, k=0.1):\n    m = X_test.shape[0]\n    y_predict = np.zeros(m)\n    for i in range(m):\n        y_predict[i] = LWLR(X_test[i],X_train,y_train,k)\n    return y_predict\n\n\ndef LRTest(X_test, X_train, y_train):\n    X_train = np.mat(X_train)\n    y_train = np.mat(y_train)\n    y_train = y_train.reshape(y_train.size, 1)\n    m = X_train.shape[0]\n    XTX=X_train.T*X_train\n    if np.linalg.det(XTX) == 0.0:\n        print('This matrix is singular')\n        return\n    theta = XTX.I*X_train.T*y_train\n    return X_test * theta\n\n\n# LWLR(Locally Weighted Linear Regression)\nX, y = LoadData('ex0.txt')\nplt.scatter(X[:,1], y)\nX_test = X.copy()\nX_test.sort(0)\ny_predict = LWLRTest(X_test, X, y, k=0.01)\nplt.plot(X_test[:,1], y_predict, color='r', label='LWLR')\n\n# LR(Linear Regression)\nX_test = X.copy()\nX_test.sort(0)\ny_predict = LRTest(X_test, X, y)\nplt.plot(X_test[:,1],y_predict,color='b', label='LR')\nplt.legend()\nplt.show()\n\n```\n效果图\n{% asset_img 2019072215220616.png %}\n#### 2019/07/30 写的多项式拟合(Polynomial regression)\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef LoadData(fileName):\n    n = len(open(fileName).readline().split('\\t'))\n    fr = open(fileName)\n    dataSet = []\n    for oneRowData in fr.readlines():\n        oneRow = oneRowData.split('\\t')\n        dataSet = dataSet + [[float(var) for var in oneRow]]\n    dataSet = np.array(dataSet)\n    X, y = dataSet[:,:-1], dataSet[:,-1]\n    return X, y\n\ndef LeastSquare(X_train, y_train, X_test, degree =1):\n    m = X_train.shape[0]\n    X = np.zeros((m,degree +1))\n    for j in range(degree +1):\n        X[:,j] = X_train[:,1]**j\n    XTX = X.T@X\n    if np.linalg.det(XTX) == 0.0:\n        print('This matrix is singular')\n        return\n    theta = np.linalg.inv(XTX)@X.T@y_train\n    m = X_test.shape[0]\n    X = np.zeros((m,degree +1))\n    for j in range(degree +1):\n        X[:,j] = X_test**j\n    return X@theta\n\ndef plot_subplot(X_train, y_train, X_test, degree ):\n    y_test = LeastSquare(X_train, y_train, X_test, degree )\n    plt.plot(X_test, y_test, color='b', label='LR')\n    plt.scatter(X_train[:, 1], y_train)\n    plt.title('degree='+str(degree))\n\n# 多项式拟合\nX_train, y_train = LoadData('ex0.txt')\nx0_min,x0_max = X_train[:,1].min(), X_train[:,1].max()\nX_test = np.linspace(x0_min, x0_max, 500)\nfig = plt.figure(figsize=(15,10))\nfor i in range(1,6):\n    for j in range(1,6):\n        fig.add_subplot(5,5,(i-1)*5+j)\n        plot_subplot(X_train, y_train, X_test, degree=(i-1)*5+j)\nplt.show()\n```\n效果如下\n{% asset_img 2019073018251430.png %}\n将数据集改变一下,改成$sin(x)$并加点噪声,看看效果\n```python\nX_train = np.array([[1,var] for var in np.linspace(0, 2 * np.pi, 30)])\ny_train = np.sin((X_train[:,1]) + np.random.normal(0, 0.1, 30))\n```\n数据集改变后的效果如下\n{% asset_img 2019073018430931.png %}\n#### 2019/07/30 岭回归的实现(在损失函数中加入正则化项)\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef LoadData(fileName):\n    n = len(open(fileName).readline().split('\\t'))\n    fr = open(fileName)\n    dataSet = []\n    for oneRowData in fr.readlines():\n        oneRow = oneRowData.split('\\t')\n        dataSet = dataSet + [[float(var) for var in oneRow]]\n    dataSet = np.array(dataSet)\n    X, y = dataSet[:,:-1], dataSet[:,-1]\n    return X, y\n\ndef LeastSquare(X_train, y_train, X_test, lamb=0.0001, degree =1):\n    m = X_train.shape[0]\n    X = np.zeros((m,degree +1))\n    for j in range(degree +1):\n        X[:,j] = X_train[:,1]**j\n    XTX = X.T@X+np.eye(degree+1)*lamb\n    if np.linalg.det(XTX) == 0.0:\n        print('This matrix is singular')\n        return\n    theta = np.linalg.inv(XTX)@X.T@y_train\n    m = X_test.shape[0]\n    X = np.zeros((m,degree +1))\n    for j in range(degree +1):\n        X[:,j] = X_test**j\n    return X@theta\n\ndef plot_subplot(X_train, y_train, X_test, lamb, degree ):\n    y_test = LeastSquare(X_train, y_train, X_test, lamb=lamb ,degree=degree )\n    plt.plot(X_test, y_test, color='b', label='LR')\n    plt.scatter(X_train[:, 1], y_train)\n    plt.title('lambda=%.3f'%lamb + ', degree='+str(degree))\n\n# 多项式拟合\nX_train = np.array([[1,var] for var in np.linspace(0, 2 * np.pi, 5)])\ny_train = np.sin((X_train[:,1]) + np.random.normal(0, 0.1, 5))\nx0_min,x0_max = X_train[:,1].min(), X_train[:,1].max()\nX_test = np.linspace(x0_min, x0_max, 500)\nfig = plt.figure()\nplt.subplots_adjust(hspace=0.5)\nfig.add_subplot(1, 2, 1)\nplot_subplot(X_train, y_train, X_test, lamb=0, degree=5)\nfig.add_subplot(1, 2, 2)\nplot_subplot(X_train, y_train, X_test, lamb=0.001, degree=5)\nplt.show()\n```\n效果对比如下(左图没有加入正则化项的效果**Polynomial Regression**,右图是加入正则化项后的效果**Ridge Regression**)\n{% asset_img 2019073019423032.png %}\n### 参考\n1. [<<机器学习实战>>第8章 预测数值型数据:回归](https://book.douban.com/subject/24703171/)","tags":["机器学习实战"],"categories":["机器学习"]},{"title":"[leetcode]377.组合总和IV","url":"%2Fposts%2F41efd2d8%2F","content":"{% asset_img 2019062015331916.png %}\n\n### 解题思路 \n`法1:组合数DP思想`\ndfs会超时\n(这道题如果不记得怎么做了,像下面例子那样手推一遍就明白了)\n使用dp数组，dp[i]代表组合数为i时使用nums中的数能组成的组合数的个数\n举个例子:nums=[1,2,4,6],target=6\ndp[0]=1\ndp[1]=dp[0]=1\ndp[2]=dp[1]+dp[0]=2\ndp[3]=dp[2]+dp[1]=3\ndp[4]=dp[3]+dp[2]+dp[0]=6\ndp[5]=dp[4]+dp[3]+dp[1]=10\ndp[6]=dp[5]+dp[4]+dp[2]+dp[0]=19\n这里的dp[6]=(1+dp[5])+(2+dp[4])+(4+dp[2])+(6+dp[0])\n```cpp\nusing ull=unsigned long long;\nint combinationSum4(vector<int>& nums, int target) {\n    vector<ull> dp(target+1);\n    dp[0]=1;\n    //是为了算上自己的情况，比如dp[1]可以由dp[0]和1这个数的这种情况组成。\n    if (nums.empty()) return 0;\n    for (int i=*min_element(nums.begin(), nums.end());i<=target;++i)\n    {\n        for(int j=0;j<nums.size();++j)\n            if (i>=nums[j])\n                dp[i]+=dp[i-nums[j]];\n    }\n    return dp.back();\n}\n```\n`法2:回溯`\n超时了\n```cpp\nvector<vector<int>> res;\nvoid backTrace(vector<int>& nums, vector<int> tmp, int remain)\n{\n    if (remain<0) return;\n    else if (remain==0)\n        res.push_back(tmp);\n    else\n    {\n        for(int l=0;l<nums.size();++l)\n        {\n            tmp.push_back(nums[l]);\n            backTrace(nums,tmp,remain-nums[l]);\n            tmp.pop_back();\n        }\n    }\n}\nint combinationSum4(vector<int>& nums, int target) {\n    backTrace(nums,vector<int>(),target);\n    return res.size();\n}\n```\n### 参考","tags":["dp"],"categories":["OJ"]},{"title":"[leetcode]216.组合总和III","url":"%2Fposts%2Fc6295a8d%2F","content":"{% asset_img 2019062014551115.png %}\n\n### 解题思路 \n`回溯法`\n```cpp\nvector<vector<int>> res;\nvoid backTrace(int val, int k, int remain, vector<int> tmp)\n{\n    if (remain<0) return;\n    else if (remain==0)\n    {\n        if (tmp.size()==k)\n            res.push_back(tmp);\n        return;\n    }\n    else\n    {\n        if (tmp.size()>=k) return;\n        for (int v=val;v<=9;++v)\n        {\n            tmp.push_back(v);\n            backTrace(v+1,k,remain-v,tmp);\n            tmp.pop_back();\n        }\n    }\n}\nvector<vector<int>> combinationSum3(int k, int n) {\n    backTrace(1,k,n,vector<int>());\n    return res;\n}\n```\n### 参考","tags":["leetcode"],"categories":["OJ"]},{"url":"%2Fposts%2F0%2F"},{"title":"[leetcode]40.组合总和II","url":"%2Fposts%2F96ce9be2%2F","content":"{% asset_img 2019062014325813.png %}\n\n### 解题思路 \n`回溯+去重`\n```cpp\nvector<vector<int>> res;\nvoid backTrace(vector<int>& nums, vector<int> tmp, int from, int remain)\n{\n    if (remain<0) return;\n    else if(remain==0) res.push_back(tmp);\n    else\n    {\n        for(int l=from;l<nums.size();++l)\n        {\n            if (nums[l]>remain) return;\n            if (l>from&&nums[l-1]==nums[l]) // 去重\n                continue;\n            tmp.push_back(nums[l]);\n            backTrace(nums,tmp,l+1,remain-nums[l]);\n            tmp.pop_back();\n        }\n    }\n}\nvector<vector<int>> combinationSum2(vector<int>& nums, int target) {\n    sort(nums.begin(),nums.end());\n    backTrace(nums,vector<int>(),0,target);\n    return res;\n}\n```\n\n### 参考","tags":["leetcode"],"categories":["OJ"]},{"title":"[leetcode]13.罗马数字转整数","url":"%2Fposts%2Fd59c100a%2F","content":"{% asset_img 2019062012155312.png %}\n\n### 解题思路 \n`规律1`:\nV/I=L/X=D/C=5\nX/I=C/X=M/C=10\n`规律2`:\nIV=I+V-2*I=4\nIX=I+X-2*I=9\nXL=X+L-2*X=40\nXC=X+C-2*X=90\nCD=C+D-2*C=400\nCM=C+M-2*C=900\n```cpp\nint romanToInt(string s) {\n    map<char,int> dict{\n        {'I',1},\n        {'V',5},\n        {'X',10},\n        {'L',50},\n        {'C',100},\n        {'D',500},\n        {'M',1000}\n    };\n    int sum=dict[s[0]];\n    for(int i=1;i<s.size();++i)\n    {\n        if (dict[s[i]]/dict[s[i-1]]==5||dict[s[i]]/dict[s[i-1]]==10)\n            sum+=dict[s[i]]-2*dict[s[i-1]];\n        else\n            sum+=dict[s[i]];\n    }\n    return sum;\n}\n```\n\n\n### 参考","tags":["数学方法"],"categories":["OJ"]},{"title":"[机器学习实战]第7章 AdaBoost元算法","url":"%2Fposts%2F82acf56%2F","content":"\n### 介绍\n提升方法AdaBoost基于这样一种思想:对于一个复杂任务来说,将多个专家的判断进行适当的综合所得出的判断,要比其中任何一个专家单独的判断好.实际上,就是\"三个臭皮匠顶个诸葛亮\"的道理\n**强可学习**:一个概念,如果存在一个多项式的学习算法能够学习它,并且正确率很高,那么就称这个概念是强可学习的.\n**弱可学习**:一个概念,如果存在一个多项式的学习算法能够学习它,它的正确率仅比随机猜测略好,那么就称这个概念是弱可学习.\n提升方法就是从弱学习算法出发,反复学习,得到一系列弱分类器(又称为基本分类器),然后组合这些弱分类器,构成一个强分类器.\n\n### 算法\n输入:训练数据集$T=\\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\\}$,其中$x_i\\in{\\chi}\\subseteq\\R^n,y_i\\in{\\mathcal{Y}}=\\{-1,+1\\}$;弱学习算法;\n输出:最终分类器$G(x)$.\n- (1) 初始化训练数据的权值分布\n  $$\\begin{aligned}\n      D_1=(w_{11},...,w_{1i},...,w_{1N}),w_{1i}=\\frac{1}{N},\\ i=1,2,...,N\n  \\end{aligned}$$\n- (2) 对$m=1,2,...,M$\n  - (a) 使用具有权值分布$D_m$的训练数据集学习,得到基本分类器\n  $$\\begin{aligned}\n      G_m(x):\\chi\\to\\{-1,+1\\}\n  \\end{aligned}$$\n  - (b) 计算$G_m(x)$在训练数据集上的分类误差率\n  $$\\begin{aligned}\n      e_m=\\sum_{i=1}^N{P(G_m(x_i)\\neq{y_i})}=\\sum_{i=1}^N{w_{mi}I(G_m(x_i)\\neq{y_i})}\\tag{8.1}\n  \\end{aligned}$$\n  - (c) 计算$G_m(x)$的系数\n  $$\\begin{aligned}\n      \\alpha_m=\\frac{1}{2}\\log\\frac{1-e_m}{e_m}\\tag{8.2}\n  \\end{aligned}$$\n  这里的对数是自然对数.\n  - (d) 更新训练数据集的权值分布\n  $$\\begin{aligned}\n      D_{m+1}&=(w_{m+1,1},...,w_{m+1,i},...,w_{m+1,N})\\qquad (8.3)\\\\\n      w_{m+1,i}&=\\frac{w_{mi}}{Z_m}\\exp(-\\alpha_my_iG_m(x_i)),\\ i=1,2,...,N\\qquad (8.4)\n  \\end{aligned}$$\n  这里,$Z_m$是规范化因子\n  $$\\begin{aligned}\n      Z_m=\\sum_{i=1}^N{w_{mi}\\exp(-\\alpha_my_iG_m(x_i))}\\tag{8.5}\n  \\end{aligned}$$\n  它使$D_{m+1}$成为一个概率分布.\n- (3) 构建基本分类器的线性组合\n  $$\\begin{aligned}\n      f(x)=\\sum_{m=1}^M\\alpha_mG_m(x)\\tag{8.6}\n  \\end{aligned}$$\n  得到最终分类器\n  $$\\begin{aligned}\n      G(x)=sign(f(x))=sign\\left[\\sum_{m=1}^M{\\alpha_mG_m(x)}\\right]\\tag{8.7}\n  \\end{aligned}$$\n以下代码对应于李航统计学习方法公式\n```python\nimport numpy as np\nfrom sklearn.datasets import load_iris\n\ndef loadSimpData():\n    datMat = np.mat([[1., 2.1],\n                     [2., 1.1],\n                     [1.3, 1.],\n                     [1., 1.],\n                     [2., 1.]])\n    classLabels = [1.0, 1.0, -1.0, -1.0, 1.0]\n    return datMat, classLabels\n\n\ndef stumpClassify(dataMatrix, dimen, threshVal, threshIneq):\n    '''\n    通过阈值比较对数据进行分类,分类标准是threshIneq=['lt','gt'],如果为'lt',那么≤阈值的为-1,>阈值的为+1\n    :param dataMatrix:数据集\n    :param dimen:维度\n    :param threshVal:阈值\n    :param threshIneq:分类标准['lt','gt']\n    :return: 返回分类后的结果\n    '''\n    # 进行划分,划分标准是threshIneq=['lt','gt'],如果为'lt',那么≤阈值的为-1,>阈值的为+1\n    retArray = np.ones((np.shape(dataMatrix)[0], 1))\n    if threshIneq == 'lt':\n        retArray[dataMatrix[:, dimen] <= threshVal] = -1.0\n    else:\n        retArray[dataMatrix[:, dimen] > threshVal] = -1.0\n    return retArray\n\n\ndef buildStump(dataArr, classLabels, D):\n    '''\n    找到数据集上最佳的单层决策树(即从哪个维度,阈值多少进行分类能获得最小的错误率)\n    :param dataArr:数据集\n    :param classLabels:类别标签\n    :param D:权重向量\n    :return:最佳单层决策树的相关信息(维度,阈值,分类标准),最小错误率,最好的分类结果\n    '''\n    dataMatrix = np.mat(dataArr);\n    labelMat = np.mat(classLabels).T\n    m, n = np.shape(dataMatrix)\n    numSteps = 10.0;\n    bestStump = {};\n    bestClasEst = np.mat(np.zeros((m, 1)))\n    minError = np.inf\n    for i in range(n):\n        rangeMin = dataMatrix[:, i].min()\n        rangeMax = dataMatrix[:, i].max()\n        stepSize = (rangeMax - rangeMin) / numSteps\n        for j in range(-1, int(numSteps) + 1):\n            for inequal in ['lt', 'gt']:\n                # 计算新的阈值\n                threshVal = (rangeMin + float(j) * stepSize)\n                # 根据该阈值进行的类别划分的值\n                predictedVals = stumpClassify(dataMatrix, i, threshVal, inequal)\n                # 计算错误率\n                errArr = np.mat(np.ones((m, 1)))\n                errArr[predictedVals == labelMat] = 0\n                weightedError = (D.T * errArr).flat[0]\n                print(\"split: dim {0}, thresh {1:.2f}, thresh inequal: {2}, the weighted error is {3:.3f}\".format(i,threshVal,inequal,weightedError))\n                # 获取最小错误率所对应的最佳的单层决策树\n                if weightedError < minError:\n                    minError = weightedError\n                    bestClasEst = predictedVals.copy()\n                    bestStump['dim'] = i\n                    bestStump['thresh'] = threshVal\n                    bestStump['ineq'] = inequal\n    return bestStump, minError, bestClasEst\n\n\ndef adaBoostTrainDS(dataArr, classLabels, numIt=40):\n    weakClassArr = []\n    m = np.shape(dataArr)[0]\n    # (1)初始的权重D_1\n    D = np.mat(np.ones((m, 1)) / m)\n    # 类别估计累计值\n    aggClassEst = np.mat(np.zeros((m, 1)))\n    for i in range(numIt):\n        # (2)-a 得到基本分类器G_m(x):根据权重向量D(错分的数据权重较高,正确的数据权重较低),找到最佳的单层决策树\n        # (2)-b 计算G_m(x)在数据集上的分类误差率\n        bestStump, error, classEst = buildStump(dataArr, classLabels, D)\n        print(\"D:{0}\".format(D.T))\n        # (2)-c 计算G_m(x)的系数\n        alpha = float(0.5 * np.log((1.0 - error) / max(error, 1e-16)))\n        # 记录最佳决策树的信息\n        bestStump['alpha'] = alpha\n        # 将最佳决策树加入到单层决策树数组\n        weakClassArr.append(bestStump)\n        print(\"classEst: {0}\".format(classEst.T))\n        # (2)-d 更新训练数据集的权值分布D_{m+1}\n        expon = np.multiply(-1 * alpha * np.mat(classLabels).T, classEst)\n        D = np.multiply(D, np.exp(expon))\n        D = D / D.sum()\n        # (3) 构建基本分类器的线性组合:计算累积类别估计值\n        aggClassEst += alpha * classEst\n        print(\"aggClassEst: {0}\".format(aggClassEst.T))\n        # 根据累积类别估计值来计算错误率\n        aggErrors = np.multiply(np.sign(aggClassEst) != np.mat(classEst), np.ones((m, 1)))\n        errorRate = aggErrors.sum() / m\n        print(\"total error: {0}\\n\".format(errorRate))\n        # 无错,则直接退出\n        if errorRate == 0.0:\n            break\n    return weakClassArr\n\n\ndef adaClassify(datToClass, classifierArr):\n    dataMatrix = np.mat(datToClass)\n    m = np.shape(dataMatrix)[0]\n    aggClassEst = np.mat(np.zeros((m, 1)))\n    for i in range(len(classifierArr)):\n        classEst = stumpClassify(dataMatrix, classifierArr[i]['dim'],\n                                 classifierArr[i]['thresh'],\n                                 classifierArr[i]['ineq'])\n        aggClassEst += classifierArr[i]['alpha'] * classEst\n        print(aggClassEst)\n    return np.sign(aggClassEst)\n\ndef auto_norm(X):\n    \"\"\"特征归一化(或特征缩放)\n\n    Arguments:\n        X {array} -- 数据集\n\n    Returns:\n        array -- 返回归一化后的数据集\n    \"\"\"\n    X = np.array(X)\n    minVals = X.min(0)\n    maxVals = X.max(0)\n    newVals = (X-minVals)/(maxVals-minVals)\n    return newVals\n\n\ndef loadDataSet_iris():\n    \"\"\"数据集生成\n\n    Returns:\n        array -- 数据集\n        array -- 标签集\n    \"\"\"\n    dataMat, labelMat = load_iris(return_X_y=True)\n    dataMat, labelMat = dataMat[:100,:2], labelMat[:100]\n    return dataMat, labelMat\n\n\n\n# 读取数据集,标签集\ndataMat, labelMat = loadDataSet_iris()\ntmp=[]\nfor var in labelMat:\n    if var==0:\n        tmp.append(-1)\n    else:\n        tmp.append(1)\nlabelMat=tmp\n\nm = len(dataMat)\n# 特征归一化(特征缩放)\ndataMat[:, :] = auto_norm(dataMat[:, :])\n# 所有数据的特征增加一列x0为1\ndataMat = np.column_stack((np.ones(m), dataMat))\n# 交叉验证:将数据打乱\nrndidx = np.arange(m)\nnp.random.shuffle(rndidx)\nshuffledX = []\nshuffledy = []\nfor i in range(m):\n    shuffledX.append(dataMat[rndidx[i]])\n    shuffledy.append(labelMat[rndidx[i]])\ndataMat, labelMat = np.array(shuffledX), np.array(shuffledy)\nX, y = np.array(dataMat), np.array(labelMat)\nmTrain = int(0.75*m)\nmTest = m-mTrain\n# 获取前mTrain个数据做训练数据,用于训练模型\nXtrain, ytrain = np.array(dataMat[:mTrain]), np.array(labelMat[:mTrain])\n\n# 获取后mTest个数据做测试数据,用于测试预测准确率\nXtest, ytest = np.array(dataMat[-mTest:]), np.array(labelMat[-mTest:])\n\nclassifieryArray = adaBoostTrainDS(Xtrain, ytrain)\nypredict = adaClassify(Xtest, classifieryArray)\nerrors=(ypredict!=np.mat(ytest).T)\nerrorRate=errors.sum()/len(ytest)\nprint(errorRate)\n```\n结果:\n{% asset_img 2019061916570111.png %}\n\n### 参考\n1. [<<机器学习实战>>第4章 朴素贝叶斯](https://book.douban.com/subject/24703171/)\n2. [<<统计学习方法>>第4章 朴素贝叶斯法](https://book.douban.com/subject/10590856/)","tags":["机器学习实战"],"categories":["机器学习"]},{"title":"[leetcode]29.两数相除","url":"%2Fposts%2F57c9c2d7%2F","content":"{% asset_img 201906191319478.png %}\n\n### 解题思路 \n要知道2^31-1=2147483647,-2^31=-2147483648\n用$dividend/2^i>=divisor$(i=31,...,0)时,则$res+=2^i$,$dividend-=divisor<<i$\n比如:100/3,可以计算为$100/2^i>=3$,那么$i=5$,则$res+=2^5(32)$,$dividend-=3<<5(96)$,则$dividend=4$因此最后结果为32\n由于题目限制只能用32位也就是int或unsigned,那么INT_MIN=-2^31=-2147483648,INT_MAX=2^31-1=2147483647\n由于要求绝对值,当dividend或divisor有一个为INT_MIN时,绝对值就会溢出int类型,那么应当使用unsigned来存放\n\n```cpp\nint divide(int dividend, int divisor) {\n    if (dividend==0)\n        return 0;\n    if (dividend==INT_MIN&&divisor==-1)\n        return INT_MAX;\n    int negative=1;\n    negative=(dividend^divisor)<0;\n    unsigned int t=dividend==INT_MIN?0x80000000:abs(dividend);\n    unsigned int d=divisor==INT_MIN?0x80000000:abs(divisor);\n    unsigned result=0;\n    for(int i=31;i>=0;--i)\n    {\n        if((t>>i)>=d)\n        {\n            result+=1<<i;\n            t-=d<<i;\n        }\n    }\n    return negative?-result:result;\n}\n```\n\n### 参考","tags":["数学方法"],"categories":["OJ"]},{"title":"[leetcode]209.长度最小的子数组","url":"%2Fposts%2F172cb71%2F","content":"{% asset_img 201906191044597.png %}\n\n### 解题思路 \n`滑动窗口思想`,窗口内始终保持总和`sum>=s`\n```cpp\nconst int INF=10000000;\nint minSubArrayLen(int s, vector<int>& nums) {\n    int minL=INF,sum=0;\n    for(int l=0,r=0;r<nums.size();++r)\n    {\n        sum+=nums[r];\n        while(sum>=s)\n        {\n            minL=min(minL,r-l+1);\n            sum-=nums[l++];\n        }\n    }\n    return minL==INF?0:minL;\n}\n```\n### 参考","categories":["OJ"]},{"title":"[leetcode]7.整数反转","url":"%2Fposts%2Fbb67fbcb%2F","content":"{% asset_img 201906190940496.png %}\n\n### 解题思路 \n本题虽是简单题,但临界点判断确实有些棘手,要知道`2^31-1=2147483647,-2^31=-2147483648`\n整数倒序实际是栈操作,那么用`pop`来表示每次出的栈顶元素\n判断正溢出:如果`x`为正数,如何判断`x*10+pop`是否溢出呢?\n1) 如果`x>INT_MAX/10`,那么必然`x*10>INT_MAX`时溢出\n2) 如果`x==INT_MAX/10`,那么必然`x*10+pop>INT_MAX`时溢出,即此时`pop>7`,因为`INT_MAX=2^31-1=2147483647`\n判断负溢出:如果`x`为负数,如何判断`x*10+pop`是否溢出呢?\n1) 如果`x<INT_MIN/10`,那么必然`x*10<INT_MIN`时溢出\n2) 如果`x==INT_MIN/10`,那么必然`x*10+pop<INT_MIN`时溢出,即此时`pop<-8`,因为`INT_MIN=-2^31=-2147483648`\n```cpp\nint reverse(int x) {\n    int res=0;\n    while(x)\n    {\n        int pop=x%10;\n        x/=10;\n        if (res>INT_MAX/10||(res==INT_MAX/10&&pop>7)) return 0;\n        if (res<INT_MIN/10||(res==INT_MIN/10&&pop<-8)) return 0;\n        res=res*10+pop;\n    }\n    return res;\n}\n```\n### 参考","categories":["OJ"]},{"title":"[leetcode]992.K个不同整数的子数组","url":"%2Fposts%2F28058be4%2F","content":"{% asset_img 201906181102084.png %}\n\n### 解题思路 \n划分为`两个好求解的子问题`,即求出最多`K`个不同整数构成的子数组的个数$f(k)$与最多`K-1`个不同整数构成的子数组的个数$f(k-1)$,相减就能得到恰好`K`个不同整数构成的子数组的个数$a(k)$,核心公式如下\n$$\\begin{aligned}\n    a(k)=f(k)-f(k-1)\n\\end{aligned}$$\n```cpp\n// 求出恰好K个不同整数构成的子数组的个数\nint subarraysWithKDistinct(vector<int>& A, int K) {\n    return AtMostK(A,K)-AtMostK(A,K-1);\n}\n// 求出最多K不同整数构成的子数组的个数\nint AtMostK(vector<int>& A, int K)\n{\n    int res=0;\n    map<int,int> dict;\n    for(int l=0,r=0;r<A.size();++r)\n    {\n        dict[A[r]]++;\n        while(dict.size()>K)\n        {\n            dict[A[l]]--;\n            if (dict[A[l]]==0)\n                dict.erase(A[l]);\n            l++;\n        }\n        res+=r-l+1;\n    }\n    return res;\n}\n```\n### 参考","tags":["滑动窗口"],"categories":["OJ"]},{"title":"[leetcode]300.最长上升子序列","url":"%2Fposts%2F60a9e9e%2F","content":"{% asset_img 201906171019152.png %}\n\n### 解题思路 \n`方法1: deque+二分法`\n维持一个队列,用于保存已知的递增序列,每遇到一个新的数,则更新队列中的元素,让队列中始终保持最小的上升子序列,这样才能有更多的数接在后面,查找更新的元素位置时用的是二分法\n```cpp\nint lengthOfLIS(vector<int>& nums) {\n    int len=nums.size();\n    if (len==0) return 0;\n    deque<int> dq(1,nums[0]);\n    for(int i=1;i<len;++i)\n    {\n        if (nums[i]>dq.back())\n            dq.push_back(nums[i]);\n        else if (nums[i]<dq.back())\n        {\n            int l=0,r=dq.size()-1;\n            for(;l<=r;)\n            {\n                int m=(l+r)>>1;\n                if (dq[m]==nums[i])\n                {l=m;break;}\n                else if (dq[m]<nums[i])\n                    l=m+1;\n                else\n                    r=m-1;\n            }\n            dq[l]=nums[i];\n        }\n    }\n    return dq.size();\n}\n```\n时间复杂度: $O(nlogn)$\n`方法2: dp的思想`\n用`dp[i]`来表示以下标`i`(>0)结尾的数字的`最长上升子序列长度`,那么每遇到一个数,都要与之前所有已知的`dp[i]`进行比较,选取满足条件的最大的加1\n```cpp\nint lengthOfLIS(vector<int>& nums) {\n    int len=nums.size();\n    if (len==0) return 0;\n    vector<int> dp(len+1,1);\n    int maxL=1;\n    for(int i=2;i<=len;++i)\n    {\n        for(int j=i-1;j>=1;--j)\n        {\n            if (nums[i-1]>nums[j-1])\n                dp[i]=max(dp[i],dp[j]+1);\n        }\n        maxL=max(maxL,dp[i]);\n    }\n    return maxL;\n}\n```\n时间复杂度: $O(n^2)$\n### 参考\n- [liweiwei1419题解](https://leetcode-cn.com/problems/two-sum/solution/dong-tai-gui-hua-er-fen-cha-zhao-tan-xin-suan-fa-p/)","categories":["OJ"]},{"title":"[leetcode]718.最长重复子数组","url":"%2Fposts%2Ff4fb85e5%2F","content":"{% asset_img 201906170922441.png %}\n\n### 解题思路 \n`dp思想`,`dp[i][j]`表示A以下标i作为匹配的最后一个数字,B以下标j作为匹配的最后一个数字的匹配长度\n最后一个字符不匹配,那么直接`dp[i][j]=0`\n核心公式如下\n$$\\begin{aligned}\n    dp[i][j]=1+\\max(dp[i][j],dp[i-1][j-1])\n\\end{aligned}$$\n代码如下\n```cpp\nint findLength(vector<int>& A, vector<int>& B) {\n    int m=A.size(),n=B.size(),maxL=0;\n    vector<vector<int>> dp(m+1,vector<int>(n+1));\n    for(int i=1;i<=m;++i)\n    {\n        for(int j=1;j<=n;++j)\n        {\n            if (A[i-1]==B[j-1])\n                dp[i][j]=max(dp[i][j],1+dp[i-1][j-1]);\n            maxL=max(maxL,dp[i][j]);\n        }\n    }\n    return maxL;\n}\n```\n时间复杂度: $O(n^2)$\n### 参考","tags":["dp"],"categories":["OJ"]},{"title":"[leetcode]1091.二进制矩阵中的最短路径","url":"%2Fposts%2F33b9c83%2F","content":"{% asset_img 201906161401402.png %}\n\n### 解题思路 \n常规的迷宫题,全部距离默认为无穷`INF`,当起点与终点有一个不可达时,都返回`-1`\n\n```cpp\nusing pii=pair<int,int>;\nconst int INF=100000000;\nint shortestPathBinaryMatrix(vector<vector<int>>& G) {\n    int dx[]={-1,-1,-1,0,0,1,1,1};\n    int dy[]={-1,0,1,-1,1,-1,0,1};\n    int m=G.size();\n    if (G[0][0]||G[m-1][m-1]) return -1;\n    vector<vector<int>> d(m,vector<int>(m,INF));\n    d[0][0]=0;\n    queue<pii> Q;\n    Q.push(pii(0,0));\n    while(!Q.empty())\n    {\n        int x=Q.front().first;\n        int y=Q.front().second;\n        Q.pop();\n        for(int i=0;i<8;++i)\n        {\n            int nx=x+dx[i],ny=y+dy[i];\n            if (0<=nx&&nx<m&&0<=ny&&ny<m&&!G[nx][ny]&&d[nx][ny]>d[x][y]+1)\n            {\n                Q.push(pii(nx,ny));\n                d[nx][ny]=d[x][y]+1;\n            }\n        }\n    }\n    if (d[m-1][m-1]==INF) return -1;\n    return d[m-1][m-1]+1;\n}\n```\n### 参考","tags":["leetcode"],"categories":["OJ"]},{"title":"[c++11]notes","url":"%2Fposts%2F78499bb0%2F","content":"## Function Object\n### Binder\n由于预定义的函数对象(function object)有一个参数的,也有两个参数的,有时需要使用两个参数的函数对象,并给定一个参数,这时就可以使用特殊的函数适配器(function adapter)即binder,将预定义的函数对象和其他数值结合为一体\n内置的有\n#### Arithmetic operations\n- plus\n- minus\n- multiplies\n- divides\n- modulus\n- negate\n#### Comparisons\n- equal_to\n- not_equal_to\n- greater\n- less\n- greater_equal\n- less_equal\n    下列程序用于计算5为第几小的数字\n    {% asset_img 201906151948581.png %}\n#### Logical operations\n- logical_and\n- logical_or\n- logical_not\n#### Bitwise operations\n- bit_and\n- bit_or\n- bit_xor\n- bit_not\n\n\n### 参考","categories":["OJ"]},{"title":"[leetcode]378.有序矩阵中第K小的元素","url":"%2Fposts%2Faa3ef5c2%2F","content":"{% asset_img 201906151515365.png %}\n\n### 解题思路 \n求解第K小元素有三种方法,快排的每一轮都可以确定一个元素的位置,堆排序也只要前面几轮就可以把元素确定下来,二分法(范围),那么可以使用`划分的方式求第K小元素`,`堆的方式求第K小元素`和`基于范围的二分法求第K小元素`这三种方法.\n`划分的方式求第K小元素`\n```cpp\n#define tr(u) u/n][u%n\nint n;\nint partition(vector<vector<int>>& m, int l, int r)\n{\n    int pivotkey=m[tr(l)];\n    for(;l<r;)\n    {\n        for(;l<r&&pivotkey<=m[tr(r)];) --r;\n        m[tr(l)]=m[tr(r)];\n        for(;l<r&&pivotkey>=m[tr(l)];) ++l;\n        m[tr(r)]=m[tr(l)];\n    }\n    m[tr(l)]=pivotkey;\n    return l;\n}\nint K(vector<vector<int>>&m, int l, int r, int k)\n{\n    int pos=partition(m,l,r);\n    if (pos==k)\n        return m[tr(k)];\n    else if (pos<k)\n        return K(m,pos+1,r,k);\n    else\n        return K(m,l,pos-1,k);\n}\nint kthSmallest(vector<vector<int>>& matrix, int k) {\n    n=matrix.size();\n    return K(matrix,0,n*n-1,k-1);\n}\n```\n时间复杂度: $O(n)$\n\n`堆的方式求第K小元素`\n`基于范围的二分法求第K小元素`\n首先给出一般的`基于范围的二分法求第K小元素`的代码\n思想很简单,即找到最小最大值,然后得到中间值,判断小于等于中间值的个数有多少\n```cpp\nint K(vector<int> nums, int k)\n{\n\tint l = *min_element(nums.begin(),nums.end()), r = *max_element(nums.begin(), nums.end());\n\tfor (; l < r;)\n\t{\n\t\tint m = (l + r) >> 1, cnt=0;\n\t\tcnt += count_if(nums.begin(), nums.end(), std::bind(std::less_equal<int>(), _1,m));\n\t\tif (cnt < k)\n\t\t\tl = m + 1;\n\t\telse\n\t\t\tr = m;\n\t}\n\treturn l;\n}\n```\n下面是题解答案代码\n```cpp\nint kthSmallest(vector<vector<int>>& matrix, int k) {\n    int left = matrix[0][0];\n    int right = matrix.back().back();\n    while (left < right) {\n        int count = 0;\n        int mid = (left + right) >> 1;\n        for (const auto &item : matrix) {\n            count += count_if(item.begin(),item.end(),bind(less_equal<int>(),placeholders::_1,mid));\n        }\n        if (count < k){\n            left = mid + 1;\n        } else{\n            right = mid;\n        }\n    }\n    return left;\n}\n```\n### 参考","tags":["partition"],"categories":["OJ"]},{"title":"[leetcode]72.编辑距离","url":"%2Fposts%2F38e00663%2F","content":"{% asset_img 201906141251582.png %}\n\n### 解题思路 \n`dp[i][j]`表示`word1`到`i`位置转换成`word2`到`j`的位置需要最少步数\n当`word1[i]==word2[j]`时,则`dp[i][j]=dp[i-1][j-1]`\n当`word1[i]!=word2[j]`时,则可以有3种方式进行转换\n- 替换: 即替换`word1[i]`为`word2[j]`,则`dp[i][j]=1+dp[i-1][j-1]`\n- 删除: 即将`word1`到`i-1`位置转换成`word2`到`j`的位置,再删除`word1[i]`,则`dp[i][j]=1+dp[i-1][j]`\n- 插入: 即将`word1`到`i`位置转换成`word2`到`j-1`位置,再进行插入,则`dp[i][j]=1+dp[i][j-1]`\n因此,核心公式为\n$$\\begin{aligned}\n    dp[i][j]=1+min(dp[i-1][j-1],min(dp[i-1][j],dp[i][j-1]))\n\\end{aligned}$$\n\n```cpp\nint minDistance(string word1, string word2) {\n    int m=word1.size(),n=word2.size();\n    vector<vector<int>> dp(m+1,vector<int>(n+1));\n    if (!m||!n)\n        return m+n;\n    for(int i=0;i<=m;++i)\n    {\n        for(int j=0;j<=n;++j)\n        {\n            if (i==0||j==0)\n                dp[i][j]=i+j;\n            else if (word1[i-1]==word2[j-1])\n                dp[i][j]=dp[i-1][j-1];\n            else\n                dp[i][j]=1+min(dp[i-1][j-1],min(dp[i-1][j],dp[i][j-1]));\n        }\n    }\n    return dp[m][n];\n}\n```\n时间复杂度: $O(mn)$\n### 参考\n- [自底向上 和自顶向下](https://leetcode-cn.com/problems/edit-distance/solution/zi-di-xiang-shang-he-zi-ding-xiang-xia-by-powcai-3)","tags":["dp"],"categories":["OJ"]},{"title":"[leetcode]152.乘积最大子序列","url":"%2Fposts%2Fbad1b215%2F","content":"{% asset_img 201906140935231.png %}\n\n### 解题思路\n连续乘积,那么到某个数$i$为止时,前面的数连续乘积必然有一个`最大乘积imax`和`最小乘积imin`,且有一个关系:\n`负数*最大乘积=最小乘积`,`负数*最小乘积=最大乘积`,因此如果当前为负数,那么需要交换`最大乘积`与`最小乘积`\n```cpp\nint maxProduct(vector<int>& nums) {\n    int imin=1,imax=1,len=nums.size(),maxV=nums[0];\n    for(int i=0;i<len;++i)\n    {\n        if (nums[i]<0) swap(imax,imin);\n        imax=max(imax*nums[i],nums[i]);\n        imin=min(imin*nums[i],nums[i]);\n        maxV=max(imax,maxV);\n    }\n    return maxV;\n}\n```\n时间复杂度: $O(n)$\n\n### 参考","tags":["交替法"],"categories":["OJ"]},{"title":"[leetcode]总结","url":"%2Fposts%2Fe52ddf3%2F","content":"### 算法\n- KMP\n    ```cpp\n    int nextval[40001];\n    void get_nextval(string pat)\n    {\n        int i=0,j=-1,len=pat.size();\n        nextval[0]=-1;\n        for(;i<len&&j<len;)\n        {\n            if (j==-1||pat[i]==pat[j])\n            {\n                i++,j++;\n                if (pat[i]==pat[j])\n                    nextval[i]=nextval[j];\n                else\n                    nextval[i]=j;\n            }\n            else\n                j=nextval[j];\n        }\n    }\n    int kmp(string txt, string pat)\n    {\n        int i=0,j=0,lenTxt=txt.size(),lenPat=pat.size();\n        get_nextval(pat);\n        for(;i<lenTxt&&j<lenPat;)\n        {\n            if (j==-1||txt[i]==pat[j])\n                i++,j++;\n            else\n                j=nextval[j];\n        }\n        if (j==lenPat)\n            return i-j;\n        return -1;\n    }\n    ```\n    时间复杂度:$O(m+n)$\n    模式串结束则匹配完成,模式串结束的条件即$j==lenPat$,否则说明匹配失败返回-1\n### 参考","tags":["TOC"],"categories":["OJ"]},{"title":"[leetcode]53.最大子序列和","url":"%2Fposts%2Fc70895a1%2F","content":"{% asset_img 201906131035011.png %}\n\n`思路一: 暴力破解`$O(n^2)$\n两层for循环,第一层for循环作为序列起点i,第二层for循环作为序列终点j,即记录$i~j$的序列和sum,每次记录都更新最大值maxSum\n```cpp\nint maxSubArray(vector<int>& nums) {\n    int len=nums.size(),maxSum=INT_MIN;\n    for(int i=0;i<len;++i)\n    {\n        int sum=nums[i];\n        maxSum=max(maxSum,sum);\n        for(int j=i+1;j<len;++j)\n        {\n            sum+=nums[j];\n            maxSum=max(maxSum,sum);\n        }\n    }\n    return maxSum;\n}\n```\n\n`思路二: DP`$O(n)$\n用$dp[i]$表示以序列中第i个数结尾的最大序列和,那么计算公式如下:\n$$\\begin{aligned}\n    dp[i]=nums[i]+(dp[i-1]>0?dp[i-1]:0)\n\\end{aligned}$$\n```cpp\nint dp[40000];\nint maxSubArray(vector<int>& nums) {\n    // dp[i]表示以i结尾的最大序列和\n    int len=nums.size(),maxSum=nums[0];\n    dp[0]=nums[0];\n    for (int i=1;i<len;++i)\n    {\n        dp[i]=nums[i]+(dp[i-1]>0?dp[i-1]:0);\n        maxSum=max(maxSum,dp[i]);\n    }\n    return maxSum;\n}\n```\n另一种$O(n)$算法\n```cpp\nint maxSubArray(vector<int>& nums) {\n    int sum=nums[0],maxSum=nums[0],len=nums.size();\n    for(int i=1;i<len;++i)\n    {\n        sum=nums[i]+(sum>0?sum:0);\n        maxSum=max(sum,maxSum);\n    }\n    return maxSum;\n}\n```\n`思路三: 分治法`$O(logn)$\n","tags":["dp"],"categories":["OJ"]},{"title":"[leetcode]686.重复叠加字符串匹配","url":"%2Fposts%2F136ccaf5%2F","content":"{% asset_img 2019060414363024.png %}\n\n### 解题思路 \n这道题关键在于明白最后求出的`A重复叠加后的长度<len(B)+2*len(A)`,然后便是常规KMP算法了,为啥这道题要记录,等会会说明原因\nKMP算法是固定格式,这里直接求出模式串B的nextval\n然后就是模式串匹配了,即主串A不断倍增,直到模式串匹配为止\n`代码1`:\n```cpp\nfor(i=0,j=0;i<A.size()&&j<B.size()&&A.size()>=B.size();)\n{\n    if (j==-1||A[i]==B[j])\n        i++,j++;\n    else\n        j=nextval[j];\n}\n```\n大家看代码1有什么问题吗?很有趣的一点是,第一遍提交`代码1`时结果返回不对,后来在调试代码,j=-1的时候无缘无故跳出了循环,后面想到size()返回的是size_t格式,是无符号整形,那么-1也被转为无符号了,反而成为了最大值,下次一定要注意,当然即使这里改成代码2也会有一定问题,只有size()超过了有符号范围,那么必然溢出,不过还好题目要求的数量只有1~10000区间内\n`代码2`:\n```cpp\nfor(i=0,j=0;i<(int)A.size()&&j<(int)B.size()&&A.size()>=B.size();)\n{\n    if (j==-1||A[i]==B[j])\n        i++,j++;\n    else\n        j=nextval[j];\n}\n```\n\n\n### 代码实现 \n```cpp\nint nextval[40001];\nvoid get_nextval(string pat)\n{\n    int i=0,j=-1,len=pat.size();\n    nextval[0]=-1;\n    for(;i<len&&j<len;)\n    {\n        if (j==-1||pat[i]==pat[j])\n        {\n            i++,j++;\n            if (pat[i]==pat[j])\n                nextval[i]=nextval[j];\n            else\n                nextval[i]=j;\n        } else j=nextval[j];\n    }\n}\nint repeatedStringMatch(string A, string B) {\n    int maxLen=2*A.size()+B.size(),times=1,i=0,j=0;\n    string tmp=A;\n    get_nextval(B);\n    while(A.size()<maxLen)\n    {\n        for(i=0,j=0;i<(int)A.size()&&j<(int)B.size()&&A.size()>=B.size();)\n        {\n            if (j==-1||A[i]==B[j])\n                i++,j++;\n            else\n                j=nextval[j];\n        }\n        if (j==B.size())\n            return times;\n        A+=tmp;\n        ++times;\n    }\n    return -1;\n}\n```\n","tags":["KMP"],"categories":["OJ"]},{"title":"[leetcode]459.重复的子字符串","url":"%2Fposts%2F6d263cd4%2F","content":"{% asset_img 2019060413565823.png %}\n\n### 解题思路 \n周期串为`s`,那么设定`t`表示周期,值在`[1,len(s)-1]`的范围,那么剩下的就是依次对每个`t`值进行遍历看是否真的满足周期为`t`,数学中周期的表达式是\n$$\\begin{aligned}\n    f(x+t)=f(x)\n\\end{aligned}$$\n那么代码中可以这样写`s[i%t]==s[i]`\n\n### 代码实现 \n```cpp\nbool repeatedSubstringPattern(string s) {\n    int len=s.size(),i=0,t=0;\n    for(t=1;t<=len/2;++t)\n    {\n        if (len%t) continue;    // 有余数,一定不为周期串\n        for (i=t;i<len&&s[i%t]==s[i];++i);\n        if (i==len) return true;\n    }\n    return false;\n}\n```\n\n### 参考\n1. [<<算法竞赛入门经典>>](https://book.douban.com/subject/4138920/)","tags":["周期串"],"categories":["OJ"]},{"title":"[高等数学]第4章 向量代数与空间解析几何","url":"%2Fposts%2Fcb4e5d34%2F","content":"## 向量代数\n### 向量的运算及性质\n- 数量积(点积,内积)\n  - 几何表示: $\\mathbf{a\\cdot b}=\\mathbf{\\vert{a}\\vert\\vert{b}\\vert}\\cos\\theta$,其中$\\theta=\\lang{\\mathbf{a,b}}\\rang$\n  - 代数表示: $\\mathbf{a}=\\{a_x,a_y,a_z\\},\\mathbf{b}=\\{b_x,b_y,b_z\\}$,则$\\mathbf{a\\cdot b}=a_xb_x+a_yb_y+a_zb_z$\n- 向量积(叉积,外积)\n  - 几何表示: $\\mathbf{a\\times b}$是一向量\n  - 模: $\\vert\\mathbf{a\\times b}\\vert=|\\mathbf a||\\mathbf b|\\sin\\theta$,其中$\\theta=\\lang\\mathbf{a,b}\\rang$\n  - 方向: $\\mathbf{a\\times b}$同时垂直于$\\mathbf a$和$\\mathbf b$,且符合右手法则.\n  - 代数表示:\n    $$\\mathbf{a\\times b}=\\begin{vmatrix}\n        \\mathbf{i}&\\mathbf{j}&\\mathbf{k}\\\\\n        a_x&a_y&a_z\\\\\n        b_x&b_y&b_z\n    \\end{vmatrix}$$\n  - 运算规律:\n    - $\\mathbf{b\\times a}=-(\\mathbf{a\\times b})$\n    - 分配律: $\\mathbf{a\\times(b+c)}=\\mathbf{a\\times b+a\\times c}$\n    - 与数乘的结合律: $\\mathbf{(\\lambda{a})\\times b}=\\mathbf{a\\times(\\lambda{b})}=\\mathbf{\\lambda(a\\times b)}$\n  - 向量积在几何上的应用\n    - 求同时垂直于$\\mathbf a$和$\\mathbf b$的向量:$\\mathbf{a\\times b}$\n    - 求以$\\mathbf a$和$\\mathbf b$为邻边的平行四边形面积: $\\mathbf{S=|a\\times b|}$\n    - 判定两向量平行: $\\mathbf{a//b}\\Lrarr\\mathbf{a\\times b}=0$\n- 混合积\n  - 定义:称$\\mathbf{(a\\times b)\\cdot c}$为三和矢量$\\mathbf{a,b,c}$的混合积.本书将混合积记为$\\mathbf{(abc)}$,即$\\mathbf{(abc)}=\\mathbf{(a\\times b)\\cdot c}$,有的书将它记为$[\\mathbf{abc}]$.\n    设$\\mathbf a=\\{a_x,a_y,a_z\\},\\mathbf b=\\{b_x,b_y,b_z\\},\\mathbf c=\\{c_x,c_y,c_z\\},$则\n    $$\\mathbf{(abc)}=\\begin{vmatrix}\n        a_x&a_y&a_z\\\\\n        b_x&b_y&b_z\\\\\n        c_x&c_y&c_z\n    \\end{vmatrix}$$ \n  - 运算规律\n    - 轮换对称性: $\\mathbf{(abc)=(bca)=(cab)}$\n    - 两向量互换,混合积变号: $\\mathbf{(abc)=-(acb)=-(cba)=-(bac)}$\n  - 混合积在几何上的应用\n    求以$\\mathbf{a,b,c}$为棱的平行六面体体积:$V_{six}=|\\mathbf{(abc)}|$\n    判定三向量共面: $\\mathbf{a,b,c}$共面$\\Lrarr\\mathbf{(abc)}=0$\n### 例题\n{% asset_img 2019060416012625.png %}\n{% asset_img 2019060416033926.png %}\n{% asset_img 2019060416041027.png %}\n{% asset_img 2019060416042728.png %}\n### 参考\n1. [复习全书](https://book.douban.com/subject/6101482/)\n","tags":["高等数学"],"categories":["高等数学"]},{"title":"[高等数学]第5章 多元函数微分学","url":"%2Fposts%2F4c20fdbb%2F","content":"## 极值与最值\n### 无条件极值\n- 多元函数极值和极值点的定义\n  - 定义: 若存在$M_0(x_0,y_0)$点的某邻域$U_\\delta(M_0)$,使得$f(x,y)\\le f(x_0,y_0)$或$(f(x,y)\\ge f(x_0,y_0)),\\forall{(x,y)\\in{U_\\delta(M_0)}}$,则称$f(x,y)$在点$M_0(x_0,y_0)$取得极大值(极小值)$f(x_0,y_0)$,极大值与极小值统称为极值.点$M_0(x_0,y_0)$称为$f(x,y)$的极值点.\n- 多元函数驻点的定义\n  - 定义: 凡能使$f'_x(x,y)=0,f'_y(x,y)=0$同时成立的点$(x,y)$称为函数$f(x,y)$的驻点.==(驻点$\\nleftrightarrow$极值点,多元函数中,极值点与驻点没有任何关系)==\n- 多元函数取得极值的必要条件 ==(与一元函数极值必要条件相联系:点极值,导存在,点导0)==\n  - 定理: 设函数$f(x,y)$在点$M_0(x_0,y_0)$的一阶偏导数存在,且在$(x_0,y_0)$取得极值,则由此可见`具有一阶偏导数的函数的极值点一定是驻点,但驻点不一定是极值点.`\n- 二元函数取得极值的充分条件(下述定理仅适用于二元函数)\n  - 定理: 设函数$z=f(x,y)$在点$(x_0,y_0)$的某邻域内有连续的二阶偏导数,且$f'_x(x_0,y_0)=0,f'_y(x_0,y_0)=0$.令$f''_{xx}(x_0,y_0)=A,f''_{xy}(x_0,y_0)=B,f''_{yy}(x_0,y_0)=C$,则\n    (1).$AC-B^2>0$时,$f(x,y)$在点$(x_0,y_0)$取极值,且$\\begin{cases}\n        minimal\\ value& A<0\\\\\n        maximal\\ value& A>0\n    \\end{cases}$\n    (2).$AC-B^2<0$时,$f(x,y)$在点$(x_0,y_0)$无极值.==(内部取不到极值,看边界情况即条件极值)==\n    (3).$AC-B^2=0$时,不能确定$f(x,y)$在点$(x_0,y_0)$是否有极值,还需进一步讨论(一般用极值定义).==(用极大值,极小值定义)==\n### 条件极值(拉格朗日乘子法)\n- 函数$f(x,y)$在条件$\\varphi(x,y)=0$下的极值的必要条件\n  解决此类问题的一般方法是拉格朗日乘数法:\n  先构造拉格朗日函数$F(x,y,\\lambda)=f(x,y)+\\lambda\\varphi(x,y)$,然后解方程组\n  $$\\begin{cases}\n      \\frac{\\partial F}{\\partial x}=\\frac{\\partial f}{\\partial x}+\\lambda\\frac{\\partial\\varphi}{\\partial x}=0,\\\\\n      \\frac{\\partial F}{\\partial y}=\\frac{\\partial f}{\\partial y}+\\lambda\\frac{\\partial\\varphi}{\\partial y}=0,\\\\\n      \\frac{\\partial F}{\\partial\\lambda}=\\varphi(x,y)=0,\n  \\end{cases}$$\n  所有满足此方程的解$(x,y,\\lambda)$中$(x,y)$是函数$f(x,y)$在条件$\\varphi(x,y)=0$下的可能的极值点.\n- 函数$f(x,y,z)$在条件$\\varphi(x,y,z)=0,\\psi(x,y,z)=0$下的极值的必要条件\n  与上一条情况类似,构造拉格朗日函数\n  $$\\begin{aligned}\n      F(x,y,z,\\lambda,\\mu)=f(x,y,z)+\\lambda\\varphi(x,y,z)+\\mu\\psi(x,y,z),\n  \\end{aligned}$$\n  以下与上一条情况类似(略).\n- 拉格朗日乘子法的几何意义\n    举个2维的例子说明,假设有自变量$x$和$y$,给定约束条件$g(x,y)=c$,要求$f(x,y)$在约束条件$g(x,y)=c$下的极值.\n    我们可以画出$f(x,y)$的等高线图,如图.\n    {% asset_img 2019060417254130.png %}\n    此时,约束$g(x,y)=c$由于只有一个自由度,因此也是图中的一条曲线(红色).显然,当约束曲线$g(x,y)=c$与某一条等高线$f(x,y)=d_1$相切时,函数$f(x,y)$取得极值.两曲线相切等价于两曲线在切点处拥有共线的法向量.因此可得函数$f(x,y)$与$g(x,y)=c$在切点处的梯度成正比$\\lambda$,令$\\lambda>0$.假设在$(x_0,y_0)$处取得极值,即\n    $$\\begin{aligned}\n        \\nabla{f(x_0,y_0)}=-\\lambda\\nabla{g(x_0,y_0)}\\\\\n        f'_x(x_0,y_0)+\\lambda{g'_x(x_0,y_0)}=0\\\\\n        f'_y(x_0,y_0)+\\lambda{g'_y(x_0,y_0)}=0\n    \\end{aligned}$$\n    加上约束条件$g(x_0,y_0)=c$即构成了方程组.\n    问:为什么说相切才是最好的值？\n    解答:因为相切说明g在相切点的左右领域都是\"大值\"(或者\"小值\"),这就满足了极值的定义.如果不是相切,那么就是相交,相交的话,左右领域就存在\"大值\"和\"小值\",这不满足极值的定义.相离说明不满足约束条件,就不需要考虑了.\n### 扩展\n- 不等式约束优化\n  当约束加上不等式之后,情况变得更加复杂,首先来看一个简单的情况,给定如下不等式约束问题\n  $$\\begin{aligned}\n      &\\min_xf(x)\\\\\n      s.t.&\\quad g(x)\\le 0\n  \\end{aligned}$$\n  对应的Lagrangian与图形分别如下所示\n  $$\\begin{aligned}\n      L(x,\\lambda)=f(x)+\\lambda{g(x)}\n  \\end{aligned}$$\n  这时可行解必须落在约束区域$g(x)$之内,下图给出了目标函数的等高线与约束\n  {% asset_img 2019060418075931.png %}\n  由图可见可行解$x$只能在$g(x)<0$或者$g(x)=0$的区域里取得\n  - 当可行解$x$落在$g(x)<0$的区域内,此时直接极小化$f(x)$即可\n  - 当可行解$x$落在$g(x)=0$即边界上,此时等价于等式约束优化问题(用拉格朗日乘子法即可)\n  当约束区域包含目标函数原有的可行解时,此时加上约束,可行解仍落在约束区域内部,对应$g(x)<0$的情况,这时约束条件不起作用;\n  当约束区域不包含目标函数原有的可行解时,此时加上约束,可行解落在边界$g(x)=0$上.\n  下图分别描述了上面两种情况,右图表示加上约束,可行解会落在约束区域的边界上.\n  {% asset_img 2019060418131432.png %}\n  以上两种情况就是说,要么可行解落在约束边界上,即得$g(x)=0$,要么可行解落在约束区域内部,此时约束不起作用,另$\\lambda=0$消去约束即可,所以无论哪种情况都会得到\n  $$\\begin{aligned}\n      \\lambda{g(x)}=0\n  \\end{aligned}$$\n  还有一个问题是$\\lambda$的取值,在等式约束优化中,约束函数与目标函数的梯度只要满足平行即可,而在不等式约束中则不然,若$\\lambda\\ne 0$,这便说明可行解$x$是落在约束区域的边界上的,这时可行解应尽量靠近无约束时的解,所以在约束边界上,目标函数的负梯度方向应该远离约束区域朝向无约束时的解,此时正好可得约束函数的梯度方向与目标函数的负梯度方向应相同\n  $$\\begin{aligned}\n      -\\nabla_xf(x)=\\lambda\\nabla_xg(x)\n  \\end{aligned}$$\n  上式需要满足的要求是拉格朗日乘子$\\lambda>0$,这个问题可以举一个形象的例子,假设你去爬山,目标是山顶,但有一个障碍挡住了通向山顶的路,所以只能沿着障碍爬到尽可能靠近山顶的位置,然后望着山顶叹叹气,这里山顶便是目标函数的可行解,障碍便是约束函数的边界,此时的梯度方向一定是指向山顶的,与障碍的梯度同向,如下图\n  {% asset_img 2019060418214333.png %}\n  可见,`对于不等式约束,只要满足一定的条件,依然可以使用拉格朗日乘子法解决,这里的条件便是KKT条件`.\n  接下来给出形式化的KKT条件,首先给出形式化的不等式约束优化问题\n  $$\\begin{aligned}\n      &\\min_xf(x)\\\\\n      s.t.\\quad &h_i(x)=0,\\quad i=1,2,\\cdots,m\\\\\n      &g_j(x)\\le 0,\\quad j=1,2,\\cdots,n\n  \\end{aligned}$$\n  列出Lagrangian得到无约束优化问题\n  $$\\begin{aligned}\n      L(x,\\alpha,\\beta)=f(x)+\\sum_{i=1}^m{\\alpha_ih_i(x)}+\\sum_{j=1}^n\\beta_jg_j(x)\n  \\end{aligned}$$\n  经过之前的分析,便得知加上不等式约束后,可行解$x$需要满足的就是以下$KKT$条件:\n  $$\\begin{aligned}\n      \\nabla_xL(x,\\alpha,\\beta)&=0\\quad\\quad&(1)\\\\\n      \\beta_jg_j(x)&=0,\\quad j=1,2,\\cdots,n&(2)\\\\\n      h_i(x)&=0,\\quad i=1,2,\\cdots,m&(3)\\\\\n      g_j(x)&\\le 0,\\quad j=1,2,\\cdots,n&(4)\\\\\n      \\beta_j&\\ge 0,\\quad j=1,2,\\cdots,n&(5)\n  \\end{aligned}$$\n  满足KKT条件后极小化Lagrangian即可得到在不等式约束条件下的可行解. KKT条件看起来很多,其实很好理解:\n  (1): 拉格朗日取得可行解的必要条件\n  (2): 称为松弛互补条件\n  (3)~(4): 初始的约束条件\n  (5): 不等式约束的Lagrange Multiplier需要满足的条件\n  主要的KKT条件便是(3)和(5),只要满足这两个条件便可直接使用拉格朗日乘子法\n## 方向导数和梯度\n### 方向导数\n偏导数反映的是函数沿坐标轴方向的`变化率`.但许多物理现象告诉我们,只考虑函数沿坐标轴方向的变化率是不够的.例如,热空气要向冷的地方流动,气象学中就要确定大气温度,气压沿着某些方向的变化率.因此我们有必要来讨论函数沿任一指定方向的变化率问题.\n设$l$是$xOy$平面上以$P_0(x_0,y_0)$为始点的一条射线,$\\mathbf e_l=(\\cos\\alpha,\\cos\\beta)$是与$l$同方向的单位向量(图9-9).\n{% asset_img 2019060410134116.png %}\n射线$l$的参数方程为\n$$\\begin{aligned}\n    x&=x_0+t\\cos\\alpha,(t\\ge 0)\\\\\n    y&=y_0+t\\cos\\beta\n\\end{aligned}$$\n设函数$z=f(x,y)$在点$P_0(x_0,y_0)$的某个邻域$U(P_0)$内有定义,$P(x_0+t\\cos\\alpha,y_0+t\\cos\\beta)$为$l$上另一点,且$P\\in{U(P_0)}$.如果函数增量$f(x_0+t\\cos\\alpha,y_0+t\\cos\\beta)-f(x_0,y_0)$与$P$到$P_0$的距离$|PP_0|=t$的比值\n$$\\begin{aligned}\n    \\frac{f(x_0+t\\cos\\alpha,y_0+t\\cos\\beta)-f(x_0,y_0)}{t}\n\\end{aligned}$$\n当$P$沿着$l$趋于$P_0$(即$t\\to 0^+)$时的极限存在,则称此极限为函数$f(x,y)$在点$P_0$沿方向$l$的`方向导数`,记作$\\frac{\\partial{f}}{\\partial{l}}\\vert_{(x_0,y_0)}$,即\n$$\\begin{aligned}\n    \\frac{\\partial f}{\\partial l}\\vert_{(x_0,y_0)}=\\lim_{t\\to{0^+}}\\frac{f(x_0+t\\cos\\alpha,y_0+t\\cos\\beta)-f(x_0,y_0)}{t}\\tag{1}\n\\end{aligned}$$\n从方向导数的定义可知,方向导数$\\frac{\\partial f}{\\partial l}\\vert_{(x_0,y_0)}$就是函数$f(x,y)$在点$P_0(x_0,y_0)$处沿方向$l$的变化率.若函数$f(x,y)$在点$P_0(x_0,y_0)$的偏导数存在,$\\mathbf e_l=\\mathbf i=(1,0)$,则\n$$\\begin{aligned}\n    \\frac{\\partial f}{\\partial l}\\vert_{(x_0,y_0)}=\\lim_{t\\to{0^+}}\\frac{f(x_0+t,y_0)-f(x_0,y_0)}{t}=f_x(x_0,y_0)\n\\end{aligned}$$\n又若$\\mathbf e_l=\\mathbf j=(0,1)$,则\n$$\\begin{aligned}\n    \\frac{\\partial f}{\\partial l}\\vert_{(x_0,y_0)}=\\lim_{t\\to{0^+}}\\frac{f(x_0,y_0+t)-f(x_0,y_0)}{t}=f_y(x_0,y_0)\n\\end{aligned}$$\n但反之,若$\\mathbf e_l=\\mathbf i,\\frac{\\partial z}{\\partial l}\\vert_{(x_0,y_0)}$存在,则$\\frac{\\partial z}{\\partial x}\\vert_{(x_0,y_0)}$未必存在.例如,$z=\\sqrt{x^2+y^2}$在点$O(0,0)$处沿$l=\\mathbf i$方向的方向导数$\\frac{\\partial z}{\\partial l}\\vert_{(0,0)}=1$,而偏导数$\\frac{\\partial z}{\\partial x}\\vert_{(0,0)}$不存在.==(方向导数存在,偏导数未必存在)==\n关于方向导数的存在及计算,我们有以下定理.\n- <b>定理:如果函数$f(x,y)$在点$P_0(x_0,y_0)$可微分,那么函数在该点沿任一方向$l$的方向导数存在,且有</b>\n  $$\\begin{aligned}\n    \\frac{\\partial f}{\\partial l}\\vert_{(x_0,y_0)}=f_x(x_0,y_0)\\cos\\alpha+f_y(x_0,y_0)\\cos\\beta\\tag{2}\n  \\end{aligned}$$\n<b>其中$\\cos\\alpha,\\cos\\beta$是方向余弦.</b>\n> 证: 由假设,$f(x,y)$在点$(x_0,y_0)$可微分,故有\n> $$\\begin{aligned}\n    &f(x_0+\\Delta{x},y_0+\\Delta{y})-f(x_0,y_0)\\\\\n    =&f_x(x_0,y_0)\\Delta{x}+f_y(x_0,y_0)\\Delta{y}+o(\\sqrt{(\\Delta{x})^2+(\\Delta{y})^2})\n\\end{aligned}$$\n> 但点$(x_0+\\Delta{x},y_0+\\Delta{y})$在以$(x_0,y_0)$为始点的射线$l$上时,应有$\\Delta{x}=t\\cos\\alpha,\\Delta{y}=t\\cos\\beta,\\sqrt{(\\Delta{x})^2+(\\Delta{y})^2}=t$.所以\n> $$\\begin{aligned}\n    &\\lim_{t\\to{0^+}}\\frac{f(x_0+t\\cos\\alpha,y_0+t\\cos\\beta)-f(x_0,y_0)}{t}\\\\\n    =&f_x(x_0,y_0)\\cos\\alpha+f_y(x_0,y_0)\\cos\\beta\n\\end{aligned}$$\n> 这就证明了方向导数的存在,且其值为\n> $$\\begin{aligned}\n    \\frac{\\partial f}{\\partial l}\\vert_{(x_0,y_0)}=f_x(x_0,y_0)\\cos\\alpha+f_y(x_0,y_0)\\cos\\beta\n\\end{aligned}$$\n{% asset_img 2019060411312219.png %}\n{% asset_img 2019060411314420.png %}\n### 梯度\n与方向导数有关联的一个概念是函数的梯度.在二元函数的情形,设函数$f(x,y)$在平面区域$D$内具有一阶连续偏导数,则对于每一点$P_0(x_0,y_0)\\in{D}$,都可定出一个向量\n$$\\begin{aligned}\n    f_x(x_0,y_0)\\mathbf i+f_y(x_0,y_0)\\mathbf j\n\\end{aligned}$$\n这向量称为函数$f(x,y)$在点$P_0(x_0,y_0)$的梯度,记作$\\mathbf{grad}f(x_0,y_0)$或$\\nabla{f(x_0,y_0)}$,即\n$$\\begin{aligned}\n    \\mathbf{grad}f(x_0,y_0)=\\nabla{f(x_0,y_0)}=f_x(x_0,y_0)\\mathbf i+f_y(x_0,y_0)\\mathbf j\n\\end{aligned}$$\n其中$\\nabla=\\frac{\\partial}{\\partial x}\\mathbf i+\\frac{\\partial}{\\partial y}\\mathbf j$称为(二维的)向量微分算子或Nabla算子,$\\nabla{f}=\\frac{\\partial f}{\\partial x}\\mathbf i+\\frac{\\partial f}{\\partial y}\\mathbf j$.\n如果函数$f(x,y)$在点$P_0(x_0,y_0)$可微分,$\\mathbf e_l=(\\cos\\alpha,\\cos\\beta)$是与方向$l$同向的单位向量,则\n$$\\begin{aligned}\n    \\frac{\\partial f}{\\partial l}\\vert_{(x_0,y_0)}&=f_x(x_0,y_0)\\cos\\alpha+f_y(x_0,y_0)\\cos\\beta\\\\\n    &=\\mathbf{grad}f(x_0,y_0)\\cdot\\mathbf e_l=\\vert\\mathbf{grad}f(x_0,y_0)\\vert\\cos\\theta\n\\end{aligned}$$\n其中$\\theta=(\\mathbf{grad}\\widehat{f(x_0,y_0),\\mathbf e_l})$.\n这一关系式表明了函数在一点的梯度与函数在这点的方向导数间的关系.\n特别,由这关系可知:\n(1).当$\\theta=0$,即方向$\\mathbf e_l$与梯度$\\mathbf{grad}f(x_0,y_0)$的方向相同时,函数$f(x,y)$增加最快.此时,函数在这个方向的方向导数达到最大值,这个最大值就是梯度$\\mathbf{grad}f(x_0,y_0)$的模,即\n$$\\begin{aligned}\n    \\frac{\\partial f}{\\partial l}\\vert_{(x_0,y_0)}=\\vert\\mathbf{grad}f(x_0,y_0)\\vert\n\\end{aligned}$$\n这个结果也表示:函数$f(x,y)$在一点的梯度$\\mathbf{grad}f$是这样一个向量,它的方向是函数在这点的方向导数取得最大值的方向,它的模就等于方向导数的最大值.\n(2).当$\\theta=\\pi$,即方向$\\mathbf e_l$与梯度$\\mathbf{grad}f(x_0,y_0)$的方向相反时,函数$f(x,y)$减少最快,函数在这个方向的方向导数达到最小值,即\n$$\\begin{aligned}\n    \\frac{\\partial f}{\\partial l}\\vert_{(x_0,y_0)}=-\\vert\\mathbf{grad}f(x_0,y_0)\\vert\n\\end{aligned}$$\n(3).当$\\theta=\\frac{\\pi}{2}$,即方向$\\mathbf e_l$与梯度$\\mathbf{grad}f(x_0,y_0)$的方向正交时,函数的变化率为零,即\n$$\\begin{aligned}\n    \\frac{\\partial f}{\\partial l}\\vert_{(x_0,y_0)}=\\vert\\mathbf{grad}f(x_0,y_0)\\vert\\cos\\theta=0\n\\end{aligned}$$\n我们知道,一般说来二元函数$z=f(x,y)$在几何上表示一个曲面,这曲面被平面$z=c$(c是常数)所截得的曲线$L$的方程为\n$$\\begin{cases}\n    z=f(x,y)\\\\\n    z=c\n\\end{cases}$$\n这条曲线$L$在$xOy$面上的投影是一条平面曲线$L^*$(图9-10),\n{% asset_img 2019060410451818.png %}\n它在$xOy$平面直角坐标系中的方程为\n$$\\begin{aligned}\n    f(x,y)=c\n\\end{aligned}$$\n对于曲线$L^*$上的一切点,已给函数的函数值都是$c$,所以我们称平面曲线$L^*$为函数$z=f(x,y)$的`等值线`.\n若$f_x,f_y$不同时为零,则等值线$f(x,y)=c$上任一点$P_0(x_0,y_0)$处的一个单位法向量为\n$$\\begin{aligned}\n    \\mathbf n&=\\frac{1}{\\sqrt{f_x^2(x_0,y_0)+f_y^2(x_0,y_0)}}(f_x(x_0,y_0),f_y(x_0,y_0))\\\\\n    &=\\frac{\\nabla f(x_0,y_0)}{\\vert\\nabla{f(x_0,y_0)}\\vert}\n\\end{aligned}$$\n这表明函数$f(x,y)$在一点$(x_0,y_0)$的梯度$\\nabla f(x_0,y_0)$的方向就是等值线$f(x,y)=c$在这点的法线方向$\\mathbf n$,而梯度的模$\\vert\\nabla{f(x_0,y_0)}\\vert$就是沿这个法线方向的方向导数$\\frac{\\partial f}{\\partial n}$,于是有\n$$\\begin{aligned}\n    \\nabla{f(x_0,y_0)}=\\frac{\\partial f}{\\partial n}\\mathbf n\n\\end{aligned}$$\n上面讨论的梯度概念可以类似地推广到三元函数的情形.设函数$f(x,y,z)$在空间区域$G$内具有一阶连续偏导数,则对于每一点$P_0(x_0,y_0,z_0)\\in{G}$,都可定出一个向量\n$$\\begin{aligned}\n    f_x(x_0,y_0,z_0)\\mathbf{i}+f_y(x_0,y_0,z_0)\\mathbf{j}+f_z(x_0,y_0,z_0)\\mathbf{k}\n\\end{aligned}$$\n这向量称为函数$f(x,y,z)$在点$P_0(x_0,y_0,z_0)$的`梯度`,将它记作$\\mathbf{grad}f(x_0,y_0,z_0)$或$\\nabla{f(x_0,y_0,z_0)}$,即\n$$\\begin{aligned}\n    &\\mathbf{grad}f(x_0,y_0,z_0)\\\\\n    =&\\nabla{f(x_0,y_0,z_0)}\\\\\n    =&f_x(x_0,y_0,z_0)\\mathbf{i}+f_y(x_0,y_0,z_0)\\mathbf{j}+f_z(x_0,y_0,z_0)\\mathbf{k}\n\\end{aligned}$$\n其中$\\nabla=\\frac{\\partial}{\\partial x}\\mathbf{i}+\\frac{\\partial}{\\partial y}\\mathbf{j}+\\frac{\\partial}{\\partial z}\\mathbf{k}$称为(三维的)`向量微分算子`或`Nabla算子`,$\\nabla{f=\\frac{\\partial f}{\\partial x}\\mathbf{i}+\\frac{\\partial f}{\\partial y}\\mathbf{j}+\\frac{\\partial f}{\\partial z}\\mathbf{k}}$.\n经过与二元函数的情形完全类似的讨论可知,三元函数$f(x,y,z)$在一点的梯度$\\nabla{f}$是这样一个向量,它的方向是函数$f(x,y,z)$在这点的方向导数取得最大值的方向,它的模就等于方向导数的最大值.\n如果我们引进曲面\n$$\\begin{aligned}\n    f(x,y,z)=c\n\\end{aligned}$$\n为函数$f(x,y,z)$的`等值面`的概念,则可得函数$f(x,y,z)$在一点$(x_0,y_0,z_0)$的梯度$\\nabla{f(x_0,y_0,z_0)}$的方向就是等值面$f(x,y,z)=c$在这点的法线方向$\\mathbf n$,而梯度的模$\\vert\\nabla{f(x_0,y_0,z_0)}\\vert$就是函数沿这个法线方向的方向导数$\\frac{\\partial f}{\\partial n}$.\n{% asset_img 2019060411330421.png %}\n{% asset_img 2019060411333722.png %}\n\n## 曲线的切平面与法线\n曲面方程常见的两种形式:$F(x,y,z)=0$或$z=f(x,y)$\n- (1).设曲面$\\Sigma$的方程为$F(x,y,z)=0$,并设点$(x_0,y_0,z_0)\\in\\Sigma$,三个偏导数$F'_x(x_0,y_0,z_0),F'_y(x_0,y_0,z_0),F'_z(x_0,y_0,z_0)$不同时为零(以后凡讲到法向量时均如此假定).则该曲面在点$(x_0,y_0,z_0)$处的法向量为\n    $$\\begin{aligned}\n    \\mathbf n=\\{F'_x(x_0,y_0,z_0),F'_y(x_0,y_0,z_0),F'_z(x_0,y_0,z_0)\\}\n    \\end{aligned}$$\n    过点$(x_0,y_0,z_0)$的切平面和法线方程分别为\n    $$\\begin{aligned}\n    F'_x(x_0,y_0,z_0)(x-x_0)+F'_y(x_0,y_0,z_0)(y-y_0)+F'_z(x_0,y_0,z_0)(z-z_0)=0\\\\\n    \\frac{x-x_0}{F'_x(x_0,y_0,z_0)}=\\frac{y-y_0}{F'_y(x_0,y_0,z_0)}=\\frac{z-z_0}{F'_z(x_0,y_0,z_0)}\n    \\end{aligned}$$\n- (2).设曲面$\\Sigma$的方程为$z=f(x,y)$,则该曲面方程可改写为$f(x,y)-z=0$\n    设$f(x,y)$在$(x_0,y_0)$处可导,由$(1)$知,该曲面在点$(x_0,y_0,z_0)$处的法向量为$\\mathbf n=\\{f'_x(x_0,y_0),f'_y(x_0,y_0),-1\\}$.\n    切平面方程和法线方程与$(1)$类似(略).\n### 扩展(超平面)\n- 超平面\n    在样本空间中,划分超平面可通过如下线性方程来描述:\n    $$\\begin{aligned}\n        \\mathbf{\\omega^Tx}+b=0\n    \\end{aligned}$$\n    其中$\\mathbf \\omega=(\\omega_1;\\omega_2;...;\\omega_d)$为**法向量**,决定了超平面的方向;$b$为**位移项**,决定了超平面与原点之间的距离.显然,划分超平面可被法向量$\\mathbf\\omega$和位移$b$确定,下面我们将其记为$(\\mathbf\\omega,b)$.样本空间中任意点$\\mathbf x$到超平面$(\\mathbf\\omega,b)$的距离可写为\n    $$\\begin{aligned}\n        r=\\frac{|\\mathbf{\\omega^Tx}+b|}{||\\mathbf\\omega||}\n    \\end{aligned}$$\n### 参考\n1. [复习全书](https://book.douban.com/subject/6101482/)\n2. [同济 高等数学 第六版](https://book.douban.com/subject/2195654/)\n3. [如何理解拉格朗日乘子法？ - 卢健龙的回答 - 知乎](https://www.zhihu.com/question/38586401/answer/105273125)\n4. [约束优化方法之拉格朗日乘子法与KKT条件 - ooon - 博客园](https://www.cnblogs.com/ooon/p/5721119.html)\n","tags":["高等数学"],"categories":["高等数学"]},{"title":"[leetcode]37.解数独","url":"%2Fposts%2F9eebc96a%2F","content":"{% asset_img 201906032135188.png %}\n\n### 解题思路(约束法+回溯法) \n\n**约束法**:基本的意思是在放置每个数字时都设置约束。在数独上放置一个数字后立即 排除当前**行,列和子方块**对该数字的使用。这会传播 约束条件 并有利于减少需要考虑组合的个数。\n{% asset_img 201906032135189.png %}\n\n**回溯法**:根据已有的约束数组来判断当前可以放什么元素,然后不断尝试(1~9),如果找到可以放的元素,则将其记录到栈,如果当前(1~9)都不能放,则进行回溯(即倒退),将栈顶元素出栈,在上一步进行新的尝试.\n{% asset_img 201906032135190.png %}\n\n**枚举子方块**:使用`方块索引= (行 / 3) * 3 + 列 / 3 其中 / 表示整数除法`。\n{% asset_img 201906032135191.png %}\n\n### 代码实现 \n```cpp\n// 行约束,列约束,块约束\nint row_map[9][10] = { 0 }, col_map[9][10] = { 0 }, box_map[9][10] = { 0 };\n// 增加约束\nvoid constrain(int i, int j, int v)\n{\n    row_map[i][v]++;\n    col_map[j][v]++;\n    box_map[i / 3 * 3 + j / 3][v]++;\n}\n// 取消约束\nvoid deconstrain(int i, int j, int v)\n{\n    row_map[i][v]--;\n    col_map[j][v]--;\n    box_map[i / 3 * 3 + j / 3][v]--;\n}\n// 计算数独\nvoid solveSudoku(vector<vector<char>>& board) {\n    int m = board.size(), n = board[0].size();\n    stack<pair<int,int>> stk;\n    int i = 0, j = 0;\n    // 通过已知的值,计算已知的约束,避免多做无用功\n    for (i = 0; i < m; ++i)\n    {\n        for (j = 0; j < n; ++j)\n        {\n            if (board[i][j] != '.')\n                constrain(i, j, board[i][j] - '0');\n        }\n    }\n    // 计算未知的值\n    for (i=0,j=0;i < m;)\n    {\n        if (board[i][j] == '.')\n        {\n            board[i][j] = '0';\n            stk.push({ i * 9 + j,board[i][j] });\n            int flag = 0;\n            // 找到下一个可用值,没有的话则回溯\n            while (!stk.empty())\n            {\n                pair<int, int> e = stk.top();\n                stk.pop();\n                i = e.first / 9;\n                j = e.first % 9;\n                int v = e.second-'0';\n                flag = 0;\n                deconstrain(i, j, v);\n                for (v = v + 1; v <= 9; ++v)\n                {\n                    if (row_map[i][v] || col_map[j][v] || box_map[i / 3 * 3 + j / 3][v])\n                        continue;\n                    board[i][j] = v + '0';\n                    constrain(i, j, v);\n                    stk.push({ i * 9 + j,board[i][j] });\n                    flag = 1;\n                    break;\n                }\n                if (flag == 1)\n                    break;\n                board[i][j] = '.';\n            }\n        }\n        j++;\n        i = j / 9 ? i + 1 : i;\n        j = j / 9 ? j % 9 : j;\n    }\n}\n```\n\n### 参考\n1. [力扣（LeetCode）37. 解数独](https://leetcode-cn.com/problems/two-sum/solution/jie-shu-du-by-leetcode/)","tags":["数独"],"categories":["OJ"]},{"title":"[matplotlib]notes","url":"%2Fposts%2Fbbaeb0e0%2F","content":"### Matplotlib\n#### pyplot\n- **matplotlib.pyplot.figure(num=None, figsize=None, dpi=None, facecolor=None, edgecolor=None, frameon=True, FigureClass=<class 'matplotlib.figure.Figure'>, clear=False, \\*\\*kwargs)**:创建一个figure\n  > **num**:可以选数字(类似于标记)或字符串(作为标题),默认None\n  > **figsize**:(float,float),宽高inches,默认rcParams[\"figure.figsize\"]=[6.4,4.8]\n  > **dpi**:分辨率,默认rcParams[\"figure.dpi\"]=100\n  > **facecolor**:背景色,默认rcParams[\"figure.facecolor\"]='w'\n  > **edgecolor**:边框色,默认rcParams[\"figure.edgecolor\"]='w'\n  \n- **matplotlib.pyplot.gca(\\*\\*kwargs)**:在当前figure中根据关键字获取或创建坐标轴\n- **matplotlib.pyplot.imshow(X, cmap=None, norm=None, aspect=None, interpolation=None, alpha=None, vmin=None, vmax=None, origin=None, extent=None, shape=<deprecated parameter>, filternorm=1, filterrad=4.0, imlim=<deprecated parameter>, resample=None, url=None, *, data=None, **kwargs)**: 显示一幅图片,比如2D常规光栅\n  {% asset_img 2019070117075884.png %}\n- **matplotlib.pyplot.hist(x, bins=None, range=None, density=None, weights=None, cumulative=False, bottom=None, histtype='bar', align='mid', orientation='vertical', rwidth=None, log=False, color=None, label=None, stacked=False, normed=None, *, data=None, \\*\\*kwargs)**: 创建一个直方图\n  > **x**: (n,)数组或数组序列,\n\n\n\n\n#### RcParams(*args,**kwargs):字典结构,存储rc参数\n#### markers\n{% asset_img 2019062608512758.png %}\n#### Solutions\n- **中文乱码**\n  {% asset_img 2019070316225088.png %}\n","tags":["notes"],"categories":["matplotlib"]},{"title":"[leetcode]303.区域和检索-数组不可变","url":"%2Fposts%2F6c5b9a98%2F","content":"{% asset_img 201906021606251.png %}\n\n### 解题思路 \n`思路一:DP`\ndp[i]表示从1~i的累加和,那么要求从i~j(包含端点)累加和即为\n$$\\begin{aligned}\n    sum_{i,j}=dp[j]-dp[i-1]\n\\end{aligned}$$\n`思路二:树状数组`\n实际树状数组的优势在于查询和修改都是$O(logn)$,本题只是为了熟悉一下这个数据结构,详细请参考[树状数组彻底入门](https://blog.csdn.net/Small_Orange_glory/article/details/81290634)\n核心就是以下两个公式:\n$$\\begin{aligned}\n    C[i]=A[i-lowbit(i)+1]+A[i-lowbit(i)+2]...+A[i]\\tag{1}\n\\end{aligned}$$\n```cpp\nfor(int i=1;i<=m;++i)\n    for(int j=i-lowbit(i)+1;j<=i;++j)\n        C[j]+=A[j];\n```\n\n$$\\begin{aligned}\n    sum[15]&=sum[1111]=C[1111]+C[1110]+C[1100]+C[1000]\\\\\n    &=C[15]+C[14]+C[12]+C[8]\\tag{2}\n\\end{aligned}$$\n```cpp\nsum=0;\nfor(i=x;i>0;i-=lowbit(i))\n    sum+=C[i];\n```\n\n### 代码实现 \n`思路一:DP`\n```cpp\n    vector<int> dp;\n    NumArray(vector<int>& nums) {\n        int m=nums.size();\n        dp.resize(m,0);\n        for(int i=0;i<m;++i)\n            dp[i]=dp[i-1>=0?i-1:0]+nums[i];\n    }\n    \n    int sumRange(int i, int j) {\n        int x=min(i,j),y=max(i,j);\n        return dp[y]-(x-1>=0?dp[x-1]:0);\n    }\n```\n时间复杂度为$O(n)$\n\n`思路二:树状数组`\n```cpp\n    vector<int> c;\n    NumArray(vector<int>& nums) {//树状数组,查询/更新 o(logn)\n        c = vector<int>(nums.size() + 1,0);\n        for(int i = 1;i <= nums.size();i++)\n            for(int j = i-lowbit(i)+1; j<=i; ++j)\n                c[i]+=nums[j-1];\n    }\n    int lowbit(int x){\n        return x & -x;\n    }\n    int sum(int i){\n        int res = 0;\n        for(; i > 0;i -= lowbit(i)) res += c[i];\n        return res;\n    }\n    int sumRange(int i, int j) {\n        return sum(j + 1) - sum(i);\n    }\n```\n### 参考\n1. [树状数组彻底入门](https://blog.csdn.net/Small_Orange_glory/article/details/81290634)","tags":["树状数组"],"categories":["OJ"]},{"title":"[高等数学]第1章 函数,极限,连续","url":"%2Fposts%2Fa86313a0%2F","content":"### 1.函数\n- 定义1.1.1(邻域):设$\\delta\\gt 0$,实数集$U_\\delta(x_0)=\\{x||x-x_0|<\\delta\\}$称为$x_0$的$\\delta$邻域,如果不必说及邻域半径$\\delta$的大小,则简记为$U(x_0)$,称为$x_0$的某邻域.\n  $\\mathring{U}_\\delta(x_0)=\\{x|0\\lt|x-x_0|\\lt\\delta\\}$称为$x_0$的去心$\\delta$邻域,类似地有记号$\\mathring{U}(x_0)$及相应的名称.\n- 定义1.1.3(隐函数):设$x$在某数集$X$内每取一个值时,由方程$F(x,y)=0$可唯一\n###\n### 参考\n1. [复习全书](https://book.douban.com/subject/6101482/)\n","tags":["高等数学"],"categories":["高等数学"]},{"title":"[概率与统计]第6章 数理统计的基本概念","url":"%2Fposts%2F4eb1bf74%2F","content":"### 一.总体,样本,统计量和样本数字特征\n#### 1.总体\n- 定义:数理统计中所研究对象的某项数量指标$X$的全体称为总体.\n> 注:$X$是一个随机变量,称$X$的概率分布为总体分布,$X$的数字特征为总体数字特征,总体中的每个元素称为个体.\n\n#### 2.样本\n- 定义:如果$X_1,X_2,\\cdots,X_n$相互独立且都与总体$X$同分布,则称$X_1,X_2,\\cdots,X_n$为来自总体的简单随机样本,简称为样本.$n$为样本容量,样本的具体观测值$x_1,x_2,\\cdots,x_n$称为样本值,或称总体$X$的$n$个独立观测值.\n> 注:如果总体$X$的分布为$F(x)$,则样本$X_1,X_2,\\cdots,X_n$的分布为\n> $$\\begin{aligned}\n    F_n(x_1,x_2,\\cdots,x_n)=\\prod_{i=1}^n{F(x_i)}.\n\\end{aligned}$$\n> 如果总体$X$有概率密度$f(x)$,则样本$X_1,X_2,\\cdots,X_n$的概率密度为\n> $$\\begin{aligned}\n    f_n(x_1,x_2,\\cdots,x_n)=\\prod_{i=1}^n{f(x_i)}.\n\\end{aligned}$$\n> 如果总体$X$有概率分布$P\\{X=a_j\\}=p_j,j=1,2,\\cdots,$则样本$X_1,X_2,\\cdots,X_n$的概率分布为\n> $$\\begin{aligned}\n    P\\{X_1=x_1,X_2=x_2,\\cdots,X_n=x_n\\}=\\prod_{i=1}^n{P{X_i=x_i}},\n\\end{aligned}$$\n> 其中$x_i$取$a_1,a_2,\\cdots$中的某一个数.\n#### 3.统计量\n- 定义:样本$X_1,X_2,\\cdots,X_n$的不含未知参数的函数$T=T(X_1,X_2,\\cdots,X_n)$称为统计量.\n  > 注:作为随机样本的函数,统计量本身也是一个随机变量.\n  如果$x_1,x_2,\\cdots,x_n$是样本$X_1,X_2,\\cdots,X_n$的样本值,则数值$T(x_1,x_2,\\cdots,x_n)$为统计量$T(X_1,X_2,\\cdots,X_n)$的观测值.\n#### 4.样本数字特征\n设$X_1,X_2,\\cdots,X_n$是来自总体$X$的样本,则称\n(1) 样本均值$\\overline{X}=\\frac{1}{n}\\sum_{i=1}^n{X_i}$\n(2) 样本方差$S^2=\\frac{1}{n-1}\\sum_{i=1}^n{(X_i-\\overline{X})^2}$,样本标准差$S=\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n{(X_i-\\overline{X})^2}}$\n(3) 样本$k$阶原点矩$A_k=\\frac{1}{n}\\sum_{i=1}^n{X_i^k},k=1,2,A_1=\\overline{X}$\n(4) 样本$k$阶中心矩$B_k=\\frac{1}{n}\\sum_{i=1}^n{(X_i-\\overline{X})^k},k=1,2,B_2=\\frac{n-1}{n}S^2\\neq S^2$\n> 补充:总体期望$E(X)$,总体均值$\\mu$,总体方差$D(X)$,总体标准差$\\sigma$\n\n#### 5.样本数字特征的性质\n(1) 如果总体$X$具有数学期望$E(X)=\\mu,$则\n$$\\begin{aligned}\n    E(\\overline{X})=E(X)=\\mu\n\\end{aligned}$$\n> 证:$E(\\overline{X})=\\mu$\n> $$\\begin{aligned}\n    E(\\overline{X})=E(\\frac{1}{n}\\sum_{i=1}^n{X_i})=\\frac{1}{n}\\sum_{i=1}^nE(X_i)=\\frac{1}{n}\\cdot n\\mu=\\mu\n\\end{aligned}$$\n\n(2) 如果总体$X$具有方差$D(X)=\\sigma^2$,则\n$$\\begin{aligned}\n    D(\\overline{X})&=\\frac{1}{n}D(X)=\\frac{\\sigma^2}{n}\\\\\n    E(S^2)&=D(X)=\\sigma^2\n\\end{aligned}$$\n\n> 证:$D(\\overline{X})=\\frac{\\sigma^2}{n}$\n> $$\\begin{aligned}\n    D(\\overline{X})=D(\\frac{1}{n}\\sum_{i=1}^n{X_i})=\\frac{1}{n^2}\\sum_{i=1}^n{D(X_i)}=\\frac{1}{n^2}\\cdot n\\sigma^2=\\frac{\\sigma^2}{n}\n\\end{aligned}$$\n> 证:$E(S^2)=\\sigma^2$\n> $$\\begin{aligned}\n    E(S^2)&=E\\left[\\frac{1}{n-1}\\sum_{i=1}^n{(X_i-\\overline{X})^2}\\right]=\\frac{1}{n-1}E(\\sum_{i=1}^n{X_i^2-n\\overline{X}^2})\\\\\n    &=\\frac{1}{n-1}\\left[\\sum_{i=1}^n{E(X_i^2)}-nE(\\overline{X}^2)\\right]\\\\\n    &=\\frac{1}{n-1}\\left[\\sum_{i=1}^n(\\sigma^2+\\mu^2)-n(\\frac{\\sigma^2}{n}+\\mu^2)\\right]\\\\\n    &=\\frac{1}{n-1}(n\\sigma^2+n\\mu^2-\\sigma^2-n\\mu^2)=\\sigma^2\n\\end{aligned}$$\n\n(3) 如果总体$X$的$k$阶原点矩$E(X^k)=\\mu_k,k=1,2,\\cdots$存在,则当$n\\to\\infty$时\n$$\\begin{aligned}\n    \\frac{1}{n}\\sum_{i=1}^n{X_i^k}\\xrightarrow{P}\\mu_k,\\ k=1,2,\\cdots\n\\end{aligned}$$\n\n补充:\n(1). $\\sum_{i=1}^n(X_i-\\overline{X})^2=\\sum_{i=1}^n{X_i}^2-n\\overline{X}^2$\n(2). $\\sum_{i=1}^n(X_i-\\mu)^2=\\sum_{i=1}^n(X_i-\\overline{X})^2+n(\\overline{X}-\\mu)^2$\n\n> 证(1):\n> $$\\begin{aligned}\n    \\sum_{i=1}^n(X_i-\\overline{X})^2&=\\sum_{i=1}^n(X_i^2-2X_i\\overline{X}+\\overline{X}^2)=\\sum_{i=1}^nX_i^2-2n\\overline{X}^2+n\\overline{X}^2\\\\\n    &=\\sum_{i=1}^n{X_i^2-n\\overline{X}^2}\n\\end{aligned}$$\n> 证(2): \n> $$\\begin{aligned}\n    \\sum_{i=1}^n(X_i-\\mu)^2&=\\sum_{i=1}^n[(X_i-\\overline{X})+(\\overline{X}-\\mu)]^2\\\\\n    &=\\sum_{i=1}^n(X_i-\\overline{X})^2+2\\sum_{i=1}^n(X_i-\\overline{X})(\\overline{X}-\\mu)+\\sum_{i=1}^n(\\overline{X}-\\mu)^2\\\\\n    &=\\sum_{i=1}^n(X_i-\\overline{X})^2+2\\sum_{i=1}^n(X_i\\overline{X}-\\mu{X_i}-\\overline{X}^2+\\mu\\overline{X})+\\sum_{i=1}^n{(\\overline{X}-\\mu)^2}\\\\\n    &=\\sum_{i=1}^n(X_i-\\overline{X})^2+2(n\\overline{X}^2-n\\mu\\overline{X}-n\\overline{X}^2+n\\mu\\overline{X}^2)+\\sum_{i=1}^n{(\\overline{X}-\\mu)^2}\n\\end{aligned}$$\n\n### 参考\n1. [复习全书](https://book.douban.com/subject/6101482/)\n","tags":["概率与统计"],"categories":["概率与统计"]},{"title":"[概率与统计]第5章 大数定律和中心极限定理","url":"%2Fposts%2F19f6d8b2%2F","content":"\n### 切比雪夫不等式\n- 定义:设随机变量$X$的数学期望$E(X)$和方差$D(X)$存在,则对任意$\\epsilon\\gt 0$,总有\n$$\\begin{aligned}\n    P\\{\\vert X-E(X)\\vert \\ge \\epsilon\\}\\le \\frac{D(X)}{\\epsilon^2}\n\\end{aligned}$$\n\n### <span id='jump-0'>依概率收敛</span>\n- 定义:设$X_1,X_2,\\cdots,X_n,\\cdots$是一个随机变量序列,$A$是一个常数,如果对任意$\\epsilon\\gt 0$,有\n$$\\begin{aligned}\n    \\lim_{n\\to+\\infin}P\\{|X_n-A|\\lt \\epsilon\\}=1,\n\\end{aligned}$$\n则称随机变量序列$X_1,X_2,\\cdots,X_n,\\cdots$依概率收敛于常数$A$,记作$X_n\\xrightarrow{P}A$\n\n### 切比雪夫大数定律\n- 定义:设$X_1,X_2,\\cdots,X_n,\\cdots$为<span class='my-color-b'>两两不相关</span>的随机变量序列,存在常数$C$,使<span class='my-color-b'>$D(X_i)\\le C(i=1,2,\\cdots)$</span>,则对任意$\\epsilon\\gt 0$,有\n$$\\begin{aligned}\n    \\lim_{n\\to\\infin}P=\\left\\{\\left|\\frac{1}{n}\\sum_{i=1}^n{X_i}-\\frac{1}{n}\\sum_{i=1}^n{E(X_i)}\\right|\\lt \\epsilon\\right\\}=1\n\\end{aligned}$$\n\n### 伯努利大数定律\n- 定义:设随机变量$X_n\\sim B(n,p),n=1,2,\\cdots,$则对于任意$\\epsilon\\gt 0$,有\n  $$\\begin{aligned}\n      \\lim_{n\\to +\\infin}P\\left\\{\\left|\\frac{X_n}{n}-p\\right|\\lt\\epsilon\\right\\}=1\n  \\end{aligned}$$\n\n### 辛钦大数定律\n- 定义:设随机变量$X_1,X_2,\\cdots,X_n,\\cdots$<span class='my-color-b'>独立同分布</span>,具有<span class='my-color-b'>数学期望$E(X_i)=\\mu,i=1,2,\\cdots,$</span>则对任意$\\epsilon\\gt 0$有\n$$\\begin{aligned}\n    \\lim_{n\\to +\\infin}P\\left\\{\\left|\\frac{1}{n}\\sum_{i=1}^n{X_i-\\mu}\\right|\\lt\\epsilon\\right\\}=1\n\\end{aligned}$$\n\n### 棣莫弗-拉普拉斯中心极限定理\n- 定义:设随机变量$X_n\\sim B(n,p)(n=1,2,\\cdots)$,则对于任意实数$x$,有\n  $$\\begin{aligned}\n      \\lim_{n\\to +\\infin}P\\left\\{\\frac{X_n-np}{\\sqrt{np(1-p)}}\\le x\\right\\}=\\Phi(x)\n  \\end{aligned}$$\n其中$\\Phi(x)$是标准正太的分布函数\n> 注:定理表面当$n$充分大时,服从$B(n,p)$ 的随机变量$X_n$经标准化后得$\\frac{X_n-np}{\\sqrt{np(1-p)}}$近似服从标准正态分布$N(0,1)$,或者说$X_n$近似地服从$N(np,np(1-p))$\n\n### 列维-林德伯格中心极限定理\n- 定义:设随机变量$X_1,X_2,\\cdots,X_n,\\cdots$独立同分布,具有数学期望与方差,$E(X_n)=\\mu,D(X_n)=\\sigma^2,n=1,2,\\cdots,$则对于任意实数$x$,有\n$$\\begin{aligned}\n    \\lim_{n\\to\\infin}P\\left\\{\\frac{\\sum_{i=1}^n{X_i-n\\mu}}{\\sqrt{n}\\sigma}\\le x\\right\\}=\\Phi(x)\n\\end{aligned}$$\n> 注:定理表明当$n$充分大时$\\sum_{i=1}^n{X_i}$的标准化$\\frac{\\sum_{i=1}^n{X_i-n\\mu}}{\\sqrt{n}\\sigma}$近似服从标准正态分布$N(0,1)$,或者说$\\sum_{i=1}^n{X_i}$近似地服从$N(n\\mu,n\\sigma^2)$\n### 参考\n1. [复习全书](https://book.douban.com/subject/6101482/)\n","tags":["概率与统计"],"categories":["概率与统计"]},{"title":"[概率与统计]第7章 参数估计","url":"%2Fposts%2F19f58936%2F","content":"### 一.点估计\n#### 1.点估计\n- 定义:用样本$X_1,X_2,\\cdots,X_n$构造的统计量$\\hat{\\theta}(X_1,X_2,\\cdots,X_n)$来估计未知参数$\\theta$称为点估计.统计量$\\hat{\\theta}(X_1,X_2,\\cdots,X_n)$称为估计量.\n> 注: 估计量是随机变量,它所取得的观测值$\\hat\\theta(x_1,x_2,\\cdots,x_n)$称为估计值.有时将$\\theta$的估计量和估计值统称为$\\theta$的估计.\n\n#### 2.无偏估计量\n- 定义:设$\\hat\\theta$是$\\theta$的估计量,如果$E(\\hat\\theta)=\\theta$,则称$\\hat\\theta=\\hat\\theta(X_1,X_2,\\cdots,X_n)$是未知参数$\\theta$的无偏估计量.\n> 证1:$\\overline{X}$是$\\mu$的无偏估计量\n> $$\\begin{aligned}\n    E(\\overline{X})=E\\left(\\frac{1}{n}\\sum_{i=1}^n{X_i}\\right)=\\frac{1}{n}\\sum_{i=1}^n{E(X_i)}=\\mu\n\\end{aligned}$$\n> 证2:$S^2$是$\\sigma^2$的无偏估计量\n> $$\\begin{aligned}\n    E(S^2)&=\\frac{1}{n-1}E\\left[\\sum_{i=1}^n(X_i-\\overline{X})^2\\right]=\\frac{1}{n-1}E\\left[\\sum_{i=1}^n(X_i^2-2X_i\\overline{X}+\\overline{X}^2)\\right]\\\\\n    &=\\frac{1}{n-1}E\\left[\\sum_{i=1}^nX_i^2-2\\overline{X}\\sum_{i=1}^n{X_i}+n\\overline{X}^2\\right]=\\frac{1}{n-1}\\left[\\sum_{i=1}^nX_i^2-n\\overline{X}^2\\right]\\\\\n    &=\\frac{1}{n-1}\\left[n(\\sigma^2+\\mu^2)-n[D(\\overline{X})+(E\\overline{X})^2]\\right]=\\frac{1}{n-1}\\left[(\\sigma^2+\\mu^2)-\\left(\\frac{\\sigma^2}{n}+\\mu^2\\right)\\right]\\\\\n    &=\\sigma^2\n\\end{aligned}$$\n\n#### 3.更有效估计量\n- 定义:设$\\hat\\theta_1$和$\\hat\\theta_2$都是$\\theta$的无偏估计量,且$D(\\hat\\theta_1)\\le D(\\hat\\theta_2)$,则称$\\hat\\theta_1$比$\\hat\\theta_2$更有效,或$\\hat\\theta_1$比$\\hat\\theta_2$更有效估计量.\n\n#### 4.一致估计量\n- 定义:设$\\hat\\theta(X_1,X_2,\\cdots,X_n)$是$\\theta$的估计值,如果$\\hat\\theta$[依概率收敛](../19f6d8b2/#jump-0)于$\\theta$,则称$\\hat\\theta(X_1,X_2,\\cdots,X_n)$为$\\theta$的一致估计量.\n\n### 二.估计量的求法和区间估计\n#### 1.矩估计法\n- 定义:用样本估计相应的总体矩,用样本矩的函数估计总体矩相应的函数,然后求出要估计的参数,称这种估计法为矩估计法.\n#### 2.矩估计法步骤\n- 设总体X的分布含有未知参数$\\theta_1,\\theta_2,\\cdots,\\theta_k,\\alpha_l=E(X^l)$存在,显然它是$\\theta_1,\\theta_2,\\cdots,\\theta_k$的函数,记作$\\alpha_l(\\theta_1,\\theta_2,\\cdots,\\theta_k),l=1,2,\\cdots,k$.样本的$l$阶原点矩为$A_l=\\frac{1}{n}\\sum_{i=1}^n{X_i^l}$.令\n$$\\begin{aligned}\n    \\alpha_l(\\theta_1,\\theta_2,\\cdots,\\theta_k)=A_l,l=1,2,\\cdots,k.\n\\end{aligned}$$\n从这$k$个方程组中,可以解得$\\theta_1,\\theta_2,\\cdots,\\theta_k$.\n矩估计法不需要知道总体的具体分布数学形式,只要知道各阶矩存在.\n如果不用原点矩,而用中心矩也可以求解:用样本中心矩等于总体中心矩来建立方程组.\n求$k$个参数的估计一般就列出一阶矩到$k$阶矩的方程.\n设$g(\\alpha_1,\\alpha_2)$是一阶矩$\\alpha_1$和二阶矩$\\alpha_2$的函数,而$\\hat\\alpha_1$和$\\hat\\alpha_2$分别为$\\alpha_1$和$\\alpha_2$的矩估计,则$g(\\hat\\alpha_1,\\hat\\alpha_2)$的矩估计.\n#### 3.最大似然估计法\n设$X_1,X_2,\\cdots,X_n$是来自总体$X$的样本,$x_1,x_2,\\cdots,x_n$是样本值,$\\theta$是待估参数.\n- **似然函数**\n    定义:对于离散型总体$X$,设其概率分布为$P{X=a_i}=p(a_i;\\theta),i=1,2,\\cdots,$称函数\n    $$\\begin{aligned}\n        L(\\theta)=L(X_1,X_2,\\cdots,X_n;\\theta)=\\prod_{i=1}^n{p(X_i;\\theta)}\n    \\end{aligned}$$\n    为参数$\\theta$的似然函数.\n    对于连续型总体$X$,概率密度为$f(x;\\theta)$,则称函数\n    $$\\begin{aligned}\n        L(\\theta)=L(X_1,X_2,\\cdots,X_n;\\theta)=\\prod_{i=1}^n{f(X_i;\\theta)}\n    \\end{aligned}$$\n    为参数$\\theta$的似然函数.\n- **最大似然估计法**\n    定义:对于给定的样本值$(x_1,x_2,\\cdots,x_n)$,使似然函数$L(x_1,x_2,\\cdots,x_n;\\theta)$达到最大值的参数值$\\hat\\theta=\\hat\\theta(x_1,x_2,\\cdots,x_n)$称为未知参数$\\theta$的最大似然估计值,相应的使似然函数$L(X_1,X_2,\\cdots,X_n;\\theta)$达到最大值的参数值$\\hat\\theta=\\hat\\theta(X_1,X_2,\\cdots,X_n)$称为$\\theta$的最大似然估计量.一般统称为$\\theta$的最大似然估计.称这种估计法为最大似然估计法.\n- **最大似然估计法步骤**\n    如果$L(\\theta)$或$lnL(\\theta)$关于$\\theta$可微,值$\\hat\\theta$往往可以从方程\n    $$\\begin{aligned}\n        \\frac{dL(\\theta)}{d\\theta}=0\\ or\\ \\frac{dln(\\theta)}{d\\theta}=0\n    \\end{aligned}$$\n    中求解,称这两个方程为似然方程.\n    如果要估计的参数是两个$\\theta_1$和$\\theta_2$,则得似然方程组\n    $$\\begin{cases}\n        \\displaystyle\\frac{\\partial L(\\theta)}{\\partial \\theta_1}=0,\\\\\n        \\displaystyle\\frac{\\partial L(\\theta)}{\\partial \\theta_2}=0\n    \\end{cases}\n    or\n    \\begin{cases}\n        \\displaystyle\\frac{\\partial \\ln L(\\theta)}{\\partial \\theta_1}=0,\\\\        \n        \\displaystyle\\frac{\\partial \\ln L(\\theta)}{\\partial \\theta_2}=0      \n    \\end{cases}$$\n    解这两个方程组,可以得到$\\hat\\theta_1$和$\\hat\\theta_2$.\n    有时,使$L(\\theta)$或$\\ln L(\\theta)$达到最大值的$\\hat\\theta$不一定是$L(\\theta)$或$\\ln L(\\theta)$驻点,这时不能用似然方程来求解,应采用其他方法求最大似然估计.\n\n### 参考\n1. [复习全书](https://book.douban.com/subject/6101482/)\n","tags":["概率与统计"],"categories":["概率与统计"]},{"title":"[leetcode]221.最大正方形","url":"%2Fposts%2Feefa3a9f%2F","content":"{% asset_img 2019053113453649.png %}\n\n### 解题思路 \n首先当然是对图形进行处理,将其看作是柱状图,如何看呢?\n即,这样看类似于`自顶向下`,先房顶,再房身,最后看地基\ni=0\n`1 0 1 0 0`\ni=1\n`1 0 1 0 0`\n`2 0 2 1 1`\ni=2\n`1 0 1 0 0`\n`2 0 2 1 1`\n`3 1 3 2 2`\ni=3\n`1 0 1 0 0`\n`2 0 2 1 1`\n`3 1 3 2 2`\n`4 0 0 3 0`\n\n`思路一(单调栈)`: 跟之前[[leetcode]84.柱状图中最大的矩形](../f814d224)思路一样,只不过在选最大矩形的时候变成了最大正方形\n`思路二(DP)`: 把dp[i][j]看作是以(i,j)为结尾的最大正方形,那么需要确定上边长,与左边长,取较小者\nmin(dp[i-1][j],dp[i][j-1])\n然后再与dp[i-1][j-1]比较,取最小,那么最后的式子为\n$$\\begin{aligned}\n    dp[i][j]=min(dp[i-1][j-1],min(dp[i-1][j],dp[i][j-1]))    \n\\end{aligned}$$\n\n### 代码实现 \n`思路一(单调栈)`:\n```cpp\n// 求出直方图中最大矩形\nint maxSquare(vector<int>& heights)\n{\n    heights.push_back(0);\n    stack<int> stk;\n    int len=heights.size(),maxArea=0;\n    for(int i=0;i<len;++i)\n    {\n        while(!stk.empty()&&heights[i]<heights[stk.top()])\n        {\n            int h=heights[stk.top()];\n            stk.pop();\n            int l=(stk.empty()?0:stk.top()+1),r=i;\n            h=min(h,r-l);\n            maxArea=max(maxArea,h*h);\n        }\n        stk.push(i);\n    }\n    return maxArea;\n}\n// 所有直方图中最大矩形\nint maximalSquare(vector<vector<char>>& matrix) {\n    if (matrix.empty()) return 0;\n    int m=matrix.size(),n=matrix[0].size();\n    vector<vector<int>> M(m,vector<int>(n,0));\n    for(int i=0;i<m;++i)\n    {\n        for(int j=0;j<n;++j)\n        {\n            if (i==0)\n                M[i][j]=matrix[i][j]-'0';\n            else if (matrix[i][j]!='0')\n                M[i][j]=M[i-1][j]+matrix[i][j]-'0';\n        }\n    }\n    int maxArea=0;\n    for(int i=0;i<m;++i)\n        maxArea=max(maxArea,maxSquare(M[i]));\n    return maxArea;\n}\n```\n\n`思路二(DP)`:\n```cpp\nint maximalSquare(vector<vector<char>>& matrix) {\n    if (matrix.empty()) return 0;\n    int m=matrix.size(),n=matrix[0].size(),maxL=0;\n    vector<vector<int>> dp(m,vector<int>(n,0));\n    for(int i=0;i<m;++i)\n    {\n        for(int j=0;j<n;++j)\n        {\n            if (i==0||j==0)\n                dp[i][j]=matrix[i][j]-'0';\n            else if (matrix[i][j]!='0')\n                dp[i][j]=min(dp[i-1][j-1],min(dp[i-1][j],dp[i][j-1]))+1;\n            maxL=max(maxL,dp[i][j]);\n        }\n    }\n    return maxL*maxL;\n}\n```","tags":["单调栈"],"categories":["OJ"]},{"title":"[life]notes","url":"%2Fposts%2Fb320b84b%2F","content":"### 学习\n- 先看视频,再看书更有效率\n- 心不要急,稳扎稳打,书读百遍,其义自见\n- 看纸质书比电子书效率高,且不伤眼睛\n### Leetcode\n- 算法是内功,每天坚持2道,并回顾\n- 注意\n  - 不要在循环条件处用s.size(),应在开头len=s.size()\n### 参考","tags":["life"],"categories":["life"]},{"title":"[机器学习实战]第6章 支持向量机(SVM)","url":"%2Fposts%2F80b6a5fe%2F","content":"{% asset_img 2019053016131743.png %}\n### 了解\n\n### 点导超平面S的距离\n输入空间中任意$x_0$到超平面$S$的距离为:\n$$\\begin{aligned}\n    d=\\frac{|\\mathbf {w\\cdot x_0}+b|}{||\\mathbf{w}||}\n\\end{aligned}\n$$\n$\\mathbf {w,x_0,x}$都为$N$维向量,点代表内积,$\\mathbf {w\\cdot x_0}$为一个数\n推导: 点$\\mathbf x_0$到超平面$S: \\mathbf{w\\cdot x}+b=0$距离$d$的计算过程如下:\n1. 设点$x_0$在平面$S$上的投影为$x_1$,则$\\mathbf{w\\cdot x_1}+b=0$\n2. 由于向量$\\overrightarrow{\\mathbf{x_0x_1}}$与$S$平面的法向量$\\mathbf w$平行,所以\n$$\\begin{aligned}\n    |\\mathbf{w\\cdot\\overrightarrow{x_0x_1}}|=|\\mathbf w|\\cdot|\\overrightarrow{\\mathbf{x_0x_1}}|=\\sqrt{(w^1)^2+\\cdots+(w^N)^2}\\cdot d=||\\mathbf w||\\cdot d\n\\end{aligned}\n$$\n又\n$$\\begin{aligned}  \n\\mathbf w\\cdot \\overrightarrow{\\mathbf{x_0x_1}}&=w^1(x_0^1-x_1^1)+w^2(x_0^2-x_1^2)+\\cdots+w^N(x_0^N-x_1^N)\\\\\n&=w^1x_0^1+w^2x_0^2+\\cdots+w^Nx_0^N-(w^1x_1^1+w^2x_1^2+\\cdots+w^Nx_1^N)\\\\\n&=w^1x_0^1+w^2x_0^2+\\cdots+w^Nx_0^N-(-b)\\\\\n&=|\\mathbf{w\\cdot x_0}+b|\n\\end{aligned}\n$$\n因此$||\\mathbf w||\\cdot d=|\\mathbf{w\\cdot x_0}+b|$,推出$d=\\frac{\\displaystyle|\\mathbf {w\\cdot x_0}+b|}{\\displaystyle||\\mathbf{w}||}$\n### 相关\n$$\\begin{aligned}\n    u^Tv=p\\cdot\\Vert{u}\\Vert\n\\end{aligned}$$\nSVM有三宝,间隔对偶和核技巧\nSVM分为:\n- hard-margin svm\n- soft-margin svm\n- kernel svm\n判别模型\n$$\\begin{aligned}\n    f(w)=sign(w^Tx+b)\n\\end{aligned}$$\n最大间隔分类器\n$$\\begin{aligned}\n    \\max margin(w,b)\\\\\n    s.t. y_i\n\\end{aligned}$$\n### 超平面\n- 定义: 在样本空间中,划分超平面可通过如下线性方程来描述:\n    $$\\begin{aligned}\n        \\mathbf{\\omega^Tx+b=0}\n    \\end{aligned}$$\n    其中$\\mathbf\\omega=(\\omega_1;\\omega_2;...;\\omega_d)$为法向量,决定了超平面的方向;$b$为位移项,决定了超平面与原点之间的距离.显然,划分超平面可被法向量$\\mathbf\\omega$和位移$\\mathbf b$确定,下面我们将其记为$(\\omega,b)$.样本空间中任意点$\\mathbf x$到超平面$(\\mathbf{\\omega,b})$的距离可写为\n    $$\\begin{aligned}\n        d=\\frac{|\\mathbf{\\omega^Tx+b}|}{\\Vert\\mathbf\\omega\\Vert}\n    \\end{aligned}$$\n    约束问题可表示为:\n    $$\\begin{cases}\n        \\min{\\frac{1}{2}\\Vert\\omega\\Vert^2}\\\\\n        s.t.\\ y_i(\\mathbf{\\omega^Tx_i+b})\\ge 1,i=1,2,...,n\n    \\end{cases}$$\n    剩下的看下面参考链接\n### 参考\n- [机器学习实战教程（八）：支持向量机原理篇之手撕线性SVM](https://cuijiahua.com/blog/2017/11/ml_8_svm_1.html)","tags":["机器学习实战"],"categories":["机器学习"]},{"title":"[leetcode]84.柱状图中最大的矩形","url":"%2Fposts%2Ff814d224%2F","content":"{% asset_img 2019053014352639.png %}\n\n### 了解\n单调栈(Monotone Stack)是一种特殊的栈,特殊之处在于栈内的元素都保持一个单调性,可能`单调递减`(如:5 4 4 3 2 1),也可能`单调递增`(如: 1 2 2 3 4)\n\n### 解题思路 \n单调栈用来存放柱子下标,只保存高度是`单调递增`的柱子,对于每一个柱子,我们都求出以该柱子为高的矩形最大面积\n- 能完全覆盖第0个柱子的最大矩形\n    {% asset_img 20190328100531585.png %}\n- 能完全覆盖第1个柱子的最大矩形\n    {% asset_img 20190328100617165.png %}\n- 能完全覆盖第2个柱子的最大矩形\n    {% asset_img 20190328100648369.png %}\n- 能完全覆盖第3个柱子的最大矩形\n    {% asset_img 20190328100625809.png %}\n- 能完全覆盖第4个柱子的最大矩形\n    {% asset_img 20190328100655726.png %}\n- 能完全覆盖第5个柱子的最大矩形\n    {% asset_img 20190328100702406.png %}\n首先,我们将其看成下图形式\n{% asset_img 2019053015105342.png %}\n柱子中间数字表示编号,下面数字即为边界,每一个柱子都有左边下标和右边下标,例如第0个柱子,左边为0,右边为1\n<span class='my-color-b'>注意：基于各个高度的最大矩形是在出栈的时候计算的，因此必须要让所有高度都出栈。这里是利用单调栈的性质让其全部出栈，即在原始数组后添一个0.</span>\n那么,为了确定每个柱子为高的最大矩形面积,我们就需要找到最近的与之相邻的比它小的柱子(即确定左边界和右边界)\n如:\n第2根柱子,右边界即4,左边界即2,那么该柱子确定的矩形面积为5*(4-2)=10\n第3根柱子,右边界即4,左边界即3,那么该柱子确定的矩形面积为6*(4-3)=6\n- 确定左边界\n    该柱子左边第一个比当前柱子高度小的柱子,它的右下标\n    栈顶元素作为要确定的柱子,因此左边比它高度的小柱子必然是该元素抛出后的下一个栈顶元素的右边下标,就确定了左边界\n- 确定右边界\n    该柱子右边第一个比当前柱子高度小的柱子,它的左下标\n    即单调栈的栈顶必然为当前最高的柱子,只要遍历时发现有比栈顶高度小的柱子的左边下标,就确定了右边界\n### 代码实现 \n```cpp\nint largestRectangleArea(vector<int>& heights)\n{\n    heights.push_back(0);\n    stack<int> stk;\n    int maxArea=0,len=heights.size();\n    for(int i=0;i<len;++i)\n    {\n        while(!stk.empty()&&heights[i]<heights[stk.top()])\n        {\n            int h=heights[stk.top()]; // 要确定的柱子的高度\n            stk.pop();  // 为了得到下一个栈顶元素\n            int l=(stk.empty()?0:stk.top()+1),r=i; // 如果栈空,就说明左边元素都比它大,则左边界为0\n            maxArea=max(maxArea,h*(r-l)); // 选最大的矩形面积\n        }\n        stk.push(i);\n    }\n    return maxArea;\n}\n```\n时间复杂度$O(n)$\n### 参考\n1. [](https://blog.csdn.net/Zolewit/article/details/88863970)","tags":["单调栈"],"categories":["OJ"]},{"title":"[leetcode]31.下一个排列","url":"%2Fposts%2F428400a2%2F","content":"{% asset_img 2019052912303931.png %}\n\n### 解题思路 \n可以看到,`只要不是单调递减的序列,下一个排列的数一定是比当前数大的`,但`同时也应当是比当前数大的所有数当中最小的`\n因此,可分为两种情况:\n- **不单调**: 则找到下一个比它大的数,且这个数是所有排列中比当前数大的最小的一个\n    例如: 找`1 5 8 4 7 6 5 3 1`的下一个排列\n    方法是: 低位到高位找到第一个不是单调递减的数,这里是`4`,`4`后面的数都满足单调递减,则从后面单调递减的数中找到一个比4大的最小的数进行交换,然后把`4`后的数进行逆序即可\n    如果不理解,我们不妨这样看,单独看`4 7 6 5 3 1`,它后面一个排列应该是什么呢?由于`7 6 5 3 1`是单调递减,已经达到了该排列的最大值,因此`4 7 6 5 3 1`中只能更换`4`的值,当然是比`4`大的数中尽可能的小的数,那么就是`5`,交换后是这样`5 7 6 4 3 1`(交换后必然还是后面的数单调递减(即是以`5`开头的最大的排列)),这样就将后面的数逆序,即可保障是`5`开头的最小的一个排列`5 1 3 4 6 7`\n- **单调递减**: 即所有排列中最大的数,那么它的下一个排列必然是所有排列中最小的数,那么只需逆序即可\n    例如: `7 6 5 4 3 2 1`逆序为`1 2 3 4 5 6 7`\n### 代码实现 \n\n```cpp\n    void nextPermutation(vector<int>& nums) {\n        if (nums.size()<=1) return ;\n        int i=nums.size()-2,j=nums.size()-1;\n        for(;i>=0&&nums[i]>=nums[i+1];--i);\n        if (i>=0) // i<0,则说明为单调递减,i>=0则不单调\n        {\n            for(;j>=0&&nums[i]>=nums[j];--j);\n            swap(nums[i],nums[j]);\n        }\n        reverse(nums.begin()+i+1,nums.end());\n    }\n```\n算法时间复杂度$O(n)$","tags":["排列"],"categories":["OJ"]},{"title":"[python]notes","url":"%2Fposts%2Ffd60ea9c%2F","content":"### Python3\n#### 循环\n{% asset_img 2019052911252030.png %}\n{% asset_img 2019072113585011.png %}\n- **map(function, iterable, ...)**\n  返回一个迭代器,它将应用函数到每一个可迭代的值上,并产生结果\n  {% asset_img 2019062421400048.png %}\n  可用于批量类型转换\n  {% asset_img 2019062421423849.png %}\n#### 形参*args,**args\n  {% asset_img 201907080943282.png %}\n  {% asset_img 201907082116358.png %}\n  在实参中使用*arg,则表示unpack,如下所示\n  {% asset_img 2019070921035710.png %}","tags":["python"],"categories":["python"]},{"title":"[leetcode]32.最长有效括号","url":"%2Fposts%2Ff8bebee9%2F","content":"{% asset_img 201905261449501.png %}\n\n### 题目含义\n\n### 解题思路\nDP思想:\n初始dp数组为0,dp[i]表示以下标为i的字符结尾的最长有效字符串的长度.\n那么很明显,有效字符串一定以')'结尾,进一步得出以'('结尾的字符串对应的dp数组位置上都为0,则\n1. $s[i]=')'$且$s[i-1]='('$,即形如$\"...()\"$,可以推出:\n    $$\\begin{aligned}\n        i\\lt 2&\\;\\Rightarrow\\;dp[i]=2\\\\\n        i\\ge 2&\\;\\Rightarrow\\;dp[i]=dp[i-2]+2\n    \\end{aligned}\n    $$\n    即在上一个有效字符串长度之上增加了2(增加的是最后一个()长度)\n2. $s[i]=')'$且$s[i-1]=')'$,即形如$\"...))\"$,可以推出:\n    $$\\begin{aligned}\n        i-dp[i-1]-2<0&\\;\\Rightarrow\\;dp[i]=dp[i-1]+2\\\\\n        i-dp[i-1]-2\\ge 0&\\;\\Rightarrow\\;dp[i]=dp[i-1]+2+dp[i-dp[i-1]-2]\n    \\end{aligned}\n    $$\n    这个式子的推导可以用下图表示\n    $$\\begin{aligned}\n    \\underbrace{(\\ )\\ (\\ )}_{dp[i-dp[i-1]-2]}\\ (\\ \\underbrace{(\\ (\\ )\\ (\\ )\\ )}_{dp[i-1]}\\underbrace{\\ )}_i\n    \\end{aligned}\n    $$\n    可以这样理解,$\"()()()\"$表示同级括号(1级),则最长为6,$\"((()))\"$表示不同级括号,1级为最外层,2级往内一层,3级最里层\n    那么,求dp[i]也就是找以i结尾的有效1级括号的长度,那么我们可以先得到内部一级也就是dp[i-1]的长度,然后加2就为当前以i结尾的一个括号的长度,然而还得求所有有效的同级括号,因此找到前面也就是$\"\\underbrace{(\\cdots)(\\cdots)}_{many\\ brackets}\\underbrace{(\\cdots)}_{final\\ bracket}\"$,即最后那个括号的前面所有同级括号长度$dp[i-dp[i-1]-2]$,由于要找最长的有效括号长度,因此需要不断更新最大值\n### 代码实现 \n- **思路清晰版**\n    ```cpp\n    int longestValidParentheses(string s) {\n        if (s.empty()) return 0;\n        vector<int> dp(s.size(),0);\n        int maxn=0;\n        for(int i=1;i<s.size();++i)\n        {\n            if (s[i]==')')\n            {\n                if (s[i-1]=='(')    \\\\ 形如\"...()\"\n                {\n                    if (i>=2)   \\\\ 形如\"...()\"\n                        dp[i]=dp[i-2]+2;\n                    else    \\\\ \"()\"\n                        dp[i]=2;\n                }\n                else if(i-dp[i-1]-1>=0&&s[i-dp[i-1]-1]=='(')    \\\\ 形如\"...))\"\n                {\n                    if (i-dp[i-1]-2>=0) \\\\ 形如\"...(...)((...))\",实际等价于\"...(...)(...)\"\n                        dp[i]=dp[i-1]+2+dp[i-dp[i-1]-2];\n                    else    \\\\ 形如\"((...))\"\n                        dp[i]=dp[i-1]+2;\n                }\n            }\n            maxn=max(maxn,dp[i]);   \\\\ 求出最长有效括号长度\n        }\n        return maxn;\n    }\n    ```\n    算法时间复杂度$O(n)$\n- **精简版**\n    ```cpp\n    int longestValidParentheses(string s) {\n        if (s.empty()) return 0;\n        vector<int> dp(s.size()+1,0);\n        int maxn=0;\n        for(int i=1;i<s.size();++i)\n        {\n            if (s[i]==')')\n            {\n                if (s[i-1]=='(')\n                    dp[i]=(i>=2?dp[i-2]:0)+2;\n                else if(i-dp[i-1]>=1&&s[i-dp[i-1]-1]=='(')\n                    dp[i]=dp[i-1]+2+dp[i-dp[i-1]>=2?i-dp[i-1]-2:0];\n                maxn=max(maxn,dp[i]);\n            }\n        }\n        return maxn;\n    }\n    ```\n    算法时间复杂度$O(n)$","tags":["dp"],"categories":["OJ"]},{"title":"[mathjax]MathJax符号整理","url":"%2Fposts%2F682ccd58%2F","content":"### 字母对应表\n|                          文本                          |                           显示                           |                           文本                            |                            显示                             |                           文本                           |                            显示                            |      文本       |        显示         |\n| :----------------------------------------------------: | :------------------------------------------------------: | :-------------------------------------------------------: | :---------------------------------------------------------: | :------------------------------------------------------: | :--------------------------------------------------------: | :-------------: | :-----------------: |\n|                         \\alpha                         |                     $\\alpha,\\Alpha$                      |                           \\beta                           |                        $\\beta,\\Beta$                        |                           \\nu                            |                         $\\nu,\\Nu$                          |       \\xi       |      $\\xi,\\Xi$      |\n|                         \\gamma                         |                     $\\gamma,\\Gamma$                      |                          \\delta                           |                       $\\delta,\\Delta$                       |                            o                             |                           $o,O$                            |       \\pi       |      $\\pi,\\Pi$      |\n|                  \\epsilon,\\varepsilon                  |             $\\epsilon,\\Epsilon,\\varepsilon$              |                           \\zeta                           |                        $\\zeta,\\Zeta$                        |                           \\rho                           |                        $\\rho,\\Rho$                         |     \\sigma      |   $\\sigma,\\Sigma$   |\n|                          \\eta                          |                       $\\eta,\\Eta$                        |                          \\theta                           |                       $\\theta,\\Theta$                       |                           \\tau                           |                        $\\tau,\\Tau$                         |    \\upsilon     | $\\upsilon,\\Upsilon$ |\n|                         \\iota                          |                      $\\iota,\\Iota$                       |                          \\kappa                           |                       $\\kappa,\\Kappa$                       |                       \\phi,\\varphi                       |                    $\\phi,\\Phi,\\varphi$                     |      \\chi       |     $\\chi,\\Chi$     |\n|                        \\lambda                         |                    $\\lambda,\\Lambda$                     |                            \\mu                            |                          $\\mu,\\Mu$                          |                           \\psi                           |                        $\\psi,\\Psi$                         |     \\omega      |   $\\omega,\\Omega$   |\n|                         \\nabla                         |                         $\\nabla$                         |                                                           |                                                             |                                                          |                                                            |                 |                     |\n|                          \\pm                           |                          $\\pm$                           |                          \\times                           |                          $\\times$                           |                           \\div                           |                           $\\div$                           |      \\mid       |       $\\mid$        |\n|                         \\nmid                          |                         $\\nmid$                          |                           \\cdot                           |                           $\\cdot$                           |                          \\circ                           |                          $\\circ$                           |      \\ast       |       $\\ast$        |\n|                        \\bigodot                        |                        $\\bigodot$                        |                        \\bigotimes                         |                        $\\bigotimes$                         |                        \\bigoplus                         |                        $\\bigoplus$                         |      \\leq       |       $\\leq$        |\n|                         \\vert                          |                      $\\vert,\\Vert$                       |                           \\sim                            |                           $\\sim$                            |                     \\xrightarrow{P}                      |                     $\\xrightarrow{P}$                      |                 |                     |\n|                       \\mathring                        |                      $\\mathring{U}$                      |                        \\lang,\\rang                        |                        $\\lang,\\rang$                        |                          \\Lrarr                          |                          $\\Lrarr$                          |                 |                     |\n|                          \\int                          |                          $\\int$                          |                          \\pm,\\mp                          |                          $\\pm,\\mp$                          |                                                          |                                                            |                 |                     |\n|                          \\cup                          |                          $\\cup$                          |                           \\cap                            |                           $\\cap$                            |                        \\setminus                         |                        $\\setminus$                         |     \\subset     |      $\\subset$      |\n|                       \\subseteq                        |                       $\\subseteq$                        |                        \\subsetneq                         |                        $\\subsetneq$                         |                         \\supset                          |                         $\\supset$                          |       \\in       |        $\\in$        |\n|                         \\notin                         |                         $\\notin$                         |                         \\emptyset                         |                         $\\emptyset$                         |                       \\varnothing                        |                       $\\varnothing$                        |                 |                     |\n|                       \\overline                        |                      $\\overline{A}$                      |                        \\widetilde                         |                       $\\widetilde{A}$                       |                         \\widehat                         |                       $\\widehat{A}$                        |      \\vec       |      $\\vec{a}$      |\n|                          \\bar                          |                        $\\bar{a}$                         |                          \\tilde                           |                         $\\tilde{a}$                         |                        \\mathring                         |                       $\\mathring{A}$                       |                 |                     |\n| \\approx \\sim \\simeq \\cong \\equiv \\prec \\lhd \\therefore | $\\approx \\sim \\simeq \\cong \\equiv \\prec \\lhd \\therefore$ | \\to \\rightarrow \\leftarrow \\Rightarrow \\Leftarrow \\mapsto | $\\to \\rightarrow \\leftarrow \\Rightarrow \\Leftarrow \\mapsto$ | \\land \\lor \\lnot \\forall \\exists \\top \\bot \\vdash \\vDash | $\\land \\lor \\lnot \\forall \\exists \\top \\bot \\vdash \\vDash$ | \\binom{n+1}{2k} |  $\\binom{n+1}{2k}$  |\n|              \\succ \\succeq \\prec \\preceq               |              $\\succ \\succeq \\prec \\preceq$               |                     \\gg \\ll \\ggg \\lll                     |                     $\\gg \\ll \\ggg \\lll$                     |                                                          |                                                            |                 |                     |\n|                                                        |                                                          |                                                           |                                                             |                                                          |                                                            |                 |                     |\n|                                                        |                                                          |                                                           |                                                             |                                                          |                                                            |                 |                     |\n|                                                        |                                                          |                                                           |                                                             |                                                          |                                                            |                 |                     |\n\n### 参考\n1. [MathJax basic tutorial and quick reference](MathJax basic tutorial and quick reference - Mathematics Meta Stack Exchange.html)\n2. [mathjax语法介绍](https://www.jianshu.com/p/a7fa1ed4ca20)","tags":["mathjax"],"categories":["mathjax"]},{"title":"[杂谈]亲戚关系图谱","url":"%2Fposts%2F9c4487da%2F","content":"### 前言\n你是否经常在亲戚朋友聚会时会碰到喊不出称呼的尴尬场景,下面这张`亲戚关系图谱`将帮你好好理清一下亲戚关系.\n{% asset_img 0bf9c8fa0ec09fac7aa.jpg %}\n\n### 亲戚关系图谱\n根据上图做的部分修改,适用于南方这边\n```mermaid\ngraph TD\n    waigong[\"外公,外婆\"]---jiujiu[\"<b>舅舅</b>,舅妈\"]\n    waigong---yifu[\"<b>姨妈</b>,姨父\"]\n    waigong---baba[\"<b>妈妈,爸爸</b>\"]\n    yeye[\"爷爷,奶奶\"]---baba\n    yeye---bobo[\"<b>伯伯</b>,伯妈\"]\n    yeye---gudie[\"姑爹,<b>姑妈</b>\"]\n    jiujiu---biaojiemei[\"表姐妹\"]\n    jiujiu---biaoxiongdi1[\"表兄弟\"]\n    yifu---yibiaozimei[\"姨表姊妹\"]\n    yifu---yibiaoxiongdi[\"姨表兄弟\"]\n    baba---zimeizhangdi[\"姊妹丈弟,<b>姊妹妹妹</b>\"]\n    baba---laogong[\"<b>老婆,老公(你在这)</b>\"]\n    baba---xiongdi[\"<b>兄弟兄弟</b>,兄嫂弟妹\"]\n    bobo---tangxiongdi[\"堂兄弟,堂姐妹\"]\n    gudie---biaoxiongdi2[\"表兄弟\"]\n    zimeizhangdi---waisheng[\"外甥女,外甥\"]\n    laogong---nver[\"儿婿,<b>女儿</b>\"]\n    laogong---erzi[\"<b>儿子</b>,儿媳妇\"]\n    xiongdi---zhizi[\"侄女,侄子\"]\n    nver---sunzi1[\"孙女,孙子\"]\n    erzi---sunzi2[\"孙女,孙子\"]\n```\n> 1. **堂表之分**: 堂兄弟是爸爸的兄弟的儿子,表兄弟是爸爸的姐妹的儿子和妈妈的兄弟姐妹的儿子,堂姐妹和表姐妹也一样区分.简而言之,一个姓的都是堂\n> 2. **加深颜色**: 图中加深颜色表示与父节点有**直接血缘关系**\n\n","tags":["杂谈"],"categories":["杂谈"]},{"title":"[paper 2]Semi-supervised Feature Selection via Rescaled Linear Regression Xiaojun","url":"%2Fposts%2F531010d9%2F","content":"\n### Abstract\n- 基于特征选择的线性回归方法通常学习一个[投影矩阵(projection matrix)](../e772abbc)并通过它来评估特征的重要性,然而这些方法不能找到**投影矩阵的全局和稀疏解**,新的方法通过用一系列的缩放因子重新调节最小二乘法回归模型的系数,本文将给出理论诠释为何可以使用投影矩阵来给特征排序\n\n### Introduction\n- 标记信息能否获得,决定了特征选择能否在监督学习和无监督学习环境下进行\n  - 在监督特征选择中,特征的相关性可以通过特征和类别的关系来评估\n  - 无监督特征选择中,没有标签信息,特征的相关性可以通过特征的依赖性或相似性来评估\n  - 通常我们只能得到一小部分带有标记的数据集和大部分的未标记的数据集,基于这两部分数据集来进行特征选择,这种方法叫做\"半监督特征选择\"\n  > 附加:无标签数据一般是有标签数据中的某一个类别的（不要不属于的，也不要属于多个类别的）\n  有标签数据的标签应该都是对的\n  无标签数据一般是类别平衡的（即每一类的样本数差不多）\n  无标签数据的分布应该和有标签的相同或类似 等等。\n  ","tags":["paper"],"categories":["paper"]},{"title":"[机器学习实战]第5章 Logistic回归","url":"%2Fposts%2Ff0799167%2F","content":"### 最大似然估计法\n- **定义[离散型]**: 若总体$X$属离散型,其分布律$P\\{X=x\\}=p(x;\\theta),\\theta\\isin\\Theta$的形式为已    知,$\\theta$为待估参数,$\\Theta$是$\\theta$可能取值的范围.设$X_1,X_2,\\cdots,X_n$是来自$X$的样本,则$X_1,X_2,\\cdots,X_n$的联合分布律为\n    $$\\begin{aligned}\n        \\prod_{i=1}^n{p(x_i;\\theta)}\n    \\end{aligned}\n    $$\n    又设$x_1,x_2,\\cdots,x_n$是相应与$X_1,X_2,\\cdots,X_n$的一个样本值.易知样本$X_1,X_2,\\cdots,X_n$取到观察值$x_1,x_2,\\cdots,x_n$的概率,亦即事件$\\{X_1=x_1,X_2=x_2,\\cdots,X_n=x_n\\}$发生的概率为\n    $$\\begin{aligned}\n        L(\\theta)=L(x_1,x_2,\\cdots,x_n;\\theta)=\\prod_{i=1}^n{p(x_i;\\theta)},\\theta\\isin\\Theta\\quad (1.1)\n    \\end{aligned}\n    $$\n    这一概率随$\\theta$的取值而变化,它是$\\theta$的函数,$L(\\theta)$成为样本的 **似然函数** (注意,这里$x_1,x_2,\\cdots,x_n$是已知的样本值,它们都是常数)\n    由费希尔(R.A.Fisher)引进的最大似然估计法,就是固定样本观测值$x_1,x_2,\\cdots,x_n,$在$\\theta$取值的可能范围$\\Theta$内挑选使似然函数$L(x_1,x_2,\\cdots,x_n;\\theta)$达到最大的参数值$\\hat\\theta$,作为参数$\\theta$的估计值.即取$\\hat\\theta$使\n    $$\\begin{aligned}\n        L(x_1,x_2,\\cdots,x_n;\\hat\\theta)=\\max_{\\theta\\isin\\Theta}{L(x_1,x_2,\\cdots,x_n;\\theta)}\\quad (1.2)\n    \\end{aligned}\n    $$\n    这样得到的$\\hat\\theta$与样本值$x_1,x_2,\\cdots,x_n$有关,常记为$\\hat\\theta(x_1,x_2,\\cdots,x_n)$,成为参数$\\theta$的 **最大似然估计值** ,而相应的统计量$\\hat\\theta(X_1,X_2,\\cdots,X_n)$称为参数$\\theta$的 **最大似然估计量**.\n- **定义[连续型]**: 若总体$X$属连续型,其概率密度$f(x;\\theta),\\theta\\isin\\Theta$的形式已知,$\\theta$为待估参数,$\\Theta$是$\\theta$可能取值的范围.设$X_1,X_2,\\cdot,X_n$是来自$X$的样本,则$X_1,X_2,\\cdots,X_n$的联合密度为\n    $$\\begin{aligned}\n        \\prod_{i=1}^n{f(x_i,\\theta)}\n    \\end{aligned}\n    $$\n    设$x_1,x_2,\\cdots,x_n$是相应于样本$X_1,X_2,\\cdots,X_n$的一个样本值,则随机点$(X_1,X_2,\\cdots,X_n)$落在点$(x_1,x_2,\\cdots,x_n)$的邻域(边长分别为$dx_1,dx_2,\\cdots,dx_n$的$n$维立方体)内的概率近似地为\n    $$\\begin{aligned}\n        \\prod_{i=1}^n{f(x_i;\\theta)dx_i}\\quad (1.3)\n    \\end{aligned}\n    $$\n    其值随$\\theta$的取值而变化.与离散型的情况一样,我们取$\\theta$的估计值$\\hat\\theta$使概率(1.3)取到最大值,但因子$\\prod_{i=1}^n{dx_i}$不随$\\theta$而变,故只需考虑函数\n    $$\\begin{aligned}\n        L(\\theta)=L(x_1,x_2,\\cdots,x_n;\\theta)=\\prod_{i=1}^n{f(x_i;\\theta)}\\quad (1.4)\n    \\end{aligned}\n    $$\n    的最大值.这里$L(\\theta)$称为样本的 **似然函数**.若\n    $$\\begin{aligned}\n        L(x_1,x_2,\\cdots,x_n;\\hat\\theta)=\\max_{\\theta\\isin\\Theta}{L(x_1,x_2,\\cdots,x_n;\\theta)},\n    \\end{aligned}\n    $$\n    则称$\\hat\\theta(x_1,x_2,\\cdots,x_n)$为$\\theta$的 **最大似然估计值**,称$\\hat\\theta(X_1,X_2,\\cdots,X_n)$为$\\theta$的 **最大似然估计量**.\n    > 在很多情况下,$p(x;\\theta)$和$f(x;\\theta)$关于$\\theta$可微,这时$\\hat\\theta$常可从方程$\\frac{dL(\\theta)}{d\\theta}=0$解得,又因$L(\\theta)$与$lnL(\\theta)$在同一$\\theta$处取到极值,因此,$\\theta$的最大似然估计$\\theta$也可以从方程$\\frac{dlnL(\\theta)}{d\\theta}=0$求得,而从后一方程求解往往比较方便.该方程被称为 **对数似然方程**.\n\n### Sigmoid函数\n  - **Sigmoid函数定义**: **S函数**得名因其形状像S字母,是一种常见的**逻辑函数(logistic function)**\n    $$\\begin{aligned}\n        S(t)=\\frac{1}{1+e^{-t}}\\quad (1)\n    \\end{aligned}\n    $$\n    其级数展开为:\n    $$\\begin{aligned}\n        S(t)=\\frac{1}{2}+\\frac{1}{4}t-\\frac{1}{48}t^3+\\frac{1}{480}t^5-\\frac{17}{80640}t^7+\\frac{31}{1451520}t^9-\\frac{691}{319334400}t^{11}+O(t^{12})\n    \\end{aligned}\n    $$\n    {% asset_img 201905250800123432.png %}\n    <p style=\"text-align: center;\" >图5-1 Sigmoid图像</p>\n    公式(1)自变量取值为任意实数,值域为$[0,1]$\n    将任意的输入映射到$[0,1]$区间,我们在线性回归中可以得到一个预测值,再将该值映射到Sigmoid函数中这样就完成了由值到概率的转换,也就是分类任务\n    python代码实现\n    \n    ```python\n    def sigmoid(z):\n        return 1 / (1+np.exp(-z))\n    ```\n\n### Logistic回归模型\n1. Logistic分布\n    - **定义(李航书):** 设$X$是连续随机变量,$X$服从Logistic分布是指$X$具有下列分布函数和密度函数:\n    $$\\begin{aligned}\n    F(x)=P(X\\le x)=\\frac{1}{1+e^{-\\frac{x-\\mu}{\\gamma}}}&\\quad (2)\\\\\n    f(x)=F'(x)=\\frac{e^{-\\frac{x-\\mu}{\\gamma}}}{\\gamma(1+e^{-\\frac{x-\\mu}{\\gamma}})^2}&\\quad (3)\n    \\end{aligned}\n    $$\n    式中,$\\mu$为位置参数,$\\gamma\\gt 0$为形状参数\n    - **说明**: 逻辑回归算法是分类算法,我们将它作为分类算法使用.有时候可能因为这个算法的名字出现了\"回归\"使你感到困惑,但逻辑回归算法实际上是一种分类算法,它适用于标签$y$取值离散的情况,如:$1\\ 0\\ 0\\ 1$\n    - **假设函数**: $h_\\theta(x)=g(\\theta x)=S(\\theta x)$,其中h表示假设函数(hypothesis function),g表示逻辑函数(logistic function),S表示Sigmoid函数(Sigmoid function)\n    - **决策边界()**: Sigmoid函数是常用的一个逻辑函数,我们通过将输入(样本)$x$映射到Sigmoid函数,从而把输出控制在[0,1]范围,对于分类问题,我们需要输出0或1,我们可以预测:\n      - 当$h_\\theta(x)\\ge 0.5$时,预测$y=1$\n      - 当$h_\\theta(x)\\lt 0.5$时,预测$y=0$\n        {% asset_img 2019052814183916.png %}\n        决策边界(decision boundary)代码实现如下\n        ```python\n        def decision_boundary(prob):\n            return prob > 0.5*np.ones(len(prob))\n        ```\n\n### 对比线性回归和Logistic回归模型\n1. 线性回归的方法:\n   1. 假设函数$h_\\theta(x)=\\theta x$(这样多项式的形式可以表示所有的值,$\\theta$为未知参数,我们要求出一个$\\theta$,对于每一个样本$x$放进去$h_\\theta(x)$得出的预测值都可以尽可能的匹配观测值)\n   2. 线性回归的代价函数时所有模型误差的平方和(这样可以保障是凸函数),代价函数为\n    $$\\begin{aligned}\n        J(\\theta)=\\frac{1}{2m}\\sum_{i=0}^m(h_\\theta(x^{(i)})-y^{(i)})^2\n    \\end{aligned}$$\n   3. 为使预测值与观测值最接近,从而应当使代价函数$J(\\theta)$最小,虽然可以采用求导的方法求最值,但求导属于一元函数(即1维空间,1个特征值),但是现实生活中每个样本的特征值远远超过1个,因此要采用偏导数(适用于多元函数)的方法的方法求最值,梯度是指向值变化最大的方向,用于求多元函数的最值再适合不过,因此采用梯度下降的方法来求得代价函数的最小值,由于梯度方向只是指向值变化最大的方向,而我们仍需要尽可能沿着观测的值走(不能一步太大跑偏了),需要在到达最小值的过程中不断的重新计算梯度方向,因此引入$\\alpha$作为学习率(learning rate),决定了我们沿着能让代价函数下降程度最大的方向迈出的步子有多大,这个过程便是梯度下降的过程,梯度下降有几种实现方式:\n         1. 批量梯度下降(BGD: Batch Gradient Descent):即每一次都同时让所有的参数减去学习速率乘以代价函数$J(\\theta)$的偏导数\n            $$\\begin{aligned}\n                &J(\\theta)=\\frac{1}{2m}\\sum_{i=0}^m(h_\\theta(x^{(i)})-y^{(i)})^2\\\\\n                &\\frac{\\partial}{\\partial\\theta_i}{J(\\theta)}=\\frac{1}{m}\\sum_{j=0}^m(h_\\theta(x^{(j)})-y^{(j)})x_i^{(j)}\\\\\n            &\\theta_i=\\theta_i-\\alpha\\frac{\\partial}{\\partial\\theta_i}{J(\\theta)}=\\theta_i-\\alpha\\sum_{j=0}^m(h_\\theta(x^{(j)})-y^{(j)})x_i^{(j)}\n            \\end{aligned}$$\n            <span class='my-color-b'>梯度下降法把$\\frac{1}{m}$放到了学习率$\\alpha$中,因此最后只需保留$\\alpha$即可</span>\n         2. 随机梯度下降(SGD: Stochastic Gradient Descent):与批量梯度下降的区别在于,随机梯度下降仅仅选取一个样本j来求梯度.公式为:\n            $$\\begin{aligned}\n                \\theta_i=\\theta_i-\\alpha(h_\\theta(x^{j})-y^{(j)})x_i^{(j)}\n            \\end{aligned}\n            $$\n2. Logistic回归的方法:\n   1. 假设函数$h_\\theta(x)=g(\\theta x)=\\frac{1}{1+e^{-\\theta x}}$\n        假设函数$h_\\theta(x)$代码实现如下\n        ```python\n        def hypo(theta, x):\n            \"\"\"假设函数(hypothesis function)\n\n            Arguments:\n                theta {array} -- 参数值\n                x {array} -- 一个数据或数据集\n\n            Returns:\n                array -- 通过假设函数得到的预测值\n            \"\"\"\n            return sigmoid(np.dot(x, theta))\n        ```\n   2. 我们不能将$h_\\theta(x)$带入线性回归的代价函数(即所有模型误差的平方和)中,因为这样得到的代价函数会是一个非凸函数,意味着我们的代价函数有许多局部最小值,这回影响梯度下降算法寻找全局最小值,Logistic的代价函数定义为\n        $$\\begin{aligned}\n            J(\\theta)=-\\frac{1}{m}\\sum_{i=0}^m[y^{(i)}log(h_\\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\\theta(x^{(i)}))]\n        \\end{aligned}\n        $$\n        代价函数$J(\\theta)$实现代码如下\n        ```python\n        def jcost(theta, X, y):\n            \"\"\"J代价函数\n\n            Arguments:\n                theta {array} -- 参数值\n                X {array} -- 数据集\n                y {array} -- 标签集\n\n            Returns:\n                array -- 代价\n            \"\"\"\n            hX = hypo(theta, X)\n            # 元素级乘法获取第一项的值(数组)\n            first = np.multiply(y, np.log(hX))\n            # 元素级乘法获取第二项的值(数组)\n            second = np.multiply(np.ones(len(y))-y, np.log(np.ones(len(hX))-hX))\n            return -(first+second)/len(X)\n        ```\n   3. 同线性回归方法一样,为了使代价函数$J(\\theta)$最小,采用梯度下降方式\n      1. 批量梯度下降(BGD: Batch Gradient Descent):即每一次都同时让所有的参数减去学习速率乘以代价函数$J(\\theta)$的偏导数\n        $$\\begin{aligned}\n            &J(\\theta)=-\\frac{1}{m}\\sum_{i=0}^m[y^{(i)}log(h_\\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\\theta(x^{(i)}))]\\\\\n            &\\frac{\\partial}{\\partial \\theta_i}J(\\theta)=\\frac{1}{m}\\sum_{j=0}^m(h_\\theta(x^{(j)})-y^{(j)})x_i^{(j)}=\\frac{1}{m}\\sum_{j=0}^m(S(\\theta x^{(j)})-y^{(j)})x_i^{(j)}\\\\\n            &\\theta_i=\\theta_i-\\alpha\\frac{\\partial}{\\partial\\theta_i}J(\\theta)=\\theta_i-\\alpha\\sum_{j=0}^m(h_\\theta(x^{(j)})-y^{(j)})x_i^{(j)}=\\theta_i-\\alpha\\sum_{j=0}^m(S(\\theta x^{(j)})-y^{(j)})x_i^{(j)}\n        \\end{aligned}\n        $$\n        <span class='my-color-b'>梯度下降法把$\\frac{1}{m}$放到了学习率$\\alpha$中,因此最后只需保留$\\alpha$即可\n        虽然Logistic回归和线性回归的代价函数的偏导数都一样,但并非所有代价函数最终求导都为同一形式</span>\n        我们来推导一下Logistic回归的代价函数的偏导数的式子是如何求出的\n        首先\n        $$\\begin{aligned}\n            &y^{(j)}log(h_\\theta(x^{(j)}))+(1-y^{(j)})log(1-h_\\theta(x^{(j)}))\\\\\n            =&y^{(j)}log(S(\\theta x^{(j)}))+(1-y^{(j)})log(1-S(\\theta x^{(j)}))\\\\\n            =&y^{(j)}log(\\frac{1}{1+exp(-\\theta x^{(j)})})+(1-y^{(j)})log(1-\\frac{1}{1+exp(-\\theta x^{(j)})})\\\\\n            =&-y^{(j)}log(1+exp(-\\theta x^{(j)}))-(1-y^{(j)})log(1+exp(\\theta x^{(j)}))\n        \\end{aligned}$$\n        所以\n        $$\\begin{aligned}\n            \\frac{\\partial}{\\partial\\theta_i}J(\\theta)&=\\frac{\\partial}{\\partial\\theta_i}[-\\frac{1}{m}\\sum_{j=0}^m[-y^{(j)}log(1+e^{-\\theta x^{(j)}})-(1-y^{(j)})log(1+exp(\\theta x^{(j)}))]]\\\\\n            &=-\\frac{1}{m}\\sum_{j=0}^m[-y^{(j)}\\frac{-x_i^{(j)}e^{-\\theta x^{(j)}}}{1+e^{-\\theta x^{(j)}}}-(1-y^{(j)})\\frac{x_i^{(j)}e^{\\theta x^{(j)}}}{1+e^{\\theta x^{(j)}}}]\\\\\n            &=-\\frac{1}{m}\\sum_{j=0}^m[y^{(j)}\\frac{x_i^{(j)}}{1+e^{\\theta x^{(j)}}}-(1-y^{(j)})\\frac{x_i^{(j)}e^{\\theta x^{(j)}}}{1+e^{\\theta x^{(j)}}}]\\\\\n            &=-\\frac{1}{m}\\sum_{j=0}^m[\\frac{y^{(j)}x_i^{(j)}-x_i^{(j)}e^{\\theta x^{(j)}}+y^{(j)}x_i^{(j)}e^{\\theta x^{(j)}}}{1+e^{\\theta x^{(j)}}}]\\\\\n            &=-\\frac{1}{m}\\sum_{j=0}^m[\\frac{y^{(j)}(1+e^{\\theta x^{(j)}})-e^{\\theta x^{(j)}}}{1+e^{\\theta x^{(j)}}}]x_i^{(j)}\\\\\n            &=-\\frac{1}{m}\\sum_{j=0}^m[y^{(j)}-\\frac{e^{\\theta x^{(j)}}}{1+e^{\\theta x^{(j)}}}]x_i^{(j)}\\\\\n            &=-\\frac{1}{m}\\sum_{j=0}^m[y^{(j)}-\\frac{1}{1+e^{-\\theta x^{(j)}}}]x_i^{(j)}\\\\\n            &=-\\frac{1}{m}\\sum_{j=0}^m[y^{(j)}-S(\\theta x^{(j)})]x_i^{(j)}\\\\\n            &=-\\frac{1}{m}\\sum_{j=0}^m[y^{(j)}-h_\\theta(x^{(j)})]x_i^{(j)}\\\\\n            &=\\frac{1}{m}\\sum_{j=0}^m[h_\\theta(x^{(j)})-y^{(j)}]x_i^{(j)}\n        \\end{aligned}$$\n        证毕\n        <span class='my-color-b'>注:虽然得到的梯度下降算法表面上看上去与线性回归的梯度下降算法一样,但是这里的$h_\\theta(x)=g(\\theta x)$与线性回归中不同,所以实际上是不一样的.另外,在运行梯度下降算法之前,进行[特征缩放(特征归一化)](../75b8f8af#jump-autonorm)依旧是非常必要的.</span>\n        批量梯度代码实现如下\n        ```python\n        def partial_jcost(theta, X, y, x):\n            \"\"\"J代价函数关于theta的偏导数\n\n            Arguments:\n                theta {array} -- 参数值\n                X {array} -- 数据集\n                y {array} -- 标签集\n                x {array} -- 一个数据或数据集\n\n            Returns:\n                array -- 偏导值\n            \"\"\"\n            return np.dot(hypo(theta, X)-y, x)\n\n        def batch_gradient_desc(X, y, alpha=0.01, numIterations=500):\n            \"\"\"BGD下降法(批量梯度下降法)\n\n            Arguments:\n                X {array} -- 数据集\n                y {array} -- 标签集\n\n            Keyword Arguments:\n                alpha {float} -- 步长(学习率) (default: {0.01})\n                numIterations {int} -- 迭代次数 (default: {500})\n\n            Returns:\n                array -- 返回参数值\n            \"\"\"\n            # 获取特征数n\n            n = len(X[0])\n            theta = np.ones(n)\n            # 批量梯度下降法\n            for i in range(numIterations):\n                theta = theta-alpha*partial_jcost(theta, X, y, X)\n            return theta\n        ```\n\n        1. 随机梯度下降(SGD: Stochastic Gradient Descent):与批量梯度下降的区别在于,随机梯度下降仅仅选取一个样本j来求梯度.公式为:\n        $$\\begin{aligned}\n            \\theta_i=\\theta_i-\\alpha(h_\\theta(x^{j})-y^{(j)})x_i^{(j)}=\\theta_i-\\alpha(S(\\theta x^{(j)})-y^{(j)})x_i^{(j)}\n        \\end{aligned}\n        $$\n        随机梯度下降法代码实现如下\n        ```python\n        def stochastic_gradient_desc(X, y, alpha=0.01, numIterations=100):\n            \"\"\"SGD下降法(随机梯度下降法)\n\n            Arguments:\n                X {array} -- 数据集\n                y {array} -- 标签集\n\n            Keyword Arguments:\n                alpha {float} -- 步长(学习率) (default: {0.01})\n                numIterations {int} -- 迭代次数 (default: {100})\n\n            Returns:\n                array -- 返回参数值\n            \"\"\"\n            # 获取样本数m,特征数n\n            m, n = len(X), len(X[0])\n            theta = np.ones(n)\n            # 随机梯度下降法\n            for k in range(numIterations):\n                dataIndex = range(m)\n                for i in range(m):\n                    alpha = 4/(1.0+k+i)+0.01\n                    randIndex = int(np.random.uniform(0,len(dataIndex)))\n                    theta=theta-alpha*np.multiply(hypo(X[randIndex], theta)-y[randIndex],X[randIndex])\n            return theta\n        ```\n\n### 最优化理论中常用的优化算法\n#### 梯度下降法和梯度上升法\n- **梯度下降法wiki**: 如果$F(x)$在点$a$附近的邻域是有定义且可微的,那么函数$F(x)$在点$a$沿着负梯度方向值减少得最快(下降得最快),遵循下列公式\n    $$\\begin{aligned}\n        a_{n+1}=a_n-\\gamma{\\nabla{F(a_n)}}\\tag 7\n    \\end{aligned}\n    $$\n    对于$\\gamma\\isin\\Reals_+$为一个足够小的数值时成立,那么$F(a)\\ge F(b)$\n    考虑到这一点,我们可以从函数$F$的局部极小值的初始估计$x_0$出发,并考虑如下序列$x_0,x_1,\\cdots$使得\n    $$\\begin{aligned}\n        x_{n+1}=x_n-\\gamma_n\\nabla{F(x_n)},\\quad n\\ge 0\\tag 8\n    \\end{aligned}\n    $$\n    因此可以得到\n    $$\\begin{aligned}\n        F(x_0)\\ge F(x_1)\\ge F(x_2)\\ge\\cdots,\n    \\end{aligned}\n    $$\n    如果顺利的话序列$(X_n)$将收敛到期望的局部极小值.注意:每次迭代过程中$\\gamma$值都可以改变.\n    {%asset_img 2019052618559123.png %}\n    当函数$F$是凸函数,所有的局部最小值也是全局最小值,因此,上面这幅图梯度下降可以收敛到全局解(即最小值).上图中,$F$被定义在一个\"碗\"状图形上.蓝色的为轮廓线,轮廓线上$F$的值是不变的.红色箭头表示的方向是箭头原点的负梯度方向,该方向与该原点所在的轮廓线相互正交.图中梯度下降法最终引向了\"碗底\",该点便是$F$的最小值点.\n- **梯度下降的相关概念**\n    {% asset_img v2-77ab0b5fab73c_r.jpg %}\n    1. **步长(learning rate)**:步长决定了在梯度下降迭代的过程中，每一步沿梯度负方向前进的长度。用上面下山的例子，步长就是在当前这一步所在位置沿着最陡峭最易下山的位置走的那一步的长度。\n    2. **特征(feature)**:指的是样本中输入部分，比如2个单特征的样本$(x^{(0)},y^{(0)}),(x^{(1)},y^{(1)})$,则第一个样本特征为$x^{(0)}$，第一个样本输出为$y^{(0)}$。\n    3. **假设函数(hypothesis function)**:在监督学习中，为了拟合输入样本，而使用的假设函数，记为$h_\\theta(x)$。比如对于单个特征的$m$个样本$(x^{(i)},y^{(i)})(i=1,2,...m)$,可以采用拟合函数如下： $h_\\theta(x_1)=\\theta_0+\\theta_1{x_1}$,为了更好的表示,则增加$x_0=1$,则$h_\\theta(x)=h_\\theta(x_0,x_1)=\\theta_0x_0+\\theta_1x_1$\n    4. **损失函数或代价函数(loss function或cost function)**：为了评估模型拟合的好坏，通常用代价函数来度量拟合的程度。代价函数极小化，意味着拟合程度最好，对应的模型参数即为最优参数。在线性回归中，代价函数通常为预测值与观测值的差取平方。比如对于$m$个样本$(x^{(i)},y^{(i)})(i=1,2,...m)$,采用线性回归，代价函数为：\n    $J(\\theta_0,\\theta_1)=\\frac{1}{2m}\\sum_{i=0}^m(h_\\theta(x^{(i)})-y^{(i)})^2$\n    之所以从$i=0$而不是$i=1$开始,是增加了$x^{(0)}=1$,为了方便运算,其中$x^{(i)}$表示第$i$个样本特征,$y^{(i)}$表示第$i$个样本的观测值,$h_\\theta(x^{(i)})$为假设函数,表示第$i$个样本的预测值\n- **梯度下降法的代数方式描述**\n    1. **先决条件**： 确认优化模型的假设函数和代价函数。\n        比如对于线性回归，假设函数表示为 $h_θ(x_1,x_2,...x_n)=θ_0+θ_1x_1+...+θ_nx_n$, 其中$θ_i(i = 0,1,2...n)$为模型参数，$x_i(i = 0,1,2...n)$为每个样本的$n$个特征值。这个表示可以简化，我们增加一个特征$x_0=1$ ，这样$h_\\theta(x_0,x_1,...x_n)=\\displaystyle\\sum_{i=0}^n{\\theta_ix_i}$\n        同样是线性回归，对应于上面的假设函数，代价函数为：\n        $$\\begin{aligned}\n            J(\\theta_0,\\theta_1...,\\theta_n)=\\frac{1}{2m}\\sum_{j=0}^m(h_\\theta(x_0^{(j)},x_1^{(j)},...x_n^{(j)})−y^{(j)})^2\n        \\end{aligned}$$\n        <span class='my-color-b'>比如说,$x=(x_0,x_1,x_2,\\cdots,x_n)$表示一个样本$x$有n($x_0=1$)个特征值,$h_\\theta(x)=x\\theta=θ_0x_0+θ_1x_1+...+θ_nx_n$,中间$x\\theta$表示**向量点积**,在$x$中增加$x_0$(值为1)是为了方便向量点积运算(必须长度相同),且不会产生任何影响,$\\theta=(\\theta_0,\\theta_1,\\cdots,\\theta_n)$</span>\n    2. **算法相关参数初始化**：主要是初始化$\\theta_0,\\theta_1...,\\theta_n$,算法终止距离$\\epsilon$以及步长$\\alpha$。在没有任何先验知识的时候，我喜欢将所有的$\\theta$初始化为1， 将步长初始化为0.01。在调优的时候再 优化。\n    3. **算法过程(批量梯度下降法)**：\n       1). 确定当前位置的代价函数的梯度，对于$\\theta_i$,其梯度表达式如下：\n       $$\\begin{aligned}\n           \\frac{\\partial}{\\partial\\theta_i}{J(\\theta_0,\\theta_1,\\cdots,\\theta_n)}\n       \\end{aligned}\n       $$\n       2). 用步长乘以代价函数的梯度，得到当前位置下降的距离，即$\\alpha\\frac{\\partial}{\\partial\\theta_i}{J(\\theta_0,\\theta_1,\\cdots,\\theta_n)}$对应于前面登山例子中的某一步。\n       3). 确定是否所有的$\\theta_i$,梯度下降的距离都小于$\\epsilon$，如果小于$\\epsilon$则算法终止，当前所有的$θ_i(i=0,1,...n)$即为最终结果。否则进入步骤4\n       4). 更新所有的$\\theta$，对于$\\theta_i$，其更新表达式如下。更新完毕后继续转入步骤1.\n        $$\\begin{aligned}\n            \\theta_i=\\theta_i-\\alpha\\frac{\\partial}{\\partial\\theta_i}{J(\\theta_0,\\theta_1,\\cdots,\\theta_n)}\n        \\end{aligned}\n        $$\n        下面用线性回归的例子来具体描述梯度下降。假设我们的样本是\n        $(x_1^{(0)},x_2^{(0)},\\cdots,x_n^{(0)},y_0),(x_1^{(1)},x_2^{(1)},\\cdots,x_n^{(1)},y_1),\\cdots,(x_1^{(m)},x_2^{(m)},\\cdots,x_n^{(m)},y_m)$,代价函数如前面先决条件所述：\n        $$\\begin{aligned}\n            J(\\theta_0,\\theta_1,\\cdots,\\theta_n)=\\frac{1}{2m}\\sum_{j=0}^m(h_\\theta(x_0^{(j)},x_1^{(j)},\\cdots,x_n^{(j)})-y^{(j)})^2\n        \\end{aligned}\n        $$\n        则在算法过程步骤1中对于$\\theta_i$ 的偏导数计算如下： \n        $$\\begin{aligned}\n            \\frac{\\partial}{\\partial\\theta_i}{J(\\theta_0,\\theta_1,\\cdots,\\theta_n)}=\\frac{1}{m}\\sum_{j=0}^m(h_\\theta(x_0^{(j)},x_1^{(j)},\\cdots,x_n^{(j)})-y^{(j)})x_i^{(j)}\n        \\end{aligned}\n        $$\n        由于样本中没有$x_0$上式中令所有的$x_0^{(j)}$为1.\n        步骤4中$\\theta_i$的更新表达式如下：\n        $$\\begin{aligned}\n            \\theta_i=\\theta_i-\\alpha\\frac{1}{m}\\sum_{j=0}^m(h_\\theta(x_0^{(j)},x_1^{(j)},\\cdots,x_n^{(j)})-y^{(j)})x_i^{(j)}\n        \\end{aligned}\n        $$\n        从这个例子可以看出当前点的梯度方向是由所有的样本决定的，加$\\frac{1}{m}$ 是为了好理解。由于步长也为常数，他们的乘积也为常数，所以<span class='my-color-b'>这里$\\alpha\\frac{1}{m}$可以用一个常数来处理,一般在程序中的处理是只保留$\\alpha$</span>。\n        在下面会详细讲到的梯度下降法的变种，他们主要的区别就是对样本的采用方法不同。这里我们采用的是用所有样本(即批量梯度下降法)。\n- **梯度下降法的矩阵方式描述**\n    1. **先决条件**： 和上面类似， 需要确认优化模型的假设函数和代价函数。对于线性回归，假设函数$h_θ(x_1,x_2,...x_n)=θ_0+θ_1x_1+...+θ_nx_n$的矩阵表达方式为：\n        $$\\begin{aligned}\n            h_\\theta(X)=X\\theta=(x_1,x_2,\\cdots,x_m)\\theta,\\quad\\theta=(\\theta_0, \\theta_1,\\cdots,\\theta_n)\n        \\end{aligned}\n        $$\n        其中， 假设函数$h_\\theta(X)$为mx1的向量,$\\theta$为(n+1)x1的向量，里面有n+1个代数法的模型参数。$X$为mx(n+1)维的矩阵。m代表样本的个数，n+1代表样本的特征数。\n        代价函数的表达式为：$J(\\theta)=\\frac{1}{2}(X\\theta−Y)^T(X\\theta−Y)$, 其中$Y$是样本的输出向量，维度为mx1.\n        <span class='my-color-b'>这里$X=(x_1,x_2,\\cdots,x_m)$,表示m个样本的集合(每个样本有n+1个特征值$x_0,x_1,\\cdots,x_n$,多的1为增加的$x0=1$)</span>\n    2. **算法相关参数初始化**: $\\theta$向量可以初始化为默认值，或者调优后的值。算法终止距离$\\epsilon$，步长$\\alpha$和代数形式比没有变化。\n    3. **算法过程(批量梯度下降法)**:\n        1). 确定当前位置的代价函数的梯度，对于$\\theta$向量,其梯度表达式如下：$\\frac{\\partial}{\\partial\\theta}J(\\theta)$\n        2). 用步长乘以代价函数的梯度，得到当前位置下降的距离，即$\\alpha\\frac{\\partial}{\\partial\\theta}J(\\theta)$对应于前面登山例子中的某一步。\n        3). 确定$\\theta$向量里面的每个值,梯度下降的距离都小于$\\epsilon$，如果小于$\\epsilon$则算法终止，当前$\\epsilon$向量即为最终结果。否则进入步骤4.\n        4). 更新$\\theta$向量，其更新表达式如下。更新完毕后继续转入步骤1.\n        $$\\begin{aligned}\n            \\theta=\\theta-\\alpha\\frac{\\partial}{\\partial\\theta}J(\\theta)\n        \\end{aligned}\n        $$\n        还是用线性回归的例子来描述具体的算法过程。\n        代价函数对于$\\theta$向量的偏导数计算如下：\n        $$\\begin{aligned}\n          \\frac{\\partial}{\\partial\\theta}J(\\theta)=X^T(X\\theta-Y)  \n        \\end{aligned}\n        $$\n        步骤4中θ向量的更新表达式如下：$\\theta=\\theta-\\alpha{X^T}(X\\theta-Y)$\n        对于之前的代数法,可以看到矩阵法要简洁很多。这里面用到了矩阵求导链式法则，和两个矩阵求导的公式。\n        这里面用到了矩阵求导链式法则，和两个个矩阵求导的公式。\n        公式1: $\\frac{\\partial}{\\partial x}(x^Tx)=2x$,$x$为向量\n        公式2: $\\nabla_X{f(AX+B)}=A^T\\nabla_Y{f},\\ Y=AX+B$, $f(Y)$为标量\n        如果需要熟悉矩阵求导建议参考张贤达的《矩阵分析与应用》一书。\n- **程序演示**\n    我们通过构造函数$f(x)=(x-1)(x-2)(x-3)=x^3-6x^2+11x-6$来展示二维空间(平面)上的梯度上升和梯度下降过程\n    {% asset_img 20190526192019192.png %}\n    上图中,左图为二维空间的梯度上升示意图,红色点即为每次梯度上升迭代的过程,可以看到点与点之间的距离是逐步缩小的,只到最后收敛为0,即点取得极大值(注意:只是极大值(局部解),不是最大值,如果是凸函数,则为最大值(全局解)),右图为二维空间的梯度下降示意图,蓝色点即为每次梯度下降迭代的过程,同样最终收敛点取得极小值(局部解) \n#### 批量梯度下降法(BGD: Batch Gradient Descent)\n批量梯度下降法，是梯度下降法最常用的形式，具体做法也就是在更新参数时使用所有的样本来进行更新，这个方法对应于前面代数形式的线性回归的梯度下降算法，也就是说3.3.1的梯度下降算法就是批量梯度下降法。　\n$$\\begin{aligned}\n    \\theta_i=\\theta_i-\\alpha\\sum_{j=0}^m(h_\\theta(x_0^{(j)},x_1^{(j)},\\cdots,x_n^{(j)})-y^{(j)})x_i^{(j)}\n\\end{aligned}\n$$　\n或\n{% asset_img v2-5809743fd06c4ff8.jpg %}\n<span class='my-color-b'>一般记住第一个,用预测值减观测值$\\sum_{j=0}^m(h_\\theta(x_0^{(j)},x_1^{(j)},\\cdots,x_n^{(j)})-y^{(j)})$,由于是梯度下降,因此是减$\\alpha$(虽然第二福图是加,我们这样记只是为了方便记忆)</span>\n由于我们有m个样本，这里求梯度的时候就用了所有m个样本的梯度数据。\n\n#### 随机梯度下降法(SGD: Stochastic Gradient Descent)\n随机梯度下降法，其实和批量梯度下降法原理类似，区别在与求梯度时没有用所有的m个样本的数据，而是仅仅选取一个样本j来求梯度。对应的更新公式是：\n$$\\begin{aligned}\n    \\theta_i=\\theta_i-\\alpha(h_\\theta(x_0^{(j)},x_1^{(j)},\\cdots,x_n^{(j)})-y^{(j)})x_i^{(j)}\n\\end{aligned}\n$$\n或\n{% asset_img v2-b3f14a09ad27df9_r.jpg %}\n<span class='my-color-b'>一般记住第一个,用预测值减观测值$\\sum_{j=0}^m(h_\\theta(x_0^{(j)},x_1^{(j)},\\cdots,x_n^{(j)})-y^{(j)})$,由于是梯度下降,因此是减$\\alpha$(虽然第二福图是加,我们这样记只是为了方便记忆)</span>\n随机梯度下降法，上面的批量梯度下降法是两个极端，一个采用所有数据来梯度下降，一个用一个样本来梯度下降。自然各自的优缺点都非常突出。对于训练速度来说，随机梯度下降法由于每次仅仅采用一个样本来迭代，训练速度很快，而批量梯度下降法在样本量很大的时候，训练速度不能让人满意。对于准确度来说，随机梯度下降法用于仅仅用一个样本决定梯度方向，导致解很有可能不是最优。对于收敛速度来说，由于随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解。\n那么，有没有一个中庸的办法能够结合两种方法的优点呢？有！这就是小批量梯度下降法。\n#### 小批量随机梯度下降法(MBGD: Mini-batch Gradient Descent)\n小批量梯度下降法是批量梯度下降法和随机梯度下降法的折衷，也就是对于m个样本，我们采用x个样子来迭代，$1\\lt x\\lt m$。一般可以取x=10，当然根据样本的数据，可以调整这个x的值。对应的更新公式是：\n$$\\begin{aligned}\n    \\theta_i=\\theta_i-\\alpha\\sum_{j=t}^{t+x-1}(h_\\theta(x_0^{(j)},x_1^{(j)},\\cdots,x_n^{(j)})-y^{(j)})x_i^{(j)}\n\\end{aligned}\n$$　\n或\n{% asset_img v2-96181aa7bcd.jpg %}\n#### 牛顿法和拟牛顿法(Quasi-Newton Methods)\n#### DFP算法\n#### 局部优化法(BFGS)\n#### 有限内存局部优化法(L-BFGS)\n#### 共轭梯度法(Conjugate Gradient)\n#### 拉格朗日乘数法\n#### 启发式优化算法-智能算法\n1. 人工神经网络\n2. 模拟退火算法\n3. 禁忌搜索算法\n4. 粒子群算法\n5. 蚁群算法\n6. 鱼群算法\n7. 布谷鸟算法\n8. 遗传算法\n9. 免疫算法\n10. 进化多目标算法\n### Logistic二分类程序运行结果\n{% asset_img 2019052817502312.png %}\n可以看出程序预测结果还是比较准确的$(0.98,1.00)$,由于每次运行程序都会重新打乱样本,则训练样本和测试样本会有所不同,因此多次运行会得到不同的精确度,上图是先将特征值进行了归一化的结果,将特征值的范围都限定在了$[0,1]$之间可以更好的把握特征的分布,不至于图像过大(由于有些特征之间值的差距过大),数据点分布过于稀疏\n### Logistic二分类代码实现\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\n\n\ndef loadDataSet_iris():\n    \"\"\"数据集生成\n\n    Returns:\n        array -- 数据集\n        array -- 标签集\n    \"\"\"\n    dataMat, labelMat = load_iris(return_X_y=True)\n    dataMat, labelMat = dataMat[:100,:2], labelMat[:100]\n    return dataMat, labelMat\n\n\ndef sigmoid(z):\n    return 1 / (1+np.exp(-z))\n\n\ndef hypo(x, theta):\n    \"\"\"假设函数(hypothesis function)\n\n    Arguments:\n        theta {array} -- 参数值\n        x {array} -- 一个数据或数据集\n\n    Returns:\n        array -- 通过假设函数得到的预测值\n    \"\"\"\n    return sigmoid(np.dot(x, theta))\n\n\ndef jcost(theta, X, y):\n    \"\"\"J代价函数\n\n    Arguments:\n        theta {array} -- 参数值\n        X {array} -- 数据集\n        y {array} -- 标签集\n\n    Returns:\n        array -- 代价\n    \"\"\"\n    hX = hypo(X, theta)\n    # 元素级乘法获取第一项的值(数组)\n    first = np.multiply(y, np.log(hX))\n    # 元素级乘法获取第二项的值(数组)\n    second = np.multiply(np.ones(len(y))-y, np.log(np.ones(len(hX))-hX))\n    return -(first+second)/len(X)\n\n\ndef partial_jcost(theta, X, y, x):\n    \"\"\"J代价函数关于theta的偏导数\n\n    Arguments:\n        theta {array} -- 参数值\n        X {array} -- 数据集\n        y {array} -- 标签集\n        x {array} -- 一个数据或数据集\n\n    Returns:\n        array -- 偏导值\n    \"\"\"\n    return np.dot(hypo(X, theta)-y, x)\n\n\ndef decision_boundary(prob):\n    \"\"\"决策边界:\n           概率>=0.5时,为1\n           概率<0.5时,为0\n\n    Arguments:\n        prob {array} -- 一组概率值\n\n    Returns:\n        array -- 一组类别值\n    \"\"\"\n    return prob >= 0.5*np.ones(len(prob))\n\n\ndef accuracy_rate(X, y, theta):\n    \"\"\"计算预测准确率\n\n    Arguments:\n        X {array} -- 预测数据集\n        y {array} -- 已知观测值\n        theta {array} -- 参数值\n\n    Returns:\n        float -- 返回预测准确率\n    \"\"\"\n    y_predict = classify(hypo(X, theta))\n    trueCount = np.sum(y_predict == y)\n    return float(trueCount)/len(y)\n\n\ndef batch_gradient_desc(X, y, alpha=0.01, numIterations=500):\n    \"\"\"BGD下降法(批量梯度下降法)\n\n    Arguments:\n        X {array} -- 数据集\n        y {array} -- 标签集\n\n    Keyword Arguments:\n        alpha {float} -- 步长(学习率) (default: {0.01})\n        numIterations {int} -- 迭代次数 (default: {500})\n\n    Returns:\n        array -- 返回参数值\n    \"\"\"\n    # 获取特征数n\n    n = len(X[0])\n    theta = np.ones(n)\n    # 批量梯度下降法\n    for i in range(numIterations):\n        theta = theta-alpha*partial_jcost(theta, X, y, X)\n    return theta\n\n\ndef stochastic_gradient_desc(X, y, alpha=0.01, numIterations=100):\n    \"\"\"SGD下降法(随机梯度下降法)\n\n    Arguments:\n        X {array} -- 数据集\n        y {array} -- 标签集\n\n    Keyword Arguments:\n        alpha {float} -- 步长(学习率) (default: {0.01})\n        numIterations {int} -- 迭代次数 (default: {100})\n\n    Returns:\n        array -- 返回参数值\n    \"\"\"\n    # 获取样本数m,特征数n\n    m, n = len(X), len(X[0])\n    theta = np.ones(n)\n    # 随机梯度下降法\n    for k in range(numIterations):\n        for i in range(m):\n            theta=theta-alpha*np.multiply(hypo(X[i], theta)-y[i],X[i])\n    return theta\n\n\ndef classify(prob):\n    \"\"\"由概率返回分类类别\n\n    Arguments:\n        prob {array} -- 每个样本的概率\n\n    Returns:\n        array -- 返回分类类别\n    \"\"\"\n    return decision_boundary(prob)\n\n\ndef auto_norm(X):\n    \"\"\"特征归一化(或特征缩放)\n\n    Arguments:\n        X {array} -- 数据集\n\n    Returns:\n        array -- 返回归一化后的数据集\n    \"\"\"\n    X = np.array(X)\n    minVals = X.min(0)\n    maxVals = X.max(0)\n    newVals = (X-minVals)/(maxVals-minVals)\n    return newVals\n\n\ndef plotBestFit(theta, dataMat, labelMat, title='Gradient Descent', subplt=111):\n    \"\"\"绘制图像\n\n    Arguments:\n        theta {array} -- 参数列表\n        dataMat {array} -- 样本集\n        labelMat {array} -- 标签集\n\n    Keyword Arguments:\n        title {str} -- 图像标题 (default: {'Gradient Descent'})\n        subplt {int} -- 子图 (default: {111})\n    \"\"\"\n    # 存储分类,所有标签值0的为一类,标签值1的为一类\n    xcord0 = []\n    ycord0 = []\n    xcord1 = []\n    ycord1 = []\n    # 分类\n    for i in range(len(dataMat)):\n        if labelMat[i] == 1:\n            xcord1.append(dataMat[i][1])\n            ycord1.append(dataMat[i][2])\n        else:\n            xcord0.append(dataMat[i][1])\n            ycord0.append(dataMat[i][2])\n    plt.subplot(subplt)\n    plt.scatter(xcord0, ycord0, s=30, c='red', marker='s', label='failed')\n    plt.scatter(xcord1, ycord1, s=30, c='green', label='success')\n    x0_min = dataMat[:, 1].min()\n    x0_max = dataMat[:, 1].max()\n    x = np.arange(x0_min, x0_max, 0.1)\n    y = (-theta[0]-theta[1]*x)/theta[2]\n    plt.plot(x, y)\n    plt.xlabel('X1')\n    plt.ylabel('X2')\n    plt.title(title)\n    plt.legend()\n\n\nif __name__ == \"__main__\":\n    # 读取数据集,标签集\n    dataMat, labelMat = loadDataSet_iris()\n    m = len(dataMat)\n    # 特征归一化(特征缩放)\n    dataMat[:, :] = auto_norm(dataMat[:, :])\n    # 所有数据的特征增加一列x0为1\n    dataMat = np.column_stack((np.ones(m), dataMat))\n    # 交叉验证:将数据打乱\n    rndidx = np.arange(m)\n    np.random.shuffle(rndidx)\n    shuffledX = []\n    shuffledy = []\n    for i in range(m):\n        shuffledX.append(dataMat[rndidx[i]])\n        shuffledy.append(labelMat[rndidx[i]])\n    dataMat, labelMat = np.array(shuffledX), np.array(shuffledy)\n    X, y = np.array(dataMat), np.array(labelMat)\n    mTrain = int(0.75*m)\n    mTest = m-mTrain\n    # 获取前mTrain个数据做训练数据,用于训练模型\n    Xtrain, ytrain = np.array(dataMat[:mTrain]), np.array(labelMat[:mTrain])\n    # 获取后mTest个数据做测试数据,用于测试预测准确率\n    Xtest, ytest = np.array(dataMat[-mTest:]), np.array(labelMat[-mTest:])\n    # 画布大小\n    plt.figure(figsize=(13, 6))\n    # 使用BGD批量梯度下降法\n    theta = batch_gradient_desc(Xtrain, ytrain)\n    # 拿测试数据放入模型,计算预测准确率\n    bgd_acc = accuracy_rate(Xtest, ytest, theta)\n    # 绘图\n    plotBestFit(theta, X, y, 'BGD(Batch Gradient Descent) accuracy %.2f' %\n                bgd_acc, 121)\n    # 使用SGD随机梯度下降法\n    theta = stochastic_gradient_desc(Xtrain, ytrain)\n    # 拿测试数据放入模型,计算预测准确率\n    sgd_acc = accuracy_rate(Xtest, ytest, theta)\n    # 绘图\n    plotBestFit(theta, X, y, 'SGD(Stochastic Gradient Descent) accuracy %.2f' %\n                sgd_acc, 122)\n    plt.show()\n\n```\n上述代码文件[下载](LogisticRegression.py)\n### Logistic回归用于多分类(one-vs-all)介绍\n这个思想很容易理解,我们假设现在是三分类问题,如下图\n{% asset_img 2019052823285119.png %}\n现在有一个数据集,有3个类别,我们要做的是选定其中一个类别数据作为正类$y=1$,其余类别作为负类,这样就转换为二分类问题了,这正是Logistic回归擅长处理的问题,得到这个模型,记为$h_\\theta^{(1)}(x)$,接着选另一类作为正类$(y=1)$,将其余类作为负类,同样用Logistic回归处理,得到模型,记为$h_\\theta^{(2)}(x)$,最后同样处理得到$h_\\theta^{(3)}(x)$\n这样,我们就得到了一系列模型简记为:$h_\\theta^{(i)}=p(y=i|x;\\theta)$其中$i=(1,2,\\cdots,k)$(k表示共有k个类)\n{% asset_img 2019052823372620.png %}\n如上图所示,我们每幅图都选中了一个类别作为正类(为该类原本的形状),其余类别作为负类(为圆形)\n现在要做的就是在这些分类器里,输入$x$,然后选择一个让$h_\\theta^{(i)}(x)$最大的$i$,即$max_i{h_\\theta^{(i)}}\n$\n### Logistic回归用于多分类(one-vs-all)程序运行结果\n以下是自己写的logistic回归实现的多分类方法和使用`sklearn.linear_model`中的`LogisticRegression`来进行对比\n{% asset_img 2019052917235832.png %}\n左边为原始数据展示,中间为自己实现的one-vs-all多分类测试结果,右边为sklearn测试结果,预测率一模一样,在调试过程中,两个预测准确率的精度连小数都是一模一样,有时候自己的反而更高,说明方法应该没有差错\n### Logistic回归用于多分类(one-vs-all)代码实现\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import make_classification\n\ndef loadDataSet_iris():\n    \"\"\"数据集生成\n\n    Returns:\n        array -- 数据集\n        array -- 标签集\n    \"\"\"\n    dataMat, labelMat = load_iris(return_X_y=True)  # 多分类(3类)\n    #dataMat, labelMat = dataMat[:100],labelMat[:100] # 二分类\n    n = len(dataMat[0])\n    return dataMat, labelMat\n\n\ndef sigmoid(z):\n    return 1 / (1+np.exp(-z))\n\n\ndef hypo(x, theta):\n    \"\"\"假设函数(hypothesis function)\n\n    Arguments:\n        theta {array} -- 参数值\n        x {array} -- 一个数据或数据集\n\n    Returns:\n        array -- 通过假设函数得到的预测值\n    \"\"\"\n    return sigmoid(np.dot(x, theta))\n\n\ndef partial_jcost(theta, X, y, x):\n    \"\"\"J代价函数关于theta的偏导数\n\n    Arguments:\n        theta {array} -- 参数值\n        X {array} -- 数据集\n        y {array} -- 标签集\n        x {array} -- 一个数据或数据集\n\n    Returns:\n        array -- 偏导值\n    \"\"\"\n    return np.dot((hypo(X, theta)-y), x)\n\n\ndef predict(X, hDict):\n    \"\"\"预测结果\n\n    Arguments:\n        X {array} -- 测试数据集\n        hDict {dict} -- 存储每个类别的参数值\n\n    Returns:\n        array -- 返回预测类别结果\n    \"\"\"\n    m = len(X)\n    y_predict = []\n    for i in range(m):\n        maxPro = -1.0\n        maxLabel = 0\n        for key in hDict.keys():\n            theta = hDict[key]\n            pro = hypo(X[i], theta)\n            if pro > maxPro:\n                maxPro = pro\n                maxLabel = key\n        y_predict.append(maxLabel)\n    return y_predict\n\n\ndef accuracy_rate(X, y, hDict):\n    \"\"\"计算预测准确率\n\n    Arguments:\n        X {array} -- 测试数据集\n        y {array} -- 已知测试结果\n        hDict {dict} -- 存储每个类别的参数值\n\n    Returns:\n        float -- 准确率\n    \"\"\"\n    m, n = len(X), len(X[0])\n    y_predict = predict(X, hDict)\n    true_count = np.sum(y_predict == y)\n    return float(true_count)/m\n\n\ndef batch_gradient_desc(X, y, alpha=0.01, numIterations=500):\n    \"\"\"BGD下降法(批量梯度下降法)\n\n    Arguments:\n        X {array} -- 数据集\n        y {array} -- 标签集\n\n    Keyword Arguments:\n        alpha {float} -- 步长(学习率) (default: {0.01})\n        numIterations {int} -- 迭代次数 (default: {500})\n\n    Returns:\n        array -- 返回参数值\n    \"\"\"\n    # 获取特征数n\n    n = len(X[0])\n    theta = np.ones(n)\n    # 批量梯度下降法\n    for i in range(numIterations):\n        theta = theta-alpha*partial_jcost(theta, X, y, X)\n    return theta\n\n\ndef stochastic_gradient_desc(X, y, alpha=0.01, numIterations=100):\n    \"\"\"SGD下降法(随机梯度下降法)\n\n    Arguments:\n        X {array} -- 数据集\n        y {array} -- 标签集\n\n    Keyword Arguments:\n        alpha {float} -- 步长(学习率) (default: {0.01})\n        numIterations {int} -- 迭代次数 (default: {100})\n\n    Returns:\n        array -- 返回参数值\n    \"\"\"\n    # 获取样本数m,特征数n\n    m, n = len(X), len(X[0])\n    theta = np.ones(n)\n    # 随机梯度下降法\n    for k in range(numIterations):\n        dataIndex = range(m)\n        for i in range(m):\n            alpha = 4/(1.0+k+i)+0.01\n            randIndex = int(np.random.uniform(0,len(dataIndex)))\n            theta=theta-alpha*np.multiply(hypo(X[randIndex], theta)-y[randIndex],X[randIndex])\n    return theta\n\n\ndef auto_norm(X):\n    \"\"\"特征归一化(或特征缩放)\n\n    Arguments:\n        X {array} -- 数据集\n\n    Returns:\n        array -- 返回归一化后的数据集\n    \"\"\"\n    X = np.array(X)\n    minVals = X.min(0)\n    maxVals = X.max(0)\n    newVals = (X-minVals)/(maxVals-minVals)\n    return newVals\n\n\ndef plotBestFit(dataMat, labelMat, title='Gradient Descent', subplt=111):\n    \"\"\"绘制图像\n\n    Arguments:\n        theta {array} -- 参数列表\n        dataMat {array} -- 样本集\n        labelMat {array} -- 标签集\n\n    Keyword Arguments:\n        title {str} -- 图像标题 (default: {'Gradient Descent'})\n        subplt {int} -- 子图 (default: {111})\n    \"\"\"\n    dataDict = {}\n    # 分类绘图\n    for i in range(len(dataMat)):\n        if labelMat[i] not in dataDict:\n            dataDict[labelMat[i]]=[]\n        dataDict[labelMat[i]].append(dataMat[i])\n    plt.subplot(subplt)\n    for y in dataDict.keys():\n        X=np.array(dataDict[y])\n        plt.scatter(X[:,1], X[:,2], s=30, label=y)\n    plt.xlabel('X1')\n    plt.ylabel('X2')\n    plt.title(title)\n    plt.legend()\n\n\ndef train(X, y):\n    \"\"\"训练模型,获取每个类别的参数值\n    \n    Arguments:\n        X {array} -- 输入训练数据集\n        y {array} -- 输入训练标签集\n    \n    Returns:\n        dict -- 返回每个类别的参数值\n    \"\"\"\n    dataDict = {}\n    m = len(y)\n    for i in range(m):\n        if y[i] not in dataDict.keys():\n            dataDict[y[i]] = []\n        dataDict[y[i]].append(X[i])\n    hDict = {}\n\n    for key in dataDict.keys():\n        dataX = []\n        dataY = []\n        for k, v in dataDict.items():\n            dataX.extend(v)\n            if k == key:\n                dataY.extend(np.ones(len(v)))\n            else:\n                dataY.extend(np.zeros(len(v)))\n        theta = batch_gradient_desc(dataX, dataY)\n        hDict[key] = theta\n    return hDict\n\n\nif __name__ == \"__main__\":\n    # 读取数据集,标签集\n    dataMat, labelMat = loadDataSet_iris()\n    m = len(dataMat)\n    # 特征归一化(特征缩放)\n    dataMat[:, :] = auto_norm(dataMat[:, :])\n    # 所有数据的特征增加一列x0为1\n    dataMat = np.column_stack((np.ones(m), dataMat))\n    # 交叉验证:将数据打乱\n    rndidx = np.arange(m)\n    np.random.shuffle(rndidx)\n    shuffledX = []\n    shuffledy = []\n    for i in range(m):\n        shuffledX.append(dataMat[rndidx[i]])\n        shuffledy.append(labelMat[rndidx[i]])\n    dataMat, labelMat = np.array(shuffledX), np.array(shuffledy)\n    X, y = np.array(dataMat), np.array(labelMat)\n    mTrain = int(0.75*m)\n    mTest = m-mTrain\n    # 获取前mTrain个数据做训练数据,用于训练模型\n    Xtrain, ytrain = np.array(dataMat[:mTrain]), np.array(labelMat[:mTrain])\n    # 获取后mTest个数据做测试数据,用于测试预测准确率\n    Xtest, ytest = np.array(dataMat[-mTest:]), np.array(labelMat[-mTest:])\n    # 画布大小\n    plt.figure(figsize=(13, 6))\n\n    # 显示原始数据\n    plotBestFit(X, y, 'Original Data', 131)\n\n    # 自己实现的LogisticRegression预测\n    hDict = train(Xtrain, ytrain)\n    y_predict = predict(Xtest, hDict)\n    sgd_acc = accuracy_rate(Xtest, ytest, hDict)\n    plotBestFit(Xtest, y_predict, 'Prediction Data (accuracy %.2f)' %\n                sgd_acc, 132)\n\n    # sklearn LogisticRegression预测\n    clf = LogisticRegression(random_state=0, solver='lbfgs',\n                             multi_class='multinomial').fit(X, y)\n    clf.fit(Xtrain[:, 1:], ytrain)   # sklearn不需要前面的x0,直接放入特征即可\n    y_predict = clf.predict(Xtest[:, 1:])\n    sgd_acc = np.sum(y_predict == ytest)/len(ytest) # 直接用clf.score()也可以\n    plotBestFit(Xtest, y_predict,\n                'Sklearn Prediction Data (accuracy {0:.2f})'.format(sgd_acc), 133)\n\n    plt.show()\n\n```\n\n代码文件[下载](LogisticRegression(one-vs-all).py)\n### 数学补充\n- **梯度**\n    **定义**:一个标量函数$f(x_1,x_2,\\cdots,x_n)$的梯度(或梯度向量场)被表示为$\\nabla f$或者$\\overrightarrow{\\nabla}f$,$\\nabla$表示 **微分算子**.标记$gradf$也被称为梯度.$f$的梯度被定义为唯一的向量场，它与任意单位向量$\\mathbf e$在每一点$x$上的点积是$f$沿着$\\mathbf e$的方向导数,即\n    $$\\begin{aligned}\n        (\\nabla{f(x)})\\cdot\\mathbf v=D_{\\mathbf v}{f(x)}\\quad (4)\\\\\n    \\end{aligned}\n    $$\n    > **注释**: $D_\\mathbf{v}{f(x)}$表示$f(x)$沿着向量\n    > $\\mathbf v$方向的方向导数,例如:\n    > 假设在三维空间$\\mathbf v=(\\cos\\alpha,\\cos\\beta,\\cos\\gamma)$,则(4)为\n    >$$\\begin{aligned}\n    >    (\\nabla{f(x)})\\cdot\\mathbf v&=(\\frac{\\partial f}{\\partial x},\\frac{\\partial f}{\\partial y},\\frac{\\partial f}{\\partial z})\\cdot{(\\cos\\alpha,\\cos\\beta,\\cos\\gamma)}\\\\\n    >    &=\\frac{\\partial f}{\\partial x}\\cos\\alpha+\\frac{\\partial f}{\\partial y}\\cos\\beta+\\frac{\\partial f}{\\partial z}\\cos\\gamma\\\\\n    >    &=D_{\\mathbf v}f(x)\n    >\\end{aligned}\n    >$$\n    **常用**:\n    1. $\\nabla f=gradf=\\frac{\\partial f}{\\partial x}\\overrightarrow{\\mathbf{e_x}}+\\frac{\\partial f}{\\partial y}\\overrightarrow{\\mathbf{e_y}}+\\frac{\\partial f}{\\partial z}\\overrightarrow{\\mathbf{e_z}}$\n    2. 线性特性:\n       1. $\\nabla (\\alpha f+\\beta g)(a)=\\alpha \\nabla f(a)+\\beta \\nabla g(a)$\n    3. 乘积特性:\n       1. $\\nabla (fg)(a)=f(a)\\nabla g(a)+g(a)\\nabla f(a)$\n\n- **向量微分算子**\n    **定义**: $\\nabla$被称为向量微分算子,在坐标$(x_1,x_2,\\cdots,x_n)$和标准基为$\\{\\overrightarrow{\\mathbf e_1},\\cdots,\\overrightarrow{\\mathbf e_n}\\}$的笛卡尔坐标系$\\mathbf R^n$中,$\\nabla$被按照偏导数的方式定义为\n    $$\\begin{aligned}\n        \\nabla=\\sum_{i=1}^n{\\overrightarrow{\\mathbf e_i}\\frac{\\partial}{\\partial x_i}=(\\frac{\\partial}{\\partial{x_1}},\\cdots,\\frac{\\partial}{\\partial{x_n}})}\\tag 5\n    \\end{aligned}\n    $$\n    在坐标$(x,y,z)$和标准基为$\\{\\overrightarrow{\\mathbf{e_x}},\\overrightarrow{\\mathbf{e_y}},\\overrightarrow{\\mathbf{e_z}}\\}$的三维笛卡尔坐标系$\\mathbf R^3$下,$\\nabla$被写成\n    $$\\begin{aligned}\n        \\nabla=\\overrightarrow{\\mathbf e_x}\\frac{\\partial}{\\partial{x}}+\\overrightarrow{\\mathbf e_y}\\frac{\\partial}{\\partial{y}}+\\overrightarrow{\\mathbf e_z}\\frac{\\partial}{\\partial{z}}=(\\frac{\\partial}{\\partial x},\\frac{\\partial}{\\partial y},\\frac{\\partial}{\\partial z})\\tag 6\n    \\end{aligned}\n    $$\n    **常用**:\n    1. Laplacian算子$\\Delta$(标量操作符): $\\Delta=\\frac{\\partial^2}{\\partial x^2}+\\frac{\\partial^2}{\\partial y^2}+\\frac{\\partial^2}{\\partial z^2}=\\nabla\\cdot\\nabla=\\nabla^2$\n    2. 乘积规则:\n        {% asset_img 201905252315313.png %}\n\n### 参考\n1. [<<概率论与数理统计>>第7章 参数估计](https://book.douban.com/subject/3165271/)\n2. [Python3《机器学习实战》学习笔记（六）：Logistic回归基础篇之梯度上升算法](https://blog.csdn.net/c406495762/article/details/77723333)\n3. [wiki Gradient](https://en.wikipedia.org/wiki/Gradient)\n4. [梯度上升与梯度下降](https://www.cnblogs.com/hitwhhw09/p/4715030.html#commentform)\n5. [梯度下降（Gradient Descent）小结](https://www.cnblogs.com/pinard/p/5970503.html)\n6. [吴恩达机器学习个人笔记](#)","tags":["机器学习实战"],"categories":["机器学习"]},{"title":"[hdoj]1010.Tempter of the Bone","url":"%2Fposts%2F5533e030%2F","content":"{% asset_img 2019052403501231.png %}\n\n### 题目含义\n在$N$行$M$列的迷宫中,若能从起点到达终点(不能重复走已经走过的路)并恰好用$T$步到达,则输出$YES$,否则$NO$.($1\\lt N,M\\lt 7;\\ 0\\lt T\\lt 50$)\n'$X$':墙,不能通过, '$S$':起点, '$D$':终点, '.':空地\n### 解题思路(dfs(回溯)+剪枝)\n剪枝:即把不会产生答案的,或不必要的枝条\"剪掉\"\n\n剪枝(1):确定上下界,可以走的最少步>T||可以走的最少步<T,这两种情况都不可能,需要剪掉\n剪枝(2):有限步内,其它走法(绕道)到达终点要比曼哈顿距离多走偶数步\n\n\n> **曼哈顿距离** :图中红线代表曼哈顿距离，绿色代表欧氏距离，也就是直线距离，而蓝色和黄色代表等价的曼哈顿距离。\n> {% asset_img 2019052404001232.jpg %}\n\n\n### 代码实现 \n```cpp\n#include <cstdio>\n#include <cstdlib>\n#include <cstring>\n#define MAXN 10\nchar maze[MAXN][MAXN];\nint dx[4] = { -1,1,0,0 }, dy[4] = { 0,0,-1,1 };\nint N, M, T, sx, sy, ex, ey;\nbool dfs(int x, int y, int t)\n{\n    int nx, ny;\n    if (x < 0 || x >= N || y < 0 || y >= M) return false;\n    if (t == T && x == ex && y == ey)\n        return true;\n    int tmp = (T - t) - abs(x - ex) + abs(y - ey);\n    if (tmp < 0 || tmp&1)// 剪枝(1):可以走的最少步<T,剪枝(2):其它走法(绕道)到达终点要比曼哈顿距离多走偶数步\n        return false;\n    for (int i = 0; i < 4; i++)\n    {\n        nx = x + dx[i];\n        ny = y + dy[i];\n        if (maze[nx][ny] != 'X')\n        {\n            maze[nx][ny] = 'X';\n            if (dfs(nx, ny, t + 1)) return true;\n            maze[nx][ny] = '.';\n        }\n    }\n    return false;\n}\nint main()\n{\n#ifdef _L\n    read_from_file\n#endif\n    while (~scanf(\"%d%d%d\", &N, &M, &T))\n    {\n        getchar();\n        if (!N && !M && !T) break;\n        int wall = 0;\n        memset(maze, 0, sizeof(maze));\n        for (int i = 0; i < N; i++)\n        {\n            for (int j = 0; j < M; j++)\n            {\n                scanf(\"%c\", &maze[i][j]);\n                if (maze[i][j] == 'S')\n                    sx = i, sy = j;\n                else if (maze[i][j] == 'X')\n                    wall++;\n                else if (maze[i][j] == 'D')\n                    ex = i, ey = j;\n            }\n            getchar();\n        }\n        if (N*M - wall <= T)    // 剪枝(1),可以走的最多步都<T\n            printf(\"NO\\n\");\n        else\n        {\n            maze[sx][sy] = 'X';\n            if (dfs(sx, sy, 0))\n                printf(\"YES\\n\");\n            else printf(\"NO\\n\");\n        }\n    }\n    return 0;\n}\n```","tags":["回溯"],"categories":["OJ"]},{"title":"[algorithm]常用基础","url":"%2Fposts%2F72dd6e90%2F","content":"{% asset_img  %}\n\n### 数论\n1. **质数**\n   - 素数判断\n        ```cpp\n        bool is_prime(int n)\n        {\n            if (n==1) return 0;\n            int sqt = sqrt(n);\n            for (i=2;i<=sqt;++i)\n                if (n%i == 0) return 0;\n            return 1;\n        }\n        ```\n   - 获取所有n以内的素数(Eratosthenes筛选法求解质数)\n        ```cpp\n        vector<int> allPrimes(int n)\n        {\n            vector<int> primes(n+1,1);\n            int sqt=sqrt(n),i=0,j=0;\n            for(i=2;i<=sqt;++i)\n            {\n                if (primes[i])\n                {\n                    for(j=2*i;j<=n;j+=i)\n                        primes[j]=0;\n                }\n            }\n            return primes;\n        }\n        ```\n        下图为Eratosthenes筛选法求解质数的过程\n        {% asset_img 20190108153428899.gif %}","tags":["基础算法"],"categories":["OJ"]},{"title":"[leetcode]279.完全平方数","url":"%2Fposts%2F9176b286%2F","content":"{% asset_img 2019052413473237.png %}\n\n### 解题思路 \n假设dp[i]表示和为i的最少完全平方数.\n那么就有:\n$$\\begin{aligned}\n    dp[i]=min(dp[i],dp[i-j*j]+1), \\quad j=1,2,\\cdots,i-j*j\n\\end{aligned}\n$$\n$min$中的+1表示要加最后$j*j$这个数,也就是$i-j*j+j*j=i$,完全平方数的和才能为$i$\n这个式子是什么意思呢?\n比如n=10,那么\ndp[10]=min(dp[10],dp[10-1]+1), j=1, 要求出dp[9]\ndp[10]=min(dp[10],dp[10-4]+1), j=2, 要求出dp[6]\ndp[10]=min(dp[10],dp[10-9]+1), j=3, 要求出dp[1]\n求dp[9]=min(dp[9],dp[9-1]+1), j=1, 要求出dp[8]\n求dp[9]=min(dp[9],dp[9-4]+1), j=2, 要求出dp[5]\n求dp[9]=min(dp[9],dp[9-9]+1), j=3, 要求出dp[0]\n同样求dp[8],dp[5],dp[0]...\n因此,我们可以顺序求出dp[0],dp[1],...,dp[n]即可\n初始条件:dp[i]=i,每个数都可以化为i个1相加,这样组成和的完全平方数的个数最多\n边界条件:dp[0]=0,0没有完全平方数,所以个数是0\n\n### 代码实现 \n```cpp\n    int numSquares(int n) {\n        //利用动态规划 定义长度为n+1的数组 对应索引所对应的数装最少的完全平方数\n        vector<int> dp(n+1,0);\n        dp[0] = 0;  // 边界条件,0没有完全平方数\n        for (int i=1;i<=n;++i)\n        {\n            dp[i]=i;    // i个1,即初始为最多完全平方数个数\n            for (int j=1; i-j*j>=0;++j)\n                dp[i]=min(dp[i],dp[i-j*j]+1);\n        }\n        return dp[n];\n```\n时间复杂度$O(n^2)$","tags":["dp"],"categories":["OJ"]},{"title":"[leetcode]174.地下城游戏","url":"%2Fposts%2F523ea935%2F","content":"{% asset_img 2019052120381514.png %}\n\n### 题目含义\n玩过魔塔游戏的人都知道,假设你是个`肉盾`(不具备攻击力),如果你的`生命值`$HP$比恶魔的`攻击力`$DPS$高,那么必然可以扛过去,那么最后剩余的$HP_{remain}=HP-DPS$,只有当$HP_{remain}>0$时,你才能继续游戏.不过地图中也有`补给`(血瓶)可以加血,可以给自己补充`生命值`(活着才能喝)\n那么这道题的意思就是让你计算你在起点处(左上角第一个方格)的初始生命值最少为多少时,可以一路 `砍怪升(tao)级(ming)` 不**Game Over**($HP\\le 0$)的情况下找到公主(右下角第一个方格).\n\n### 解题思路 \n经过上面的抽象解释含义,大家应该对题目有所理解了\n- **首先,我们考虑从`左上至右下`的方法**\n    如果用dp[i][j]来表示骑士从起点(0,0)到达(i,j)所需的最少血量的话,那么我们就需要掌握两个因素,一个是骑士到达每一格时的血量是否比这一路来的最少血量还要少,因此要记录两个量,比较麻烦,比如dp[1][1]的最少血量如何计算呢?得知道上方一格dp[0][1]的值和最少血量,还得知道左边一个dp[1][0]的值和最少血量,然后算算min(dp[0][1]+dp[1][1],dp[0][1]处的最少血量),min(dp[1][0]+dp[1][1],dp[1][0]处的最少血量)...**我们换种思路(做题时如果发现一种思路不行一定要换个方向思考)**\n- **我们考虑从`右下至左上`的方法**\n    如果用dp[i][j]来表示骑士从点(i,j)到达终点(m-1,n-1)所需的最少血量的话,那么我们就可以倒推了,那么我们假设骑士已经到达了终点(m-1,n-1),我们来计算骑士在终点时所需的血量dp[m-1][n-1]=max(1,1-dungeon[m-1][n-1]),我们来详细解释一下这个公式的含义:\n    $$dp[m-1][n-1]=max(1,1-dungeon[m-1][n-1])\\quad (1)$$\n    骑士到达终点时,如果有恶魔那么血量就得为$HP=1-dungeon[m-1][n-1]$才能救出公主,如果没有恶魔那么骑士在终点处只要活着就行($HP=1$),再说其他情况,先说骑士不在右边缘和下边缘的情况,当骑士在(i,j)时,那么所需的血量为$dp[i][j]=max(1,min(dp[i+1][j],dp[i][j+1])-dungeon[i][j])$,这个式子有点长,我们分解一下:\n    $$\n    \\begin{aligned}\n        need &= min(dp[i+1][j],dp[i][j+1])-dungeon[i][j]\\quad &(2)\\\\\n        dp[i][j] &= max(1,need)\\quad                          &(3)\n    \\end{aligned}\n    $$\n    公式(1)中$need$表示所需的血量,要想知道骑士在$(i,j)$位置的血量,就需要知道骑士在$(i+1,j)$和$(i,j+1)$位置处的血量为多少,然后选择最小的来减去(i,j)处的值,选最小的血量(正)是为了减去$dungeon[i][j]$(有恶魔为负)保证计算得到一个较小的血量,这样就可以保证最后计算到起点时是最少的血量,而公式(2)是为了保证骑士必须活着($HP\\ge 1$),你可能会想:难道$need\\le 0$吗?是的,如果$dungeon[i][j]$处不是恶魔,而是血瓶($HP+9999999$)那么得到的$need$必然为一个负值,因此,这样一来就把骑士从$(i,j)$到终点$(m-1,n-1)$处所需的血量计算出来了,只需一步一步倒退到起点$(0,0)$即可计算出$dp[0][0]$的值,也就是骑士在$(0,0)$到终点$(m-1,n-1)$所需的最少血量.\n    哦!对了,差点忘了,还有边缘没分析,边缘情况与上面情况相同仍然可以用上面的公式(2)(3),但有一些特殊,如果骑士在右边缘,那么(2)的$min(dp[i+1][j],dp[i][j+1])$就要改为$min(dp[i+1][j],MAX\\_INT)$了,因为骑士如果越界那么必然不可能,那么可以默认骑士在墙处的生命值为极大$MAX\\_INT$,公式即为\n    $$\n    \\begin{aligned}\n        need &= min(dp[i+1][j],INT\\_MAX)-dungeon[i][j]\\quad &(4)\\\\\n        dp[i][j] &= max(1,need)\\quad                       &(5)\n    \\end{aligned}\n    $$\n    同样,如果骑士在下边缘,公式即为\n    $$\n    \\begin{aligned}\n        need &= min(dp[i][j+1],INT\\_MAX)-dungeon[i][j]\\quad &(6)\\\\\n        dp[i][j] &= max(1,need)\\quad                       &(7)\n    \\end{aligned}\n    $$\n    经过以上分析,我们就可以开始写代码了\n### 代码实现 \n- **步骤清晰的代码**\n  ```cpp\n    int calculateMinimumHP(vector<vector<int>>& dungeon) {\n        int m=dungeon.size(),n=dungeon[0].size(),i=0,j=0,need=0;\n        vector<vector<int>> dp(m,vector<int>(n,0));\n        // 计算骑士在终点时的HP\n        dp[m-1][n-1]=max(1,1-dungeon[m-1][n-1]);\n        // 计算骑士在右边缘时的HP\n        for(i=m-2,j=n-1;i>=0;--i)\n        {\n            need=min(dp[i+1][j],INT_MAX)-dungeon[i][j];\n            dp[i][j]=max(1,need);\n        }\n        // 计算骑士在下边缘时的HP\n        for(i=m-1,j=n-2;j>=0;--j)\n        {\n            need=min(dp[i][j+1],INT_MAX)-dungeon[i][j];\n            dp[i][j]=max(1,need);\n        }\n        // 计算骑士在内部(包括左边缘和上边缘)时的HP\n        for(i=m-2;i>=0;--i)\n        {\n            for(j=n-2;j>=0;--j)\n            {\n                need=min(dp[i+1][j],dp[i][j+1])-dungeon[i][j];\n                dp[i][j]=max(1,need);\n            }\n        }\n        return dp[0][0];\n    }\n  ```\n- **精炼后的代码**\n  ```cpp\n    int calculateMinimumHP(vector<vector<int>>& dungeon) {\n        // dp[i][j]表示从(i,j)到终点(m-1,n-1)需要的最少生命值\n        // 骑士在外圈时需要生命值INT_MAX(因为越界)\n        // 倒推,从终点到起点来算,每一个位置上,骑士需要的最少生命值为min(下方位置最少生命值,右方位置最少生命值)-当前位置要消耗多少生命值\n        int m=dungeon.size(),n=dungeon[0].size(),i=0,j=0;\n        vector<vector<int>> dp(m+1,vector<int>(n+1,INT_MAX));\n        dp[m-1][n]=dp[m][n-1]=1;\n        for(i=m-1;i>=0;--i)\n        {\n            for(j=n-1;j>=0;--j)\n            {\n                int need = min(dp[i+1][j],dp[i][j+1]) - dungeon[i][j];    // 计算骑士从(i,j)处到终点(m-1,n-1)所需的最少血量\n                dp[i][j] = max(1,need); // 如果所需need为负表示血量远远足够,那么必须保证骑士在该点处时的生命值为1(活着)\n            }\n        }\n        return dp[0][0];\n    }\n  ```\n时间复杂度$O(m*n)$\n### 结论\n`动态规划`真(cao)棒(dan)!","tags":["dp"],"categories":["OJ"]},{"title":"[leetcode]动态规划Top down & Bottom up","url":"%2Fposts%2F25b36638%2F","content":"### 介绍\n**动态规划(DP:dynamic programming)**与**分治法**相似,都是通过组合子问题来求解原问题,但两者是有区别的\n- **分治法:** 将问题划分为互不相交的子问题,递归的求解子问题(过程中会反复进行计算,做许多不必要的工作),再把它们的解组合起来,求出原问题的解.\n- **动态规划:** 将问题划分为重叠的子问题,即不同的子问题具有公共的子子问题,对每个子子问题只求解一次(记忆性),不再进行重复的计算,避免了不必要的计算工作(核心:问题是否能用动态规划解决取决于这些`子问题`会不会被重复调用。)\n动态规划方法通常用来求解最优化问题(optimization problem).这类问题可以有许多可行解,每个解都有一个值,我们希望寻找最优值(最小值或最大值)的解,这样的解称为**一个最优解**,因为最优解可能有多个.\n\n### 动态规划算法基本步骤\n1. 刻画一个最优解特征的模型\n2. 递归定义最优解的值\n3. 计算一个最优解的值,一般用bottom up的方法\n4. 从已经计算完的信息中构造最优解\n\n### 经典的Rod Cutting问题\n有一根长度为$n$的铁棒和一个已知的铁棒长度对应价格表,现让你将铁棒进行切割,问可以获得的最大收益为多少?\n$$\n\\begin{array}{c|lcr}\n    length\\ i &1 &2 &3 &4 &5 &6 &7 &8 &9 &10\\\\\n    \\hline\n    price\\ p_i &1 &5 &8 &9 &10 &17 &17 &20 &24 &30 \n\\end{array}\n$$\n这里$i$表示铁棒长度,$p_i$表示对应长度的价格\n- 题目分析\n    由于铁棒价格表只有长度为1~10的铁棒价格,也就是说,如果n=11,那么直接卖是行的,要切割为n=1+10(其中一种方法),那么收益为1+30元\n    假设n=4,则可以划分的方式和收益如下图所示:\n    {% asset_img 2019052211443317.png %}\n    可以看出$p_2+p_2=5+5=10$这种划分方法收益最大(是一个最优解)\n    如果铁棒长度为$n$,那么我们可以划分铁棒的方法就有$2^{n-1}$种,你可能会惊讶怎么会有这么多种?因为对于铁棒的每一处位置我们都有2种选择,切割或不切割\n- 代码实现\n  - 法一: 递归 $O(^2)$\n    ```cpp\n    int CutRod(vector<int>& p, int n)\n    {\n        if (n == 0) return 0;\n        int maxR = INT_MIN;\n        for (int i = 1; i <= min(n,10); ++i)\n            maxR = max(maxR, p[i] + CutRod(p, n - i));\n        return maxR;\n    }\n    ```\n  - 法二: top-down(记忆化递归)\n    ```cpp\n    int MemoizedCutRodAux(vector<int>& p, int n, vector<int>& r)\n    {\n        int maxR = 0;\n        if (r[n] >= 0)\n            return r[n];\n        if (n == 0)\n            maxR = 0;\n        else\n        {\n            maxR = INT_MIN;\n            for (int i = 1; i <= min(n,10); ++i)\n                maxR = max(maxR, p[i] + MemoizedCutRodAux(p, n - i, r));\n        }\n        r[n] = maxR;\n        return maxR;\n    }\n    ```\n  - 法三: bottom-up $O(n^2)$\n    ```cpp\n    int BottomUpCutRod(vector<int>& p, int n)\n    {\n        vector<int> r(n + 1, 0);\n        for (int i = 1; i <= n; ++i)\n        {\n            int maxR = INT_MIN;\n            for (int j = 1; j <= i; ++j)\n                maxR = max(maxR, p[(j-1)%10+1] + r[i - j]);\n            r[i] = maxR;\n        }\n        return r[n];\n    }\n    ```\n### 相关\n[[leetcode]动态规划(Dynamic Programming)](https://brianyi.github.io/posts/26386)\n### 扩展阅读\n[常见的动态规划问题分析与求解](https://www.cnblogs.com/wuyuegb2312/p/3281264.html)\n[漫画：什么是动态规划？](http://www.sohu.com/a/153858619_466939)\n","tags":["dp"],"categories":["OJ"]},{"title":"[机器学习实战]第4章 基于概率论的分类方法:朴素贝叶斯","url":"%2Fposts%2F28f8d320%2F","content":"### 了解\nKNN分类算法和决策树分类算法最终都是预测出实例的确定的分类结果，但是，有时候分类器会产生错误结果；本章要学的朴素贝叶斯分类算法则是给出一个最优的猜测结果，同时给出猜测的概率估计值。\n### 先验概率,后验概率,全概率公式和贝叶斯公式\n> 一种癌症,得了这种癌症的人被检测出阳性的几率为90%,未得这种癌症的人被检测出阴性的几率为90%,而人群中得这种癌症的几率为1%,一个人被检测出阳性,问这个人得癌症的几率为多少?\n\n我们用$A$表示事件\"测出为阳性\",用$B_1$表示\"得癌症\",$B_2$表示\"未得癌症\".\n则可以得到:\n$$\n\\begin{aligned}\n&P(A|B_1)=0.9,\\ P(A|B_2)=1-0.9=0.1,\\ P(B_1)=0.01,\\ P(B_2)=1-0.01=0.99\\\\\n&P(A,B_1)=P(B_1)\\cdot P(A|B_1)=0.01\\times 0.9=0.009\\\\\n&P(A,B_2)=P(B_2)\\cdot P(A|B_2)=0.99\\times 0.1=0.099\n\\end{aligned}\n$$\n一个人被检测出阳性,且这个人得癌症的几率可被表示为$P(B_1|A)$,那么根据以上结果我们来进行计算:\n$$\n\\begin{aligned}\n    P(B_1|A)&=\\frac{P(B_1)\\cdot P(A|B_1)}{P(A)}=\\frac{P(B_1)\\cdot P(A|B_1)}{P(A,B_1)+P(A,B_2)}\\quad (1)\\\\\n    &=\\frac{0.01\\times 0.9}{0.009+0.099}=\\frac{0.009}{0.108}\\approx 0.083\n\\end{aligned}\n$$\n当然也很容易求得阳性未得癌症的概率$P(B_2|A)=1-P(B_1|A)=1-0.083=0.917$\n这里$P(B_1|A),P(B_2|A)$中都多了一竖线,被称为 **后验概率** ,即在$A$的情况下$B_1$(或$B_2$)发生的概率,后验概率就是一种**条件概率**(但反之不成立), 而$P(A),P(B_1),P(B_2)$就是 **先验概率** ,上述公式(1)中,分母$P(A)=P(A,B_1)+P(A,B_2)$为 **全概率公式**,整个公式(1)就是 **贝叶斯公式**\n- **后验概率与条件概率的区别**\n    后验概率就是一种条件概率,但是与其他条件概率的不同之处在于,它限定了目标事件为隐变量取值,而其中的条件为观测结果\n    一般的条件概率,条件和事件都可以是任意的\n    举例说明:\n    1). 如果我们出门之前我们听到新闻说今天路上出了个交通事故，那么我们想算一下堵车的概率，这个就叫做条件概率 。也就是P(堵车|交通事故)。这是==有因求果==\n    2). 如果我们已经出了门，然后遇到了堵车，那么我们想算一下堵车时由交通事故引起的概率有多大，那这个就叫做后验概率 （其实也是条件概率，但是通常习惯这么说） 。也就是P(交通事故|堵车)。这是==有果求因==\n- **全概率公式**\n    设$B_1,B_2,\\cdots,B_n$满足$\\displaystyle\\bigcup_{i=1}^n B_i=\\Omega,B_iB_j=\\empty(i\\neq j)$且$P(B_k)>0,k=1,2,\\cdots,n,$则对任意事件$A$有\n    $$\\begin{aligned}\n        P(A)=\\sum_{i=1}^n{P(A,B_i)}=\\sum_{i=1}^n{P(B_i)\\cdot P(A|B_i)}\\quad (2)\n    \\end{aligned}\n    $$\n    称满足$\\displaystyle\\bigcup_{i=1}^n{B_i}=\\Omega$和$B_iB_j=\\empty(i\\neq j)$的$B_1,B_2,\\cdots,B_n$为$\\Omega$的一个完备事件组.\n- **贝叶斯公式**\n    设$B_1,B_2,\\cdots,B_n$满足$\\displaystyle\\bigcup_{i=1}^n B_i=\\Omega,B_iB_j=\\empty(i\\neq j)$且$P(A)>0,P(B_k)>0,k=1,2,\\cdots,n,$则\n    $$\\begin{aligned}\n        P(B_j|A)=\\frac{P(B_j)\\cdot P(A|B_j)}{\\sum_{i=1}^n P(B_i)\\cdot P(A|B_i)}, j=1,2,\\cdots,n.\\quad (3)\n    \\end{aligned}\n    $$\n### 朴素贝叶斯算法\n- **介绍**\n    朴素贝叶斯算法是基于贝叶斯定理与特征条件独立假设的分类方法.最为广泛的两种分类模型是决策树模型(Decision Tree)和朴素贝叶斯模型(Naive Bayesian Model,NBM).\n    朴素贝叶斯算法的使用的就是贝叶斯公式,我们将贝叶斯公式应用到机器学习领域即为:\n    $$\\begin{aligned}\n        P(class|feature)=\\frac{P(feature|class)\\cdot P(class)}{P(feature)}\\quad (4)\n    \\end{aligned}\n    $$\n    我们只需求出$P(class|feature)$来加以比较选出最大的即可\n    **朴素贝叶斯算法的朴素一词解释**\n    朴素(naive)也就是简单(天真)的意思,朴素贝叶斯算法的朴素:\n    1.假设各个特征之间相互独立\n    2.假设各个特征同等重要\n    那么就会有\n    $$\\begin{aligned}\n        P(A=a_i,B=b_i,C=c_i|D=d_i)&=P(A=a_i|D=d_i)\\cdot P(B=b_i|D=d_i)\\cdot P(C=c_i|D=d_i)\\quad &(5)\\\\\n        &\\neq P(A=a_i)\\cdot P(B=b_i)\\cdot P(C=c_i)\\quad &(6)\n    \\end{aligned}$$\n    注意:只是说特征$A,B,C$之间相互独立,而不包括D,那么公式(6)就不成立\n    如果没有朴素二字,那么特征之间不一定相互独立,则问题就会变得复杂起来,因为这3个特征的联合概率分布是3维空间,\n    {% asset_img 2019052217342720.png %}\n    现实生活中特征个数是很多的,每一个特征可以取的值也非常多,如果不假设特征之间相互独立的话,那么通过统计来估计概率的值变得几乎不可做(计算量太大)\n\n    ---\n    对于公式(4)在机器学习领域中如何应用,我们来看一个例子,如<span id='jump4-1'>图4-1</span>所示\n    {% asset_img v2-8b7031854b7c8r.jpg %}\n    <p style='text-align:center'>图4-1</p>\n    \n    **现在给我们的问题是，如果一对男女朋友，男生想女生求婚，男生的四个特点分别是不帅，性格不好，身高矮，不上进，请你判断一下女生是嫁还是不嫁？**\n    这是一个典型的分类问题，**转为数学问题就是比较p(嫁|(不帅、性格不好、身高矮、不上进))与p(不嫁|(不帅、性格不好、身高矮、不上进))的概率**，谁的概率大，就能给出嫁或者不嫁的预测\n    这里我们联系到朴素贝叶斯公式:\n    {% asset_img 62ef7babb177ea_r.jpg %}\n    那么我们需要将等式右边分子的后验概率,先验概率和分母的先验概率求出来\n    {% asset_img 2019052218075422.png %}\n    \n\n\n- **朴素贝叶斯算法定义**\n    输入:训练数据$T={(\\mathbf x_1,\\mathbf y_1),(\\mathbf x_2,\\mathbf y_2),\\cdots,(\\mathbf x_N,\\mathbf y_N)}$,其中$\\mathbf x_i=(x_i^{(1)},x_i^{(2)},\\cdots,x_i^{(n)})^T$,$x_i^{(j)}$是第$i$个样本的第$j$个特征,$x_i^{(j)}\\isin{\\{a_{j1},a_{j2},\\cdots,a_{js_j}\\}}$,$a_{jl}$是第$j$个特征可能取的第$l$个值,$j=1,2,\\cdots,n,\\ l=1,2,\\cdots,S_j,\\ y_i\\isin{\\{c_1,c_2,\\cdots,c_K\\}}$;\n    输出:实例$x$的分类.\n    (1).计算先验概率及条件概率\n    $$\\begin{aligned}\n        &P(Y=c_k)=\\frac{\\sum_{i=1}^N{I(y_i=c_k)}}{N},\\ k=1,2,\\cdots,K&\\quad (7)\\\\\n        &P(X^{(j)}=a_{jl}|Y=c_k)=\\frac{\\sum_{i=1}^N{I(x_i^{(j)}=a_{jl},y_i=c_k)}}{\\sum_{i=1}^N{I(y_i=c_k)}}&\\quad (8)\\\\\n        &j=1,2,\\cdots,n;\\ l=1,2,\\cdots,S_j;\\ k=1,2,\\cdots,K\n    \\end{aligned}\n    $$\n    > 注释: $\\sum_{i=1}^N{I(y_i=c_k)}$表示统计一下类别为$c_k$的个数\n    > $\\sum_{i=1}^N{I(x_i^{(j)}=a_{jl},y_i=c_k)}$表示统计一下类别为$c_k$且第$j$个特征的特征值为$a_{jl}$的个数\n\n    (2).对于给定实例$\\mathbf x=(x^{(1)},x^{(2)},\\cdots,x^{(n)})^T$,计算\n    $$\\begin{aligned}\n        P(Y=c_k)\\prod_{j=1}^n{P(X^{(j)}=x^{(j)}|Y=c_k)},\\ k=1,2,\\cdots,K&\\quad (9)\n    \\end{aligned}\n    $$\n    (3).确定实例$\\mathbf x$的类别\n    $$\\begin{aligned}\n        y=\\argmax_{c_k}{P(Y=c_k)}\\prod_{j=1}^n{P(X^{(j)}=x^{(j)}|Y=c_k)}&&\\quad (10)\n    \\end{aligned}\n    $$\n    > 1. 大写字母表示矩阵(如$T$),小写字母粗体表示向量(如$\\mathbf {x_i}$),常规表示数值(如$x_i^{(1)},a_{j1},y_i,c_1$)\n    > 2. $\\mathbf x=(x_1,x_2,\\cdots,x_n)^T$,表示$x$为一个含有$n$个值的列向量,线性代数中一般如果没有特殊说明,都默认为是列向量\n    > 3. $\\mathbf {x_i}$表示第i个样本,每个样本都有n个特征,那么为了方便表示第几个样本的第几个特征,我们采用上标表特征,下标表样本的形式,即$x_i^{(5)}$表示第i个样本的第5个特征的值或特征值,这个$x_i^{(5)}$为什么不是粗体了呢?是因为它是一个数值而不是一个向量\n    > 4. 指示函数$I_A{(x)}$:指示函数是定义在某集合$X$上的函数,表示其中有哪些元素属于某一子集$A$,常用在集合论中.\n    集$X$的子集$A$的指示函数是函数$1_A: X\\to\\{0,1\\}$,定义为\n    $1_A(x)=\\begin{cases}\n        1 &if\\ x\\isin A\\\\\n        0 &if\\ x\\notin A\n    \\end{cases}$\n\n    ---\n    公式解析: \n    我们设$Y$为输出随机变量,表示类别的随机变量,$X$为输入随机变量,表示样本的随机变量,实际求的是$P(Y=c_k|X=x)$,即输入随机变量为样本$\\mathbf x$时,求出类别为$c_k$的概率,利用熟知的朴素贝叶斯公式即\n    $$\\begin{aligned}\n        P(Y=c_k|X=x)&=\\frac{P(X=x|Y=c_k)\\cdot P(Y=c_k)}{P(X=x)}&\\quad (11)\\\\\n        &=\\frac{P(Y=c_k)\\cdot \\prod_{j=1}^n{P(X^{(j)}=x^{(j)}|Y=c_k)}}{P(X=x)}&\\quad (12)\n    \\end{aligned}\n    $$\n    > 公式(12)成立的前提是朴素贝叶斯公式,也即各个特征之间相互独立\n\n    我们通过改变$Y$的值(即改变类别)来找到最大的概率,由于分母不随$Y$而变化,因此可以省去计算分母的过程,直接变为求下面公式(即上面公式(10))\n    $$\\begin{aligned}\n        y=\\argmax_{c_k}P(Y=c_k)\\prod_{j=1}^n{P(X^{(j)}=x^{(j)}|Y=c_k)}\n    \\end{aligned}\n    $$\n\n- **引入**\n    假设现在我们有一个数据集,由两类数据组成,如<span id='jump-4-2'>图4-2</span>所示\n    {% asset_img 2019052215350118.png %}\n    \n    <p style='text-align:center'>图4-2</p>\n    \n    我们用$p_1(x,y)$表示数据点$(x,y)$属于类别1(图中的圆)的概率,$p_2(x,y)$表示数据点$(x,y)$属于类别2(图中的三角形)的概率,现在有一个数据为(x,y),可以用下面规则来判断:\n    如果$p_1(x,y)>p_2(x,y)$,那么类别为1\n    如果$p_2(x,y)>p_1(x,y)$,那么类别为2\n    这就是贝叶斯决策理论的核心思想,即`具有最高概率的决策`\n\n### 实践1:使用朴素贝叶斯进行文档分类\n机器学习的一个重要应用就是文档的自动分类.在文档分类中整个文档(如一封电子邮件)是 **一个实例** ,而文档中的 **某些元素则构成特征** ,我们观察文档中出现的词,并把每个词的出现或者不出现作为一个特征,这样得到的特征数目就会跟词汇表中的词目一样多.`朴素贝叶斯是介绍贝叶斯分类器的一个扩展,是用于文档分类的常用算法`\n由统计学知,如果每个特征需要N个样本,那么10个特征将需要$N^{10}$个样本,对于包含1000个特征的词汇表将需要$N^{1000}$个样本,所需要的样本会随着特征数目增大而指数上升,如果特征之间相互独立,那么样本数就可以从$N^{1000}$减少到$1000\\times N$\n根据前面已有的知识,我们可以开始编写一个可以自动将侮辱性言论分类的贝叶斯分类器\n> 以在线社区为例,了不影响社区发展,我们要屏蔽侮辱性的言论,所以要构建一个快速过滤器,如果某条留言使用了负面或者侮辱性的语言,那么就将该留言标识为内容不当.过滤这类内容是一个很常见的需求,对此问题建立两个类别:侮辱类和费侮辱类,使用1和0分别标识\n\n如何判断一个文档是否是侮辱类的文档呢?\n1. **拆分文本，准备数据**\n    要从文本中获取特征，显然我们需要先拆分文本，这里的文本指的是来自文本的词条，每个词条是字符的任意组合。词条可以为单词，当然也可以是URL，IP地址或者其他任意字符串。将文本按照词条进行拆分，根据词条是否在词汇列表中出现，将文档组成成词条向量，向量的每个值为1或者0，其中1表示出现，0表示未出现。\n    接下来，以在线社区的留言为例。对于每一条留言进行预测分类，类别两种，侮辱性和非侮辱性，预测完成后，根据预测结果考虑屏蔽侮辱性言论，从而不影响社区发展。\n    ```python\n    #---------------------------从文本中构建词条向量-------------------------\n    #1 要从文本中获取特征，需要先拆分文本，这里特征是指来自文本的词条，每个词\n    #条是字符的任意组合。词条可以理解为单词，当然也可以是非单词词条，比如URL\n    #IP地址或者其他任意字符串 \n    #  将文本拆分成词条向量后，将每一个文本片段表示为一个词条向量，值为1表示出现\n    #在文档中，值为0表示词条未出现\n\n\n    #导入numpy\n    import numpy as np\n    import math\n\n    def loadDataSet():\n    #词条切分后的文档集合，列表每一行代表一个文档\n        postingList=[['my','dog','has','flea',\\\n                    'problems','help','please'],\n                    ['maybe','not','take','him',\\\n                    'to','dog','park','stupid'],\n                    ['my','dalmation','is','so','cute',\n                    'I','love','him'],\n                    ['stop','posting','stupid','worthless','garbage'],\n                    ['my','licks','ate','my','steak','how',\\\n                    'to','stop','him'],\n                    ['quit','buying','worthless','dog','food','stupid']]\n        #由人工标注的每篇文档的类标签\n        classVec=[0,1,0,1,0,1]\n        return postingList,classVec\n\n    #统计所有文档中出现的词条列表    \n    def createVocabList(dataSet):\n        #新建一个存放词条的集合\n        vocabSet=set([])\n        #遍历文档集合中的每一篇文档\n        for document in dataSet:\n            #将文档列表转为集合的形式，保证每个词条的唯一性\n            #然后与vocabSet取并集，向vocabSet中添加没有出现\n            #的新的词条        \n            vocabSet=vocabSet|set(document)\n        #再将集合转化为列表，便于接下来的处理\n        return list(vocabSet)\n\n    #根据词条列表中的词条是否在文档中出现(出现1，未出现0)，将文档转化为词条向量    \n    def setOfWords2Vec(vocabSet,inputSet):\n        #新建一个长度为vocabSet的列表，并且各维度元素初始化为0\n        returnVec=[0]*len(vocabSet)\n        #遍历文档中的每一个词条\n        for word in inputSet:\n            #如果词条在词条列表中出现\n            if word in vocabSet:\n                #通过列表获取当前word的索引(下标)\n                #将词条向量中的对应下标的项由0改为1\n                returnVec[vocabSet.index(word)]=1\n            else: print('the word: %s is not in my vocabulary! '%'word')\n        #返回inputet转化后的词条向量\n        return returnVec\n    ```\n    需要说明的是，上面函数creatVocabList得到的是所有文档中出现的词汇列表，列表中没有重复的单词，每个词是唯一的。\n\n2. **由词向量计算朴素贝叶斯用到的概率值**\n    这里，如果我们将之前的点$(x,y)$换成词条向量$\\mathbf w$(各维度的值由特征是否出现的0或1组成)，在这里词条向量的维度和词汇表长度相同。\n    $$p(c_i|\\mathbf w)=\\frac{p(\\mathbf w|c_i)\\cdot p(c_i)}{p(\\mathbf w)}$$\n    使用该公式计算文档词条向量属于各个类的概率,然后比较概率大小,从而预测出分类结果。\n    具体地，首先，可以通过统计各个类别的文档数目除以总得文档数目，计算出相应的$p(c_i)$；然后，基于条件独立性假设，将$\\mathbf w$展开为一个个的独立特征，那么就可以将上述公式写为$p(\\mathbf w|c_i)=p(w_0|c_i)\\cdot p(w_1|c_i)\\cdots p(w_N|c_i)$,这样就很容易计算，从而极大地简化了计算过程。\n    函数的伪代码为：\n    ```python\n    计算每个类别文档的数目\n    计算每个类别占总文档数目的比例\n    对每一篇文档：\n    　　对每一个类别：\n    　　　　如果词条出现在文档中->增加该词条的计数值 #统计每个类别中出现的词条的次数\n    　　　　增加所有词条的计数值 #统计每个类别的文档中出现的词条总数 \n    　　对每个类别：\n    　　　　将各个词条出现的次数除以类别中出现的总词条数目得到条件概率\n    返回每个类别各个词条的条件概率和每个类别所占的比例\n    ```\n    代码如下:\n    ```python\n    #训练算法，从词向量计算概率p(w0|ci)...及p(ci)\n    #@trainMatrix：由每篇文档的词条向量组成的文档矩阵\n    #@trainCategory:每篇文档的类标签组成的向量\n    def trainNB0(trainMatrix,trainCategory):\n        #获取文档矩阵中文档的数目\n        numTrainDocs=len(trainMatrix)\n        #获取词条向量的长度\n        numWords=len(trainMatrix[0])\n        #所有文档中属于类1所占的比例p(c=1)\n        pAbusive=sum(trainCategory)/float(numTrainDocs)\n        #创建一个长度为词条向量等长的列表\n        p0Num = np.ones(numWords)  # 用于统计Y=0(正常文档)对应的各个特征下特征值的数量总和,默认为全是1的向量而不是0,是为了避免后面累乘时为0\n        p1Num = np.ones(numWords)  # 用于统计Y=1(侮辱性文档)对应的各个特征下特征值的数量总和\n        p0Denom = 2.0 + sum(trainCategory==0) # 拉普拉斯平滑\n        p1Denom = 2.0 + sum(trainCategory) # 拉普拉斯平滑\n        #遍历每一篇文档的词条向量\n        for i in range(numTrainDocs):\n            #如果该词条向量对应的标签为1\n            if trainCategory[i]==1:\n                #统计所有类别为1的词条向量中各个词条出现的次数\n                p1Num+=trainMatrix[i]\n            else:\n                #统计所有类别为0的词条向量中各个词条出现的次数\n                p0Num+=trainMatrix[i]\n        #利用NumPy数组计算p(wi|c1)\n        p1Vect=np.log(p1Num/p1Denom)  #为避免下溢出问题，变成log()\n        #利用NumPy数组计算p(wi|c0)\n        p0Vect=np.log(p0Num/p0Denom)  #为避免下溢出问题，变成log()\n        return p0Vect,p1Vect,pAbusive\n    ```\n3. **针对算法的部分改进**\n    1. 计算概率时，需要计算多个概率的乘积以获得文档属于某个类别的概率，即计算$p(w_0|c_i)\\cdot p(w_1|c_i)\\cdots p(w_N|c_i)$，然后当其中任意一项的值为0，那么最后的乘积也为0.为降低这种影响，采用拉普拉斯平滑，在分子上添加a(一般为1)，分母上添加ka(k表示类别总数)，即在这里将所有词的出现数初始化为1，并将分母初始化为2*1=2\n        > 拉普拉斯平滑: 其实就是计算概率的时候，对于分子+1，避免出现概率为0。这样乘起来的时候，不至于因为某个量x，在观察样本库（训练集）中没有出现过，会导致整个实例的概率结果是0。在文本分类的问题中，当一个词语没有在训练样本中出现，该词语概率为0，使用连乘计算文本出现概率时也为0。这是不合理的，不能因为一个事件没有观察到就武断的认为该事件的概率是0。\n\n        ```python\n        p0Num = np.ones(numWords)  # 用于统计Y=0(正常文档)对应的各个特征下特征值的数量总和,默认为全是1的向量而不是0,是为了避免后面累乘时为0\n        p1Num = np.ones(numWords)  # 用于统计Y=1(侮辱性文档)对应的各个特征下特征值的数量总和\n        p0Denom = 2.0 + sum(trainCategory==0) # 拉普拉斯平滑\n        p1Denom = 2.0 + sum(trainCategory) # 拉普拉斯平滑\n        ```\n    2. 解决下溢出问题\n        正如上面所述，由于有太多很小的数相乘。计算概率时，由于大部分因子都非常小，最后相乘的结果四舍五入为0,造成下溢出或者得不到准确的结果，所以，我们可以对乘积取自然对数，在代数中有$ln(a\\cdot b)=lna+lnb$,于是通过求对数可以避免下溢出或者浮点数舍入导致的错误。同时采用自然对数处理不会有任何损失。\n        {% asset_img 2019052408342026.png %}\n        于是可以将原代码改为下面代码\n        ```python\n        p0Vect=log(p0Num/p0Denom);p1Vect=log(p1Num/p1Denom)\n        ```\n    下面是朴素贝叶斯分类函数的代码：\n    ```python\n    #朴素贝叶斯分类函数\n    #@vec2Classify:待测试分类的词条向量\n    #@p0Vec:类别0所有文档中各个词条出现的频数p(wi|c0)\n    #@p0Vec:类别1所有文档中各个词条出现的频数p(wi|c1)\n    #@pClass1:类别为1的文档占文档总数比例\n    def classifyNB(vec2Classify,p0Vec,p1Vec,pClass1):\n        #根据朴素贝叶斯分类函数分别计算待分类文档属于类1和类0的概率\n        p1=sum(vec2Classify*p1Vec)+np.log(pClass1)\n        p0=sum(vec2Classify*p0Vec)+np.log(1.0-pClass1)\n        if p1>p0:\n            return 1\n        else:\n            return 0\n\n    #分类测试整体函数        \n    def testingNB():\n        #由数据集获取文档矩阵和类标签向量\n        listOPosts,listClasses=loadDataSet()\n        #统计所有文档中出现的词条，存入词条列表\n        myVocabList=createVocabList(listOPosts)\n        #创建新的列表\n        trainMat=[]\n        for postinDoc in listOPosts:\n            #将每篇文档利用words2Vec函数转为词条向量，存入文档矩阵中\n            trainMat.append(setOfWords2Vec(myVocabList,postinDoc))\\\n        #将文档矩阵和类标签向量转为NumPy的数组形式，方便接下来的概率计算\n        #调用训练函数，得到相应概率值\n        p0V,p1V,pAb=trainNB0(array(trainMat),array(listClasses))\n        #测试文档\n        testEntry=['love','my','dalmation']\n        #将测试文档转为词条向量，并转为NumPy数组的形式\n        thisDoc=np.array(setOfWords2Vec(myVocabList,testEntry))\n        #利用贝叶斯分类函数对测试文档进行分类并打印\n        print(testEntry,'classified as:',classifyNB(thisDoc,p0V,p1V,pAb))\n        #第二个测试文档\n        testEntry1=['stupid','garbage']\n        #同样转为词条向量，并转为NumPy数组的形式\n        thisDoc1=np.array(setOfWords2Vec(myVocabList,testEntry1))\n        print(testEntry1,'classified as:',classifyNB(thisDoc1,p0V,p1V,pAb))\n    ```\n    这里需要补充一点，上面也提到了关于如何选取文档特征的方法，上面用到的是==词集模型==，`即对于一篇文档，将文档中是否出现某一词条作为特征，即特征只能为0不出现或者1出现`；然后，`一篇文档中词条的出现次数也可能具有重要的信息，于是我们可以采用`==词袋模型==，`在词袋向量中每个词可以出现多次，这样，在将文档转为向量时，每当遇到一个单词时，它会增加词向量中的对应值`\n    > **词集模型(Set Of Words)**： 单词构成的集合，集合自然每个元素都只有一个，也即词集中的每个单词都只有一个。\n    **词袋模型(Bag Of Words)**： 如果一个单词在文档中出现不止一次，并统计其出现的次数（频数）。\n\n    具体将文档转为词袋向量的代码为:\n    ```python\n    def bagOfWords2VecMN(vocabList,inputSet):\n    #词袋向量\n    returnVec=[0]*len(vocabList)\n    for word in inputSet:\n        if word in vocabList:\n            #某词每出现一次，次数加1\n            returnVec[vocabList.index(word)]+=1\n    return returnVec\n    ```\n    利用`词集模型`或`词袋模型`的程序运行结果(本例结果一样):\n    {% asset_img 2019052408541227.png %}\n\n### 实践2:使用朴素贝叶斯过滤垃圾邮件\n{% asset_img 2019052410014929.png %}\n- 准备数据:切分文本\n    由于前面讲述的例子中词向量是预先给定的(通过loadDataSet()),下面介绍如何从文本文档中构建自己的词列表\n    我们使用python中的正则表达式语法和内嵌语法,来实现一封完整的电子邮件的处理,数据集[下载email.zip](email.zip)\n    {% asset_img 2019052410581331.png %}\n    \n- 测试算法:使用朴素贝叶斯进行交叉验证\n  > **交叉验证**: 交叉验证是在机器学习建立模型和验证模型参数时常用的办法。交叉验证，顾名思义，就是重复的使用数据，把得到的样本数据进行切分，组合为不同的训练集和测试集，用训练集来训练模型，用测试集来评估模型预测的好坏。在此基础上可以得到多组不同的训练集和测试集，某次训练集中的某样本在下次可能成为测试集中的样本，即所谓“交叉”。　\n\n    下面将文本解析器集成到一个完整分类器中\n    ```python\n    def textParse(bigString):\n    \"\"\"解析邮件为单词列表\n    \n    Arguments:\n        bigString {str} -- 一封邮件内容\n    \"\"\"\n    # 导入正则表达式\n    import re\n    # 使用正则表达式\n    listOfTokens = re.split(r'\\W+', bigString)\n    # 将所有长度>2的单词小写并构成一个单词列表\n    return [tok.lower() for tok in listOfTokens if len(tok) > 2]\n\n    def spamTest():\n        \"\"\"\n        对贝叶斯垃圾邮件分类器进行自动化处理\n        选取40封邮件做训练数据集,10封邮件做测试数据集,计算分类器的错误率\n        \"\"\"\n        import random\n        docList = []; classList = []; fullText = []\n        # 遍历垃圾邮件,选取50封邮件(垃圾邮件,正常邮件各25封),穿插进行文本提取,标记类别\n        for i in range(1,26):\n            # 解析垃圾邮件为单词列表\n            wordList = textParse(open('email/spam/%d.txt' % i).read())\n            # 加入文档列表集中  \n            docList.append(wordList) \n            # 扩展到全文中   \n            fullText.extend(wordList)   \n            # 标记类别:垃圾邮件\n            classList.append(1)\n            # 解析正常邮件为单词列表\n            wordList = textParse(open('email/ham/%d.txt' % i, encoding='utf-8', errors='ignore').read())\n            # 加入文档列表集中\n            docList.append(wordList)\n            # 扩展到全文中\n            fullText.extend(wordList)\n            # 标记类别:正常邮件\n            classList.append(0)\n        # 创建词汇表\n        vocabList = createVocabList(docList)\n        # 随机生成50个数的列表\n        trainingSet = list(range(50)); testSet = []\n        # 随机选取10个邮件作为测试数据集\n        for i in range(10):\n            randIndex = int(random.uniform(0, len(trainingSet)))\n            testSet.append(trainingSet[randIndex])\n            del(trainingSet[randIndex])\n        trainMat = []; trainClasses = []\n        # 选取剩下的40个邮件作为训练数据集\n        for docIndex in trainingSet:\n            # 邮件转为词向量并追加到训练数据集\n            trainMat.append(setOfWords2Vec(vocabList, docList[docIndex]))\n            # 已知的邮件类别追加到训练数据集\n            trainClasses.append(classList[docIndex])\n        # 获取p(w|c=0),p(w|c=1),p(c=1)\n        # p(c|w)=p(w|c)*p(c)/p(w),由于p(w)不变,可以直接比较p(w|c)*p(c)/p(w)\n        p0V,p1V,pSpam = trainNB0(np.array(trainMat), np.array(trainClasses))\n        errorCount = 0\n        for docIndex in testSet:\n            # 邮件转为词向量\n            wordVector = setOfWords2Vec(vocabList, docList[docIndex])\n            # 计算判错类别的个数\n            if classifyNB(np.array(wordVector), p0V, p1V, pSpam) != classList[docIndex]:\n                errorCount += 1\n                print('classification error ', docList[docIndex])\n        # 显示分类器的错误率\n        print('the error rate is: ', float(errorCount)/len(testSet))\n    ```\n    程序运行结果如下:\n    {% asset_img 2019052412390434.png %}\n    {% asset_img 2019052412395035.png %}\n    spamTest()会输出在10封随机选择的电子邮件上的分类错误率,因为电子邮件是随机选择的,所以每次输出结果可能有些差别.如果发现错误的话,函数会输出错分文档的词表,这样就可以了解到底是哪篇文档发生了错误,总体来说错误率还是相当低的,维持在4%以下(自己测了多次)\n\n### 总结\n- 我们的目的是计算$P(Y=y_j|X=x),j=1,2,\\cdots,n$,求出$\\argmax_{y_j}P(Y=y_j|X=x)$,然而:\n    $$\\begin{aligned}\n        P(Y=y_j|X=x)&=\\frac{P(Y=y_j)\\cdot{P(X=x|Y=y_j)}}{P(X=x)}\\quad j=1,2,\\cdots,n\\\\\n        &=\\frac{P(Y=y_j)\\cdot\\prod_{i=1}^n{P(X^{(i)}=x^{(i)}|Y=y_j)}}{P(X=x)}\\quad i,j=1,2,\\cdots,n\n    \\end{aligned}\n    $$\n    $$\\begin{aligned}\n    \\argmax_{y_j}{P(Y=y_j|X=x)}&=\\argmax_{y_j}\\frac{P(Y=y_j)\\cdot\\prod_{i=1}^n{P(X^{(i)}=x^{(i)}|Y=y_j)}}{P(X=x)}\\\\&=\\argmax_{y_j} P(Y=y_j)\\cdot\\prod_{i=1}^n{P(X^{(i)}=x^{(i)}|Y=y_j)}\\quad i,j=1,2,\\cdots,n\\quad (13)\n    \\end{aligned}$$\n    最终所求即为(13)式(各特征之间相互独立为前提)\n- 对于分类而言,使用概率有时要比使用硬规则更为有效,贝叶斯概率及贝叶斯准则提供了一种利用已知值来估计未知概率的有效方法\n- 之所以被称为朴素贝叶斯,是因为该理论假设各特征之间相互独立(独立性假设),因此降低了对数据量的需求,独立性假设是指一个词的出现概率并不依赖于文档中的其他词,尽管条件独立性假设并不正确,但是朴素贝叶斯仍然是一种有效的分类器\n\n### 参考\n1. [你对贝叶斯统计都有怎样的理解？](https://www.zhihu.com/question/21134457/answer/169523403)\n2. [<<2019考研数学复习全书(数学一)>>](https://book.douban.com/subject/26354557/)\n3. [后验概率与条件概率区别](https://www.cnblogs.com/gczr/p/10154451.html)\n4. [<<机器学习实战>>第4章 朴素贝叶斯](https://book.douban.com/subject/24703171/)\n5. [<<统计学习方法>>第4章 朴素贝叶斯法](https://book.douban.com/subject/10590856/)\n6. [带你理解朴素贝叶斯分类算法](https://zhuanlan.zhihu.com/p/26262151)\n7. [机器学习实战之朴素贝叶斯](https://www.cnblogs.com/zy230530/p/6847243.html)\n","tags":["机器学习实战"],"categories":["机器学习"]},{"title":"[math]投影矩阵","url":"%2Fposts%2Fe772abbc%2F","content":"> From: [什么是projection matrix （投影矩阵）](https://www.zhihu.com/question/62801807/answer/341425237)\n\n那从 **投影(Projection)** 说起吧，假设我们有一条直线 determined by vector $a$，还有一个 vector $b$，想把 $b$ 投影到 $a$ 上，得到 $p$：\n{% asset_img v2-408fdce89f9c9b.jpg %}\n要想得到 $p$，线性代数的办法是：因为 $p$ 在 $a$ 上，所以 $p=xa$ , 再定义 $e = b - p$ （从 $p$ 指向 $b$), 那么 $a \\perp e$，所以：\n$$\n\\begin{aligned}\n    a^T(b-xa)&=0\\\\\n    xa^Ta&=a^Tb\\\\\nx&=\\frac{\\displaystyle a^Tb}{\\displaystyle a^Ta}\n\\end{aligned}\n$$\n得到 $p=ax=a\\frac{\\displaystyle a^Tb}{\\displaystyle a^Ta}$ ，进一步再把这个投影写成 **投影矩阵(Projection Matrix)** 的形式：\n$p=Pb$,其中 P 是矩阵，那么： \n$$p=ax=\\frac{\\displaystyle aa^Tb}{\\displaystyle a^Ta}\\\\\nP=\\frac{\\displaystyle aa^T}{\\displaystyle a^Ta}$$\n关键性质大概有三条： \n1. $Rank(P)=1$。想想$P=\\frac{\\displaystyle aa^T}{\\displaystyle a^Ta}$ 的column space，分母是个 scalar，分子是个矩阵并且每一列是 $a$ 向量本身的常数倍，$Rank$ 是 1 没毛病。所以任何 vector $b$ 乘以 $P$，会落在 $a$ 所在直线上。接着从线性代数的角度考虑，$Pb$ 相当于在 $P$ 的 column space 里搞事情，$P$ 的 column space 就是 $a$ 那条直线，随便怎么搞都在 $a$ 上。\n2. $P^T=P$ 。因为$P^T=\\frac{\\displaystyle (aa^T)^T}{\\displaystyle a^Ta}=\\frac{\\displaystyle aa^T}{\\displaystyle a^Ta}=P$\n3. $P^2=P$ 。因为$P^2=\\frac{\\displaystyle aa^T}{\\displaystyle a^Ta}\\frac{\\displaystyle aa^T}{\\displaystyle a^Ta}=\\frac{\\displaystyle a(a^Ta)a^T}{\\displaystyle (a^Ta)a^Ta}=\\frac{\\displaystyle aa^T}{\\displaystyle a^Ta}=P$  。所以，随便你用多少次~\n","tags":["matrix"],"categories":["math"]},{"title":"[leetcode]63.不同路径II","url":"%2Fposts%2F6e2a1502%2F","content":"{% asset_img 201905201512007.png %}\n\n### 解题思路 \n请先参考上一题 [**[leetcode]62.不同路径**](https://brianyi.github.io/posts/ef733dfb) 的做法\n本题在上一题的基础上增加了限制条件,地图上多了障碍物.但仍然是dp的思路,dp[i][j]用来存储从$(0,0)$到$(i,j)$的路径数\n因为给了地图数组,且障碍物为1,空地为0,那么正好可以利用这个地图数组作为dp状态\n那么仍然先考虑特例,边缘情况,如果边缘没有障碍物,那么$dp[i][0]$或$dp[0][j]$为1,如果边缘有障碍物,那么$dp[i][0]$或$dp[0][j]$为0(错!我开始就是这么简单考虑,后来想到...)\n实际,如果边缘某处有障碍物,那么从该处开始的后面都应当认为无法到达,即$dp[i][0]$或$dp[0][j]$为0\n再考虑不在边缘的情况,没有障碍物时\n$$\ndp[i][j]=dp[i-1][j]+dp[i][j-1]\n$$\n有障碍物时直接赋值,$dp[i][j]=0$\n`本来用的原地图数组来做dp,结果在提交时有一个特例竟然说int无法表示,即溢出了,那么就改成了unsinged long long`\n### 代码实现 \n```cpp\n    using ull = unsigned long long;\n    int uniquePathsWithObstacles(vector<vector<int>>& s) {\n        vector<vector<ull>> dp;\n        for(auto it:s)\n        {\n            vector<ull> t;\n            for(auto it2:it)\n                t.push_back(it2);\n            dp.push_back(t);\n        }\n        int m=dp.size(),n=dp[0].size();\n        /* 如果为障碍1,那么取反为0,表示有0条路径到达\n         * 如果为空地0,那么取反为1,表示有1条路径到达 */\n        dp[0][0]=!dp[0][0]; \n        for(int i=1;i<m;++i) // 左边缘:空地为则dp为1,遇到第一个障碍物,则当前及后面方格dp都为0\n        {\n            dp[i][0]=!dp[i][0];\n            if (!dp[i-1][0])\n                dp[i][0]=0;\n        }\n        for(int j=1;j<n;++j) // 上边缘:空地为则dp为1,遇到第一个障碍物,则当前及后面方格dp都为0\n        {\n            dp[0][j]=!dp[0][j];\n            if (!dp[0][j-1])\n                dp[0][j]=0;\n        }\n        for (int i=1;i<m;++i) // 内部包含下,右边缘:障碍物则dp为0,否则为左边方格可达的路径数与上边方格可达的路径数之和\n        {\n            for(int j=1;j<n;++j)\n            {\n                if (dp[i][j])\n                    dp[i][j]=0;\n                else\n                    dp[i][j]=dp[i-1][j]+dp[i][j-1];\n            }\n        }\n        return dp[m-1][n-1];\n    }\n```\n算法时间复杂度$O(m*n)$\n","tags":["dp"],"categories":["OJ"]},{"title":"[leetcode]62.不同路径","url":"%2Fposts%2Fef733dfb%2F","content":"{% asset_img 201905201405595.png %}\n### 解题思路 \n看到这道题我以为是迷宫算法呢,像以前一样用dfs结果超时,后来看题解才明白可以用 **排列组合** 或 **动态规划(DP)** 来做\n### 思路一: 排列组合\n因为机器人只能向右或者向下走,那么最终肯定只能有m+n-2步到达终点,且这m+n-2步中必然有m-1步向下走,n-1步向右走,那么就简单了,求总共有多少不同路径也就是求在这m+n-2步中m-1步向下走的走法有多少种(或n-1步向右走的走法有多少种)?\n{% asset_img 201905201414106.png %}\n那么答案公式即为\n$$\nC_{m+n-2}^{m-1}=\\frac{\\displaystyle A_{m+n-2}^{m-1}}{\\displaystyle (m-1)!}\n$$\n`代码实现`\n```cpp\nusing ld = long double;\nint uniquePaths(int m, int n) {\n    ld a = 1, b = 1;\n    for(int i=n,k=m-1;i<=m+n-2;++i)\n    {\n        a*=i;\n        b*=k;\n        k=k==1?1:k-1;\n    }\n    return a/b+0.5; // 四舍五入\n}\n```\n时间复杂度为$O(n)$\n### 思路二: DP\n用$dp[i][j]$表示从点$(0,0)$到点$(i,j)$可以有多少种走法.\n先看特例:\n因为只能向下和向右,那么靠边界的一边必然走法只有一种即\n$$dp[i][0]=1(0\\le i\\le m-1)$$\n$$dp[0][j]=1(0\\le j\\le n-1)$$\n再看其他位置,如果当前在$(i,j)$位置$(i\\neq 0,j\\neq 0)$,则上一步位置可以是左边也可以是上边方格,那么当前到达位置的走法必然为上边方格走法与左边方格走法之和\n$$dp[i][j]=dp[i-1][j]+dp[i][j-1]$$\n`代码实现`\n```cpp\nint uniquePaths(int m, int n) {\n    vector<vector<int>> dp(m,vector<int>(n, 0));\n    for(int i=0;i<m;++i)\n        dp[i][0]=1;\n    for(int j=0;j<n;++j)\n        dp[0][j]=1;\n    for(int i=1;i<m;++i)\n        for(int j=1;j<n;++j)\n            dp[i][j]=dp[i-1][j]+dp[i][j-1];\n    return dp[m-1][n-1];\n}\n```\n时间复杂度为$O(m*n)$\n\n  ","tags":["dp"],"categories":["OJ"]},{"title":"[leetcode]34. 在排序数组中查找元素的第一个和最后一个位置","url":"%2Fposts%2Fe8124129%2F","content":"{% asset_img 2019051919134438.png %}\n\n### 解题思路 \n已知题目数组为有序,且要求算法时间复杂度$O(log_2n)$那么明显是二分查找\n首先,进行二分查找,找到元素后进行左遍历找到目标值的开始位置,右遍历找到目标值的结束位置\n### 代码实现 \n```cpp\nvector<int> searchRange(vector<int>& nums, int target) {\n    int len=nums.size();\n    int l=0,r=len-1,idx=-1;\n    while(l<=r)\n    {\n        int m=(l+r)>>1;\n        if (nums[m]==target)\n        {\n            idx=m;\n            break;\n        }\n        else if (nums[m]<target)\n            l=m+1;\n        else\n            r=m-1;\n    }\n    if (idx==-1)\n        return vector<int>{-1,-1};\n    l=r=idx;\n    while(l-1>=0&&nums[l-1]==target) --l;   // 这里比较巧妙,先判断越界,再判断值相等,最后进行移位\n    while(r+1<len&&nums[r+1]==target) ++r;\n    return vector<int>{l,r};\n}\n```\n### 结论\n题目比较简单,记录一下","tags":["二分查找"],"categories":["OJ"]},{"title":"[leetcode]10. 正则表达式匹配","url":"%2Fposts%2F886a4a97%2F","content":"{% asset_img 2019051917403936.png %}\n### 解题思路 \n{% asset_img 2019051917424837.png %}\n算法时间复杂度$O(m*n)$\n### 代码实现 \n```cpp\nint dp[500][500];\nbool isMatch(string s, string p) {\n\ts.insert(s.begin(), '#');\n\tp.insert(p.begin(), '#');\n\tif (p.size()==1 && s.size()==1) return true;\n\tint len_s = s.size()-1;\n\tint len_p = p.size()-1;\n\n\t// s为空,p为#*#*#*... ,匹配\n\tdp[0][0] = 1;   // s=空,p=空,必然匹配\n\tfor (int j = 1; j <= len_p; ++j)\n\t\tif (p[j] == '*'&&dp[0][j - 2])\n\t\t\tdp[0][j] = 1;\n\n\n\tfor (int i = 1; i <= len_s; ++i)\n\t{\n\t\tfor (int j = 1; j <= len_p; ++j)\n\t\t{\n\t\t\tif (p[j] == s[i] || p[j] == '.')  // p[j]是字符且能匹配s[i],则结果取决于前面j-1个长度串\n\t\t\t\tdp[i][j] = dp[i - 1][j - 1];\n\t\t\tif (p[j] == '*')\n\t\t\t{\n\t\t\t\tif (p[j - 1] != s[i] && p[j - 1] != '.')\t// 前面为字母且不等于s[i],那么#*就算作空,则匹配长度由dp[i][j-2]决定\n\t\t\t\t\tdp[i][j] = dp[i][j - 2];\n\t\t\t\telse // 前面字符(可以为.)与s[i]相匹配,则*表示至少一次(>=1),这里不明白(硬记吧)\n\t\t\t\t\tdp[i][j] = (dp[i][j - 1] || dp[i][j - 2] || dp[i - 1][j]);\n\t\t\t}\n\t\t}\n\t}\n\treturn dp[len_s][len_p];\n}\n```\n### 结论\n这道题有些难,不愧是困难级别的,平时正则也用得多,但没想到实现起来如此难","tags":["dp"],"categories":["OJ"]},{"title":"[english]idiomatic expression","url":"%2Fposts%2F381dde6d%2F","content":"### The birds and the bees\n\"The birds and the bees\" is an English-language idiomatic expression and euphemism that refers to courtship and sexual intercourse. The \"Birds and the Bees talk\" is generally the event in most children's lives in which the parents explain what sexual relationships are.\n### Casabianca\n\"Casabianca\" is a poem by the English poet Felicia Dorothea Hemans, first published in The Monthly Magazine, Vol 2, August 1826.[1]\nThe poem starts:\nThe boy stood on the burning deck\nWhence all but he had fled;\nThe flame that lit the battle's wreck\nShone round him o'er the dead.\nThe poem commemorates an actual incident that occurred in 1798 during the Battle of the Nile aboard the French ship Orient. The young son Giocante (his age is variously given as ten, twelve and thirteen) of commander Louis de Casabianca remained at his post and perished when the flames caused the magazine to explode.\n### Waste not, want not\n精打细算,不愁吃穿\n","tags":["idiomatic expression"],"categories":["english"]},{"title":"[机器学习实战]第3章 决策树","url":"%2Fposts%2F49e197e8%2F","content":"### 了解\n如果已知一个结论:一个动物如果不浮出水面,有鱼鳍,那么它一定是鱼.那么现在你有很多的动物,你怎么判断哪些是鱼类哪些不是呢?\n我们可以通过将文字转换为数值的形式来表示,即:\n`不浮出水面用1表示(0则表示浮出水面),有鱼鳍用1表示(0则表示没有鱼鳍),是鱼类用1表示(0则表示非鱼类)`\n**特征标签(即属性):** 不浮出水面,有鱼鳍\n**特征值(特征的数字表示):** 0或1\n**类别标签(即类别名):** 鱼类\n**类别值(即类别的数字表示):** 0或1\n\n**例如:** \n```python\nfeatLabels = ['不浮出水面','有鱼鳍']\nclassLabels = ['鱼类']\n```\n一般,一个已知的数据集都是以向量形式表示为 **[特征值1,特征值2,...,特征值n,类别值]** ,而要预测的数据集则为 **[特征值1,特征值2,...,特征值n]** ,如果你有一个动物的数据为 **[1,1]** ,根据开头已知的结论可以得出 **[1(不浮出水面),1(有鱼鳍)]** 一定是鱼\n\n### 决策树的含义\n顾名思义,用于进行决策的一种树状结构\n{% asset_img 2019051819365621.png %}\n\n### 决策树创建的伪代码\n```python\n    def createBranch():\n        检测数据集中的每个子项是否属于同一分类:\n            If so return 类标签\n            Else\n                寻找划分数据集的最好特征\n                划分数据集\n                创建分支节点\n                    for每个划分的子集\n                        调用函数createBranch并增加返回结果到分直节点中\n                return 分支结点\n```\n\n### 信息增益\n- **自信息**: 在信息论中,自信息(英语：self-information),由**克劳德·香农**提出,是与概率空间中的单一事件或离散随机变量的值相关的信息量的量度.自信息的期望值就是信息论中的**熵**,它反映了随机变量采样时的平均不确定程度.\n  如果待分类的事物可能划分在多个分类之中,则符号$x_i$的自信息定义为\n    $$I(X)=-log{p(x_i)}$$\n    $p(x_i)$是选择该分类的概率\n- 熵或香农熵(entropy): 信息熵是考虑该随机变量的所有可能取值，即所有可能发生事件所带来的信息量的期望。熵定义为自信息的期望值,即度量数据的无序(混乱)程度,**越乱则熵越大,越纯则熵越小**,公式为 \n  $$H(X)=\\sum_{i=1}^n p(x_i)I(x_i)=-\\sum_{i=1}^n p(x_i)log p(x_i)$$\n  熵越大,随机变量的不确定性就越大.从定义可验证\n  $$0\\leq H(p)\\leq logn$$\n  当随机变量只取两个值,例如1,0时,即$X$的分布为\n  $$P(X=1)=p,\\ P(X=0)=1-p,\\ 0\\leq p\\leq 1$$\n  熵为\n  $$H(p)=-plog_2p-(1-p)log_2{(1-p)}$$\n  这时,熵$H(p)$随概率$p$变化的曲线如下图所示\n    {% asset_img Binary_entropy_plot.svg.png %}\n- 条件熵: 条件熵$H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性。条件熵$H(Y|X)$定义为$X$给定条件下$Y$的条件概率分布的熵对$X$的数学期望：\n  {% asset_img 130152886.png %}\n  \n  > **注意**: \n  > 1. $X$表示变量,变量$X$中的每个值都会取,而$x$表示具体某个值,因此$H(Y|X=x)=-\\sum_{y}p(y|x)logp(y|x)$\n  > 2. $\\sum_{x,y}=\\sum_x\\sum_y$(两个累加可以写成一个),$\\sum{a_i}=\\sum_{i=1}^n{a_i}$(非正式写法会省略下面的$i$)\n  > 3. $p(X)=\\sum_xp(x)=\\sum_{i=1}^np(x_i)$\n\n  条件熵$H(Y|X)$相当于联合熵$H(X,Y)$减去单独的熵$H(X)$,即\n  $H(Y|X)=H(X,Y)−H(X)$,证明如下：\n  {% asset_img 1537760199.png %}\n\n- 计算数据的香农熵\n  首先导入必要的包\n    `import numpy as np`\n    `import math`\n    `import operator`\n    ```python\n    def calcShannonEnt(dataSet):\n        '''\n        :param dataSet: 数据集\n        :return shannonEnt: 香农熵值\n        '''\n        numEntries = len(dataSet)   # 获取数据集总数\n        classValCounts = {}    # 字典存放每个类别对应数据集的数量\n        for featVec in dataSet:\n            classVal = featVec[-1]  # 获取类别值\n            if classVal not in classValCounts.keys():\n                classValCounts[classVal] = 0\n            classValCounts[classVal] += 1\n        shannonEnt = 0.0\n        for key in classValCounts:\n            prob = float(classValCounts[key])/numEntries\n            shannonEnt += -prob * math.log(prob,2)\n        return shannonEnt\n    ```\n- 实验\n  我们实验一下是否数据集越纯,熵值越小,数据集越乱,熵值越大\n  ```python\n  def createDataSet():\n    dataSet = [[1, 1, 'yes'],\n              [1, 1, 'yes'],\n              [1, 0, 'no'],\n              [0, 1, 'no'],\n              [0, 1, 'no']]\n    featLabels = ['no surfacing','flippers']\n    return dataSet, featLabels\n    \n    >>> dataSet,featLabels = createDataSet()    # 获取数据集和特征标签\n    >>> calcShannonEnt(dataSet) # 计算香农熵\n    0.9709505944546686\n  ```\n  我们试试如果让数据集变纯会怎样?\n  ```python\n    >>> dataSet[0][-1]=dataSet[1][-1]='no';dataSet\n    [[1, 1, 'no'], [1, 1, 'no'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]\n    >>> calcShannonEnt(dataSet) # 计算香农熵\n    0.0\n  ```\n  果然变小了,再试试数据集变乱?\n  ```python\n    >>> dataSet[1][-1]='yes';dataSet[2][-1]='possible';dataSet[3][-1]='impossible';dataSet[4][-1]='may';dataSet\n    [[1, 1, 'no'],\n    [1, 1, 'yes'],\n    [1, 0, 'possible'],\n    [0, 1, 'impossible'],\n    [0, 1, 'may']]\n    >>> calcShannonEnt(dataSet) # 计算香农熵\n    2.321928094887362\n  ```\n  变得非常大了\n### 划分数据集\n- 根据给定特征划分数据集\n  当我们按照某个特征划分数据集时,就需要将所有符合要求的元素抽取出来\n  ```python\n    def splitDataSet(dataSet, axis, value):\n        '''\n        :param dataSet: 待划分的数据集\n        :param axis: 划分数据集的特征\n        :param value: 特征值\n        :return retDataSet: 返回划分后的数据集\n        '''\n        retDataSet = []\n        for featVec in dataSet:\n            if featVec[axis] == value:  # 只选择包含选定特征值的数据集\n                reducedFeatVec = featVec[:axis] # 该行及下一行将选定数据集的选定特征值去掉(即用于下轮其他特征值的划分)\n                reducedFeatVec.extend(featVec[axis+1:])\n                retDataSet.append(reducedFeatVec)\n        return retDataSet\n  ```\n  下面用<span id='jump-3-2'>图3-2</span>来说明上面函数**splitDataSet**的用途\n  {% asset_img 2019051911283026.png %}\n  这是一个根据特征(是否浮出水面(No sufacting), 是否有鳍(Filppers))来判断是否是鱼类(Fish)的决策图,已有5个数据集数据,我们来将数据进行划分,看是否符合图示\n  首先构造图中数据集\n  {% asset_img 2019051911325428.png %}\n  根据**No surfacing**特征来划分数据\n  - **No(0)**\n    {% asset_img 2019051911385529.png %}\n    可以发现分类结果都为no,即不是鱼,因此纯度已经很高,无需再划分了\n  - **Yes(1)**\n    {% asset_img 2019051911392930.png %}\n    结果中仍然有不同类别,有yes,有no,因此还可以根据下一个特征继续划分\n    从数据集中排除已经划分过的特征,从剩下的数据集中的特征继续划分\n    {% asset_img 2019051911410131.png %}\n    根据Flippers特征又可以继续划分数据\n    - **No(0)**\n    {% asset_img 2019051911412932.png %}\n    - **Yes(1)**\n    {% asset_img 2019051911415033.png %}\n- 选择最好的数据集划分方式\n  每一次划分都选择当前信息增益最大的特征进行划分\n  ```python\n    def chooseBestFeatureToSplit(dataSet):\n        '''\n        :param dataSet: 数据集\n        :return bestFeature: 返回获取信息增益最大的特征索引\n        '''\n        numFeatures = len(dataSet[0]) - 1 # 获取总特征数量(减1是去掉尾部的分类标签值)\n        baseEntropy = calcShannonEnt(dataSet)   # 计算未划分数据时的香农熵(或熵)\n        bestInfoGain = 0.0\n        bestFeature = -1\n        for i in range(numFeatures):    # 计算按每一个特征进行划分时,所得到的信息增益,并选取能得到最大信息增益的特征\n            featList = [example[i] for example in dataSet]  # 获取第i个特征的所有特征值\n            uniqueVals = set(featList)  # 特征值去重\n            newEntropy = 0.0\n            for value in uniqueVals:    # 计算按该特征划分后的香农熵\n                subDataSet = splitDataSet(dataSet, i, value)\n                prob = len(subDataSet)/float(len(dataSet))\n                newEntropy += prob * calcShannonEnt(subDataSet)\n            infoGain = baseEntropy - newEntropy # 计算信息增益\n            if infoGain > bestInfoGain: # 选取最大的信息增益,和特征\n                bestInfoGain = infoGain\n                bestFeature = i\n        return bestFeature\n  ```\n### 递归构造决策树\n- 介绍\n  从原始数据集中,基于最好的特征(即**信息增益最大,从混乱到越纯的程度越高**)划分数据集,每一个分支都是根据一个特征的数据划分,在该分支下的子分支都将不再包含该特征(因为前门已经根据该特征划分过了),每次划分,子分支也可能有多个($\\ge 2$),划分的数据将传递到子分支上(利用信息增益作为特征划分的依据,越划分则下面分支上的数据越少也就越纯),因此我们可以采用递归的原则处理数据集\n  **递归结束的条件**(满足**任意一个**):\n    - 程序遍历完所有划分数据集的特征(若叶节点不纯则采用多数表决策略)    $\\quad (1)$\n    - 每个分支下的数据都是相同的分类(比如上图示的叶节点)    $\\quad (2)$\n- 多数表决策略\n  有时候,我们在划分数据时,已经遍历完了所有特征,但叶节点上的类标签依然不是唯一的,此时我们可以采取多数表决的策略,也就是少数服从多数,选择类别较多的那个类标签作为该叶节点的分类\n  > 上述情况易发生在特征少,而数据量较多的情况\n  ```python\n    def majorityCnt(classValList):\n        '''\n        :param classValList: 数据集类别值列表\n        :return sortedClassCount[0][0]: 返回出现次数最多的类别值\n        '''\n        classValCount = {}\n        for vote in classValList:  # 字典存储每个类别的数据集数量\n            if vote in classValCount.keys():\n                classValCount[vote] = 0\n            classValCount[vote] += 1\n        sortedClassValCount = sorted(classValCount.items(), key=operator.itemgetter(1), reverse=True) # 根据类别值出现次数来排序(逆序),出现次数最多类别值的为第一个\n        return sortedClassValCount[0][0]   # 返回出现次数最多的类别值\n  ```\n- 递归构造决策树代码(这里采用字典结构来存储树,当然实际也可创建数据结构来存储)\n  ```python\n    def createTree(dataSet, featureLabels):\n        '''\n        :param dataSet: 数据集\n        :param featureLabels: 特征标签\n        :return myTree: 构造好的决策树\n        '''\n        featLabels = featureLabels[:]\n        classValList = [example[-1] for example in dataSet]    # 获取数据集对应的类别\n        if classValList.count(classValList[0]) == len(classValList): # 检测到数据集已为同一类别,满足递归结束的条件$(2)$,终止递归\n            return classValList[0]\n        if len(dataSet[0]) == 1:    # 遍历完所有特征,满足递归结束的条件$(1)$,终止递归\n            return majorityCnt(classValList)\n        bestFeat = chooseBestFeatureToSplit(dataSet)    # 选择信息增益最大的特征\n        bestFeatLabel = featLabels[bestFeat]    # 获取特征的名称\n        myTree = {bestFeatLabel:{}} # 在该分支结点注明特征名称\n        del(featLabels[bestFeat])   # 已经遍历过的特征从特征标签中去掉\n        featValues = [example[bestFeat] for example in dataSet] # 获取该特征的所有值\n        uniqueVals = set(featValues)    # 去重\n        for value in uniqueVals:\n            subFeatLabels = featLabels[:]   # 获取剩下的特征标签(还未进行划分的特征)\n            myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subFeatLabels)   # 为该特征下的每个分支创建子树(递归进行)\n        return myTree\n  ```\n  来进行测试一下\n  {% asset_img 201905201643038.png %}\n  果然,决策树跟[图3-2](#jump-3-2)中一样\n\n  > 步骤\n  > 1. 写好递归结束条件\n  > 2. 选择最好特征(信息增益最大),创建该特征结点\n  > 3. 根据该特征进行划分数据集,并递归建树\n\n### 测试算法: 使用决策树执行分类\n- 使用决策树来预测数据类别\n  经过以上部分,将已知类别的训练数据进行训练后,生成的决策树就可以用来进行实际数据的分类了.现在来编写使用决策树预测类别的函数\n- 决策树预测函数的代码\n  ```python\n    def classify(decisionTree, featLabels, featVec):\n        '''\n        :param decisionTree: 决策树,根据样本特征来预测样本的类别\n        :param featLabels: 特征标签\n        :param featVec: 样本的特征向量\n        '''\n        firstStr = list(decisionTree.keys())[0]   # 获取当前用于决策的特征标签\n        secondDict = decisionTree[firstStr]    # 获取该特征标签下的所有不同特征值分支(所有子树)\n        featIndex = featLabels.index(firstStr)  # 获取该特征标签对应的特征索引\n        for featVal in secondDict.keys():   # 遍历特征标签下的所有不同特征值\n            if featVec[featIndex] == featVal:   # 若当样本的特征值相同则进入下一个分支\n                if type(secondDict[featVal]).__name__ == 'dict':    # 下一个分支是否是子树\n                    classLabel = classify(secondDict[featVal], featLabels, featVec) # 在子树中继续递归决策(选择分支)\n                else: classLabel = secondDict[featVal]  # 下一个分支是叶子节点,那么可以直接确定类别了\n        return classLabel   # 返回预测的类别标签\n  ```\n  来进行测试一下鱼类样本 **[1,1]**\n  {% asset_img 201905202100249.png %}\n  果然,结果为`yes`,判断正确!不过我们用的训练数据只有5条,所以差异性也有可能存在,在实际应用中要尽量采用较多的训练数据来训练模型,从而达到较高的预测准确率.\n\n### 使用算法: 决策树的存储\n- 介绍\n  由于每次构造决策树都要耗费大量时间,那么可以在最开始构造好决策树后将它序列化存储到磁盘,并在需要的时候读出来\n- 使用 **pickle模块** 存储和读取决策树\n  - **存储决策树** 到磁盘\n从```python\n    def saveTree(decisionTree, fileName):\n        '''\n        :param decisionTree: 决策树\n        :param fileName: 要存储的文件名\n        '''\n        import pickle\n        fw = open(fileName, 'wb')\n        pickle.dump(decisionTree, fw)\n        fw.close()\n  ```\n  - **读取决策树** 到内存\n  ```python\n    def readTree(fileName):\n        '''\n        :param fileName: 要读取的文件名\n        :return decisionTree: 返回决策树\n        '''\n        import pickle\n        fr = open(fileName, 'rb')\n        return pickle.load(fr)\n  ```\n  来进行测试一下\n  {% asset_img 2019052021361210.png %}\n  还是很方便,其实 **存储决策树** 就是把数据从`内存`中拷贝到了`硬盘`而已(以二进制的形式), **读取决策树** 就是将数据从`硬盘`读到`内存`,二进制只占了很小的硬盘空间,数据文件如下图`红圈`所示`decision.data`.\n  {% asset_img 2019052021460911.png %}\n  可以发现,一棵决策树只占了`84字节`,相比每次都要构造决策树来说,从内存中读取决策树将大大节省了时间消耗\n### 总结\n- 信息公式$I(x_i)=-logp(x_i)$\n- 熵\n  信息熵,香农熵(因为是香农发明的),熵都是一个东西,用于表示数据的混乱程度,**越混乱,熵越大,越纯,熵越小**\n  公式 $H(X)=\\sum_x{p(x)I(x)}=-\\sum_x{p(x)logp(x)}$\n- 信息增益\n  也就是熵的变化,信息增益越大就表明变得纯度越高,信息增益越小就表明变得纯度稍低\n  而决策树中一般选择使得信息增益最大的特征作为分类的特征\n- 决策树预测\n  数据实例最终会被明确划分到某个分类中\n\n### 参考\n1. [详解机器学习中的熵、条件熵、相对熵和交叉熵](http://www.cnblogs.com/kyrieng/p/8694705.html)\n2. [<<机器学习实战>>第3章 决策树](https://book.douban.com/subject/24703171/)\n3. [<<统计学习方法>>第5章 决策树](https://book.douban.com/subject/10590856/)","tags":["机器学习实战"],"categories":["机器学习"]},{"title":"[numpy]notes","url":"%2Fposts%2Ffd60ea9b%2F","content":"\n### Numpy\n#### **np**\n- **numpy.array(object, dtype=None, copy=True, order='K', subok=False, ndmin=0)**: 创建一个数组\n    {% asset_img 2019052717422813.png %}\n    +,-操作\n    {% asset_img 2019062119322528.png %}\n    切片操作\n    `[start:end:interval,start:end:interval]`切片用于在范围内提取元素,间隔为`-1`表示逆序,如下所示\n    {% asset_img 2019062720352571.png %}\n    思考,如何将`x`按行逆序后每行提取最大的2个元素?\n    答案:`x[:,:-3:-1]`\n    {% asset_img 2019062720475372.png %}\n- **numpy.asfarray(a,dtype=<class 'numpy.float64'>)**: 返回一个转换为float类型的数组\n- **numpy.dot(a,b,out=None)**: 两个数组点积\n  - 如果a,b都为1维数组,直接向量的内积\n  - 如果a,b都为$\\ge$2维数组,使用矩阵乘法,但使用的是`np.matmul`或`a@b`(矩阵乘法)而不是`np.multiply(a,b)`或`a*b`(二者为元素级乘法)\n    {% asset_img 201905271213131.png %}\n    - 计算$\\mathbf {ab}=\\sum_{i=0}^m{ab}=a_1b_1+a_2b_2+\\cdots+a_mb_m$,可以分开为$a$和$b$,则可以直接写成`np.dot(a,b)`或`a@b`\n    - 计算$\\sum_{j=0}^n(\\theta x-y)x$,可以分开为$(\\theta x-y)$和$x$,则可以直接写成`np.dot(θx-y,x)`或`(θx-y)@x`\n    - 计算$\\mathbf {ab}$(元素级)=$[a_1b_1,a_2b_2,\\cdots,a_nb_n]$,可以直接写成`np.multiply(a,b)`或`a*b`\n    - 计算$a_i-a_ib_i,i=0,1,2,\\cdots,n$,可以直接写成`a-np.multiply(a,b)`或`a-a*b`\n  - 如果a或b为0维(也就是标量数值),等价于使用`multiply`,可以用`np.multiply(a,b)`或`a*b`(二者为元素级乘法)\n    {% asset_img 201905271214352.png %}\n  - 如果a为N维,b为1维数组,则是最后一个axis的a与b的积之和\n    {% asset_img 201905271621045.png %}\n    {% asset_img 201905271217153.png %}\n- **numpy.full(shape, fill_value, dtype=None, order='C')**: 返回一个给定`shape`和`type`,填充`fill_value`的数组\n    {% asset_img 2019052717324610.png %}\n- **numpy.full_like(a, fill_value, dtype=None, order='K', subok=True, shape=None)**: 返回一个与`a`相同`shape`且填充`fill_value`的数组\n    {% asset_img 2019052717344811.png %}\n- **numpy.ones(shape, dtype=None, order='C')**: 返回一个`shape`大小的`全1数组`\n- **numpy.ones_like(a, dtype=None, order='K', subok=True, shape=None)**: 返回一个与`a`相同`shape`大小的`全1数组`\n- **numpy.row_stack**: 将1D数组作为2D数组的行,等价于`vstack`\n- **numpy.column_stack**: 将1D数组作为2D数组的列等价于\n  - 1D情况\n    {% asset_img 2019052909404427.png %}\n  - 2D情况\n    {% asset_img 2019052909501128.png %}\n- **numpy.vstack**: 并列扩展,等价于`row_stack`\n- **numpy.hstack**: 并行扩展\n    {% asset_img 2019052909522629.png %}\n- **numpy.r_**: 并行连接   \n- **numpy.c_**: 并列连接\n    {% asset_img 2019052909242822.png %}\n    `r_`,`c_`都可通过沿一个轴叠加数字来创建数组\n    {% asset_img 2019052909285523.png %}\n- **numpy.meshgrid(\\*xi, \\*\\*kwargs)**:由坐标向量返回坐标矩阵\n  即,我只需给横坐标数组x,纵坐标数组y,就可构成一张网\n  - **indexing**:{'xy','ij'},可选.表示输出矩阵时,索引选用Cartesian(默认为'xy')或matrix('ij')\n  当选用Cartesian(默认为'xy')时,输入长度为$M,N$,则输出为$(N,M)$矩阵,如下图\n  {% asset_img 201906031709414.png %}\n  当选用matrix('ij')时,输入长度为$M,N$,则输出为$(M,N)$矩阵,如下图\n  {% asset_img 201906031747526.png %}\n  上图每个点是由$(x,y)_{ij}$构成,如果我们一个一个坐标输入,要输入$3\\times 10$个点,要做很多重复工作,因此这个函数只需提供x轴和y轴坐标的坐标数组即可,值得注意的是,画线方式默认是按**二维数组的列**(不是图像的列)进行\n  用于画3d图像时给定2维坐标\n  {% asset_img 201906031815497.png %}\n- **numpy.log(x, out=None, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])**\n  以`e`为底,等同于`ln`(numpy中没有),相似的用途有`log10,log2,log1p`\n  {% asset_img 201906181523445.png %}\n- **numpy.mean(a, axis=None, dtype=None, out=None, keepdims=<no value>)**\n  沿axis计算算术平均值,`因而可以用来求期望`\n  默认是作为展开的数组的平均值\n  axis=0时,列平均值;axis=1时,行平均值\n  {% asset_img 2019062115283220.png %}\n  求期望值E(X)\n  {% asset_img 2019062117204025.png %}\n- **numpy.reshape(a, newshape, order='C')**\n  给数组新的维度,并不改变原始数据(ndim<=2)\n  {% asset_img 2019062115245819.png %}\n- **numpy.var(a, axis=None, dtype=None, out=None, ddof=0, keepdims=<no value>)**\n  计算方差,等同于下面公式\n  $$\\begin{aligned}\n      var(X)=\\frac{\\sum_{i=1}^n(X_i-\\bar{X})(X_i-\\bar{X})}{n}\n  \\end{aligned}$$\n  `var = mean(abs(x - x.mean())**2)`\n  千万不要与样本方差弄混淆,样本方差是`除n-1`\n  默认是作为展开的数组的\n  {% asset_img 2019062118533426.png %}\n  axis=0时,按列求每列的方差;axis=1时,按行求每行的方差\n  {% asset_img 2019062119091527.png %}\n- <span id='cov'>**numpy.cov(m, y=None, rowvar=True, bias=False, ddof=None, fweights=None, aweights=None)**</span>\n  根据给予的数据和权重评估矩阵的协方差,协方差反映的是两个变量的不同程度.\n  $X=[x_1,x_2,...,x_N]^T$,协方差矩阵元素$C_{ij}$是$x_i$和$x_j$的协方差,$C_{ii}$是$x_i$的方差\n  > - **m**: array_like\n  >          1D或者2D数组,包含多个变量和观测值.m的每行表示一个变量,每列表示所有这些变量的观测值\n  > - **y**: array_like, optional\n  >          额外的变量和观测集合,y与m的形式一致\n  > - **rowvar**: bool, optional\n  >               默认为True,每行表示一个变量,列为观测值.否则,则为转置后的关系.\n  > - **bias**: bool, optional\n  >             默认`(False)`标准化是除以`(N-1)`,`N`是观测值(无偏估计)的数量.如果`bias`为`True`,标准化则除以`N`.这些值可以使用`ddof`来进行重写\n  \n  协方差计算公式如下:\n  $$\\begin{aligned}\n      Cov(x,y)&=E[(x-E(x))(y-E(y))]\\\\\n              &=\\frac{1}{n}\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})\n  \\end{aligned}$$\n  用numpy表示:\n  当`bias=False`时: `Cov(x,y)=sum((x-mean(x,axis=0))*(y-mean(y,axis=0)))*1.0/n`\n  当`bias=True`时: `Cov(x,y)=sum((x-mean(x,axis=0))*(y-mean(y,axis=0)))*1.0/(n-1)`\n  协方差矩阵计算公式如下:\n  $$\\begin{aligned}\n      &X=[x_1,x_2]^T,Y=[y_1,y_2]^T\\\\\n      &Cov(X,X)=\n      \\begin{bmatrix}\n          Cov(x_1,x_1)&Cov(x_1,x_2)\\\\\n          Cov(x_2,x_1)&Cov(x_2,x_2)\n      \\end{bmatrix}\\\\\n      &Cov(X,Y)=\n      \\begin{bmatrix}\n          Cov(x_1,x_1)&Cov(x_1,x_2)&Cov(x_1,y_1)&Cov(x_1,y_2)\\\\\n          Cov(x_2,x_1)&Cov(x_2,x_2)&Cov(x_2,y_1)&Cov(x_2,y_2)\\\\\n          Cov(y_1,x_1)&Cov(y_1,x_2)&Cov(y_1,y_1)&Cov(y_1,y_2)\\\\\n          Cov(y_2,x_1)&Cov(y_2,x_2)&Cov(y_2,y_1)&Cov(y_2,y_2)\n      \\end{bmatrix}\n  \\end{aligned}$$\n  以下为`bias=False`(默认)的例子\n  {% asset_img 2019062716000370.png %}\n- **numpy.nonzero(a)**\n  返回非零元素的索引,返回维度个数个array,每一个array对应一个维度,表示该维度下不为0的索引\n  {% asset_img 2019062420145147.png %}\n  `只有2个维度时,一个array返回行索引,另一个array返回列索引`\n  上图x有2个维度,即`3x3`,第一维度(最外层)中含有非零元的索引为`[0,1,2,2]`,第二维度(即每行内的元素)中含有非零元的索引为`[0,1,0,1]`\n  实际这两个维度合起来即`[[0,0],[1,1],[2,0],[2,1]]`正好把非零元标记出\n- **numpy.linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None, axis=0)**:\n    按指定间隔值返回均匀的数\n    返回`num`个均匀分布的数,间隔通过`[start,stop]`计算出\n    间隔最后的点可以选择是否包含在内\n    {% asset_img 201905271228234.png %}\n- **numpy.matmul**: 等价于`numpy.dot(a,b)`,`a@b`(数组的矩阵乘法)\n- **numpy.matrix**\n  - **Attributes**\n    - A: 返回ndarray(维数不变)\n      {% asset_img 201905271719369.png %}\n    - A1: 返回ndarray(一维)\n      {% asset_img 201905271719048.png %}\n    - H: 返回(复数)共轭转置[(complex)conjugate transpose]\n    - I: 返回逆矩阵\n    - T: 返回转置矩阵\n    - flags: 内存布局\n    - size: 返回总元素个数\n  - **getA(self)**\n    - 也可直接`self.A`,将自己作为ndarray返回,等价于`numpy.asarry(self)`\n        {% asset_img 2019051819061420.png %}\n  - **getA1(self)**\n    {% asset_img 201906142211264.png %}\n  - **flat**\n    {% asset_img 201906142209483.png %}\n- **numpy.pad(array, pad_width, mode='constant', \\*\\*kwargs)**: 填充数组\n  > **array: array_like of rank N**: 数组\n  > **pad_width: {sequence, array_like, int}**: 被填充到每个维度边缘的值的数量`((before_1,after_1),...,(before_N,after_N))`每一对括号表示在`第N维`的两边填充多少个元素.\n  > **mode: str or function, optional**: 填充模式\n  > - **'constant'(default)**: 填充常量\n  > - **'edge'**: 给边缘填充数组\n  > - **'maximum'**: 给每个axis填充该维度或全部维度的最大值\n  > - **'mean'**: 给每个axis填充该维度或全部维度的平均值\n  > - **'median'**: 给每个axis填充该维度或全部维度的中位数\n  > - **'minimum'**: 给每个axis填充该维度或全部维度的最小值  \n  {% asset_img 2019071011094711.png %}\n- **numpy.frombuffer(buffer, dtype=float, count=-1, offset=0)**: 将buffer看作1-D array\n  > **buffer: buffer_like**: 缓冲区\n  > **dtype: data-type, optional**: 返回数组的数据类型;默认为:float\n  > **count: int, optional**: 读取的数量.`-1`表示读取所有数据\n  > **offset: int, optional**: 从offset的位置开始读取缓冲区;默认为:0\n\n#### **numpy.random**\n- **numpy.random.rand(d0, d1, …, dn)** 生成shape大小的随机数(0~1)\n  {% asset_img 2019062115313422.png %}\n- **numpy.random.randn(d0, d1, …, dn)** 生成shape大小的标准正态分布\n  要表示正太分布 $N(\\mu, \\sigma^2)$, 用下面形式:\n  `sigma * numpy.random.randn(...) + mu`\n  根据N(3, 6.25)随机生成2x4数组大小的样例:\n  {% asset_img 2019062115321523.png %}\n- **numpy.random.randint(low[, high, size, dtype])**\t从小(包含)到大(不包含)返回随机整数.\n  {% asset_img 2019062115305121.png %}\n- **numpy.random.shuffle(x)** 随机重排序列\n  - **Parameters:**\t\n      x : 要被重排的array或list\n  - **Examples**\n      {% asset_img 201905251718352.png %}\n      多维数组只会沿第一个轴进行重排\n      {% asset_img 201905251719553.png %}\n- **numpy.random.normal(loc=0.0, scale=1.0, size=None)**\n  根据normal(Gaussian) distribution生产随机样本\n  Gaussian distribution的概率密度函数为\n  $$\\begin{aligned}\n      p(x)=\\frac{1}{\\sqrt{2\\pi{\\sigma^2}}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n  \\end{aligned}$$\n  $\\mu$是均值,$\\sigma$是标准偏差(standard deviation),$\\sigma^2$被称为方差(variance)\n  > **loc: float or array_like of floats**: 分布中心\n  > **scale: float or array_like of floats**: 分布的标准偏差\n  > **size: int or tuple of ints, optional**: 输出shape(大小)\n  > {% asset_img  %}\n- **numpy.random.choice(a, size=None, replace=True, p=None, axis=0)**\n  根据给定的1-D数组产生随机样本\n  - **a: 1-D array-like or int**: 若为`array-like`,则随机样本产生于该array.若为int,则产生于`np.arange(a)`.\n  - **size: int or tuple of ints, optional**: 输出shape.不选则默认输出1个\n  - **p: 1-D array-like, optional**: 与每条entry相关的概率.未给出则是uniform distribution(均匀分布).\n  {% asset_img 20190705102527121.png %}\n- **numpy.random.uniform(low=0.0, high=1.0, size=None)**\n  根据均匀分布产生随机样本\n  样本在半开区间均匀分布`[low,high)`\n  - **low: float or array_like of floats, optional**: 输出值的下界,默认为0\n  - **high: float or array_like of floats**: 输出值的上界,默认为1.0\n  - **size: int or tuple of ints, optional**: 输出shape.\n  {% asset_img 201907081017533.png %}\n#### **numpy.ndarray(为数组array的最原始类型)**\n- **numpy.ndarray**\n  +,-,*,/算术运算和广播\n  当数组形状相同或可兼容,数组间逐个元素的操作才是有效的.\n  为了判断两个形状是否是可兼容的,numpy从最后开始往前逐个比较它们的维度大小,比较过程中,如果两者的对应维度相同,或者其中之一等于1,比较继续进行指导最前面的维度.\n  {% asset_img 201907092005319.png %}\n- **numpy.ndarray.argmax(axis=None, out=None)**: 返回沿给定轴的最大值的索引\n- **numpy.ndarray.argmin(axis=None, out=None)**: 返回沿给定轴的最小值的索引\n- **numpy.ndarray.min(axis=None, out=None, keepdims=False, initial=<no value>, where=True)** 返回沿给定轴的最小值\n  - **Examples**\n    axis=0时,在列中选取最小值,axis=1时,在行中选取最小值\n  {% asset_img 2019051819061419.png %}\n- **numpy.ndarray.astype(dtype, order='K', casting='unsafe', subok=True, copy=True)**: 返回类型为指定类型的一份array的拷贝\n  {% asset_img 2019070410390194.png %}\n- **numpy.ndarray.size**: 数组元素的总数量\n  {% asset_img 2019071309532642.png %}\n#### **numpy.linalg**\n- **numpy.linalg.norm(x, ord=None, axis=None, keepdims=False)**:\n  返回矩阵或向量的范数\n  根据`ord`的值,函数可返回8种不同的矩阵范数,或无穷多个向量范数中的一个\n  > **x: array_like**: 输入数组.如果`axis`是`None`,则`x`必须为`1-D`或`2-D`\n  > **ord:{non-zero int,inf,-inf,'fro','nuc'},optional**: 范数的顺序\n  > {% asset_img 2019062908090974.png %}\n- **numpy.linalg.eig(a)**\n  计算方阵的特征值和特征向量\n- **numpy.linalg.svd(a, full_matrices=True, compute_uv=True)**\n  奇异值分解(SVD,Singular Value Decomposition)\n  $$\\begin{aligned}\n      Data_{m\\times{n}}=U_{m\\times{m}}\\Sigma_{m\\times{n}}V_{n\\times{n}}^T\\approx{U_{m\\times{k}}\\Sigma_{k\\times{k}}V_{k\\times{n}}^T}\n  \\end{aligned}$$\n","tags":["numpy"],"categories":["numpy"]},{"title":"[机器学习实战]第2章 k-近邻算法","url":"%2Fposts%2F75b8f8af%2F","content":"### k-近邻算法概述\nk-近邻算法采用测量不同特征值之间距离进行分类\n- 优点: 精度高,对异常值不敏感,无数据输入假定\n- 缺点: 计算复杂度高,空间复杂度高\n- 使用数据范围: 数值型和标称型\n\n### k-近邻算法的伪代码\n对位未知类别属性的数据集中的每个点依次执行以下操作:\n1. 计算已知类别数据集中的点与当前点之间的距离\n2. 按照距离递增次序排序\n3. 选取与当前点距离最小的k个点\n4. 确定前k个点所在类别的出现频率\n5. 返回前k个点出现频率最高的类别作为当前点的预测分类\n  \n### k-近邻算法核心代码\n```python\ndef classify0(inX, dataSet, labels, k):\n  '''\n  :param inX: 预测数据\n  :param dataSet: 已知数据集\n  :param labels: 已知数据所属标签值\n  :param k: 取前k个点出现频率最高的类别作为当前点的预测分类\n  :return:  \n  '''\n  dataSetSize = dataSet.shape[0]  # 获取一维长度(即样本数量)\n  diffMat = inX - dataSet # 获取差异矩阵\n  sqDiffMat = diffMat**2  # 取平方\n  sqDistances = sqDiffMat.sum(axis=1) # 二维数据累加,即计算每个点与预测点距离的平方和\n  distances = sqDistances**0.5  # 开根号\n  sortedDistIndicies = distances.argsort()  # 返回排序后的索引值(数据顺序不变)\n  classCount = {}\n  for i in range(k):\n    voteILabel = labels[sortedDistIndicies[i]]\n    classCount[voteILabel] = classCount.get(voteILabel, 0) + 1  #get(key, default): Return the value for key if key is in the dictionary, else default.\n  sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse=True) # 逆序后选择发生频率最高的标签\n  return sortedClassCount[0][0]\n```\n\n### <span id='jump-autonorm'>特征值归一化(特征缩放)</span>\n特征中,有些特征的数字差值对计算结果影响较大,而实际每个特征应当是同等重要的,因此需要进行归一化处理,将数值都转为比重(0.0~1.0)\n公式: $$newValue = \\frac{oldValue-minValue}{maxValue-minValue}$$\n```python\ndef auto_norm(X):\n    \"\"\"特征归一化(或特征缩放)\n    \n    Arguments:\n        X {array} -- 数据集\n    \n    Returns:\n        array -- 返回归一化后的数据集\n    \"\"\"\n    X=np.array(X)   \n    # 获取各column最小值\n    minVals=X.min(0)    \n    # 获取各column最大值    \n    maxVals=X.max(0)    \n    # 计算每个数据所在比重(0.0~1.0)\n    newVals=(X-minVals)/(maxVals-minVals)\n    return newVals\n```\n### 完整实现代码\n数据集下载: [digits.zip](digits.zip)\n```python\nimport numpy as np\nfrom os import listdir\n\n# K近邻类\nclass NearestNeighbor:\n    def __init__(self):\n        pass\n\n    # 训练\n    def train(self, X, y):\n        X = np.array(X)\n        y = np.array(y)\n        self.Xtr = X\n        self.ytr = y\n\n    # 预测\n    def predict(self, X):\n        X = np.array(X)\n        num_test = X.shape[0]\n        Ypred = np.zeros(num_test, dtype = self.ytr.dtype)\n\n        for i in range(num_test):\n            # 当前点的各个维度值与所有点的各个维度值进行相减取绝对值,找出最小值(即距离最近)\n            distances = np.sum(np.abs(self.Xtr - X[i,:]), axis=1)\n            min_index = np.argmin(distances)\n            Ypred[i] = self.ytr[min_index]\n\n        return Ypred\n\n# 文件内容转矩阵\ndef file2matrix(filename):\n    fr = open(filename)\n    arrayOfLines = fr.readlines()\n    numberOfLines = len(arrayOfLines)\n    returnMat = np.zeros((numberOfLines, 3))\n    classLabelVector = []\n    index = 0\n    for line in arrayOfLines:\n        line = line.strip()\n        listFromLine = line.split('\\t')\n        returnMat[index, :] = listFromLine[0:3]\n        classLabelVector.append(int(listFromLine[-1]))\n        index += 1\n    return returnMat, classLabelVector\n\n# 图片转vector\ndef img2vector(filename):\n    returnVect = np.zeros((1, 1024))\n    fr = open(filename)\n    for i in range(32):\n        lineStr = fr.readline()\n        for j in range(32):\n            returnVect[0, i * 32 + j] = int(lineStr[j])\n    return returnVect\n\n# 获取训练数据集\ntrainFiles = listdir('trainingDigits')\nlenTrainFiles = len(trainFiles)\ntrainDataSet = np.zeros((lenTrainFiles, 1024))\ntrainLabels = []\nfor i in range(lenTrainFiles):\n    fileFullNameStr = trainFiles[i]\n    fileNameStr = fileFullNameStr.split('.')[0]\n    classNumber = int(fileNameStr.split('_')[0])\n    trainLabels.append(classNumber)\n    trainDataSet[i,:] = img2vector('trainingDigits/{}'.format(trainFiles[i]))\n\n# 获取测试数据集\ntestFiles = listdir('testDigits')\nlenTestFiles = len(testFiles)\ntestDataSet = np.zeros((lenTestFiles, 1024))\ntestLabels = []\nfor i in range(lenTestFiles):\n    fileFullNameStr = testFiles[i]\n    fileNameStr = fileFullNameStr.split('.')[0]\n    classNumber = int(fileNameStr.split('_')[0])\n    testLabels.append(classNumber)\n    testDataSet[i, :] = img2vector('testDigits/{}'.format(fileFullNameStr))\n\n# k近邻预测\nnear = NearestNeighbor()\nnear.train(trainDataSet, trainLabels)\ny = near.predict(testDataSet)\nerrorCount = np.sum(y != testLabels)\nprint(\"error rate:{0:f}\".format(errorCount*1.0/lenTestFiles))\n```\n结果如下\n```\nerror rate:0.013742\n```\n### 小结\nk-近邻算法对于数据量不是很大的情况还是比较适宜的,因为效率不高,不过可以采用建立k-tree的方法来提高搜索效率","tags":["机器学习实战"],"categories":["机器学习"]},{"title":"[机器学习实战]第1章 机器学习基础","url":"%2Fposts%2F60021%2F","content":"### 机器学习主要任务\n- 监督学习主要任务: \n  - 分类: 将实例数据划分到合适的类别\n  - 回归: 用于预测数值型数据\n  - 分类和回归属于监督学习,之所以为监督学习,因为算法必须知道预测什么(即已经知道了分类信息)\n- 无监督学习主要任务:\n  - 聚类: 此时数据没有类别信息,也不会给定目标值,将数据集合分成类似的对象组成的多个类的过程称为聚类\n  - 密度估计: 将寻找描述数据统计值的过程称为密度估计\n### 吴恩达视频表示方法<span class='my-color-b'>(博客采用)</span>\n$m$表示训练集中实例的数量,$n$表示样本的特征数\n$x$代表特征/输入变量\n$y$代表目标变量/输出变量\n$(x,y)$代表训练集中的实例\n<b>$(x^{(i)},y^{(i)})$代表第$i$个观测实例</b>\n<b>$x_j^{(i)}$代表矩阵第$i$行的第$j$个特征(或第$i$个样本的第$j$个特征)</b>\n<b>$x_i$代表第$i$个样本</b>\n<b>$(x_i,y_i)$训练集中第i个实例</b>\n$h$代表学习算法的解决方案或函数也称为假设(hypothesis)\n### 李航书表示方法\n$x$代表输入,实例\n$y$代表输出,标记\n$(x_i,y_i)$第i个样本的训练数据点\n$x=(x^{(1)},x^{(2)},\\cdots,x^{(n)})$输入向量,$n$维实数向量\n$x_i^{(j)}$输入向量$x_i$的第$j$分量\n> 注意:吴恩达老师和李航老师,两者在数据表示方法时有不同,比如$x_i^{(j)}$意思完全相反\n> <span class='my-color-b'>本博客将采用吴恩达老师的表示方法</span>\n\n### 开发机器学习应用程序的步骤\n1. 收集数据: 利用各种方法收集样本数据\n2. 准备输入数据: 得到收集的数据后,必须确保数据格式符合要求\n3. 分析输入数据: 主要是人工分析以前得到的数据. 主要作用是确保数据集中没有垃圾数据(数据为空,或者与其他数据存在明显的差异)\n4. 训练算法: 机器学习算法从这一步才真正开始学习. 如果使用无监督学习算法,由于不存在目标变量值,也就不需要训练算法,则直接到第5步\n5. 测试算法: 对于监督学习,必须已知用于评估算法的目标变量值. 对于无监督学习,必须用其他的评测手段来检验算法的成功率. 不满意算法的输出结果则回到4步改正再测试.\n6. 使用算法: 将机器学习算法转为应用程序.\n\n### 总结\n这一章只是大概讲了一下机器学习的主要任务,即监督学习,无监督学习(应当还有半监督学习),然后就是实施步骤","tags":["机器学习实战"],"categories":["机器学习"]},{"title":"[leetcode]双指针法","url":"%2Fposts%2F54332%2F","content":"### 实战1\n{% asset_img 2019051715574813.png %}\n\n- **解题思路** \n    此题首先想到的当然是暴力破解,需要$O(n^2)$,如果采用双指针法,则需要$O(n)$即可\n    \n- **代码实现** \n    ```cpp\n    int maxArea(vector<int>& height) {\n        int maxarea = 0;\n        int i=0,j=height.size()-1;\n        while(i<j)\n        {\n            maxarea=max(maxarea,min(height[i],height[j])*(j-i));\n            if (height[i]<height[j])    // 高度较小的指针进行移动\n                i++;\n            else\n                j--;\n        }\n        return maxarea;\n    }\n    ```\n    **疑惑**: 高度较小的指针进行移动可以保证答案最大吗,难度不会漏掉其他有可能最大的结果吗?\n    **解答**: 可以用草稿纸画一下,由题意可以知道,面积是由底边宽和最短指针高度决定的,那么如果移动较高指针,情况只可能有两种:\n    - 较高指针的高度变得比较矮指针的高度还小(或等于),那么面积必然比原来小\n    - 较高指针的高度变得比较矮指针的高度大,由于高度还是由较矮指针决定,而底边却比原来短了1,那么面积必然比原来小\n    \n    因此,综上两种情况,只能移动较矮指针才有可能获取到更大的面积\n\n### 实战2\n{% asset_img 2019051716023414.png %}\n\n- **解题思路**\n  先排序$O(nlog_2n)$,然后采用$i\\lt left\\lt right$的方式,固定$i$,然后$left$和$right$进行双指针筛选\n- **代码实现**\n    ```cpp\n    int threeSumClosest(vector<int>& nums, int target) {\n        sort(nums.begin(),nums.end());  // 可以用堆排序或其他$(nlog_2n)$的算法\n        int minVal = 0,len=nums.size(),delt=999999;\n        for(int i=0;i<len-2;++i)    // 固定第一个数,后面的数双指针法进行筛选\n        {\n            while(i>0&&i<len-2&&nums[i]==nums[i-1]) ++i;    // i去重\n            int l=i+1,r=len-1;\n            while(l<r)\n            {\n                int sum=nums[i]+nums[l]+nums[r];\n                if (abs(sum-target)<delt)\n                    minVal = sum, delt=abs(sum-target);\n                else\n                {\n                    if (sum-target>0)\n                    {\n                        --r;\n                        while(l<r&&nums[r]==nums[r+1]) --r; // r去重\n                    }\n                    else\n                    {\n                        --l;\n                        while(l<r&&nums[l]==nums[l-1]) ++l; // l去重\n                    }\n                }\n            }\n        }\n        return minVal;\n    }\n    ```\n\n### 实战3\n{% asset_img 2019051719562317.png %}\n\n- **解题思路**\n  先排序$O(nlog_2n)$,然后按照$i\\le j\\le left\\le right$的方法,对$i,j,left,right$进行去重(去重时,应该先加完,判断,再去重),`三数之和,两数之和同理`\n- **代码实现**\n    ```cpp\n        vector<vector<int>> fourSum(vector<int>& nums, int target) {\n        vector<vector<int>> ans;\n        int len=nums.size();\n        sort(nums.begin(),nums.end());\n        for(int i=0;i<len-3;++i)\n        {\n            while(i>0&&i<len-3&&nums[i]==nums[i-1]) i++;    // i去重\n            for (int j=i+1;j<len-2;++j)\n            {\n                while(j>i+1&&j<len-2&&nums[j]==nums[j-1]) j++;  // j去重\n                int l=j+1,r=len-1,sum=target-(nums[i]+nums[j]);\n                while(l<r)\n                {\n                    int diff=nums[l]+nums[r]-sum;\n                    if (diff==0)\n                    {\n                        ans.push_back(vector<int>{nums[i],nums[j],nums[l],nums[r]});\n                        l++,r--;\n                        while(l<r&&nums[l]==nums[l-1]) l++; // l去重\n                        while(l<r&&nums[r]==nums[r+1]) r--; // r去重\n                    }\n                    else if (diff<0)\n                    {\n                        l++;\n                        while(l<r&&nums[l]==nums[l-1]) l++; // l去重\n                    }\n                    else\n                    {\n                        r--;\n                        while(l<r&&nums[r]==nums[r+1]) r--; // r去重\n                    }\n                }\n            }\n        }\n        return ans;\n    }\n    ```","tags":["双指针法"],"categories":["OJ"]},{"title":"[paper 1]Generalized Uncorrelated Regression with Adaptive Graph for Unsupervised Feature Selection","url":"%2Fposts%2F443%2F","content":"\n\n### unknown knowledge\n1. Stiefel manifold\n2. conventional ridge regression model\n3. uncorrelated constraint\n4. closed-form solution\n5. graph regularization term\n6. Generalized uncorrelated constraint\n7. manifold learning\n8. sparsity regularization models\n9. geometrical manifold learning\n10. spectral regression models\n11. Spectral feature selection\n12. l1-regularized regression model\n13. robust nonnegative matrix factorization\n14. local learning\n15. robust feature learning\n16. ridge regression\n17. dimensionality reduction\n18. indicator matrix\n19. subspace learning\n20. embedding-based feature selection method\n21. Graph Regularization\n22. closed-form solution\n23. clustering accuracy\n24. normalized mutual information\n\n### notation\n- **RBF kernel(Radial basis function kernel)**\n  - introduction\n  In machine learning, the radial basis function kernel, or **RBF kernel**, is a popular kernel function used in various kernelized learning algorithms. In particular, it is commonly used in **support vector machine classification**.\n  The **RBF kernel** on two samples x and x', represented as feature vectors in some input space, is defined as $K(\\mathbf x,\\mathbf x')=e^{-\\frac{||\\mathbf x-\\mathbf x'||^2}{2\\sigma^2}}$\n  $||\\mathbf x-\\mathbf x'||^2$ may be recognized as the squared Euclidean distance between the two feature vectors. $\\sigma$ is a free parameter. An equivalent definition involves a parameter $\\gamma ={\\tfrac {1}{2\\sigma ^{2}}}$: $K(\\mathbf {x} ,\\mathbf {x'} )=e^{-\\gamma \\|\\mathbf {x} -\\mathbf {x'} \\|^{2}}$\n  Since the value of the **RBF kernel** decreases with distance and ranges between **zero** (in the limit) and **one** (when $\\mathbf {x = x'}$), it has a ready interpretation as a similarity measure.\n  The feature space of the kernel has an infinite number of dimensions; for $\\sigma =1$, its expansion is:\n  {% asset_img c8d959c116119a.svg %}\n\n- **trivial solution**\n  A solution or example that is ridiculously simple and of little interest. Often, solutions or examples involving the number 0 are considered trivial. Nonzero solutions or examples are considered nontrivial.\n  For example, the equation x + 5y = 0 has the trivial solution x = 0, y = 0. Nontrivial solutions include x = 5, y = –1 and x = –2, y = 0.4.\n\n\n> Abstract\n\nby virtue of a generalized uncorrelated constraint, present an improved sparse regression model for seeking the uncorrelated yet discriminative features.\n\n> Introduction\n\nIt is natural that features always have many different clusters and each cluster has a mass of features for high-dimensional data. For example, there are eyes, nose, and mouth features in a face image. Therefore, the selected features with high redundancy lose their diversity and degrade their performance in clustering or classification tasks.\nTo address this problem, we present a generalized uncorre-lated regression model (GURM). Subsequently, an Uncorrelated Regression with Adaptive graph for unsupervised Feature Selection (URAFS) is proposed. The main contributions of this brief are summarized as follows.\n\n\n\n\n> References\n\n## feature selection\n\n### clustering\n[1] F. Nie, X. Wang, and H. Huang, “Clustering and projected clustering\nwith adaptive neighbors,” in Proc. ACM SIGKDD Int. Conf. Knowl.\nDiscovery Data Mining, 2014, pp. 977–986.\n[2] R. Zhang, F. Nie, and X. Li, “Projected clustering via robust orthogonal\nleast square regression with optimal scaling,” in Proc. Int. Joint Conf.\nNeural. Netw., 2017, pp. 2784–2791.\n\n### classification\n[3] J. Gui, T. Liu, D. Tao, Z. Sun, and T. Tan, “Representative vector\nmachines: A unified framework for classical classifiers,” IEEE Trans.\nCybern., vol. 46, no. 8, pp. 1877–1888, Aug. 2016.\n[4] J. Gui, D. Tao, Z. Sun, Y. Luo, X. You, and Y. Y. Tang, “Group\nsparse multiview patch alignment framework with view consistency\nfor image classification,” IEEE Trans. Image Process., vol. 23, no. 7,\npp. 3126–3132, Jul. 2014.\n\n### face recognition\n[5] C. Y. Lu, H. Min, J. Gui, L. Zhu, and Y. K. Lei, “Face recognition\nvia weighted sparse representation,” J. Vis. Commun. Image Represent.,\nvol. 24, no. 2, pp. 111–116, Feb. 2013.\n[6] J.-X. Mi, D. Lei, and J. Gui, “A novel method for recognizing face with\npartial occlusion via sparse representation,” Optik—Int. J. Light Electron\nOpt., vol. 124, no. 24, pp. 6786–6789, 2013.\n\n---\n\n## unsupervised feature selection\n\n### filter-based methods\n[7] X. He, D. Cai, and P. Niyogi, “Laplacian score for feature selection,”\nin Proc. 19th Annu. Conf. Neural Inf. Process. Syst., Vancouver, BC,\nCanada, Dec. 2005, pp. 507–514.\n[8] Z. Zhao and H. Liu, “Spectral feature selection for supervised\nand unsupervised learning,” in Proc. Conf. Mach. Learn., 2007,\npp. 1151–1157.\n[9] M. Qian and C. Zhai, “Robust unsupervised feature selection,” in Proc.\nIJCAI, 2013, pp. 1621–1627.\n\n### wrapper-based methods\n[10] S. Tabakhi, P. Moradi, and F. Akhlaghian, “An unsupervised feature\nselection algorithm based on ant colony optimization,” Eng. Appl. Artif.\nIntell., vol. 32, pp. 112–123, Jun. 2014.\n\n### embedding-based methods(most popular these years) 11-15\n### conventional spectral-based feature selection methods 11-14,17,18,35\n[11] D. Cai, C. Zhang, and X. He, “Unsupervised feature selection for multi￾cluster data,” in Proc. ACM SIGKDD Int. Conf. Knowl. Discovery Data\nMining, 2010, pp. 333–342.\n[12] Z. Zhao, L. Wang, and H. Liu, “Efficient spectral feature selection\nwith minimum redundancy,” in Proc. Nat. Conf. Artif. Intell., 2010,\npp. 673–678.\n[13] C. Hou, F. Nie, X. Li, D. Yi, and Y. Wu, “Joint embedding learning\nand sparse regression: A framework for unsupervised feature selection,”\nIEEE Trans. Cybern., vol. 44, no. 6, pp. 793–804, Jun. 2014.\n[14] Z. Li, Y. Yang, J. Liu, X. Zhou, and H. Lu, “Unsupervised feature\nselection using nonnegative spectral analysis,” in Proc. Nat. Conf. Artif.\nIntell., 2012, pp. 1026–1032.\n[15] S. Wang, J. Tang, and H. Liu, “Embedded unsupervised feature selec￾tion,” in Proc. Nat. Conf. Artif. Intell., 2015, pp. 470–476.\n\n### an overview of recent structured sparsity-inducing feature selection methods(another expression of embedding-based methods)\n[16] J. Gui, Z. Sun, S. Ji, D. Tao, and T. Tan, “Feature selection based on\nstructured sparsity: A comprehensive study,” IEEE Trans. Neural Netw.\nLearn. Syst., vol. 28, no. 7, pp. 1490–1507, Jul. 2017.\n[17] Q. Gu, Z. Li, and J. Han, “Joint feature selection and subspace learning,”\nin Proc. Nat. Conf. Artif. Intell., 2011, pp. 1294–1299.\n[18] H. Liu, M. Shao, and Y. Fu, “Consensus guided unsupervised feature\nselection,” in Proc. Nat. Conf. Artif. Intell., 2012, pp. 1874–1880.\n[19] F. Nie, W. Zhu, and X. Li, “Unsupervised feature selection with\nstructured graph optimization,” in Proc. Nat. Conf. Artif. Intell., 2016,\npp. 1302–1308.\n[20] Y. Yang, H. T. Shen, Z. Ma, Z. Huang, and X. Zhou, “2, 1-norm\nregularized discriminative feature selection for unsupervised learning,”\nin Proc. IJCAI, vol. 22, 2011, p. 1589.\n[21] L. Shi, L. Du, and Y.-D. Shen, “Robust spectral learning for unsuper￾vised feature selection,” in Proc. IEEE Conf. Data Mining, Dec. 2014,\npp. 977–982.\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 1595\n[22] J. Gui, Z. Sun, W. Jia, R. Hu, Y. Lei, and S. Ji, “Discrimi\u0002nant sparse neighborhood preserving embedding for face recognition,”\nPattern Recognit., vol. 45, no. 8, pp. 2884–2893, 2012.\n[23] W. Karush, “Minima of functions of several variables with inequalities\nas side constraints,” M.S. thesis, Dept. Math., Univ. Chicago, Ghicago,\nIL, USA, 1939.\n[24] J. Huang, F. Nie, H. Huang, and C. Ding, “Robust manifold nonnegative\nmatrix factorization,” ACM Trans. Knowl. Discovery Data, vol. 8, no. 3,\n2014, Art. no. 11.\n[25] F. Nie, R. Zhang, and X. Li, “A generalized power iteration method for\nsolving quadratic problem on the Stiefel manifold,” Sci. China Inf. Sci.,\nvol. 60, no. 11, p. 112101, 2017.\n[26] F. Nie, H. Huang, X. Cai, and C. H. Ding, “Efficient and\nrobust feature selection via joint \u00022, 1-norms minimization,” in Proc.\nAdv. Neural Inf. Process. Syst., Vancouver, BC, Canada, 2010,\npp. 1813–1821.\n[27] J. J. Hull, “A database for handwritten text recognition research,”\nIEEE Trans. Pattern Anal. Mach. Intell., vol. 16, no. 5, pp. 550–554,\nMay 1994.\n[28] C. E. Thomaz and G. A. Giraldi, “A new ranking method for principal\ncomponents analysis and its application to face image analysis,” Image\nVis. Comput., vol. 28, no. 6, pp. 902–913, 2010.\n[29] J. Gui, Z. Sun, G. Hou, and T. Tan, “An optimal set of code words and\ncorrentropy for rotated least squares regression,” in Proc. IEEE Int. Joint\nConf. Biometrics, Sep. 2014, pp. 1–6.\n[30] M. M. Nordstrøm, M. Larsen, J. Sierakowski, and M. B. Stegmann,\n“The IMM face database-an annotated dataset of 240 face images,”\nInform. Math. Modelling, Tech. Univ. Denmark, DTU, Kongens Lyngby,\nKingdom of Denmark, Tech. Rep. DK-2800, May 2004.\n[31] S. A. Nene, S. K. Nayar, and H. Murase, “Columbia object image library\n(COIL-20),” Dept. Comput. Sci., Columbia Univ., New York, NY, USA,\nTech. Rep. CUCS-005-96, Feb. 1996.\n[32] C. H. Papadimitriou and K. Steiglitz, Combinatorial Optimization:\nAlgorithms and Complexity. Upper Saddle River, NJ, USA: Prentice\u0002Hall, 1982.\n[33] A. Strehl and J. Ghosh, “Cluster ensembles—A knowledge reuse frame\u0002work for combining multiple partitions,” J. Mach. Learn. Res., vol. 3,\npp. 583–617, Dec. 2002.\n[34] K. Fan, “On a theorem of weyl concerning eigenvalues of linear trans\u0002formations,” Proc. Nat. Acad. Sci. USA, vol. 35, no. 11, pp. 652–655,\n1949.\n[35] R. Zhang, F. Nie, and X. Li, “Feature selection under regularized orthog\u0002onal least square regression with optimal scaling,” Neurocomputing,\nvol. 273, pp. 547–553, Jan. 2018.","tags":["paper"],"categories":["paper"]},{"title":"[leetcode]动态规划(Dynamic Programming)","url":"%2Fposts%2F26386%2F","content":"\n> 转载自[浅谈什么是动态规划以及相关的「股票」算法题](https://mp.weixin.qq.com/s/p91e-EuSuVK3bfOc7uJplg)\n\n\n### 概念\n- **动态规划**算法是通过拆分问题，定义问题状态和状态之间的关系，使得问题能够以递推（或者说分治）的方式去解决。在学习动态规划之前需要明确掌握几个重要概念。\n- **阶段：** 对于一个完整的问题过程，适当的切分为若干个相互联系的子问题，每次在求解一个子问题，则对应一个阶段，整个问题的求解转化为按照阶段次序去求解。\n- **状态：** 状态表示每个阶段开始时所处的客观条件，即在求解子问题时的已知条件。状态描述了研究的问题过程中的状况。\n- **决策：** 决策表示当求解过程处于某一阶段的某一状态时，可以根据当前条件作出不同的选择，从而确定下一个阶段的状态，这种选择称为决策。\n- **策略：** 由所有阶段的决策组成的决策序列称为全过程策略，简称策略。\n- **最优策略：** 在所有的策略中，找到代价最小，性能最优的策略，此策略称为最优策略。\n- **状态转移方程：** 状态转移方程是确定两个相邻阶段状态的演变过程，描述了状态之间是如何演变的。\n\n### 使用场景\n- 能采用动态规划求解的问题一般要具有3个性质:\n  - **最优化：** 如果问题的最优解所包含的子问题的解也是最优的，就称该问题具有最优子结构，即满足最优化原理。子问题的局部最优将导致整个问题的全局最优。换句话说，就是问题的一个最优解中一定包含子问题的一个最优解。\n    > 最优化原理: 一个过程的最优策略具有这样的性质，即无论其初始状态及初始决策如何，其以后诸决策对以第一个决策所形成的状态作为初始状态的过程而言，必须构成最优策略。\n  - **无后效性：** 即某阶段状态一旦确定，就不受这个状态以后决策的影响。也就是说，某状态以后的过程不会影响以前的状态，只与当前状态有关，与其他阶段的状态无关，特别是与未发生的阶段的状态无关。\n  - **重叠子问题：** 即子问题之间是不独立的，一个子问题在下一阶段决策中可能被多次使用到。（该性质并不是动态规划适用的必要条件，但是如果没有这条性质，动态规划算法同其他算法相比就不具备优势）\n\n### 算法流程\n- **划分阶段：** 按照问题的时间或者空间特征将问题划分为若干个阶段。\n- **确定状态以及状态变量：** 将问题的不同阶段时期的不同状态描述出来。\n- **确定决策并写出状态转移方程：** 根据相邻两个阶段的各个状态之间的关系确定决策。\n- **寻找边界条件：** 一般而言，状态转移方程是递推式，必须有一个递推的边界条件。\n- **设计程序：** 解决问题\n\n### 实战练习\n\n#### 实战1\n- 题目描述\n{% asset_img 201905152254244.png %}\n\n- 题目解析\n  - 状态: 有 **买入(buy)** 和 **卖出(sell)** 两种状态\n  - 转移方程: \n    对于买来说，买之后可以卖出（进入卖状态），也可以不再进行股票交易（保持买状态）。\n    对于卖来说，卖出股票后不在进行股票交易（还在卖状态）。\n    **所以我们只要考虑当天买和之前买哪个花费更低，当天卖和之前卖哪个收益更高。**\n    $buy = min(buy,prices[i])$\n    $sell = max(sell,prices[i]-buy)$\n  - 边界\n    buy = INT_MAX, sell = 0，最后返回 sell 即可。\n- 代码实现\n  - 方法一: DP\n    ```cpp\n    int maxProfit(vector<int>& prices) {\n        int len=prices.size();\n        if (len<=1) return 0;\n        int buy=INT_MAX,sell=0;\n        for (int i=0;i<len;++i)\n        {\n            buy=min(buy,prices[i]);         // 当天买与之前买选取花费最少的\n            sell=max(sell,prices[i]-buy);   // 当天卖与之前卖选取收益最多的\n        }\n        return sell;\n    }\n    ```\n  - 方法二: Greedy\n    ```cpp\n    int maxProfit(vector<int>& prices) {\n        int len=prices.size();\n        if (len<=1) return 0;\n        int maxVal=0;\n        for(int i=1;i<len;++i)  // 增量就是纯收益\n            if (prices[i]>prices[i-1])\n                maxVal+=prices[i]-prices[i-1];\n        return maxVal;\n    }\n    ```\n\n#### 实战2\n- 题目描述\n  {% asset_img 201905152324445.png %}\n\n- 题目解析\n  - 状态: 有 **买入(buy)** 和 **卖出(sell)** 两种状态\n  - 转移方程: \n    对比上题，这里**可以有无限次的买入和卖出**，也就是说**买入状态之前可拥有卖出状态**，所以买入的转移方程需要变化。\n    **所以我们只要考虑当天买和之前买哪个花费更低，当天卖和之前卖哪个收益更高。**\n    $buy = min(buy,prices[i]-sell)$\n    $sell = max(sell,prices[i]-buy)$\n  - 边界\n    buy = INT_MAX, sell = 0，最后返回 sell 即可。\n- 代码实现\n    ```cpp\n    int maxProfit(vector<int>& prices) {\n        int len=prices.size();\n        if (len<=1) return 0;\n        int buy=INT_MAX,sell=0;\n        for (int i=0;i<len;++i)\n        {\n            buy=min(buy,prices[i]-sell);    // 当天买与之前买选取花费最少的\n            sell=max(sell,prices[i]-buy);   // 当天卖与之前卖选取收益最多的\n        }\n        return sell;\n    }\n    ```\n#### 实战3\n- 题目描述\n  {% asset_img 201905161307449.png %}\n  \n- 题目解析\n  - 状态: 有 **第一次买入（fstBuy） 、 第一次卖出（fstSell）、第二次买入（secBuy） 和 第二次卖出（secSell）** 这四种状态。\n  - 转移方程:\n    这里可以有两次的买入和卖出，也就是说 **买入** 状态之前可拥有 **卖出** 状态，所以买入和卖出的转移方程需要变化。\n    $fstBuy=min(fstBuy,prices[i])$\n    $fstSell=max(fstSell,prices[i]-fstBuy)$\n    $secBuy=min(secBuy,prices[i]-fstSell)$\n    $secSell=max(secSell,prices[i]-secBuy)$\n  - 边界\n    fstBuy = INT_MAX, fstSell = 0\n    secBuy = INT_MAX, secSell = 0\n    最后返回 sell 即可\n- 代码实现\n    ```cpp\n    int maxProfit(vector<int>& prices) {\n        int len=prices.size();\n        if (len<=1) return 0;\n        int fstBuy=INT_MAX,fstSell=0,secBuy=INT_MAX,secSell=0;\n        for(int i=0;i<len;++i)\n        {\n            fstBuy=min(fstBuy,prices[i]);           // 第一次买花费的最小成本\n            fstSell=max(fstSell,prices[i]-fstBuy);  // 第一次卖出获取的最大收益\n            secBuy=min(secBuy,prices[i]-fstSell);   // 第二次买花费的最小成本\n            secSell=max(secSell,prices[i]-secBuy);  // 第二次卖出获取的最大收益\n        }\n        return secSell;\n    }\n    ```\n\n#### 实战4\n- 题目描述\n  {% asset_img 2019051613454010.png %}\n\n- 题目解析\n  - 状态: 有 **第一次买入,第n次买入,第n次卖出** 这三种状态,用$dp[i][0]$表示**第i次买入**,$dp[i][1]$表示**第i次卖出**,这里根据题意i从0开始\n  - 转移方程:\n    只有**第一次买入**时没有之前状态,以后的**买入状态之前都是卖出状态,卖出状态之前都是买入状态**\n    $dp[0][0]=min(dp[0][0],prices[i])$\n    $dp[0][1]=max(dp[0][1],prices[i]-dp[0][0])$\n    $1\\le j\\lt k$时,\n    $dp[j][0]=min(dp[j][0],prices[i]-dp[j-1][1])$\n    $dp[j][1]=max(dp[j][1],prices[i]-dp[j][0])$\n  - 边界\n    dp[i][0]=INT_MAX, dp[i][1]=0 ($0\\le i\\lt k$)\n    最后返回dp[k-1][1]即可\n- 代码实现\n    ```cpp\n    int dp[1000][2];\n    int greedy(vector<int>& prices) // greedy algorithm\n    {\n        int maxVal=0,len=prices.size();\n        for(int i=1;i<len;++i)\n            if (prices[i]>prices[i-1])\n                maxVal+=prices[i]-prices[i-1];\n        return maxVal;\n    }\n    int maxProfit(int k, vector<int>& prices) {\n        int len=prices.size();\n        if (len<=1||k==0) return 0;\n        if (k>=len/2) return greedy(prices);\n        \n        // dp(dynamic programming)\n        for(int i=0;i<k&&i<len;++i)\n            dp[i][0]=INT_MAX;\n        for(int i=0;i<len;++i)\n        {\n            dp[0][0]=min(dp[0][0],prices[i]);\n            dp[0][1]=max(dp[0][1],prices[i]-dp[0][0]);\n            for(int j=1;j<=i&&j<k;++j)\n            {\n                dp[j][0]=min(dp[j][0],prices[i]-dp[j-1][1]);\n                dp[j][1]=max(dp[j][1],prices[i]-dp[j][0]);\n            }\n        }\n        return dp[k-1][1];\n    }\n    ```\n\n### 练习题目\n- 题目描述\n  {% asset_img 2019052209205216.png %}\n\n- 题目解析\n  不难发现,这个问题可以被分解为一些包含最优结构的子问题,而这些子问题是叠加的,我们用$dp[i]$来表示到第$i$级阶梯的方法总数,那么在这一级阶梯之前,我们上一步是在哪级阶梯呢?因为一次只能爬1级或2级阶梯,那么上一步就在第$i-1$级或第$i-2$级阶梯,因此,可以得到的转移方程为:\n  $$\n  dp[i]=dp[i-1]+dp[i-2]\\quad (1)\n  $$\n  这里$dp[i-1]$和$dp[i-2]$就是$dp[i]$的最优子结构,即要想知道到第i级阶梯有多少种方法,那么先求出到第i-1级阶梯和第i-2级阶梯有多少种方法,后面的这两个问题就是第一个问题的最优子结构\n  如果一个问题没有边界,那么就无法得到有限的结果,那么我们再来想边界条件:\n  爬到第一级阶梯需要:$dp[1]=1$种方法\n  爬到第二级阶梯需要:$dp[2]=2$种方法\n- 代码实现\n  ```cpp\n    int climbStairs(int n) {\n        vector<int> dp(n+1,0);\n        dp[1]=1;\n        dp[2]=2;\n        for(int i=3;i<=n;++i)\n            dp[i]=dp[i-1]+dp[i-2];\n        return dp[n];\n    }\n  ```\n  算法时间复杂度$O(n)$\n- 进一步思考\n  针对这道题,实际还有更优的算法,熟悉$Fibonacci$数列的同学可能发现了,其实公式(1)就是$Fibonacci$数列公式,而找第$n$个$Fibonacci$数的公式为:\n  $$\\begin{aligned}\n  F_n=\\frac{1}{\\sqrt{5}}\\left[(\\frac{1+\\sqrt{5}}{2})^n-(\\frac{1-\\sqrt{5}}{2})^n\\right]\n  \\end{aligned}$$\n  具体公式是如何推导出来的,具体可以参考[**知乎:斐波那契数列通项公式是怎样推导出来的？**](https://www.zhihu.com/question/25217301)\n- 代码实现\n  ```cpp\n    int climbStairs(int n) {\n        double sqrt5 = sqrt(5.0);\n        double fibn = pow((1.0+sqrt5)/2,n+1)-pow((1.0-sqrt5)/2,n+1);\n        return (int)(fibn/sqrt5);\n    }\n  ```\n  算法时间复杂度$O(1)$\n  当然还有很多的实现方法,比如:暴力破解 $O(2^n)$,记忆化递归 $O(n)$,Binets $O(logn)$就不一一介绍了","tags":["dp"],"categories":["OJ"]},{"title":"[math]notes","url":"%2Fposts%2F56623%2F","content":"# 0. Notation\n\n> Refer to: [List of mathematical symbols](https://en.wikipedia.org/wiki/List_of_mathematical_symbols)\n> notice:\n> - $\\mathbb B$ is equal to $\\mathbf B$, represented boolean domain\n\n# 1. Norm\n- **Definition**\n- **Absolute-value norm**\n  $||x||=|x|\\quad (1)$\n  实数或复数构成的一维向量空间的norm\n  (1)也为**L1 norm**\n- **Euclidean norm(欧几里得范数)**\n  - n维欧几里得空间$R^n$中,$\\bm x=(x_1,x_2,...,x_n)$\n    $||\\bm x||_2(or ||\\bm x||):=\\sqrt{x_1^2+\\dots+x_n^2}\\quad (2)$\n    (2)为**Euclidean norm**,给出了原点到$X$点的一般距离\n    **注:** $||\\cdot||_2,||\\cdot||$都表示$L_2$范数\n  - n维附属空间$C^n$,可表示为\n    $||\\bm z||_2(or ||\\bm z||):=\\sqrt{|z_1|^2+\\dots+|z_n|^2}=\\sqrt{z_1\\bar{z_1}+\\dots+z_n\\bar{z_n}}\\quad (3)$\n  - $\\bm x$表示列向量时,$\\bm x=(x_1,x_2,\\dots,x_n)^T$,$\\bm x^*$表示**共轭转置(conjugate transpose)**,可表示为\n    $||\\bm x||:=\\sqrt{x^*x}\\quad (4)$\n  - **Euclidean norm**也被称为**Euclidean length,$L^2$distance,$L^2$norm**\n- **Manhattan norm(Taxicab norm)**\n  - 名字即为出租车从矩形街道原点到$\\bm x$点的距离\n    $\\begin{aligned}\n    ||\\bm x||_1:=\\sum_{i=1}^n|x_i|\\quad (5)\n    \\end{aligned}$\n    (5)也被称为**L1 norm**\n- **p-norm**\n  - $p\\geq 1$的实数,向量$\\bm x=(x_1,\\dots,x_n)^T$的p-norm表示为\n    $\\begin{aligned}\n    ||\\bm x||_p:=(\\sum_{i=1}^n|x_i|^p)^\\frac{1}{p}\\qquad (6)\n    \\end{aligned}$\n    当$p=1$时,为**Manhattan norm**\n    当$p=2$时,为**Euclidean norm**\n- **Matrix norm**\n  - a matrix norm is a vector norm in a vector space whose elements (vectors) are matrices (of given dimensions).\n  - $A_{m\\times n}$,第$i$行表示为$\\bm a^i=(a_{i1},\\dots,a_{in})$,第$j$列表示为$\\bm a_j=(a_{1j},\\dots,a_{mj})$,第$i$行$j$列表示为$a_{ij}$,\n  - 将$m\\times n$的矩阵看作$m$个行向量,每个行向量有$n$个元素,即$A=(\\bm a^1,\\dots,\\bm a^m)^T, \\bm a^1=(a_{11},...,a_{1n})^T$\n    $\\begin{aligned}\n    ||A||_p=||vec(A)||_p=(\\sum_{i=1}^m\\sum_{j=1}^n|a_{ij}|^p)^\\frac{1}{p}\n    \\end{aligned}$\n  - **$L_{2,1}$ and $L_{p,q}$ norms**\n    - $p,q\\ge 1$,$L_{p,q}$ norm表示为\n    $\\begin{aligned}\n    ||A||_{p,q}=(\\sum_{i=1}^{m}||\\bm a^i||_p^q)^\\frac{1}{q}=(\\sum_{i=1}^m(\\sum_{j=1}^n|a_{ij}|^p)^\\frac{q}{p})^\\frac{1}{q}\n    \\end{aligned}$\n    - $L_{2,1}$ norm表示为\n    $\\begin{aligned}\n        ||A||_{2,1}=\\sum_{i=1}^m||\\bm a^i||_2=\\sum_{i=1}^m(\\sum_{j=1}^n|a_{ij}|^2)^\\frac{1}{2}\n    \\end{aligned}$\n- **Frobenius norm(F-norm)**\n  - $A_{m\\times n}$,当$L_{p,q}$中,$p=q=2$时,$L_{2,2}$被称为**Frobenius norm(or Hilbert-Schmidt norm)**,可以表示为\n    $$\\begin{aligned}\n        ||A||_F&=||A||_{2,2}\\\\\n        &=\\sum_{i=1}^m||\\bm a^i||_2^2=(\\sum_{i=1}^m(\\sum_{j=1}^n|a_{ij}|^2)^\\frac{2}{2})^\\frac{1}{2}=\\sqrt{\\sum_{i=1}^m\\sum_{j=1}^n|a_{ij}|^2}\\\\\n        &=\\sqrt{trace(A^TA)}=\\sqrt{\\sum_{i=1}^{min\\{m,n\\}}\\sigma_i^2(A)}\n    \\end{aligned}$$\n\n# 2. Space\n- **Definition**\n  - 一个空间是一个有附加结构的集合(set)\n- **Euclidean spaces**\n  - Definition\n    encompasses **two-dimensional Euclidean plane**,**three-dimensional space of Euclidean geometry**, and similar spaces of **higher dimension**\n  - Euclidean distance\n    vector $\\bm x=(x_1,\\dots,x_n)$\n    $||\\bm x||=\\sqrt{\\bm x\\cdot\\bm x}=\\sqrt{\\sum_{i=1}^n x_i^2}\\quad (1)$\n    上式(1)为内积,表示向量$\\bm x$的长度,同时也满足norm的标准,被称为$R^n$空间的**Euclidean norm**\n    $d(\\bm x,\\bm y)=||\\bm x-\\bm y||=\\sqrt{\\sum_{i=1}^n(x_i-y_i)^2}\\quad (2)$\n    上式(2)为距离函数,被称为**Euclidean distance(Euclidean metric)**,为**Pythagorean theorem**(勾股定理)的一个特例\n  - Squared Euclidean distance(SED)\n    $d^2(\\bm x, \\bm y)=||\\bm x-\\bm y||^2(or ||\\bm x-\\bm y||_2^2)=\\sum_{i=1}^n(x_i-y_i)^2$\n- **Linear spaces**\n- **Topological spaces**\n- **Hilbert spaces**\n\n# 3. Matrix\n- **Toeplitz matrix**，形如\n   {% asset_img 18f00e8851de7fb2e91e743abfb00b41.png %}\n- **Hankel matix**，形如\n   {% asset_img 7a76bd718b5429e22c1f320ebb6400bf.png %}\n   刚好和就是toeplitz的transpose\n- **Degree matrix**，这个和拓扑学有关了，此矩阵只有main diagonal上有非零值，代表的是对应edge(node)所连接的vetices的数量（如果自循环则算两个）\n   $G=(V,E), |V|=n$\n   {% asset_img 2019050920271027.png %}\n- **Adjacency matrix**，也和拓扑学有关，为仅有1或者0的矩阵。\n   如果两个edge之间有vertex相连，则对应位置填1。因为这个性质，此矩阵为symmetric的，main diagonal上的1表示自循环。\n\n|             Labeled graph             |           Degree matrix            |        Adjacency matrix         |          Laplacian matrix           |\n| :-----------------------------------: | :--------------------------------: | :-----------------------------: | :---------------------------------: |\n| {% asset_img 175px-6n-graf.svg.png %} | {% asset_img 0836612538d722.svg %} | {% asset_img 56ef36960e2.svg %} | {% asset_img e495d70fe91b272.svg %} |\n\n- **Laplacian matix**。由上面两位计算得到:\n   $L=D-A$\n\n- **Circulant matrix**, T的变种，如下\n   {% asset_img cb126605ae067e1f4bae13598a2a39f8.png %}\n\n- **Symplectic matrix**\n   指满足这个条件的$M_{2n\\cdot{2n}}$矩阵：$M^T\\Omega M=\\Omega$.\n   其中,另一个矩阵必须是nonsingular, skew-symmetric matrix.，例如选$\\Omega=\\begin{bmatrix}\n   0 & I_n \\\\ -I_n & 0 \\\\    \n   \\end{bmatrix}$ 是一个block matrix,I是单位矩阵(identity matrix)\n\n- **Vandermonde matrix**,形如\n   {% asset_img 642ce6b42c22729068792a6496d81ee7.png %}\n\n-  **Hessenberg matrix**\n    Hessenberg matrix is a special kind of square matrix, one that is \"almost\" triangular. To be exact, an upper Hessenberg matrix has zero entries below the first subdiagonal, and a lower Hessenberg matrix has zero entries above the first superdiagonal\n    例如：upper Hessenberg matrix\n    {% asset_img dd7c78e1ed6e6c999036fed54fe648d0.png %}\n\n- **Hessian matrix**\n    $f : \\mathbb{R}^{n} \\rightarrow \\mathbb{R}$，输入向量$\\mathbf{x} \\in \\mathbb{R}^{n}$,输出标量$f(\\mathbf{x}) \\in \\mathbb{R}$,定义如下\n    {% asset_img f7296865484b39fcbac598a99b7f3dbb.png %}\n    有如下性质:\n    $\\mathbf{H}(f(\\mathbf{x}))=\\mathbf{J}(\\nabla f(\\mathbf{x}))^{\\mathrm{T}}$\n\n- **Jacobian matrix**\n    $\\mathbf{f} : \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}$,输入向量$\\mathbf{x} \\in \\mathbb{R}^{n}$,输出向量$\\mathbf{f}(\\mathbf{x}) \\in \\mathbb{R}^{m}$,定义如下\n    $$\\begin{aligned}\n        \\mathbf{J}=\\left[\\begin{array}{ccc}{\\frac{\\partial \\mathbf{f}}{\\partial x_{1}}} & {\\cdots} & {\\frac{\\partial \\mathbf{f}}{\\partial x_{n}}}\\end{array}\\right]=\\left[\\begin{array}{ccc}{\\frac{\\partial f_{1}}{\\partial x_{1}}} & {\\cdots} & {\\frac{\\partial f_{1}}{\\partial x_{n}}} \\\\ {\\vdots} & {\\ddots} & {\\vdots} \\\\ {\\frac{\\partial f_{m}}{\\partial x_{1}}} & {\\cdots} & {\\frac{\\partial f_{m}}{\\partial x_{n}}}\\end{array}\\right]\n    \\end{aligned}$$\n\n- **Idempotent matrix(幂等阵)**\n    Definition: $M^2=M$\n    性质:\n    - $\\lambda$只能为0或1\n    - $A\\cdot(A-E)=0$,$A-E$的每列都为$Ax=0$的解 \n    - $E+A$可逆(因为$E+A$所有$\\lambda$大于0)\n    - $\\lambda_1=1$是$r(A)$重根,$\\lambda_2=0$是$n-r(A)$重根,且有n个线性无关特征向量,即$\\exist$可逆$P$,使$P^{-1}AP=\\begin{bmatrix}\n        E_r & 0 \\\\\n        0   & 0 \\\\\n    \\end{bmatrix}$\n    - 幂等阵必相似于对角阵,$\\exist$可逆$P$,使$P^{-1}AP=\\Lambda=\\begin{bmatrix}\n        E_r&0\\\\\n        0&0\\\\\n    \\end{bmatrix}$\n      > 证明如下:\n      $A^2=A\\implies(E-A)\\cdot{A}=0$,$A$中有$r(A)$个线性无关列向量(即$(E-A)x=0$的解向量)$\\implies\\lambda_1=1$至少是$r(A)$重特征值$\\qquad (1)$\n      $Ax=0\\implies(0\\cdot{E}-A)x=0,\\lambda_2=0$至少是$n-r(A)$重根$\\qquad (2)$\n      由$(1),(2)\\implies\\lambda_1=1$($r(A)$重根),$\\lambda_2=0$($n-r(A)$重根)\n\n- **Orthogonal matrix(正交阵)**\n  - Definition: $A_{n\\times n},A^TA=E(A^{-1}=A^T)$,则称$A$为正交阵\n  - Properties:\n    - $A^T=A^{-1},A^TA=E$\n    - $A$的列向量都是单位向量,且两两正交\n      - 引申:若已知$A$中某$a_{ij}=1$则该$a_{ij}$所在的行与列的其他元素为0\n    - $A\\bm x=\\bm b$有唯一解$\\bm x=A^{-1}\\bm b$\n- **Centering matrix**\n    Definition: $\\bm 1=(1,\\dots,1)^T, C_n=I_n-\\frac{1}{n}\\bm {11}^T$\n    example: \n    $C_1=\\left[0\\right]$\n    $C_2=\n    \\begin{bmatrix}\n        1&0\\\\\n        0&1\\\\\n    \\end{bmatrix}-\\frac{1}{2}\n    \\begin{bmatrix}\n        1&1\\\\\n        1&1\\\\\n    \\end{bmatrix}=\n    \\begin{bmatrix}\n        \\frac{1}{2}&-\\frac{1}{2}\\\\\n        -\\frac{1}{2}&\\frac{1}{2}\\\\\n    \\end{bmatrix}$\n- **Similarity matrix(distance matrix)**\n- **Euclidean distance matrix**\n  - Definition: $\\bm x_1,\\cdots,\\bm x_n$ are defined on **m-dimensional space**,$\\bm x_1=(x_1,\\dots,x_m)$,the elements of $A$ are given by \n  $A=(a_{ij})$\n  $a_{ij}=d_{ij}^2=||\\bm x_i-\\bm x_j||_2^2$\n  where $||\\cdot||_2$ denotes **2-norm** on $\\bm R^m$\n  {% asset_img 79b8411936e4b4f.svg %}\n- **Scatter matrix**\n  - Definition: $n$ samples of m-dimensional data, represented as $m\\times n$ matrix, $X=(\\bm x_1,\\dots,\\bm x_n)$, sample mean is\n  $\\overline \\bm x = \\frac{1}{n}\\sum_{j=1}^n \\bm x_j$\n  where $\\bm x_j=(x_{1j},\\dots,x_{mj})^T$ is j-th column of $X$\n  The **scatter matrix** is $m\\times m$ positive **semi-definite matrix**\n  $S=\\sum_{j=1}^n(\\bm x_j - \\overline \\bm x)(\\bm x_j - \\overline \\bm x)^T=\\sum_{j=1}^n(\\bm x_j - \\overline \\bm x)\\otimes(\\bm x_j - \\overline \\bm x)=(\\sum{j=1}^n\\bm x_j\\bm x_j^T)-n\\overline\\bm x \\overline\\bm x^T$\n  mutiplication is regarded as outer product,the scatter matrix can be also expressed as\n  $S=XC_n X^T$\n  where $C_n$ is $n\\times n$ centering matrix\n- **Projection matrix**\n  \n- **Transformation matrix**\n  - Definition: If $T$ is a linear transformation mapping $\\mathbb {R} ^{n}$ to $\\mathbb {R} ^{m}$ and $\\vec {x}$ is a column vector with $n$ entries, then\n    $T( \\vec x ) = \\mathbf{A} \\vec x$\n    for some $m\\times n$ matrix $A$, called the transformation matrix of $T$. Note that $A$ has $m$ rows and $n$ columns, whereas the transformation $T$ is from $\\mathbb {R} ^{n}$ to $\\mathbb {R} ^{m}$. \n  - Non-linear transformations\n    - affine transformations\n    - projective transformations\n- **Affine transformation matrix**\n- **Projective transformation matrix**\n# 4. Definiteness of a matrix\n- **Definitions for real matrices**\n  A $n\\times n$ symmetric real matrix $M$ is said to be **positive definite** if $\\bm x^TM\\bm x>0$ for all non-zero $\\bm x$ in $\\mathbb {R} ^{n}$. Formally,\n  $$\\boxed{M{\\text{ positive definite}}\\quad \\iff \\quad \\bm x^{\\textsf {T}}M\\bm x>0{\\text{ for all }}\\bm x\\in \\mathbb {R} ^{n}\\setminus \\mathbf {0} }$$\n  \n  A $n\\times n$ symmetric real matrix $M$ is said to be **positive semidefinite** or **non-negative definite** if $\\bm x^{\\textsf {T}}M\\bm x\\geq 0$ for all non-zero $\\bm x$ in $\\mathbb {R} ^{n}$. Formally,\n  $$\\boxed{\\displaystyle M{\\text{ positive semi-definite}}\\quad \\iff \\quad \\bm x^{\\textsf {T}}M\\bm x\\geq 0{\\text{ for all }}\\bm x\\in \\mathbb {R} ^{n}\\setminus \\mathbf {0} }$$\n\n  A $n\\times n$ symmetric real matrix $M$ is said to be **negative definite** if $\\bm x^{\\textsf {T}}M\\bm x<0$ for all non-zero $\\bm x$ in $\\mathbb {R} ^{n}$. Formally,\n  $$\\boxed{\\displaystyle M{\\text{ negative definite}}\\quad \\iff \\quad \\bm x^{\\textsf {T}}M\\bm x<0{\\text{ for all }}\\bm x\\in \\mathbb {R} ^{n}\\setminus \\mathbf {0} }$$\n\n  A $n\\times n$ symmetric real matrix $M$ is said to be **negative semidefinite** or **non-positive definite** if $\\bm x^{\\textsf {T}}M\\bm x\\leq 0$ for all non-zero $\\bm x$ in $\\mathbb {R} ^{n}$. Formally,\n  $$\\boxed{\\displaystyle M{\\text{ negative semi-definite}}\\quad \\iff \\quad \\bm x^{\\textsf {T}}M\\bm x\\leq 0{\\text{ for all }}\\bm x\\in \\mathbb {R} ^{n}\\setminus \\mathbf {0} }$$\n\n  A $n\\times n$ symmetric real matrix which is neither **positive semidefinite** nor **negative semidefinite** is called **indefinite**.","tags":["notes"],"categories":["math"]},{"title":"[leetcode]5055.困于环中的机器人","url":"%2Fposts%2F272%2F","content":"{% asset_img 2019051215261229.png %}\n\n### 解题思路\n- 检测最终状态,如果改变了方向(无论过程中移动到了何处),经过了≤4轮,最终一定会回到变为初始状态(回到原处且面向北方),那么必然是无法离开\n- 如果指令结束时回到了原点,那么必然是无法离开\n\n### 代码实现\n\n方法一: 4轮指令过程中一定可以验证是否可以离开\n```cpp\nbool isRobotBounded(string ins) \n{\n    set<tuple<int,int,int,int>> dict;\n    int dx[]={-1,0,1,0},dy[]={0,-1,0,1};\n    int x=0,y=0,len=ins.size(),dir=0;\n    for (int i=0;i<=4;++i)\n    {\n        for (int j=0;j<len;++j)\n        {\n            if (ins[j]=='L')\n                dir++;\n            else if (ins[j]=='R')\n                dir+=3;\n            else if (ins[j]=='G')\n            {\n                x+=dx[dir];\n                y+=dy[dir];\n            }\n            dir%=4;\n            if (dict.count(make_tuple(x,y,j,dir)))\n                return true;\n            dict.insert(make_tuple(x,y,j,dir));\n        }\n    }\n    return false;\n}\n```\n\n方法二: $O(n)$\n```cpp\nbool isRobotBounded(string instructions)\n{\n    int dx[]={0,1,0,-1},dy[]={-1,0,1,0}, x=0,y=0,dir=0; // 左下右上\n    for (int i=0;i<ins.size();++i)\n    {\n        if (ins[i]=='L')\n            dir=(dir+1)%4;\n        else if (ins[i]=='R')\n            dir=(dir+3)%4;\n        else\n        {\n            x+=dx[dir];\n            y+=dy[dir];\n        }\n    }\n    return (x==0&&y==0)||(dir>0);\n}\n```\n\n### 结论\n- 只要最终回到原点,那么一定是bounded\n- 只要最终面向的不是北方,那么将会在剩下1到3轮回到最开始的状态(即指令结束时只要改变了方向,那么每轮都会改变方向,最终回到原点)","tags":["leetcode"],"categories":["OJ"]},{"title":"[schedule]TODO","url":"%2Fposts%2F55269%2F","content":"### Problem Solution\n1. zhihu\n2. baidu\n3. bilibili\n4. google\n5. paper\n\n### Book list\n- [ ] 深度学习入门\n- [ ] Python神经网络编程\n- [ ] 动手学深度学习\n- [ ] Deep Learning\n\n### 1.Days work(High efficiency)\n- [x] 300 vocabulary\n- [x] 10 page reading\n- [x] leetcode review 2\n- [x] leetcode 2 AC\n- [ ] adaboost(2)\n- [ ] 矩阵分析与应用\n- [ ] 凸优化\n\n### 2.Extensive reading(One book at least twice for learning)\n| Book                                   | Date | Progress |\n| :------------------------------------- | :--- | :------- |\n| Grade4RS                               | 6/24 | 87-98    |\n| Social Studies For Our Children Book 1 |      |          |\n| Social Studies For Our Children Book 2 |      |          |\n| Social Studies For Our Children Book 3 |      |          |\n| Social Studies For Our Children Book 4 |      |          |\n| Social Studies For Our Children Book 5 |      |          |\n| Social Studies For Our Children Book 6 |      |          |\n\n### 3.Papers\n- 5/20 Semi-supervised feature selection via rescaled linear regression\n- 5/20 Semi-supervised Feature Selection Based on Least Square Regression with Redundancy Minimization\n\n### 当前\n- [ ] 没有原域样本,怎么进行transfer\n- [ ] 支持向量机\n- [ ] EM算法\n- [ ] 图聚类算法\n- [ ] 流形学习\n- [ ] 稀疏分解算法\n- [ ] 矩阵低秩分解算法\n### 计划(一年)\n- 目标\n  - 广泛了解,为后续阅读论文打基础\n  - 详细理解代码,并能自己构造,为工程打基础\n  - 谨记:不要求快\n- 6月(第一遍目标:广泛了解)\n  - [x] 机器学习实战(第三部分)\n  - [ ] 矩阵分析与应用\n  - [ ] 矩阵低秩分解\n- 7月(第一遍目标:广泛了解)\n  - [ ] 西瓜书\n    - [ ] 先整体仔细过一遍(包括公式推导)\n    - [ ] 第2遍仔细过一遍\n  - [ ] 深度学习入门\n    - [x] 先整体过一遍\n    - [ ] 第2遍一定要把代码全部理解,并能自己构造\n  - [x] Python神经网络编程\n    - [x] 可以自己构造\n  - [ ] CS229,CS231n(深度学习),CS系列\n  - [ ] PyTorch\n  - [ ] FastRCNN\n  - [ ] DeepLearning-500-questions\n  - [ ] 深度学习(迁移学习 行人再识别)\n  - [ ] 动手学深度学习\n  - [ ] Deep Learning\n  - [ ] 矩阵分析与应用\n  - [ ] VGG,ResNet,GoogLeNet\n  - [ ] cuDNN,CUDA\n  - [ ] 分布式学习是一个很好的探索反向(节省大量跑数据的时间)\n  - [ ] 无监督学习名作:Deep Belief Network,Deep Boltzmann Machine\n  - [ ] DCGAN,GAN\n  - [ ] 自动驾驶是一个不错的方向:SegNet\n- 业余时间\n  - [ ] 复习总结\n  - [ ] 博客完善\n  - [ ] 凸优化\n### 了解一下\n- 概率图模型，近似推断，采样方法，自回归模型，高斯过程，强化学习\n","tags":["TODO"],"categories":["schedule"]},{"title":"[leetcode]375. Guess Number Higher or Lower II","url":"%2Fposts%2F42608%2F","content":"{% asset_img 2019050820074824.png %}\n\n### 题目含义\n\n给定一个n值为最大值,我从中选出一个数,然后你来猜,猜错了我会告诉你你的数字大了还是小了,并罚你所说数字的钱数,然后再猜,问你最少有多少钱才能保证你一定能猜对?\n\n### 解题思路\n\n没想到是dp的思想,关键在于这个最少是多少钱如何理解.\n假设n=5,你可能会想到,如果你有2+3+4+5=14元,那就一定能猜对,因为我选的数字一定会$\\ge$1,没错,但是这不是最少的钱数,即应该说如果你运气足够差的情况下(即每次都猜错),所花的最少钱数,在leetcode的discuss版块看到的解释:\n\n{% asset_img 2019050820370125.png %}\n\n这道题的意思是,你足够聪明能够选到一种策略,每次都按这个策略来,然后所花费的钱一定会最少,比如:\n\n| n值    | 猜测序列(罚款最多的猜法) | 罚款 |\n| :----- | :----------------------: | :--- |\n| n=1时  |                          | 0    |\n| n=2时  |            1             | 1    |\n| n=3时  |            2             | 2    |\n| n=4时  |           1->3           | 4    |\n| n=5时  |           2->4           | 6    |\n| n=6时  |           3->5           | 8    |\n| n=7时  |           4->6           | 10   |\n| n=8时  |           5->7           | 12   |\n| n=9时  |           6->8           | 14   |\n| n=10时 |           7->9           | 16   |\n\n上述表格的意思,拿n=10来说,我只要有16元,我就一定能够猜对,n=9来说,我只要有14元,我就一定能够猜对,下面来分析,比如n=10时:\n- 如果选的数是8,先猜7,再猜9,最后必然猜对,则花16元,花光了\n- 如果选的数是4,先猜7,而这时你知道是比7小了,那么即n=6了,在按n=6的方法来猜,再猜3,再猜5,最后猜对了, 总共花的钱数为7+3+5=15元,手里还剩1元\n\n如果上面没懂,再来分析下面,即:\n1. 如果n=2\n   - 你猜1(错了),再猜就必然正确,罚1元\n   - 你猜2(错了),再猜就必然正确,罚2元\n   - n=2时,罚款最少只要1元\n2. 如果n=3\n   - 你猜1(错了),再猜2(错了),再猜就必然正确,罚3元\n   - 你猜1(错了),再猜3(错了),再猜就必然正确,罚5元\n   - 你猜2(错了),再猜就必然正确,罚2元\n   - 你猜3(错了),再猜1(错了),再猜就必然正确,罚4元\n   - 你猜3(错了),再猜2(错了),再猜就必然正确,罚5元\n   - n=3时,罚款最少只要2元\n\n通过总结以上规律,假设要猜数的范围在[i,j],每次你猜一个数k,就可以划分出两个区域[i,k-1],[k+1,j],然后就可以确定一个区域必然没有我选的数,因此,需要罚款k元,那么你要付的钱即为:k+max([i,k-1]范围猜对最少要花的钱,[k+1,j]范围猜对最少要花的钱),而k可以取的范围为[i,j],即你可以猜的数为[i,j],那么综合出所有猜的结果,选出最少花费的钱数,核心公式即为:\n\n$$\ndp[i][j]=min(dp[i][j],k+max(dp[i][k-1],dp[k+1][j]))\n$$\n> 提示1: $k+max(dp[i][k-1],dp[k+1][j])$ 即运气足够差的情况下,要花的钱\n> 提示2: $min(dp[i][j],k+max(dp[i][k-1],dp[k+1][j]))$ 即选出所有选择中要花的最少的钱\n\n### 代码实现: DP思想\n时间复杂度$O(n^3)$\n```cpp\nint dp[500][500];\nint getMoneyAmount(int n) {\n    for (int d=0;d<n;++d)\n    {\n        for (int i=1;i+d<=n;++i)\n        {\n            int j=i+d;\n            dp[i][j]=i==j?0:INT_MAX;\n            for (int k=i;k<=j;++k)\n                dp[i][j]=min(dp[i][j],k+max(dp[i][k-1],dp[k+1][j]));\n        }\n    }\n    return dp[1][n];\n}\n```\n\n### 结论\n就像汉诺塔的递归一样,要细抠一层层递归真的很难理解,但是如果记住大体思想就能写出来.这道题也是这样,记住了核心公式的含义就能解出来,即,**假如我当前猜k,没猜对,然后我已经知道了如果要猜的数落在[i,k-1]范围,我要花费多少钱就一定能猜对,如果落在[k+1,j]范围,我要花多少钱就一定能猜对,那么选取这两个中花钱最多的与k相加就是我本轮在先猜k的条件下赢得比赛(最终猜出来)需要要花的最多钱数.**没想到代码这么少,细抠却如此难理解,终于理解为什么这道题有695个鄙视了\n\n---\n\n最后,再给出我是如何得到n=1~10的那张表格的代码部分\n\n```cpp\nint dp[500][500];\nstring dps[500][500];\nint getMoneyAmount(int n) \n{\n    for (int d = 0; d < n; ++d)\n    {\n        for (int i = 1; i + d <= n; ++i)\n        {\n            int j = i + d;\n            dp[i][j] = i==j?0:INT_MAX;\n            for (int k = i; k <= j; ++k)\n            {\n                string str = to_string(k);\n                if (dp[i][k - 1]!=0|| dp[k + 1][j]!=0)\n                {\n                    if (dp[i][k - 1] >= dp[k + 1][j])\n                        str += \"->\" + dps[i][k - 1];\n                    else\n                        str += \"->\" + dps[k + 1][j];\n                }\n                if (dp[i][j] > k + max(dp[i][k - 1], dp[k + 1][j]))\n                    dps[i][j] = str;\n                dp[i][j] = min(dp[i][j], k + max(dp[i][k - 1], dp[k + 1][j]));\n            }\n        }\n    }\n    return dp[1][n];\n}\n\nint main()\n{\n    int n = 10;\n    int a = getMoneyAmount(n);\n    for (int i = 1; i < n; ++i)\n        for (int j = i + 1; j <= n; ++j)\n            printf(\"[%d,%d]: %s\\n\", i, j, dps[i][j].c_str());\n    return 0;\n}\n```\n","tags":["dp"],"categories":["OJ"]},{"title":"[leetcode]312. Burst Balloons","url":"%2Fposts%2F31611%2F","content":"{% asset_img 2019050817052623.png %}\n\n> 题目含义: n个气球,下标从0到n-1,对应于数组nums.每个气球都有一个编号nums[i],每当你扎破一个气球就可以得到$nums[left]\\cdot{nums[i]}\\cdot{nums[right]}$的硬币,这里的left和right是紧挨着气球i的两个气球,当气球i被扎破后,气球left和right就相邻了.请你求出将所有气球扎破后,你能获取的最大的金币数.\n\n注意: 题目要求你假设nums[-1]=nums[n]=1,也就是说当你要扎破第一个气球时,得到的硬币是$nums[-1]\\cdot{nums[0]}\\cdot nums[1]$,扎破最后一个气球时,得到的硬币是$nums[n-2]\\cdot{nums[n-1]}\\cdot{nums[n]}$的,即假设只有一个气球,且编号为3,那么扎破它得到的硬币数为$nums[-1]\\cdot{nums[0]}\\cdot{nums[1]} = 1\\cdot{3}\\cdot{1} = 3$\n\n### 解题思路: DP的思想\n\n该问题与[[leetcode]1039. Minimum Score Triangulation of Polygon](/2019/05/07/leetcode-1039-Minimum-Score-Triangulation-of-Polygon/)相似,DP的思想即将一个大问题,划分为多个小问题来求解,并且大问题和小问题应当存在一种递推(堆叠)的关系.\n在此问题中,我们将$dp[i][j]$用于表示把气球$i$到气球$j$全部扎破后所得到的最大金币数.假设我们最后扎破的气球为$k$,满足$i\\leq{k}\\leq{j}$的关系,那么这里就有一个递推关系,如下图所示\n\n{% asset_img 20190508052601.png %}\n\n> 如何理解上述的递推关系呢?\n\n我们做下处理将原题的下标0~n-1变成1~n(后面会解释原因),假设现在有n个气球,则:\n$1$表示左边界,$n$表示右边界,那么两边确定了,通过枚举最后扎破的气球k来解答.\n假设要扎破气球$1~n$,那么最后扎破的气球可能为$1,...,n-1,n$有如下三种情况\n- $k=1$时,即最后扎破的气球为序号最前的气球,那么最大硬币数为$dp[1][n]=0+dp[k+1][n]+1\\cdot{nums[k]}\\cdot{1}$\n- $k=n$时,即最后扎破的气球为序号最后的气球,那么最大硬币数为$dp[1][n]=dp[1][k-1]+0+1\\cdot{nums[k]}\\cdot{1}$\n- $1\\le{k}\\le{n}$时,即最后扎破的气球为中间的这些气球的一个(不在两端),那么最大硬币数为$dp[1][n]=dp[1][k-1]+dp[k+1][n]+1\\cdot{nums[k]}\\cdot{1}$\n\n需要注意的是,从1到n的总硬币数是要从小到大来计算的,即,上述的d[k+1][n],d[1][k-1],d[k+1][n]都是通过之前的计算算出,那么递推公式如下,假设要扎破气球$i~j(1\\le{i}\\leq{j}\\le{n})$,那么最后扎破的气球可能为$i,...,j$有如下三种情况\n- $k=i$,即最后扎破的气球为序号最前的气球,那么最大硬币数为$dp[i][j]=0+dp[k+1][j]+nums[i-1]\\cdot{nums[k]}\\cdot{nums[k]}$\n- $k=j$,即最后扎破的气球为序号最后的气球,那么最大硬币数为$dp[i][j]=dp[i][k-1]+0+nums[i-1]\\cdot{nums[k]}\\cdot{nums[k]}$\n- $i\\le{k}\\le{j}$,即最后扎破的气球为中间的这些气球的一个(不在两端),那么最大硬币数为$dp[i][j]=dp[i][k-1]+dp[k+1][j]+nums[i-1]\\cdot{nums[k]}\\cdot{nums[k]}$\n\n将以上两种情况都考虑进去(包含两端和不包含两端的),则即为代码部分的特殊处理,代码用A来代表nums了,将A的首部插入一个1,尾部也插入一个1,而气球真正的数量为n个,气球的标号为A[1]~A[n],A[0]和A[1]设置为1,即为题目的条件nums[-1]=nums[n]=1,则计算过程中无论是否是端点的情况,都可以正常计算了,说起来很麻烦,如果文字部分没理解的话可以看看下面的视频讲解\n\n\n下面是一位老哥的视频讲解:\n{% youtube IFNibRVgFBo %}\n\n### 实现代码: DP的思想\n\n```cpp\nint dp[600][600];\nint maxCoins(vector<int>& A) {\n    if (A.empty()) return 0;\n    int len=A.size();\n    A.insert(A.begin(),1);\n    A.push_back(1);\n    for (int d=0;d<len;++d) // d为i与j之间的间隔\n    {\n        for (int i=1;i+d<=len;++i)\n        {\n            int j=i+d;\n            for (int k=i;k<=j;++k)  // 在i与j之间(包含i,j)枚举k\n                // start i, end j, final burst k\n                dp[i][j] = max(dp[i][j], dp[i][k-1]+A[i-1]*A[k]*A[j+1]+dp[k+1][j]);\n        }\n    }\n    return dp[1][len];\n}\n```","tags":["dp"],"categories":["OJ"]},{"title":"[leetcode]1039. Minimum Score Triangulation of Polygon","url":"%2Fposts%2F35653%2F","content":"{% asset_img 20190507422058.png %}\n题目含义: 一个凸多边形由$N$个顶点构成,每个顶点有一个数值,顶点顺时针排列为$A[0],A[1],...,A[N-1]$,假设你将多边形分成$N-2$个三角形,每一个三角形的值为三个顶点值的乘积,多边形的总分数是构成它的$N-2$个三角形的值的总和,求一个$N$多边形的最小总分数.\n\n\n### 解题思路: DP动态规划的思想\n\n首先,$dp[i][j]$表示顺时针从$i$到$j$构成的多边形的最小分数,于是取一点$k$,满足$i<k<j$,再取$d$表示$i$到$j$的距离,进行枚举,在一个大的多边形中,$d=2$先把周边一圈小三角形枚举一边,$d=3$再把周边一圈的四边形枚举一遍(由$d=2$时求出了小三角形的分数计算得出四边形的最小分数),...,$d=N-1$再把$N$边形枚举一边,如下图所示:\n\n{% asset_img 20190508122646.jpg %}\n\n核心公式为: \n\n$$\ndp[i][j] = min(dp[i][j], dp[i][k]+dp[k][j]+A[i]\\cdot{A[k]}\\cdot{A[j]})\n$$\n\n### 代码实现: DP思想\n\n```cpp\nint dp[100][100];\nint minScoreTriangulation(vector<int>& A) \n{\n    int len=A.size();\n    for (int d=2;d<len;++d) // d作为i与j之间的间距\n    {\n        for (int i=0; i+d<len; ++i)\n        {\n            int j=i+d;\n            dp[i][j]=INT_MAX;\n            for (int k=i+1;k<j;++k)\n                dp[i][j]=min(dp[i][j],dp[i][k]+dp[k][j]+A[i]*A[k]*A[j]);\n        }\n    }\n    return dp[0][len-1];\n}\n```","tags":["dp"],"categories":["OJ"]},{"title":"[algorithm]线性表","url":"%2Fposts%2F24346%2F","content":"## 一. 线性表基础算法\n\n### 1.线性表插入操作\n\n线性表插入操作(在第$i(1≤i≤L.length+1)$个位置上插入新元素$elem$)\n\n```cpp\nbool InsertSeq( SeqList& L, int i, ElemType elem )\n{\n    if ( i < 1 || i>L.length + 1 || L.length >= MAXSIZE )\n        return false;\n    for ( j = L.length - 1; j >= i - 1; j-- )\n        L.elem[j + 1] = L.elem[j];\n    L.elem[j + 1] = elem;\n    L.length++;\n    return true;\n}\n```\n\n说明:\n- 插入操作: 可选位置为$1≤i≤L.length+1$\n- 最好情况: 表尾$(i=n+1)$插入, $O(1)$\n- 最坏情况: 表头$(i=1)$插入, $O(n)$\n- 平均情况: 设 $P_i=\\frac{1}{(n+1)}$ 是在第$i$个位置插入一个结点的概率,则在长度为$n$的线性表中插入一个结点所需的移动结点的平均次数为$\\frac {n}{2}$次,即$O(n)$:\n\n$$\n\\sum_{i=1}^{n+1}{P_i}⋅(n+1−i)=\\frac{1}{n+1}\\cdot\\sum_{i=1}^{n+1}(n−i+1)=\\frac{1}{n+1}\\cdot\\frac{n(n+1)}{2}=\\frac{n}{2}\n$$\n\n### 2.线性表删除操作\n\n```cpp\nbool DeleteSeq( SeqList& L, int i, ElemType& elem )\n{\n    for ( i<1 || i>L.length ) return false;\n    elem = L.elem[i - 1];\n    for ( j = i; j < L.length; j++ )\n        L.elem[j - 1] = L.elem[j];\n    L.length--;\n    return true;\n}\n```\n\n说明:\n- 最好情况: 删除表位$(i=n)$,$O(1)$\n- 最坏情况: 删除表头$(i=1)$,$O(n)$\n- 平均情况: 设$P_i=\\frac{1}{n}$是删除第$i$个位置上结点的概率,则在长度为$n$的线性表中删除一个结点所需移动结点的平均次数为$\\frac{n−1}{2}$次,即$O(n)$:\n\n$$\n\\sum_{i=1}^{n}{Pi}\\cdot{(n−i)}=\\frac{1}{n}\\sum_{i=1}^{n}n(n−i)=\\frac{1}{n}\\cdot\\frac{n(n−1)}{2}=\\frac{n−1}{2}\n$$\n\n### 3.线性表查找操作\n\n```cpp\nint LocateSeq( SeqList& L, ElemType elem )\n{\n    for ( i = 0; i < L.length; i++ )\n        if ( L.elem[i].key == elem.key )\n            return i + 1;\n    return 0;\n}\n```\n\n说明:\n- 最好情况: 查找到表头,$O(1)$\n- 最坏情况: 查找到表尾,$O(n)$\n- 平均情况: 设$P_i=\\frac{1}{n}$是查找元素在第$i(1≤i≤L.length)$个位置上的概率,则在长度为$n$的线性表中查找值为$elem$的元素所需比较的平均次数为$\\frac{n+1}{2}$次,$O(n)$:\n\n$$\n\\sum_{i=1}^{n}P_i\\cdot{i}=\\frac{1}{n}\\cdot\\sum_{i=1}^{n}i=\\frac{1}{n}\\cdot\\frac{n(n+1)}{2}=\\frac{n+1}{2}\n$$\n\n## 二.线性表综合应用\n\n### 1.删除线性表中所有值为$x$的数据元素\n\n```cpp\nbool DeleteX( SeqList& L, ElemType x )\n{\n    int k = 1;\n    for ( i = 1; i <= L.length; i++ )\n        if ( L.elem[i].key != x.key )\n            L.elem[k++] = L.elem[i];\n    L.length = k;\n    return true;\n}\n```\n\n### 2.从有序顺序表中删除值在$[s,t]$的所有元素\n\n```cpp\nbool DeleteS2TOrderedSeq( SeqList& L, int s, int t )\n{\n    for ( i = 1; i <= L.length&&L.elem[i].key < s; i++ );    // 找≥s的第一个元素\n    for ( j = i; j <= L.length&&L.elem[j].key <= t; j++ );    // 找>t的第一个元素\n    while ( j <= L.length )\n        L.elem[i++] = L.elem[j++];\n    L.length = i;\n    return true;\n}\n```\n\n### 3.从顺序表中删除值在$[s,t]$的所有元素\n\n```cpp\nbool DeleteS2TSeq( SeqList& L, int s, int t )\n{\n    int k = 1;\n    for ( i = 1; i <= L.length; i++ )\n        if ( L.elem[i].key<s || L.elem[i].key>t )\n            L.elem[k++] = L.elem[i];\n    L.length = k;\n    return true;\n}\n```\n\n### 4.从有序顺序表中删除所有值重复的元素\n\n```cpp\nbool DeleteSameOrderedSeq( SeqList& L )\n{\n    int k = 1;\n    for ( i = 2; i <= L.length; i++ )\n        if ( L.elem[i].key != L.elem[k].key )\n            L.elem[++k] = L.elem[i];\n    L.length = k;\n    return true;\n}\n```\n\n### 5.将两个有序顺序表合并为一个新的有序顺序表\n\n```cpp\nbool Merge( SeqList A, SeqList B, SeqList& C )\n{\n    int i = 1, j = 1, k = 1;\n    while ( i<=A.length&&j<=B.length )\n    {\n        if ( A.elem[i].key <= B.elem[j].key )\n            C.elem[k++] = A.elem[i++];\n        else\n            C.elem[k++] = B.elem[j++];\n    }\n    while ( i <= A.length ) C.elem[k++] = A.elem[i++];\n    while ( j <= B.length ) C.elem[k++] = B.elem[j++];\n    C.length = k - 1;\n    return true;\n}\n```\n\n### 6.原数组$A[m+n]={a_1,a_2,...,a_m,b_1,b_2,...,b_n}$,现要求转变为$A[m+n]={b_1,b_2,...,b_n,a_1,a_2,...,a_m}$\n\n```cpp\n// 元素倒置\nvoid Reverse( ElemType A[], int s, int e )\n{\n    for ( i = s; i < ( s + e ) / 2; i++ )\n        swap( A[i], A[s + e - i - 1] );\n}\n\nvoid ExChange( ElemType A[], int m, int n )\n{\n    Reverse( A, 0, m );\n    Reverse( A, m, m + n );\n    Reverse( A, 0, m + n );\n} \n```\n\n### 7.线性表$(a_1,a_2,...,a_n)$递增有序,设计算法花最少时间找到数值为$x$的元素:\n\n> 1)找到,则与其后继元素位置互换\n> 2)未找到,将其插入表中并使表中元素仍然递增有序\n\n```cpp\n// 使用折半查找的方法\nvoid SearchExchangeInsert( ElemType A[], int n, ElemType x )\n{\n    int low = 1, high = n;\n    while ( low <= high )\n    {\n        mid = ( low + high ) / 2;\n        if ( x.key == A[mid].key )\n        {\n            if ( mid != n )\n                swap( A[mid], A[mid + 1] );\n            return;\n        }\n        else if ( x.key < A[mid].key ) high = mid - 1;\n        else low = mid + 1;\n    }\n    for ( j = n; j >= high + 1; j-- )\n        A[j + 1] = A[j];\n    A[j + 1] = x;\n}\n```\n\n### 8.设计算法将一维数组$R$中的序列循环左移$p(0<p<n)$个位置(算法思想和6.相同)\n\n```cpp\n// 元素倒置\nvoid Reverse( ElemType A[], int s, int e )\n{\n    for ( i = s; i < ( s + e ) / 2; i++ )\n        swap( A[i], A[s + e - i - 1] );\n}\n\nvoid ShiftLeft( ElemType R[], int n, int p )\n{\n    Reverse( R, 0, p );\n    Reverse( R, p, n );\n    Reverse( R, 0, n );\n}\n```\n\n### 9.长度为$L(L≥1)$的升序序列$S$,处在$⌈L2⌉$个位置的数成为$S$的中位数,设计一个在时空都尽量高效的算法找出两个等长序列$A$和$B$的中位数\n\n```cpp\nint FindMidFromABOrderedSeq( int A[], int B[], int n )\n{\n    int s1, s2, e1, e2, m1, m2;\n    s1 = s2 = 0;\n    e1 = e2 = n - 1;\n    while ( s1 != e1 || s2 != e2 )\n    {\n        m1 = ( s1 + e1 ) / 2;\n        m2 = ( s2 + e2 ) / 2;\n        if ( A[m1] == B[m2] )\n            return A[m1];\n        else if ( A[m1] < B[m2] )\n        {\n            if ( !( ( s1 + e1 ) % 2 ) )\n                s1 = m1, e2 = m2;\n            else\n                s1 = m1 + 1, e2 = m2;\n        }\n        else\n        {\n            if ( !( ( s2 + e2 ) % 2 ) )\n                s2 = m2, e1 = m1;\n            else\n                s2 = m2 + 1, e1 = m1;\n        }\n    }\n    return A[s1] < B[s2] ? A[s1] : B[s2];\n}\n```\n\n## 三.线性表的链式表示\n\n### 1.采用头插法建立单链表\n\n```cpp\nLinkList CreateList( LinkList& L )\n{\n    L = ( LinkList ) malloc( sizeof( LNode ) );\n    L->next = NULL;\n    scanf( \"%d\", &x );\n    while ( x != 9999 )\n    {\n        s = ( LNode* ) malloc( sizeof( LNode ) );\n        s->data = x;\n        s->next = L->next;\n        L->next = s;\n        scanf( \"%d\", &x );\n    }\n    return L;\n}\n```\n\n### 2.采用尾插法建立单链表\n\n```cpp\nLinkList CreateList( LinkList& L )\n{\n    L = ( LinkList ) malloc( sizeof( LNode ) );\n    L->next = NULL;\n    r = L;\n    scanf( \"%d\", &x );\n    while ( x != 9999 )\n    {\n        s = ( LNode* ) malloc( sizeof( LNode ) );\n        s->data = x;\n        r->next=s;\n        r = s;\n        scanf( \"%d\", &x );\n    }\n    r->next = NULL;\n    return L;\n}\n```\n\n## 四.线性表相关综合算法\n\n### 1.递归删除不带头结点的单列表$L$中所有值为$x$的结点\n\n```cpp\nvoid DeleteX( LinkList& L, ElemType x )\n{\n    if ( !L ) return;\n    if ( L->data == x )\n    {\n        q = L;\n        L = L->next;\n        free( q );\n        DeleteX( L, x );\n    }\n    else\n        DeleteX( L->next, x );\n} \n```\n\n### 2.删除带头结点的单链表$L$中所有值为$x$的结点\n\n```cpp\nvoid DeleteX( LinkList& L, ElemType x )\n{\n    pre = L;\n    p = L->next;\n    while ( p )\n    {\n        if ( p->data == x )\n        {\n            q = p;\n            pre->next = p->next;\n            p = p->next;\n            free( q );\n        }\n        else\n        {\n            pre = p; p = p->next;\n        }\n    }\n}\n```\n\n### 3.反向输出带头结点的单链表$L$的每个结点的值\n\n```cpp\nvoid PrintX( LinkList L )\n{\n    if ( !L )return;\n    PrintX( L->next );\n    visit( L );\n}\n```\n\n### 4.删除带头结点单链表$L$中最小值结点\n\n```cpp\nLinkList DeleteMin( LinkList& L )\n{\n    LinkList p, s, pre, q;\n    p = s = L->next;\n    pre = q = L;\n    while ( p )\n    {\n        if(p->data<s->data )\n        {\n            s = p; q = pre;\n        }\n        pre = p;\n        p = p->next;\n    }\n    q->next = s->next;\n    free( s );\n    return L;\n}\n```\n\n### 5.将带头结点的单链表就地逆置,\"就地\"指辅助空间复杂度为$O(1)$\n\n```cpp\nLinkList Reverse( LinkList L )\n{\n    LinkList p, q;\n    p = L->next;\n    L->next = NULL;\n    while ( p )\n    {\n        q = p->next;\n        p->next = L->next;\n        L->next = p;\n        p = q;\n    }\n    return L;\n}\n```\n\n### 6.将带头结点的单链表$L$排序,使其递增有序\n\n```cpp\nvoid InsertSort( LinkList& L )\n{\n    LinkList p, pre, r;\n    p = L->next; r = p->next;\n    p->next = NULL; p = r;\n    while ( p )\n    {\n        r = p->next;\n        pre = L;\n        while ( pre->next&&pre->next->data < p->data )\n            pre = pre->next;\n        p->next = pre->next;\n        pre->next = p;\n        p = r;\n    }\n}\n```\n\n### 7.在带头结点的单链表中,删除值介于$(s,t)$之间的元素\n\n```cpp\nvoid DeleteS2T( LinkList& L, int s, int t )\n{\n    LinkList pre, p;\n    pre = L; p = pre->next;\n    while ( p )\n    {\n        if ( p->data > s && p->data < t )\n        {\n            pre->next = p->next;\n            free( p );\n            p = pre->next;\n        }\n        else\n        {\n            pre = p;\n            p = p->next;\n        }\n    }\n}\n```\n\n### 8.找出两个单链表的公共结点\n\n```cpp\nLinkList SearchCommon( LinkList L1, LinkList L2 )\n{\n    LinkList pA, pB;\n    int lenA, lenB, dist;\n    pA = L1->next, pB = L2->next;\n    lenA = lenB = 0;\n    while ( pA ) { pA = pA->next; lenA++; }\n    while ( pB ) { pB = pB->next; lenB++; }\n    pA = L1->next, pB = L2->next;\n    if ( lenA > lenB )\n    {\n        dist = lenA - lenB;\n        while ( dist-- ) pA = pA->next;\n    }\n    else\n    {\n        dist = lenB - lenA;\n        while ( dist-- ) pB = pB->next;\n    }\n    while ( pA )\n    {\n        if ( pA == pB ) return pA;\n        pA = pA->next, pB = pB->next;\n    }\n    return NULL;\n}\n```\n\n### 9.带表头结点的单链表,按递增次序输出单链表中各结点的数据元素,并释放空间\n\n```cpp\nvoid AscDelete( LinkList& L )\n{\n    LinkList p, s, pre, r;\n    while ( L->next )\n    {\n        s = p = L->next; r = pre = L;\n        while ( p )\n        {\n            if ( p->data < s->data )\n            {\n                s = p; r = pre;\n            }\n            pre = p;\n            p = p->next;\n        }\n        r->next = s->next;\n        visit( s );\n        free( s );\n    }\n    free( L );\n}\n```\n\n### 10.将带头结点的单链表$A$分解成两个带头结点的单链表$A$和$B$,$A$中含有奇数序号元素,$B$中含有偶数序号元素且相对位置不变\n\n```cpp\n// 法一\nLinkList Split( LinkList& A )\n{\n    LinkList p, B, rA, rB;\n    int i = 0;\n    p = A->next;\n    B = ( LinkList ) malloc( sizeof( LNode ) );\n    rA = A; A->next = NULL;\n    rB = B; B->next = NULL;\n    while ( p )\n    {\n        i++;\n        if (i%2)\n        {\n            rA->next = p; rA = p;\n        }\n        else\n        {\n            rB->next = p; rB = p;\n        }\n        p = p->next;\n    }\n    rA->next = NULL;\n    rB->next = NULL;\n    return B;\n}\n```\n\n```cpp\n// 法二\nLinkList Split( LinkList& A )\n{\n    LinkList p, B, rB, pre;\n    int i = 0;\n    B = ( LinkList ) malloc( sizeof( LNode ) );\n    rB = B;\n    pre = A; p = pre->next;\n    while ( p )\n    {\n        i++;\n        if ( i % 2 == 0 )\n        {\n            pre->next = p->next;\n            rB->next = p;\n            rB = p;\n            p = pre->next;\n        }\n        else\n        {\n            pre = p;\n            p = p->next;\n        }\n    }\n    return B;\n}\n```\n\n### 11.$C={a_1,b_1,a_2,b_2,...,a_n,b_n}$为线性表,带有头结点,设计一个就地算法将其拆分为两个线性表,使$A={a_1,a_2,...,a_n}$,$B={b_n,...,b_2,b_1}$\n\n```cpp\nLinkList Split( LinkList& A )\n{\n    LinkList B, pre, p;\n    int i = 0;\n    B = ( LinkList ) malloc( sizeof( LNode ) );\n    pre = A; p = pre->next;\n    while ( p )\n    {\n        i++;\n        if ( i % 2 == 0 )\n        {\n            pre->next = p->next;\n            p->next = B->next;\n            B->next = p;\n            p = pre->next;\n        }\n        else\n        {\n            pre = p;\n            p = p->next;\n        }\n    }\n    return B;\n}\n```\n\n### 12.在递增有序的带头结点的单链表中,数值相同的只保留一个,使表中不再有重复的元素\n\n```cpp\nvoid DeleteSame( LinkList& L )\n{\n    LinkList p, q;\n    p = L->next;\n    while ( p )\n    {\n        q = p->next;\n        if ( q&&q->data == p->data )\n        {\n            p->next = q->next;\n            free( q );\n        }\n        else\n            p = p->next;\n    }\n}\n```\n\n### 13.将两个按元素值递增的单链表合并为一个按元素值递减的单链表\n\n```cpp\nvoid MergeList( LinkList& LA, LinkList& LB )\n{\n    LinkList pA, pB, q;\n    pA = LA->next; pB = LB->next;\n    LA->next = NULL;\n    while ( pA&&pB )\n    {\n        if ( pA->data <= pB->data )\n        {\n            q = pA->next;\n            pA->next = LA->next;\n            LA->next = pA;\n            pA = q;\n        }\n        else\n        {\n            q = pB->next;\n            pB->next = LA->next;\n            LA->next = pB;\n            pB = q;\n        }\n    }\n    if ( pA )\n        pB = pA;\n    while(pB )\n    {\n        q = pB->next;\n        pB->next = LA->next;\n        LA->next = pB;\n        pB = q;\n    }\n    free( LB );\n}\n```\n\n### 14.$A,B$为两个元素递增有序的单链表(带头结点),设计算法从$A,B$中公共元素产生单链表$C$,要求\n\n```cpp\nvoid MergeList( LinkList& LA, LinkList& LB )\n{\n    LinkList pA, pB, q;\n    pA = LA->next; pB = LB->next;\n    LA->next = NULL;\n    while ( pA&&pB )\n    {\n        if ( pA->data <= pB->data )\n        {\n            q = pA->next;\n            pA->next = LA->next;\n            LA->next = pA;\n            pA = q;\n        }\n        else\n        {\n            q = pB->next;\n            pB->next = LA->next;\n            LA->next = pB;\n            pB = q;\n        }\n    }\n    if ( pA )\n        pB = pA;\n    while ( pB )\n    {\n        q = pB->next;\n        pB->next = LA->next;\n        LA->next = pB;\n        pB = q;\n    }\n    free( LB );\n}\n```\n\n### 15.求两个元素递增排列的链表(带头结点)$A$和$B$的交集并存放于$A$链表中,并释放其他结点\n\n```cpp\nvoid Intersect( LinkList& LA, LinkList& LB )\n{\n    LinkList pA, pB, r, q;\n    pA = LA->next; pB = LB->next;\n    r = LA; LA->next = NULL;\n    while ( pA&&pB )\n    {\n        if ( pA->data == pB->data )\n        {\n            r->next = pA;\n            r = pA;\n            pA = pA->next;\n            q = pB;\n            pB = pB->next;\n            free( q );\n        }\n        else if ( pA->data < pB->data )\n        {\n            q = pA;\n            pA = pA->next;\n            free( q );\n        }\n        else\n        {\n            q = pB;\n            pB = pB->next;\n            free( q );\n        }\n    }\n    r->next = NULL;\n    while ( pA )\n    {\n        q = pA;\n        pA = pA->next;\n        free( q );\n    }\n    while ( pB )\n    {\n        q = pB;\n        pB = pB->next;\n        free( q );\n    }\n    free( LB );\n}\n```\n\n### 16.判断单链表序列$B$是否是$A$的连续子序列(不带头结点)\n\n```cpp\nbool IsSubsequence( LinkList A, LinkList B )\n{\n    LinkList pA, pB, h;\n    pA = A; pB = B;\n    h = pA;\n    while ( pA&&pB )\n    {\n        if ( pA->data == pB->data )\n        {\n            pA = pA->next;\n            pB = pB->next;\n        }\n        else\n        {\n            h = h->next;\n            pA = h;\n            pB = B;\n        }\n    }\n    if ( pB ) return false;\n    return true;\n}\n```\n\n### 17.判断带头结点的循环双链表是否对称\n\n```cpp\nbool IsSymmetry( DLinkList L )\n{\n    DLinkList p, q;\n    p = L->next; q = L->prior;\n    while ( p != q && q->next != p )\n    {\n        if ( p->data != q->data )\n            return false;\n        p = p->next;\n        q = q->next;\n    }\n    return true;\n}\n```\n\n### 18.将循环单链表$h2$链接到$h1$之后\n\n```cpp\nLinkList Link( LinkList& h1, LinkList& h2 )\n{\n    LinkList p;\n    p = h1;\n    while ( p->next != h1 )p = p->next;\n    p->next = h2;\n    p = h2;\n    while ( p->next != h2 )p = p->next;\n    p->next = h1;\n    return h1;\n}\n```\n\n### 19.带头结点的循环链表,按递增次序输出循环链表中各结点的数据元素,并释放空间\n\n```cpp\nvoid AscDelete( LinkList& L )\n{\n    LinkList p, s, r, pre;\n    while ( L->next != L )\n    {\n        s = p = L->next; r = pre = L;\n        while ( p != L )\n        {\n            if ( p->data < s->data )\n            {\n                s = p; r = pre;\n            }\n            pre = p;\n            p = p->next;\n        }\n        visit( s );\n        r->next = s->next;\n        free( s );\n    }\n    free( L );\n}\n```\n\n### 20.查找单链表(带头结点)中倒数第$k$个位置的结点,成功:则输出并返回$true$,否则只返回$false$\n\n```cpp\nbool SearchBackwardK( LinkList L, int k )\n{\n    LinkList p, q;\n    int count;\n    p = q = L->next;\n    count = 0;\n    while (p)\n    {\n        if ( count < k ) count++;\n        else q = q->next;\n        p = p->next;\n    }\n    if ( count < k ) return false;\n    visit( q );\n    return true;\n}\n```\n\n### 21.链表中$data$绝对值相等的点,只保留第一次出现的结点$(|data|≤n)$\n\n```cpp\nvoid DeleteSameAbs( LinkList L, int n )\n{\n    LinkList pre, p;\n    int *B, pos;\n    B = ( int * ) malloc( sizeof( int )*( n + 1 ) );\n    for ( int i = 0; i < n + 1; i++ )\n        B[i] = 0;\n    pre = L; p = L->next;\n    while ( p )\n    {\n        pos = p->data > 0 ? p->data : -p->data;\n        if ( B[pos] == 0)\n        {\n            B[pos] = 1; pre = p; p = p->next;\n        }\n        else\n        {\n            pre->next = p->next; free( p ); p = pre->next;\n        }\n    }\n    free( B );\n}\n```\n\n### 22.带头结点的循环双链表递增排序\n\n```cpp\nvoid AscSort( DLinkList L )\n{\n    DLinkList p, q, r;\n    if ( !L ) return;\n    p = L->next; q = p->next; r = q->next;\n    while ( q!=L )\n    {\n        while ( p != L && p->data > q->data )\n            p = p->prior;\n        // 脱链结点p\n        q->prior->next = r;\n        r->prior = q->prior;\n        // 插入节点p\n        q->next = p->next;\n        q->prior = p;\n        p->next->prior = q;\n        p->next = q;\n        // 归位(相对位置)\n        q = r;\n        p = q->prior;\n        r = r->next;\n    }\n}\n```","tags":["顺序表"],"categories":["algorithm"]},{"title":"[algorithm]栈和队列","url":"%2Fposts%2F6027%2F","content":"## 一.栈和队列综合(算法)\n\n### 1.判断单链表(带头结点)的结点值(字符型)是否中心对称\n\n```cpp\nbool IsSymmetry( LinkList& L )\n{\n    char S[MAXSIZE];\n    int top = -1, len = 0, i;\n    LinkList p;\n    p = L->next;\n    while ( p ) { p = p->next; len++; }\n    p = L->next;\n    for (i=0;i<len/2;i++)\n    {\n        S[++top] = p->data;\n        p = p->next;\n    }\n    i--;\n    if ( len % 2 )\n        p = p->next;\n    while ( top != -1 )\n    {\n        if ( p->data != S[top] )\n            return false;\n        top--;\n        p = p->next;\n    }\n    return true;\n}\n```\n\n### 2.共享栈由两个顺序栈S1,S2构成,总大小为100,请设计S1,S2入栈,出栈的算法\n\n```cpp\n#define MAXSIZE 100\nElemType S[MAXSIZE];\nint top[2] = { -1,MAXSIZE };\nbool Push( int i, ElemType x )\n{\n    if ( i < 0 || i>1 || top[1] - top[0] == 1 )\n        return false;\n    if ( i == 0 ) S[++top[0]] = x;\n    else S[--top[1]] = x;\n    return true;\n}\n\nbool Pop( int i, ElemType x )\n{\n    if ( i < 0 || i>1 \n         || ( i == 0 && top[0] == -1 ) \n         || ( i == 1 && top[1] == MAXSIZE ) )\n        return false;\n    if ( i == 0 ) x = S[top[0]--];\n    else x = S[top[1]++];\n    return true;\n}\n```\n\n### 3.如果希望循环队列中的元素都能得到利用,则需设置一个标志域tag,并以tag的值为0或1来区分队头指针front和队尾rear相同时的队列状态是\"空\"还是\"满\",编写与此结构相应的入队和出队算法\n\n```cpp\nElemType Q[MAXSIZE];\nint front = -1, rear = -1;\n// 队空条件:    front==rear&&tag==0\n// 队满条件:    front==rear&&tag==1\n// 进队操作:    rear=(rear+1)%MAXSIZE;\n//            Q[rear]=x;\n//            tag=1;\n// 出队操作:    front=(front+1)%MAXSIZE;\n//            x=Q[front];\n//            tag=0;\n```\n\n#### 1)\"tag\"法循环队列入队算法\n\n```cpp\nbool EnQueue( ElemType x )\n{\n    if ( front == rear && tag == 1 )\n        return false;\n    rear = ( rear + 1 ) % MAXSIZE;\n    Q[rear] = x;\n    tag = 1;\n    return true;\n}\n```\n\n#### 2)\"tag\"法循环队列出队算法\n\n```cpp\nbool DeQueue( ElemType& x )\n{\n    if ( front == rear && tag == 0 )\n        return false;\n    front = ( front + 1 ) % MAXSIZE;\n    x = Q[front];\n    tag = 0;\n    return true;\n}\n```\n\n### 4.Q是一个队列,S是一个空栈,实现将队列中的元素逆置的算法\n\n```cpp\nElemType S[MAXSIZE], Q[MAXSIZE];\nint top = -1, front = -1, rear = -1;\nvoid Inverse(ElemType S[], ElemType Q[])\n{\n    ElemType x;\n    while ( front != rear )\n    {\n        x = Q[++front];\n        S[++top] = x;\n    }\n    while ( top != -1 )\n    {\n        x = S[top--];\n        Q[++rear] = x;\n    }\n}\n```\n\n### 5.利用两个栈S1,S2模拟一个队列\n\n>   已知栈的4个运算如下:\n**    void Push(Stack& S, ElemType x);\n    void Pop(Stack& S, ElemType& x)\n    bool IsEmpty(Stack& S);\n    bool IsOverflow( Stack& S );**\n\n```cpp\nbool EnQueue( Stack& S1, Stack& S2, ElemType x )\n{\n    if ( !IsOverflow( S1 ) )\n    {\n        Push( S1, x );\n        return true;\n    }\n    if ( !IsEmpty( S2 ) )\n        return false;\n    while (!IsEmpty(S1))\n    {\n        Pop( S1, t );\n        Push( S2, t );\n    }\n    Push( S1, x );\n    return true;\n}\n\nbool DeQueue( Stack& S1, Stack& S2, ElemType& x )\n{\n    if (!IsEmpty(S2))\n    {\n        Pop( S2, x ); return true;\n    }\n    if ( IsEmpty( S1 ) )\n        return false;\n    while (!IsEmpty(S1))\n    {\n        Pop( S1, t ); \n        Push( S2, t );\n    }\n    Pop( S2, x );\n    return true;\n}\n\nbool IsEmpty( Stack& S1, Stack& S2 )\n{\n    if ( IsEmpty( S1 ) && IsEmpty( S2 ) )\n        return true;\n    return false;\n}\n```\n\n### 6.括号匹配问题:判别表达式中括号是否匹配(只含有(),[],{})\n\n```cpp\nbool IsBracketMatch( char*str )\n{\n    char S[MAXSIZE];\n    int top = -1;\n    for ( int i = 0; str[i]; i++ )\n    {\n        char c = str[i];\n        switch ( c )\n        {\n        case '(':\n        case '[':\n        case '{':\n            S[++top] = c;\n            break;\n        case ')':\n            c = S[top--];\n            if ( c != '(' )return false;\n            break;\n        case ']':\n            c = S[top--];\n            if ( c != '[' )return false;\n            break;\n        case '}':\n            c = S[top--];\n            if ( c != '{' )return false;\n            break;\n        default:\n            break;\n        }\n    }\n    return top == -1;\n}\n```\n\n### 7.利用栈实现以下递归函数的非递归计算:\n\n$$ \nPn(x)= \n\\begin{cases}\n    1,  & n=0 \\\\\n    2x, & n=1 \\\\\n    2x\\cdot{P_{n-1}}(x)-2(n-1)\\cdot{P_{n-2}}(x) & n>1\n\\end{cases}\n$$\n\n```cpp\ndouble P( int n, double x )\n{\n    struct Stack\n    {\n        int n;        // 层\n        double val;    // 数值结果\n    }S[MAXSIZE];\n    int top = -1, fv1 = 1, fv2 = 2 * x;\n    for ( int i = n; i > 1; i-- )\n        S[++top].n = i;\n    while ( top != -1 )\n    {\n        S[top].val = 2 * x*fv2 - 2 * ( S[top].n - 1 )*fv1;\n        fv1 = fv2;\n        fv2 = S[top--].val;\n    }\n    if ( n == 0 ) return fv1;\n    return fv2;\n}\n```","tags":["队列"],"categories":["algorithm"]},{"title":"[algorithm]树与二叉树","url":"%2Fposts%2F45464%2F","content":"## 一.树与二叉树相关算法\n\n### 1.二叉树按顺序结构存储,求编号为i和j的两个结点的最近公共祖先结点的值\n\n```cpp\nElemType CommonAncestor( SeqTree T, int i, int j )\n{\n    while ( i != j )\n    {\n        if ( i > j ) i /= 2;\n        else j /= 2;\n    }\n    return T[i];\n}\n```\n\n### 2.二叉树前序遍历非递归算法\n\n```cpp\nvoid PreOrder( BiTree T )\n{\n    BiTree S[MAXSIZE], p;\n    int top = -1;\n    p = T;\n    while ( p || top != -1 )\n    {\n        if (p)\n        {\n            visit( p );\n            S[++top] = p; p = p->lchild;\n        }\n        else\n        {\n            p = S[top--]; p = p->rchild;\n        }\n    }\n}\n```\n\n### 3.二叉树中序遍历非递归算法\n\n```cpp\nvoid InOrder( BiTree T )\n{\n    BiTree S[MAXSIZE], p;\n    int top = -1;\n    p = T;\n    while ( p || top != -1 )\n    {\n        if (p )\n        {\n            S[++top] = p; p = p->lchild;\n        }\n        else\n        {\n            p = S[top--]; visit( p ); p = p->rchild;\n        }\n    }\n}\n```\n\n### 4.二叉树后序遍历非递归算法\n\n```cpp\nvoid PostOrder( BiTree T )\n{\n    BiTree Q[MAXSIZE], p, r;\n    int top = -1;\n    p = T; r = NULL;\n    while ( p || top != -1 )\n    {\n        if (p)    // 走到最左边\n        {\n            S[++top] = p; p = p->lchild;\n        }\n        else    // 向右\n        {\n            p = S[top];\n            if (p->rchild&&p->rchild!=r)    // 转向右\n                p = p->rchild;\n            else    // 根\n            {\n                p = S[top--];\n                visit( p );\n                r = p;\n                p = NULL;\n            }\n        }\n    }\n}\n```\n\n### 5.二叉树层次遍历算法\n\n```cpp\nvoid LevelOrder( BiTree T )\n{\n    BiTree Q[MAXSIZE], p;\n    int front = -1, rear = -1;\n    Q[++rear] = T;\n    while ( front != rear )\n    {\n        p = Q[++front];\n        visit( p );\n        if ( p->lchild ) Q[++rear] = p->lchild;\n        if ( p->rchild ) Q[++rear] = p->rchild;\n    }\n}\n```\n\n### 6.二叉树的自下而上,从右到左的层次遍历算法\n\n```cpp\nvoid InvertLevel( BiTree T )\n{\n    BiTree S[MAXSIZE], Q[MAXSIZE], p;\n    int front = -1, rear = -1, top = -1;\n    Q[++rear] = T;\n    while ( front != rear )\n    {\n        p = Q[++front];\n        S[++top] = p;\n        if ( p->lchild ) Q[++rear] = p->lchild;\n        if ( p->rchild ) Q[++rear] = p->rchild;\n    }\n    while ( top!=-1 )\n    {\n        p = S[top--]; visit( p );\n    }\n}\n```\n\n### 7.求二叉树高度(递归)\n\n```cpp\nint BtDepth( BiTree T )\n{\n    if ( T == NULL ) return 0;\n    int ldepth, rdepth;\n    ldepth = BtDepth( T->lchild );\n    rdepth = BtDepth( T->rchild );\n    return ldepth > rdepth ? ldepth + 1 : rdepth + 1;\n}\n```\n\n### 8.求二叉树高度(非递归)\n\n> 法一思路:后序遍历,最大栈长即为树的高度\n\n```cpp\nint BtDepth( BiTree T )\n{\n    BiTree S[MAXSIZE], p, r;\n    int top = -1, depth = 0;\n    while ( p || top != -1 )\n    {\n        if ( p )\n        {\n            S[++top] = p; p = p->lchild;\n        }\n        else\n        {\n            p = S[top];\n            if ( p->rchild&&p->rchild != r )\n                p = p->rchild;\n            else\n            {\n                if (top+1>depth)\n                    depth = top + 1;\n                p = S[top--];\n                r = p;\n                p = NULL;\n            }\n        }\n    }\n    return depth;\n}\n```\n\n> 法二思路:层次遍历,层数即为高度\n\n```cpp\nint BtDepth( BiTree T )\n{\n    BiTree Q[MAXSIZE], p;\n    int front = -1, rear = -1, last = 0, depth = 0;\n    Q[++rear] = T;\n    while ( front != rear )\n    {\n        p = Q[++front];\n        if ( p->lchild )\n            Q[++rear] = p->lchild;\n        if ( p->rchild )\n            Q[++rear] = p->rchild;\n        if ( front == last )\n        {\n            depth++;\n            last = rear;\n        }\n    }\n    return depth;\n}\n```\n\n### 9.先许遍历序列和中序遍历序列分别存放于两个一维数组A[1...n],B[1...n]中,编写算法建立该二叉树的二叉链表\n\n```cpp\nBiTree PreInCreate( ElemType A[], ElemType B[], int l1, int h1, int l2, int h2 )\n{\n    BiTree root = ( BiTree ) malloc( sizeof( BiTNode ) );\n    int i, llen, rlen;\n    root->data = A[l1];\n    for ( i = l2; B[i] != root->data; i++ );\n    llen = i - l2;\n    rlen = h2 - i;\n    if ( llen )\n        root->lchild = PreInCreate( A, B, l1 + 1, l1 + llen, l2, l2 + llen - 1 );\n    else\n        root->rchild = NULL;\n    if ( rlen )\n        root->rchild = PreInCreate( A, B, h1 - rlen + 1, h1, h2 - rlen + 1, h2 );\n    else\n        root->rchild = NULL;\n    return root;\n}\n```\n\n### 10.判断二叉树是否是完全二叉树\n\n```cpp\nbool IsComplete( BiTree T )\n{\n    if ( T == NULL ) return true;\n    BiTree Q[MAXSIZE], p;\n    int front = -1, rear = -1;\n    Q[++rear] = T;\n    while ( front != rear )\n    {\n        p = Q[++front];\n        if (p)\n        {\n            Q[++rear] = p->lchild;\n            Q[++rear] = p->rchild;\n        } \n        else\n        {\n            while ( front != rear )\n            {\n                p = Q[++front];\n                if ( p ) return false;\n            }\n        }\n    }\n    return true;\n}\n```\n\n### 11.计算一棵给定二叉树的所有双分支结点个数\n\n```cpp\nint N2Nodes( BiTree T )\n{\n    if ( T == NULL ) return 0;\n    if ( T->lchild && T->rchild )\n        return N2Nodes( T->lchild ) + N2Nodes( T->rchild ) + 1;\n    return N2Nodes( T->lchild ) + N2Nodes( T->rchild );\n}\n```\n\n### 12.将二叉树中所有结点的左,右子树进行交换\n\n```cpp\nvoid SwapTree( BiTree T )\n{\n    if ( T == NULL ) return;\n    SwapTree( T->lchild );\n    SwapTree( T->rchild );\n    swap( T->lchild, T->rchild );\n}\n```\n\n### 13.求二叉树先序遍历序列中第k(1≤k≤二叉树结点个数)个结点的值\n\n```cpp\nint i = 1;\nElemType PreNodeK( BiTree T, int k )\n{\n    if ( T == NULL ) return '#';\n    if ( i == k ) return T->data;\n    i++;    // 下一个结点\n    ElemType ch = PreNodeK( T->lchild, k );\n    if ( ch != '#' ) return ch;\n    ch = PreNodeK( T->rchild, k );\n    return ch;\n}\n```\n\n### 14.二叉树中,对于每一个元素值为x的结点,删去以它为根的子树,并释放相应的空间\n\n```cpp\nvoid DeleteNode( BiTree T )\n{\n    if ( T == NULL ) return;\n    DeleteNode( T->lchild );\n    DeleteNode( T->rchild );\n    free( T );\n}\n```\n> 法一:递归\n\n```cpp\nvoid DeleteAllXNode( BiTree T, ElemType x )\n{\n    if ( T == NULL ) return;\n    if ( T->data == x )\n    {\n        DeleteNode( T ); return;\n    }\n    DeleteAllXNode( T->lchild, x );\n    DeleteAllXNode( T->rchild, x );\n}\n```\n\n> 法二:非递归\n\n```cpp\nvoid DeleteAllXNode( BiTree T, ElemType x )\n{\n    if ( T == NULL ) return;\n    BiTree Q[MAXSIZE], p;\n    int front = -1, rear = -1;\n    Q[++rear] = T;\n    while ( front != rear )\n    {\n        p = Q[++front];\n        if ( p->data == x ) DeleteNode( p );\n        else\n        {\n            if ( p->lchild ) Q[++rear] = p->lchild;\n            if ( p->rchild ) Q[++rear] = p->rchild;\n        }\n    }\n}\n```\n\n### 15.输出二叉树中值为x的结点(≤1)个的所有祖先\n\n> 法一:递归\n\n```cpp\nbool AllAncestorX( BiTree T, ElemType x )\n{\n    if ( T == NULL ) return false;\n    if ( T->data == x ) return true;\n    bool b1, b2;\n    b1 = AllAncestorX( T->lchild, x );\n    b2 = AllAncestorX( T->rchild, x );\n    if ( b1 || b2 ) visit( T );\n    return b1 || b2;\n}\n```\n\n> 法二:非递归\n> 思路: 后序遍历非递归方式中,保留在栈中所有元素(除栈顶外)必然是栈顶的祖先结点,只要找到x结点,将所有结点出栈即可\n\n```cpp\nvoid AllAncestorX( BiTree T, ElemType x )\n{\n    if ( T == NULL ) return;\n    BiTree S[MAXSIZE], p, r;\n    int top = -1;\n    p = T; r = NULL;\n    while ( p||top!=-1 )\n    {\n        if (p)\n        {\n            S[++top] = p; p = p->lchild;\n        }\n        else\n        {\n            p = S[top];\n            if ( p->rchild&&p->rchild != r )\n                p = p->rchild;\n            else\n            {\n                p = S[top--];\n                if (p->data==x)\n                {\n                    while ( top != -1 )\n                    {\n                        p = S[top--]; visit( p );\n                    }\n                }\n                r = p;\n                p = NULL;\n            }\n        }\n    }\n}\n```\n\n### 16.p,q为二叉树中任意两个结点的指针,编写算法找到p,q的最近公共祖先结点(递归)\n\n```cpp\n// 思路: ①左子树中能找到p(或q),右子树中能找到q(或p),的结点一定为p,q的最近公共结点\n//       ②p,q都在右子树上,则深度低的为公共祖先\n//       ③p,q都在左子树上,则深度低的为公共祖先\n//    三种情况      o  <-root(此时为公共祖先)     o  <-root                             o <-root\n//                / \\                            \\                                   /\n//           p-> o   o  <-q                       o  <-p(此时为公共祖先为right)      o  <-p(此时为公共祖先left)\n//                                                 \\                               /\n//                                                  o  <-q                        o  <-q\nBiTree Ancestor( BiTree root, BiTNode *p, BiTNode *q )\n{\n    if ( !root || !p || !q ) return NULL;\n    if ( p == root || q == root ) return root;\n    BiTree left, right;\n    /* \n     * ①在左子树中,若找到p,q中一个,则返回一个\n     * ②在左子树中,若找到p,q(全),则返回较近的一个(高度较低的)\n     */\n    left = Ancestor( root->lchild, p, q );\n    /*\n     * ①在右子树中,若找到p,q中一个,则返回一个\n     * ②在右子树中,若找到p,q(全),则返回较近的一个(高度较低的)\n     */\n    right = Ancestor( root->rchild, p, q );    \n    if ( left&&right ) return root;\n    return left ? left : right;\n}\n```\n\n### 17.求非空二叉树的宽度\n\n```cpp\nint TreeWidth( BiTree T )\n{\n    BiTree Q[MAXSIZE], p;\n    int front = -1, rear = -1, maxWidth = 0;\n    Q[++rear] = T;\n    while ( front != rear )\n    {\n        int width = rear - front;\n        if ( maxWidth < width )\n            maxWidth = width;\n        while ( width-- )\n        {\n            p = Q[++front];\n            if ( p->lchild ) Q[++rear] = p->lchild;\n            if ( p->rchild ) Q[++rear] = p->rchild;\n        }\n    }\n    return maxWidth;\n}\n```\n\n### 18.一棵满二叉树(所有结点值均不同),已知其先序序列为pre,设计算法求其后序序列post\n\n```cpp\n// 思路: 每次都会确定出后序的一个位置并划分为左右两块,再分别在这左右两块中继续确定其他元素 \n//  先序: x|    |    |\n//  后序:  |    |    |x\nvoid PreToPost( ElemType pre[], int l1, int h1, ElemType post[], int l2, int h2 )\n{\n    if ( h1 < l1 ) return;\n    post[h2] = pre[l1]; // 确定出一个后序位置\n    int half = ( h1 - l1 ) / 2;\n    PreToPost( pre, l1 + 1, l1 + half, post, l2, l2 + half - 1 );\n    PreToPost( pre, h1 - half + 1, h1, post, h2 - half, h2 - 1 );\n}\n```\n\n### 19.将二叉树叶子结点按从左到右连成单链表,表头指针为head,叶结点的右指针域存放单链表指针\n\n```cpp\nBiTree head, pre = NULL;\nBiTree InOrder( BiTree bt )\n{\n    if ( bt == NULL ) return NULL;\n    InOrder( bt->lchild );\n    if ( !bt->lchild && !bt->rchild )\n    {\n        if (!pre)\n        {\n            head = pre = bt;\n        }\n        else\n        {\n            pre->rchild = bt; pre = bt;\n        }\n    }\n    InOrder( bt->rchild );\n    pre->rchild = NULL;\n    return head;\n}\n```\n\n### 20.判断两棵二叉树是否相似.(注:不要求结点值相同,只要树的外形相同即可)\n\n```cpp\nbool Similar( BiTree T1, BiTree T2 )\n{\n    if ( T1 == NULL && T2 == NULL ) \n        return true;\n    else if ( T1 == NULL || T2 == NULL ) \n        return false;\n    else if ( Similar( T1->lchild, T2->lchild ) && Similar( T1->rchild, T2->rchild ) )\n        return true;\n    return false;\n}\n```\n\n### 21.将表达式树转换为等价的中缀表达式(通过括号反映操作符的计算次序)并输出\n\n```cpp\n// 思路: 表达式树的中序序列加上必要的括号即为等价的中缀表达式.除根结点外,遍历到其他结点时在遍历其左子树之前加上左括号,在遍历完右子树后加上右括号\nvoid BiTreeToExp( BiTree T, int deep )\n{\n    if ( T == NULL ) return;\n    else if ( !T->lchild && !T->rchild ) visit( T );\n    else\n    {\n        if ( deep > 1 ) printf( \"(\" );\n        BiTreeToExp( T->lchild, deep + 1 );\n        visit( T );\n        BiTreeToExp( T->rchild, deep + 1 );\n        if ( deep > 1 ) printf( \"(\" );\n    }\n}\n```\n\n### 22.求孩子兄弟表示法存储的森林的叶子节点数\n\n```cpp\ntypedef struct CSNode\n{\n    ElemType data;\n    struct CSNode *firstchild, *nextsibling;\n}CSNode, *CSTree;\n\nint Leaves( CSTree T )\n{\n    if ( T == NULL ) return 0;\n    if ( T->firstchild == NULL )\n        return 1 + Leaves( T->nextsibling );\n    else\n        return Leaves( T->firstchild ) + Leaves( T->nextsibling );\n}\n```\n\n### 23.以孩子兄弟链表为存储结构,求树的高度(深度)(递归)\n\n```cpp\nint Height( CSTree T )\n{\n    if ( T == NULL ) return 0;\n    int hc, hs;\n    hc = Height( T->firstchild ) + 1;\n    hs = Height( T->nextsibling );\n    return hc > hs ? hc : hs;\n}\n```\n\n### 24.二叉排序树的查找(非递归)\n\n```cpp\nBiTree BSTSearch( BiTree T, ElemType key )\n{\n    while ( T && key != T->data )\n    {\n        if ( key < T->data )\n            T = T->lchild;\n        else\n            T = T->rchild;\n    }\n    return T;\n}\n```\n或\n```cpp\nBiTree BSTSearch( BiTree T, ElemType key )\n{\n    while ( T )\n    {\n        if ( T->data == key ) return T;\n        else if ( T->data > key )\n            T = T->lchild;\n        else\n            T = T->rchild;\n    }\n    return T;\n}\n```\n\n### 25.二叉排序树的插入(递归)\n\n```cpp\nbool BSTInsert( BiTree& T, ElemType key )\n{\n    if (!T)\n    {\n        T = ( BiTree ) malloc( sizeof( BiTNode ) );\n        T->data = key;\n        T->lchild = T->rchild = NULL;\n        return true;\n    }\n    else if ( T->data == key ) return false;\n    else if ( T->data > key ) return BSTInsert( T->lchild, key );\n    else return BSTInsert( T->rchild, key );\n}\n```\n\n### 26.计算二叉树的带权路径长度(递归)\n\n```cpp\nint wpl = 0;\nint WPL_PreOrder( BiTree T, int deep )\n{\n    if ( T == NULL ) return 0;\n    if ( !T->lchild && !T->rchild )\n        wpl += deep * T->weight;\n    else\n    {\n        if ( T->lchild ) WPL_PreOrder( T->lchild, deep + 1 );\n        if ( T->rchild ) WPL_PreOrder( T->rchild, deep + 1 );\n    }\n    return wpl;\n}\n```\n\n### 27.计算二叉树的带权路径长度(非递归)\n\n```cpp\n// 思路: 层序遍历的思想\nint wpl = 0;\nint WPL_LevelOrder( BiTree T )\n{\n    if ( T == NULL ) return 0;\n    BiTree Q[MAXSIZE], p;\n    int front = -1, rear = -1, depth = 0, last = 0;\n    Q[++rear] = T;\n    while ( front != rear )\n    {\n        p = Q[++front];\n        if ( !p->lchild && !p->rchild )\n            wpl += depth * p->weight;\n        else\n        {\n            if ( p->lchild ) Q[++rear] = p->lchild;\n            if ( p->rchild ) Q[++rear] = p->rchild;\n        }\n        if ( front == last )\n        {\n            depth++; last = rear;\n        }\n    }\n    return wpl;\n}\n```\n\n### 28.判断二叉树是否为二叉排序树\n\n```cpp\nElemType preVal = MIN;\nbool IsBST( BiTree T )\n{\n    if ( T == NULL ) return true;\n    if ( !IsBST( T->lchild ) ) return false;\n    if ( preVal >= T->data )\n        return false;\n    else\n        preVal = T->data;\n    if ( !IsBST( T->rchild ) ) return false;\n    return true;\n}\n```\n\n### 29.求出指定结点在二叉排序树中的层次\n\n```cpp\nint Level( BiTree T, BiTree p )\n{\n    if ( T == NULL ) return 0;\n    int n = 1;\n    while ( T->data != p->data )\n    {\n        n++;\n        if ( p->data < T->data )\n            T = T->lchild;\n        else\n            T = T->rchild;\n    }\n    return n;\n}\n```\n\n### 30.判断二叉树是否为平衡二叉树\n\n```cpp\nbool IsAVL( BiTree T, int& h )\n{\n    int h1 = 0, h2 = 0;\n    if (T==NULL )\n    {\n        h = 0; return true;\n    }\n    if ( IsAVL( T->lchild, h1 ) && IsAVL( T->rchild, h2 ) )\n    {\n        if ( abs( h1 - h2 ) <= 1 )\n        {\n            h = 1 + ( h1 > h2 ? h1 : h2 );\n            return true;\n        }\n    }\n    return false;\n}\n```\n\n### 31.从大到小输出二叉排序中所有值不小于k的关键字\n\n```cpp\nvoid DesOutput( BiTree T, ElemType k )\n{\n    if ( T == NULL ) return;\n    DesOutput( T->rchild, k );\n    if ( T->data >= k )\n        visit( T );\n    else\n        return;\n    DesOutput( T->lchild, k );\n}\n```\n\n### 32.在二叉排序树上查找第k(1≤k≤n)小的元素,要求平均时间复杂度为O(log2n)二叉排序树上的每个结点中除data,lchild,rchild外,还增加一个count成员,保存以该结点为根的子树上的结点个数\n\n> 法一\n\n```cpp\nBiTree SearchSmallK( BiTree T, int k )\n{\n    if ( k<1 || k>T->count ) return NULL;\n    if ( T->lchild )\n    {\n        if ( k <= T->lchild->count )\n            return SearchSmallK( T->lchild, k );\n        else if ( k == T->lchild->count + 1 )\n            return T;\n        else\n            return SearchSmallK( T->rchild, k - ( T->lchild->count + 1 ) );\n    }\n    else\n    {\n        if ( k == 1 ) return T;\n        else return SearchSmallK( T->rchild, k - 1 );\n    }\n}\n```\n\n> 法二\n\n```cpp\nBiTree SearchSmallK( BiTree T, int k )\n{\n    if ( k<1 || k>T->count ) return NULL;\n    if ( T->lchild )\n    {\n        if ( k <= T->lchild->count )\n            return SearchSmallK( T->lchild, k );\n        else\n            k -= T->lchild->count;\n    }\n    if ( k == 1 ) return T;\n    if ( T->rchild )\n        return SearchSmallK( T->rchild, k - 1 );\n}\n```\n\n### 33.对于含有+,−,∗,/及括号的算术表达式(中缀表达式)写一个算法,将该表达式构造成相应的二叉树表示\n\n```cpp\n// 思想: 最后使用的操作符作为根.即:先+,-后*,/\n// 例如: a+b*(c-d)-e/f构造的表达式树如下:\n//                -\n//               /  \\\n//              +    /\n//             / \\  / \\\n//            a  *  e  f\n//              / \\\n//             b   -\n//                / \\\n//               c   d\n// 通过该表达式树,可以很容易得到:\n// 前缀表达式: -+a*b-cd/ef\n// 中缀表达式: a+b*c-d-e/f\n// 后缀表达式: abcd-*+ef/-\nBiTNode* BuildTree( char* exp, int s, int e )\n{\n    if ( e - s == 1 )\n    {\n        BiTNode* p = ( BiTNode* ) malloc( sizeof( BiTNode ) );\n        p->data = exp[s];\n        p->lchild = p->rchild = NULL;\n        return p;\n    }\n    int c1 = -1, c2 = -1, c = 0, i;\n    for ( i = s; i < e; i++ )\n    {\n        if ( exp[i] == '(' ) c++;\n        else if ( ( exp[i] == '+' || exp[i] == '-' ) && !c )\n            c1 = i;\n        else if ( ( exp[i] == '*' || exp[i] == '/' ) && !c )\n            c2 = i;\n    }\n    if ( c1 < 0 ) c1 = c2;\n    if ( c1 < 0 ) return BuildTree( exp, s + 1, e - 1 );\n    BiTree* p = ( BiTNode* ) malloc( sizeof( BiTNode ) );\n    p->data = exp[c1];\n    p->lchild = BuildTree( exp, s, c1 );\n    p->rchild = BuildTree( exp, c1 + 1, e );\n    return p;\n}\n```","tags":["二叉树"],"categories":["algorithm"]},{"title":"[algorithm]图","url":"%2Fposts%2F3164%2F","content":"## 一.图的算法\n\n### 1. 邻接矩阵和邻接表的表示\n\n#### 1). 邻接矩阵表示的数据结构\n\n```cpp\n#define INFINITY INT_MAX // 无穷大\n#define MAX_VERTEX_NUM 20 // 限制顶点最大数值为20个\n#define MAX_ARC_NUM  MAX_VERTEX_NUM * (MAX_VERTEX_NUM - 1) // 由n个顶点，最多可以确定n(n-2)/2条直线,有向图为2倍\n#define MAX_INFO 20 // 用户输入的弧信息，最多20个字符\n\n/*数组表示法*/\ntypedef int        VRType;\ntypedef char    InfoType;\ntypedef char    VertexType[5];\ntypedef enum    {DG, DN, UDG, UDN} GraphKind; \n\ntypedef struct ArcCell {\n    VRType adj;\n    InfoType *info;\n}ArcCell, AdjMatrix[MAX_VERTEX_NUM][MAX_VERTEX_NUM];\n\ntypedef struct {\n    VertexType    vexs[MAX_VERTEX_NUM];\n    AdjMatrix    arcs;\n    int            vexnum, arcnum;\n}MGraph;\n```\n\n#### 2). 邻接表表示的数据结构\n\n```cpp\n/*邻接表表示法*/\ntypedef struct ArcNode\n{\n    int                adjvex;\n    int                w; // 存储权值，书中的程序没有表示权值的数据成员(书中说用info来存储权值，但是上面的程序又是单独用的adj存权值，为了一致性，info还是用来存储其他信息算了)\n    struct ArcNode    *nextarc;\n    InfoType *info; // 用来存储权值以外的有关弧的信息\n}ArcNode;\n\ntypedef struct VNode\n{\n    VertexType    data;\n    ArcNode        *firstarc;\n}VNode, AdjList[MAX_VERTEX_NUM];\n\ntypedef struct\n{\n    AdjList        vertices;\n    int            vexnum, arcnum;\n    int            kind;\n}ALGraph;\n```\n\n### 2.写出从图的邻接表表示转换成邻接矩阵表示的算法\n\n```cpp\nvoid Convert( ALGraph G, int arcs[][10] )\n{\n    for ( int v = 0; v < G.vexnum; v++ )\n        for ( ArcNode* p = G.vertices[v].firstarc; p; p->nextarc )\n            arcs[v][p->adjvex] = 1;\n}\n```\n\n## 二.图的遍历\n\n>说明: 以下图的算法既可以使用邻接矩阵的方式也可以使用邻接表存储的方式,因此每种算法都可以换成另一种存储形式,只需要把MGraph(邻接矩阵存储)换成ALGraph(邻接表存储)即可\n\n### 1. 寻找邻接点\n\n#### 1). 邻接矩阵下,通用找邻接的函数:\n\n```cpp\n// 第一个邻居\nint FirstNeighbor( MGraph G, int v)\n{ \n    for ( int i = 0; i < G.vexnum; i++ )\n        if ( G.arcs[v][i] == 1 )\n            return i;\n    return -1;\n}\n// 当前的下一个邻居\nint NextNeighbor( MGraph G, int v, int w )\n{\n    for (int i = w+1; i < G.vexnum; i++ )\n        if ( G.arcs[v][i] == 1 )\n            return i;\n    return -1;\n}\n```\n\n#### 2). 邻接表下,通用找邻接的函数:\n\n```cpp\n/*全局变量*/\nbool Visited[MAX_VERTEX_NUM]; // 记录每个顶点是否被访问过\n\n// 找到第一个v相邻的顶点，返回它的下标\nint FirstAdjVex(ALGraph &AL, int v)\n{\n    ArcNode *p = NULL;\n    p = AL.vertices[v].firstarc;\n    if (p == NULL)\n        return -1;\n    else\n        return p->adjvex;\n}\n\n// 找到下一个与v相邻的顶点，返回它的下标\nint NextAdjVex(ALGraph &AL, int v, int w)\n{\n    ArcNode *p = NULL;\n    p = AL.vertices[v].firstarc;\n    while (p->adjvex != w) // 找到下标为w的结点\n        p = p->nextarc;\n    p = p->nextarc; // 指针指向下标为w的结点的后面一个结点\n    if (p == NULL)\n        return -1;\n    else\n        return p->adjvex;\n}\n```\n\n### 2. 遍历方法(BFS+DFS)\n\n#### 1). 广度优先搜索(Breadth-First-Search, BFS)\n\n>法一:采用邻接矩阵\n\n```cpp\nbool visited[MAX_VERTEX_NUM] = { false };\nvoid BFS( MGraph G, int v );\n\nvoid BFSTraverse( MGraph G )\n{\n    int Q[MAXSIZE];\n    int front = -1, rear = -1;\n    for (int v = 0; v < G.vexnum; v++ )\n        if ( !visited[v] )\n            BFS( G, v );\n}\n\nvoid BFS( MGraph G, int v )\n{\n    int Q[MAXSIZE];\n    int front = -1, rear = -1;\n    // BFS顶点三连\n    visit( v );\n    visited[v] = true;\n    Q[++rear] = v;\n    \n    while ( front != rear )\n    {\n        v = Q[++front];\n        for ( int w = FirstNeighbor( G, v ); w >= 0; w = NextNeighbor( G, v, w ) )\n        {\n            if (!visited[w])\n            {\n                // BFS顶点三连\n                visit( w );\n                visited[w] = true;\n                Q[++rear] = w;\n            }\n        }\n    }\n}\n```\n\n>法二:采用邻接表\n\n```cpp\nbool visited[MAX_VERTEX_NUM] = { false };\nvoid BFS( ALGraph G, int v );\n\nvoid BFSTraverse( ALGraph G )\n{\n    int Q[MAXSIZE];\n    int front = -1, rear = -1;\n    for (int v = 0; v < G.vexnum; v++ )\n        if ( !visited[v] )\n            BFS( G, v );\n}\n\nvoid BFS( ALGraph G, int v )\n{\n    int Q[MAXSIZE];\n    int front = -1, rear = -1;\n    // BFS顶点三连\n    visit( v );\n    visited[v] = true;\n    Q[++rear] = v;\n    \n    while ( front != rear )\n    {\n        v = Q[++front];\n        for ( int w = FirstNeighbor( G, v ); w >= 0; w = NextNeighbor( G, v, w ) )\n        {\n            if (!visited[w])\n            {\n                // BFS顶点三连\n                visit( w );\n                visited[w] = true;\n                Q[++rear] = w;\n            }\n        }\n    }\n}\n```\n\n#### 2). 深度优先搜索(Depth-First-Search, DFS)\n\n```cpp\nbool visited[MAX_VERTEX_NUM] = { false };\nvoid DFS( ALGraph &G, int v );\n\nvoid DFSTraverse( ALGraph &G )\n{\n    for ( int v = 0; v < G.vexnum; v++ )\n        if ( !visited[v] )\n            DFS( G, v );\n}\n\nvoid DFS( ALGraph &G, int v )\n{\n    visit( v );\n    visited[v] = true;\n    for ( int w = FirstNeighbor( G, v ); w >= 0; w = NextNeighbor( G, v, w ) )\n        if ( !visited[w] )\n            DFS( G, w );\n}\n```\n\n## 三.综合算法\n\n#### 1. BFS算法求解单源最短路径问题\n\n```cpp\nbool visited[MAXSIZE] = { false };\nunsigned int d[MAXSIZE] = { INFINITE };\nvoid BFS_MIN_Distance( ALGraph G, int u )\n{\n    BiTree Q[MAXSIZE];\n    int front = -1, rear = -1, v, w;\n    // BFS路径三连\n    d[u] = 0;\n    visited[u] = true;\n    Q[++rear] = u;\n    \n    while ( front != rear )\n    {\n        v = Q[++front];\n        for ( w = FirstNeighbor( G, v ); w >= 0; w = NextNeighbor( G, v, w ) )\n        {\n            if (!visited[w])\n            {\n                // BFS路径三连\n                d[w] = d[v] + 1;\n                visited[w] = true;\n                Q[++rear] = w;\n            }\n        }\n    }\n}\n```\n\n#### 2.设计一个算法,判断一个无向图G是否为一棵树\n\n```cpp\nint visited[MAXSIZE] = { 0 };\nvoid DFS( MGraph G, int v, int& Vnum, int& TD );\n\nbool IsTree( MGraph G )\n{\n    int Vnum = 0, TD = 0;    // TD=total degree总度数\n    DFS( G, 0, Vnum, TD );    // 从第一个顶点开始遍历\n    if ( Vnum == G.vexnum&&TD == 2 * ( G.vexnum - 1 ) )\n        return true;\n    return false;\n}\n\nvoid DFS( MGraph G, int v, int& Vnum, int& TD )\n{\n    visited[v] = true; Vnum++;\n    for ( int w = FirstNeighbor( G, v ); w >= 0; w = NextNeighbor( G, v, w ) )\n        if ( !visited[w] )\n            DFS( G, w, Vnum, TD );\n}\n```\n\n#### 3.写出图的深度优先搜索DFS算法的非递归算法\n\n```cpp\nbool visited[MAXSIZE] = { false };\nvoid DFS_NON_RC( MGraph G, int v )\n{\n    int S[MAXSIZE];\n    int top = -1;\n    for ( int i = 0; i < G.vexnum; i++ )\n        visited[i] = false;\n    // 顶点二连\n    visited[v] = true;\n    S[++top] = v;\n\n    while ( top != -1 )\n    {\n        v = S[top--]; \n        visit( v );\n        for ( int w = FirstNeighbor( G, v ); w >= 0; w = NextNeighbor( G, v, w ) )\n            if ( !visited[w] )\n            {\n                // 顶点二连\n                visited[w] = true;\n                S[++top] = w;\n            }\n    }\n}\n```\n\n#### 4.分别采用基于广度优先遍历和深度优先遍历算法判别以邻接表或邻接矩阵存储的有向图中是否存在由顶点v到顶点u的路径(v≠u)\n\n```cpp\n// 采用BFS的方法\nbool visited[MAXSIZE] = { false };\nbool Exist_Path_BFS( MGraph G, int v, int u )\n{\n    int Q[MAXSIZE];\n    int front = -1, rear = -1;\n    visited[v] = true;\n    Q[++rear] = v;\n    while ( front != rear )\n    {\n        v = Q[++front];\n        for ( int w = FirstNeighbor( G, v ); w >= 0; w = NextNeighbor( G, v, w ) )\n        {\n            if (!visited[w])\n            {\n                if ( w == u ) return true;\n                visited[w] = true;\n                Q[++rear] = w;\n            }\n        }\n    }\n    return false;\n}\n```\n\n```cpp\n// 采用DFS的方法\nbool visited[MAXSIZE] = { false };\nbool Exist_Path_DFS( MGraph G, int v, int u )\n{\n    if ( v == u ) return true;\n    visited[v] = true;\n    for ( int w = FirstNeighbor( G, v ); w >= 0; w = NextNeighbor( G, v, w ) )\n    {\n        if ( !visited[w] )\n        {\n            if ( Exist_Path_DFS( G, w, u ) ) return true;\n        }\n    }\n    return false;\n}\n```\n\n#### 5.拓扑排序:判断并输出有向图的拓扑序列\n\n```cpp\nbool Topological( MGraph G, int indegree[] )\n{\n    int S[MAXSIZE];\n    int top = -1, Vnum = 0, v = 0;\n    for ( v = 0; v < G.vexnum; v++ )\n    {\n        if ( indegree[v] == 0 )\n        {\n            visit( v );\n            Vnum++;\n            S[++top] = v;\n        }\n    }\n    while ( top != -1 )\n    {\n        v = S[top--];\n        for ( int w = FirstNeighbor( G, v ); w >= 0; w = NextNeighbor( G, v, w ) )\n        {\n            indegree[w]--;\n            if ( indegree[w] == 0 )\n            {\n                visit( w );\n                Vnum++;\n                S[++top] = w;\n            }\n        }\n    }\n    if ( Vnum == G.vexnum )\n        return true;\n    return false;\n}\n```\n\n#### 6.拓扑排序(DFS):有向无环图的拓扑排序\n\n```cpp\nbool visited[MAXSIZE] = { false };\nint time = 0, finishTime[MAXSIZE] = { 0 };\nvoid DFS( MGraph G, int v );\n\nvoid Topological_DFS( MGraph G )\n{\n    for ( int v = 0; v < G.vexnum; v++ )\n        if ( !visited[v] )\n            DFS( G, v );\n    for ( int t = time - 1; t >= 0; t-- )\n        visit( finishTime[t] );\n}\n\nvoid DFS( MGraph G, int v )\n{\n    visited[v] = true;\n    for ( int w = FirstNeighbor( G, v ); w >= 0; w = NextNeighbor( G, v, w ) )\n        if ( !visited[w] )\n            DFS( G, w );\n    finishTime[time++] = v;\n}\n```","tags":["图"],"categories":["algorithm"]},{"title":"[algorithm]查找","url":"%2Fposts%2F4660%2F","content":"## 一.查找的算法\n\n### 1.折半查找(二分查找)\n\n```cpp\nint binary_search( string s, char ch )\n{\n    int low = 0, high = s.size()-1;\n    while (low <= high)\n    {\n        int mid = ( low + high ) / 2;\n        if ( ch ==  s[mid] )\n            return mid;\n        else if ( ch < s[mid] )\n            high = mid - 1;\n        else\n            low = mid + 1;\n    }\n    return -1;\n}\n```\n时间复杂度: $O(log_2{n})$\n\n### 3.折半查找(二分查找)递归\n```cpp\nint binary_search_rc( string s, char ch, int low, int high )\n{\n    if ( low > high ) \n        return 0;\n    int mid = ( low + high ) / 2;\n    if ( ch == s[mid] ) \n        return mid;\n    else if ( ch < s[mid] )  \n        high = mid - 1;\n    else \n        low = mid + 1;\n    return binary_search_rc( s, ch, low, high );\n}\n```\n时间复杂度: $O(log_2{n})$\n\n---\n## 二.字符串匹配\n\n### 1.简单的模式匹配算法(朴素模式匹配算法)\n\n```cpp\nint naive_search( string S, string T )\n{\n    int i = 0, j = 0, lenS = S.size(), lenT = T.size();\n    while ( i < lenS && j < lenT )\n    {\n        if ( S[i] == T[j] ) { i++; j++; }\n        else { i = i - j + 1; j = 0; }\n    }\n    if ( j >= lenS ) return i - lenT;\n    return 0;\n}\n```\n时间复杂度: $O(m*n)$\n\n### 2.KMP算法\n\n算法需要先求出模式串的next值:\n\n```cpp\nvoid get_next( string T, int next[] )\n{\n    int i = 0, j = -1, lenT = T.size();\n    next[0] = -1;\n    while (i<lenT)\n    {\n        if (j==-1||T[i]==T[j] )\n        {\n            i++; j++; next[i] = j;\n        }\n        else j = next[j];\n    }\n}\n```\n\n也可求出改进后的nextval值:\n\n```cpp\nvoid get_nextval( string T, int nextval[] )\n{\n    int i = 0, j = -1, lenT = T.size();\n    nextval[0] = -1;\n    while ( i < lenT )\n    {\n        if ( j == -1 || T[i] == T[j] )\n        {\n            i++, j++;\n            if ( T[i] == T[j] )\n                nextval[i] = nextval[j];\n            else\n                nextval[i] = j;\n        }\n        else j = nextval[j];\n    }\n}\n```\n\n以下是KMP算法:\n```cpp\nint KMP( string S, string T, int next[], int pos )\n{\n    int i = pos, j = 0, lenS = S.size(), lenT = T.size();\n    while ( i<lenS&&j<lenT )\n    {\n        if ( j == -1 || S[i] == T[j] ) { i++; j++; }\n        else j = next[j];\n    }\n    if ( j >= lenT ) return i - lenT;\n    return 0;\n}\n```\n时间复杂度: $O(m+n)$","tags":["查找"],"categories":["algorithm"]},{"title":"[leetcode]214.最短回文串","url":"%2Fposts%2F44722%2F","content":"{% asset_img Image21.png %}\n### 方法一: KMP算法\n\n时间复杂度: $O(m+n)$\n\n> 解题思路: 实际就是求原串从左到右的最长回文串(必须包含左边所有字符),此处采用**倒置+KMP算法**来缩短匹配时间\n>   1. 将字符串倒置,原串作为模式串pat,倒置串作为主串txt\n>   2. 求出模式串pat的nextval[]值,然后进行字符串匹配,得到的模式串pat最长匹配长度即为模式串pat从第一个字符开始的最大回文串(匹配过程时间复杂度只需要$O(m+n)$)\n\n对于**KMP(Knuth–Morris–Pratt)** 算法,可以借鉴关于[查找][1]部分的介绍\n\n```cpp\nint nextval[40005];\nvoid get_nextval(string pat)\n{\n    int i=0,j=-1,len=pat.size();\n    nextval[0]=-1;\n    for (;i<len&&j<len;)\n    {\n        if (j==-1||pat[i]==pat[j])\n        {\n            i++,j++;\n            if (pat[i]==pat[j])\n                nextval[i]=nextval[j];\n            else\n                nextval[i]=j;\n        }\n        else j=nextval[j];\n    }\n}\nstring shortestPalindrome(string txt) {\n    string pat=txt;\n    reverse(txt.begin(),txt.end()); // 字符串倒置\n    get_nextval(pat); // 计算模式串的nextval值\n    int i=0,j=0,lenTxt=txt.size(),lenPat=pat.size();\n    for(;i<lenTxt&&j<lenPat;)   // 进行模式串匹配,找出最大匹配长度\n    {\n        if (j==-1||txt[i]==pat[j])\n            i++,j++;\n        else\n            j = nextval[j];\n    }\n    return txt.substr(0,lenTxt-j)+pat;\n}\n```\n\n","tags":["KMP"],"categories":["OJ"]},{"title":"[leetcode]TOC汇总","url":"%2Fposts%2F6542%2F","content":"| Date | Times | Title                                                    | Methods(t)                     | T(n)          |\n| :--- | :---- | :------------------------------------------------------- | :----------------------------- | :------------ |\n| 4/22 | 2     | 5.最长回文子串                                           | 中心扩展法                     | $O(n^2)$      |\n|      | 4     |                                                          | Manacher                       | $O(n)$        |\n|      | 3     | 214.最短回文串                                           | 逆置+KMP                       | $O(m+n)$      |\n|      | 2     | 28. 实现strStr()                                         | KMP                            | $O(m+n)$      |\n|      | 1     | (*)239. 滑动窗口最大值                                   |                                | $$            |\n| 4/23 | 1     | 53.最大子序列和                                          |                                | $$            |\n|      | 2     | 152.乘积最大子序列                                       |                                | $$            |\n|      |       | 分数拆分                                                 |                                | $$            |\n|      | 2     | 378.有序矩阵中第K小的元素                                |                                | $$            |\n| 4/24 | 1     | 718. 最长重复子数组                                      | DP                             | $O(n^2)$      |\n|      | 1     | 300. 最长上升子序列                                      | deque+二分                     | $$            |\n|      | 1     |                                                          | DP                             | $$            |\n|      | 1     | 3. 无重复字符的最长子串                                  |                                | $$            |\n| 4/25 |       | 340.Longest Substring with At Most K Distinct Characters |                                | $$            |\n|      | 3     | 992.Subarrays with K Different Integers                  |                                | $$            |\n|      | 1     | 14.最长公共前缀                                          |                                | $$            |\n|      | 1     | 7.整数反转                                               |                                | $$            |\n| 4/26 | 1     | 219. 存在重复元素 II                                     |                                | $$            |\n|      | 1     | 209. 长度最小的子数组                                    |                                | $$            |\n|      | 1     | 76. 最小覆盖子串                                         |                                | $$            |\n|      | 1     | 13. 罗马数字转整数                                       |                                | $$            |\n|      | 1     | 9. 回文数                                                |                                | $$            |\n|      | 1     | 12. 整数转罗马数字                                       |                                | $$            |\n|      | 1     | 15. 三数之和                                             | 双指针法+去重                  | $$            |\n| 4/27 | 1     | 43. 字符串相乘                                           |                                | $$            |\n|      | 1     | 67. 二进制求和                                           |                                | $$            |\n|      | 1     | 66. 加一                                                 |                                | $$            |\n|      | 1     | 100. 相同的树                                            |                                | $$            |\n|      | 1     | 94. 二叉树的中序遍历                                     |                                | $$            |\n| 4/28 | 1     | 1033. 移动石子直到连续                                   |                                | $$            |\n|      | 1     | (*)\t1036. 逃离大迷宫                                     |                                | $$            |\n| 4/29 |       | 5000. 从始点到终点的所有路径                             |                                | $$            |\n|      |       | 797. 所有可能的路径                                      |                                | $$            |\n|      | 1     | 257. 二叉树的所有路径                                    |                                | $$            |\n| 4/30 |       | (*)\t5040. 边框着色                                       |                                | $$            |\n|      |       | (*)\t133. 克隆图                                          |                                | $$            |\n|      |       | (*)\t138. 复制带随机指针的链表                            |                                | $$            |\n| 5/1  |       | 144.二叉树的前序遍历                                     |                                | $$            |\n|      |       | 94.二叉树的中序遍历                                      |                                | $$            |\n|      |       | 145.二叉树的后序遍历                                     |                                | $$            |\n| 5/2  |       | (*)\t968.监控二叉树                                       |                                | $$            |\n|      |       | 590. N叉树的后序遍历                                     |                                | $$            |\n|      |       | 429. N叉树的层序遍历                                     |                                | $$            |\n|      |       | 589. N叉树的前序遍历                                     |                                | $$            |\n|      |       | (*)\t106. 从中序与后序遍历序列构造二叉树                  |                                | $$            |\n|      |       | (*)\t105. 从前序与中序遍历序列构造二叉树                  |                                | $$            |\n|      |       | 559. N叉树的最大深度                                     |                                | $$            |\n|      |       | 104. 二叉树的最大深度                                    |                                | $$            |\n|      |       | 111. 二叉树的最小深度                                    |                                | $$            |\n|      |       | 102. 二叉树的层次遍历                                    |                                | $$            |\n|      |       | 107. 二叉树的层次遍历 II                                 |                                | $$            |\n|      |       | 637. 二叉树的层平均值                                    |                                | $$            |\n|      |       | (*)\t114. 二叉树展开为链表                                |                                | $$            |\n| 5/5  |       | 5051. 有效的回旋镖                                       |                                | $$            |\n|      |       | 5050. 从二叉搜索树到更大和树                             |                                | $$            |\n|      |       | 5.最长回文子串(manacher)                                 |                                | $$            |\n| 5/7  |       | 1039. Minimum Score Triangulation of Polygon             | DP                             | $O(n^3)$      |\n| 5/8  |       | 312. Burst Balloons                                      | DP                             | $O(n^3)$      |\n|      |       | 375. Guess Number Higher or Lower II                     | DP                             | $O(n^3)$      |\n| 5/9  |       | 118.杨辉三角                                             | DP                             | $O(n^2)$      |\n|      |       | 119.杨辉三角 II                                          | DP                             | $O(n^2)$      |\n|      |       |                                                          | 公式法                         | $O(n)$        |\n|      |       | 110. 平衡二叉树                                          | 递归                           | $O(n)$        |\n|      |       | 101. 对称二叉树                                          | 递归                           | $O(n)$        |\n|      |       | 103. 二叉树的锯齿形层次遍历                              | 层次遍历+奇数逆序              |               |\n| 5/11 |       | 17. 电话号码的字母组合                                   | 递归+回溯(1)                   |               |\n| 5/12 |       | 401.二进制手表                                           | 暴力破解                       |               |\n|      |       | 22. 括号生成                                             | 递归+回溯(1)                   |               |\n|      |       | 5055. 困于环中的机器人                                   |                                | $O(n)$        |\n| 5/14 |       | 6. Z 字形变换                                            | 找规律                         | $O(n)$        |\n|      |       | 8. 字符串转换整数 (atoi)                                 |                                | $O(n)$        |\n|      |       | 190. 颠倒二进制位                                        |                                |               |\n| 5/15 |       | 20. 有效的括号                                           |                                |               |\n|      |       | 121. 买卖股票的最佳时机                                  | DP                             | $O(n)$        |\n|      |       | 122. 买卖股票的最佳时机 II                               | DP                             | $O(n)$        |\n| 5/16 |       | 123. 买卖股票的最佳时机 III                              | DP                             | $O(n)$        |\n|      |       | 188. 买卖股票的最佳时机 IV                               | DP+Greedy                      | $O(n*k)$      |\n|      |       | 376. 摆动序列                                            | 找规律                         | $O(n)$        |\n| 5/17 |       | 11. Container With Most Water                            | 双指针法                       | $O(n)$        |\n|      |       | 19. 删除链表的倒数第N个节点                              | 插入头结点法                   | $O(n)$        |\n|      | 1     | 16. 最接近的三数之和                                     | 双指针法+去重                  | $O(n^2)$      |\n|      | 1     | 18. 四数之和                                             | 双指针法+去重                  |               |\n|      |       | 21. 合并两个有序链表                                     | 头结点+余项链接                | $O(max(m,n))$ |\n|      |       | 23. 合并K个排序链表                                      | 头结点+余项链接                |               |\n| 5/18 |       | 24. 两两交换链表中的节点                                 | 递归                           |               |\n|      |       | 25. k个一组翻转链表                                      | 递归                           |               |\n|      |       | 27. 移除元素                                             | 直接插入法                     | $O(n)$        |\n|      |       | 26. 删除排序数组中的重复项                               | 直接插入法                     | $O(n)$        |\n|      |       | 80. 删除排序数组中的重复项 II                            | 直接插入法                     | $O(n)$        |\n| 5/19 |       | (*) 10. 正则表达式匹配                                   | DP                             | $O(m*n)$      |\n|      |       | 35. 搜索插入位置                                         |                                | $O(n)         |\n|      |       | 34. 在排序数组中查找元素的第一个和最后一个位置           | 二分查找                       | $O(log_2n)$   |\n| 5/20 |       | 62. 不同路径                                             | 排列组合                       | $O(n)$        |\n|      |       |                                                          | DP                             | $O(n*m)$      |\n|      |       | 63. 不同路径II                                           | DP                             | $O(n*m)$      |\n| 5/21 |       | 64. 最小路径和                                           | DP                             | $O(m*n)$      |\n|      |       | 174. 地下城游戏                                          | DP(bottom up)                  |               |\n| 5/22 | 1     | 70.爬楼梯                                                | DP                             | $O(n)$        |\n|      |       |                                                          | Fibonacci                      | $O(1)$        |\n|      |       | RodCutting                                               | DP                             | $O(n^2)$      |\n| 5/24 |       | 204. 计数质数                                            | Eratosthenes                   |               |\n|      |       | 279. 完全平方数                                          | DP                             |               |\n| 5/29 |       | 31. 下一个排列                                           |                                | $O(n)$        |\n|      |       | 46. 全排列                                               |                                | $O(n^2)       |\n|      |       | 60. 第k个排列                                            |                                |               |\n| 5/30 |       | 84.柱状图中最大的矩形                                    | 单调栈                         | $O(n)$        |\n|      |       | 85. 最大矩形                                             | 单调栈                         |               |\n| 5/31 |       | 221. 最大正方形                                          | 单调栈                         |               |\n| 6/2  |       | 303. 区域和检索 - 数组不可变                             | DP                             | $O(n)$        |\n|      |       |                                                          | 树状数组                       | $O(logn)$     |\n|      |       | 304. 二维区域和检索 - 矩阵不可变                         | DP                             |               |\n| 6/3  |       | 36. 有效的数独                                           | 约束法:行判断+列判断+块判断    |               |\n|      |       | 37. 解数独                                               | 约束法+回溯                    |               |\n| 6/4  |       | 459. 重复的子字符串                                      | 周期法                         |               |\n|      |       | 686. 重复叠加字符串匹配                                  | KMP                            |               |\n| 6/13 |       | 88. 合并两个有序数组                                     | 直接插入                       |               |\n|      |       |                                                          | 归并                           |               |\n| 6/14 | 1     | 72.编辑距离                                              | dp                             | $O(mn)$       |\n|      |       | 583. 两个字符串的删除操作                                | dp                             |               |\n|      |       | 712. 两个字符串的最小ASCII删除和                         | dp                             |               |\n| 6/15 | 1     | 343. 整数拆分                                            | n大于4时,尽量多划分3,乘积最大  | $O(n)$        |\n| 6/16 |       | 1089.复写零                                              | 插入排序                       |               |\n|      |       | 1090.受标签影响最大的值                                  |                                |               |\n|      |       | 1091.二进制矩阵中的最短路径                              |                                |               |\n| 6/19 |       | 29.两数相除                                              | 数学方法                       |               |\n|      |       | 230. 二叉搜索树中第K小的元素                             | 中序遍历                       |               |\n| 6/20 | 1     | 39. 组合总和                                             | 递归+回溯(1)                   |               |\n|      | 1     | 40.组合总和II                                            | 回溯+去重                      |               |\n|      | 1     | 216.组合总和III                                          | 回溯                           |               |\n|      | 1     | 377.组合总和IV                                           | dp                             |               |\n|      |       |                                                          | 回溯                           |               |\n| 6/21 | 1     | 392. 判断子序列                                          |                                |               |\n|      |       | 792. 匹配子序列的单词数                                  |                                |               |\n|      | 1     | 54.螺旋矩阵                                              | 蛇形填数                       |               |\n|      | 1     | 59. 螺旋矩阵 II                                          | 蛇形填数                       |               |\n| 6/22 |       | 516. 最长回文子序列                                      | DP                             | $O(n^2)$      |\n| 6/23 |       | 1093.大样本统计                                          | 数学方法                       |               |\n|      |       | 1094.拼车                                                | 数学方法                       |               |\n| 6/24 |       | 560. 和为K的子数组                                       | 哈希表                         |               |\n|      |       | 523. 连续的子数组和                                      | 暴力破解                       |               |\n| 6/25 | 1     | 87.扰乱字符串                                            | 递归                           |               |\n|      | 1     | 443. 压缩字符串                                          |                                |               |\n| 7/21 |       | 5130. 等价多米诺骨牌对的数量                             |                                |               |\n| 7/22 | 1     | 5132.颜色交替的最短路径                                  | bfs                            |               |\n|      |       | 5131.叶值的最小代价生成树                                | dp memorize+分治法             |               |\n| 7/23 |       | 1131. 绝对值表达式的最大值                               | 数学技巧                       |               |\n|      |       | 33.搜索旋转排序数组                                      | 二分查找                       |               |\n| 7/24 |       | 198. 打家劫舍                                            | DP                             |               |\n|      |       |                                                          | 交替法                         |               |\n|      |       | 213. 打家劫舍 II                                         | DP                             |               |\n|      |       |                                                          | 交替法                         |               |\n| 7/25 |       | 337. 打家劫舍 III                                        | 递归                           |               |\n|      |       | 96. 不同的二叉搜索树                                     | Catalan:$\\frac{C_{2n}^n}{n+1}$ |               |\n|      |       |                                                          |                                |               |\n|      |       |                                                          |                                |               |\n|      |       |                                                          |                                |               |\n|      |       |                                                          |                                |               |\n|      |       |                                                          |                                |               |\n|      |       |                                                          |                                |               |\n|      |       |                                                          |                                |               |\n|      |       |                                                          |                                |               |\n|      |       |                                                          |                                |               |\n|      |       |                                                          |                                |               |\n|      |       |                                                          |                                |               |","tags":["TOC"],"categories":["OJ"]},{"title":"[algorithm]排序","url":"%2Fposts%2F16867%2F","content":"## 一.排序算法\n\n### 1.插入排序\n\n#### 1) 直接插入排序:(插入类),从0开始\n```cpp\nvoid InsertSort( ElemType R[], int n )\n{\n    for ( int i = 1; i < n; i++ )\n    {\n        if ( R[i].key < R[i - 1].key )\n        {\n            R[0] = R[i];\n            for ( int j = i - 1; j >= 0 && ( R[0].key < R[j].key ); j-- )\n                R[j + 1] = R[j];\n            R[j + 1] = R[0];\n        }\n    }\n}\n```\n最好情况(顺序有序):\n\n　　1)比较次数: $\\sum_{i=2}^{n} 1=n-1$\n\n　　2)移动次数: 0\n\n最坏情况(逆序有序):\n\n　　1)比较次数: $\\sum_{i=2}^{n} i=\\frac {(n+2)(n-1)}{2}$\n\n　　2)移动次数: $\\sum_{i=2}^{n} (i+1)=\\frac {(n+4)(n-1)}{2}$\n\n#### 2)折半插入排序:(插入类)\n```cpp\nvoid BiInsertSort( ElemType R[], int n )\n{\n    for ( int i = 2; i <= n; i++ )\n    {\n        R[0] = R[i];\n        int low = 1, high = i - 1;\n        while ( low <= high )\n        {\n            int mid = ( low + high ) / 2;\n            if ( R[0].key < R[m].key ) high = mid - 1;\n            else low = mid + 1;\n        }\n        for ( int j = i - 1; j > high; j-- )\n            R[j + 1] = R[j];\n        R[j + 1] = R[0];\n    }\n}\n```\n#### 3)希尔排序(又称缩小增量排序)(插入类)\n```cpp\n// 当dk=1时,即为直接插入排序\nvoid ShellSort( ElemType R[], int n )\n{\n    for ( int dk = n / 2; dk >= 1; dk /= 2 )\n    {\n        for ( int i = dk + 1; i <= n; i++ )\n        {\n            if ( R[i].key < R[i - dk].key )\n            {\n                R[0] = R[i];\n            for ( j = i - dk; j > 0 && ( R[0].key < R[j].key ); j -= dk )\n                    R[j + dk] = R[j];\n                R[j + dk] = R[0];\n            }\n        }\n    }\n}\n```\n### 2.交换排序\n\n#### 1)起泡排序(冒泡排序)(交换类)\n```cpp\nvoid BubbleSort( ElemType R[], int n )\n{\n    for ( int i = 1; i <= n - 1; i++ )\n    {\n        bool flag = false;\n        for ( int j = n; j > i; j-- )\n        {\n            if (R[j].key < R[j-1].key )\n            {\n                swap( R[j], R[j - 1] );\n                flag = true;\n            }\n        }\n        if ( !flag ) return;\n    }\n}\n```\n#### 2)快速排序:(交换类)\n```cpp\nvoid Partition( ElemType R[], int low, int high );\n\n// 快排\nvoid QuickSort( ElemType R[], int low, int high )\n{\n    if ( low >= high ) return;\n    int pivotpos = Partition( R, low, high );\n    QuickSort( R, low, pivotpos - 1 );\n    QuickSort( R, pivotpos + 1, high );\n}\n\n// 划分\nvoid Partition( ElemType R[], int low, int high )\n{\n    ElemType pivot = R[low];\n    while ( low < high )\n    {\n        while ( low < high && R[high].key >= pivot.key ) high--;\n        R[low] = R[high];\n        while ( low < high && R[low].key <= pivot.key ) low++;\n        R[high] = R[low];\n    }\n    R[low] = pivot;\n    return low;\n}\n```\n### 3.选择排序\n\n#### 1)简单选择排序(选择类)\n```cpp\nvoid SelectSort( ElemType R[], int n )\n{\n    for ( int i = 0; i < n - 1; i++ )\n    {\n        int min = i;\n        for ( int j = i + 1; j < n; j++ )\n        {\n            if ( R[j].key < R[min].key ) min = j;\n        }\n        if ( min != i ) swap( R[i], R[min] );\n    }\n}\n```\n#### 2)堆排序(选择类)\n```cpp\nvoid AdjustDown( ElemType R[], int s, int n );\n\nvoid HeapSort( ElemType R[], int n )\n{\n    for ( int i = n / 2; i > 0; i-- )\n        void AdjustDown( R, i, n );\n    for ( int i = n; i > 1; i-- )\n    {\n        swap( R[i], R[1] );\n        AdjustDown( R, 1, i - 1 );\n    }\n}\n\n// 向下调整\nvoid AdjustDown( ElemType R[], int s, int n )\n{\n    R[0] = R[s];\n    for ( int i = 2 * s; i <= n; i *= 2 )\n    {\n        if ( i < n&&R[i].key < R[i + 1].key ) i++;\n        if (R[0].key  >=R[i].key ) break;\n        else\n        {\n            R[s] = R[i]; s = i;\n        }\n    }\n    R[s] = R[0];\n}\n```\n```cpp\n// 向上调整\nvoid AdjustUp( ElemType R[], int s )\n{\n    R[0] = R[s];\n    int p = s / 2;\n    while ( p >&& R[p].key < R[0].key )\n    {\n        R[s] = R[p];\n        s = p;\n        p /= 2;\n    }\n    R[s] = R[0];\n}\n```\n### 4.归并排序(归并类)\n```cpp\nvoid Merge( ElemType R[], int low, int mid, int high );\n\nvoid MergeSort( ElemType R[], int low, int high )\n{\n    if ( low >= high ) return;\n    int mid = ( low + high ) / 2;\n    MergeSort( R, low, mid );\n    MergeSort( R, mid + 1, high );\n    Merge( R, low, mid, high );\n}\n\nElemType B[MAXSIZE];\nvoid Merge( ElemType R[], int low, int mid, int high )\n{\n    int i,j,k;\n    for ( i = low; i <= high; i++ )\n        B[i] = R[i];\n    i = k = low, j = mid + 1;\n    while ( i <= mid && j <= high )\n    {\n        if ( B[i].key <= B[j].key )\n            R[k++] = B[i++];\n        else\n            R[k++] = B[j++];\n    }\n    while ( i <= mid ) R[k++] = B[i++];\n    while ( j <= high ) R[k++] = B[j++];\n}\n```\n## 二.综合题(算法)\n\n1.设顺序表用数组R[]表示,表中存储在数组下标1~m+n的范围内,前m个元素递增有序,后n个元素递增有序,设计一个算法,使得整个顺序表有序\n```cpp\nvoid InsertSort( ElemType R[], int m, int n )\n{\n    for ( int i = m + 1; i <= m + n; i++ )\n    {\n        if ( R[i].key < R[i - 1].key )\n        {\n            R[0] = R[i];\n            for ( int j = i - 1; j > 0 && ( R[0].key < R[j].key ); j-- )\n                R[j + 1] = R[j];\n            R[j + 1] = R[0];\n        }\n    }\n}\n```\n2.计数排序:对表进行排序并将结果放到另一个新的表中,要求表中所有关键码互不相同\n```cpp\nvoid CountSort( ElemType A[], ElemType B[], int n )\n{\n    for ( int i = 0; i < n; i++ )\n    {\n        int cnt = 0;\n        for ( int j = 0; j < n; j++ )\n            if ( A[i].key > A[j].key )cnt++;\n        B[cnt] = A[i];\n    }\n}\n```\n3.双向冒泡排序\n```cpp\n// 思想:第一趟通过交换把最大的放最后,第二趟通过交换把最小的放最前,反复进行\nvoid BubbleSort( ElemType A[], int n )\n{\n    int low = 0, high = n - 1, i;\n    bool flag = true;\n    while ( low < high && flag )\n    {\n        flag = false;\n        for (i = low; i < high; i++ )\n        {\n            if (A[i]>A[i+1] )\n            {\n                swap( A[i], A[i + 1] ); flag = true;\n            }\n        }\n        high--;\n        for ( i = high; i > low; i-- )\n        {\n            if ( A[i] < A[i - 1] )\n            {\n                swap( A[i], A[i - 1] ); flag = true;\n            }\n        }\n        low++;\n    }\n}\n```\n4.单链表的简单选择排序(假设不带表头结点)\n```cpp\nvoid SelectSort( LinkList& L )\n{\n    LinkList h, p, s, pre, r;\n    h = L;\n    while ( h )\n    {\n        p = s = h; pre = r = NULL;\n        // 找最大结点s\n        while ( p )\n        {\n            if (p->data>s->data )\n            {\n                s = p; r = pre;\n            }\n            pre = p;\n            p = p->next;\n        }\n        // 脱链\n        if ( s == h ) h = h->next;\n        else r->next = s->next;\n        // 头插法\n        s->next = L; L = s;\n    }\n}\n```\n5.顺序表中有n个不同整数(下标1~n),设计算法把所有奇数移动到偶数前面(时,空都最少)\n```cpp\nvoid Move( ElemType A[], int n )\n{\n    int low = 1, high = n;\n    while ( low < high )\n    {\n        while ( low < high&&A[low] % 2 ) low++;\n        while ( low < high && A[high] % 2 == 0 ) high--;\n        if ( low < high )\n        {\n            swap( A[low], A[high] );\n            low++; high--;\n        }\n    }\n}\n```\n6.在顺序表中找出第k小的元素(时空最少)\n```cpp\n// 思想:划分\nint Partition( ElemType R[], int low, int high )\n{\n    int pivot = R[low];\n    while ( low < high )\n    {\n        while ( low < high && R[high].key >= pivot.key ) high--;\n        R[low] = R[high];\n        while ( low < high&& R[low].key <= pivot.key ) low++;\n            R[high] = R[low];\n    }\n    R[low] = pivot;\n    return low;\n}\n\nElemType Kth_elem( ElemType R[], int low, int high, int k )\n{\n    int pivotpos = Partition( R, low, high );\n    if ( pivotpos == k ) return R[pivotpos];\n    else if ( pivotpos > k ) return Kth_elem( R, low, pivotpos - 1, k );\n    else return Kth_elem( R, pivotpos + 1, high, k );\n}\n```\n7.n个正整数构成的集合A,将其划分为两个不相交的子集$A1,A2$,元素个数分别是n1和n2.A1和A2中元素之和分别为S1和S2.设计一个时空高效算法,使|n1-n2|最小且|s1-s1|最大.(下标从1开始)\n```cpp\nint Partition( ElemType R[], int low, int high )\n{\n    int pivot = R[low];\n    while ( low < high )\n    {\n        while ( low < high && R[high].key >= pivot.key ) high--;\n        R[low] = R[high];\n        while ( low < high&& R[low].key <= pivot.key ) low++;\n        R[high] = R[low];\n    }\n    R[low] = pivot;\n    return low;\n}\n\nint SetPartition( ElemType R[], int n, int low, int high )\n{\n    int k = n / 2, s1, s2, i;\n    int pivotpos = Partition( R, low, high );\n    if ( pivotpos == k )\n    {\n        s1 = s2 = 0;\n        for ( i = 1; i <= k; i++ ) s1 += R[i];\n        for ( j = k + 1; j <= n; j++ ) s2 += R[j];\n        return s2 - s1;\n    }\n    else if ( pivotpos > k )\n        return SetPartition( R, n, low, pivotpos - 1 );\n    else return SetPartition( R, n, pivotpos + 1, high );\n}\n```","tags":["排序"],"categories":["algorithm"]},{"title":"[leetcode]133.克隆图","url":"%2Fposts%2F12189%2F","content":"{% asset_img 453425-20190430181854853-2091334093.png %}\n### 方法一:dfs(递归)\n\n```cpp\nmap<Node*,Node*> dict;\nNode* clone(Node* node)\n{\n    if (!node) return node;\n    if (dict.count(node)) return dict[node];\n    dict[node]=new Node(node->val,vector<Node*>{});　　// 这里不能写clone(node),会导致死循环,记住,在new的时候千万不要再递归,递归最低层一定有一个明确结果,所以要把截止条件写清楚\n    for(auto it:node->neighbors)\n        dict[node]->neighbors.push_back(clone(it));\n    return dict[node];\n}\nNode* cloneGraph(Node* node) \n{\n    return clone(node);\n}\n```\n### 方法二:dfs(非递归)\n\n```cpp\nmap<Node*,Node*> dict;\nNode* cloneGraph(Node* node) \n{\n    stack<Node*> S;\n    S.push(node);\n    while (!S.empty())\n    {\n        Node *p = S.top();\n        S.pop();\n        if (!dict.count(p))　　// 从栈中出来的都是没有进行访问过的点\n            dict[p]=new Node(p->val,vector<Node*>{});\n        for (auto it:p->neighbors)\n        {\n            if (!dict.count(it))　　// 判断是否已经访问过该点\n            {\n                dict[it]=new Node(it->val,vector<Node*>{});\n                S.push(it);\n            }\n            dict[p]->neighbors.push_back(dict[it]);　　// 将新点的拷贝放入neighbors中\n        }\n    }\n    return dict[node];\n}\n```","tags":["图"],"categories":["OJ"]},{"title":"[leetcode]138.复制带随机指针的链表","url":"%2Fposts%2F48962%2F","content":"{% asset_img 453425-20190430193735900-662137168.png %}\n### 方法一:递归\n```cpp\nunordered_map<Node*,Node*> dict;\nNode* copyRandomList(Node* head) \n{\n    if (!head) return head;\n    if (dict.count(head)) return dict[head];\n    dict[head]=new Node(head->val, nullptr, nullptr);\n    dict[head]->next=copyRandomList(head->next);\n    dict[head]->random=copyRandomList(head->random);\n    return dict[head];\n}\n```\n### 方法二:非递归\n```cpp\nNode* copyRandomList(Node* head) \n{\n    if (!head) return head;\n    unordered_map<Node*,Node*> m;\n    Node *p=head;\n    while(p)    // make a copy of nodes\n    {\n        m[p]=new Node(p->val,nullptr,nullptr);\n        p=p->next;\n    }\n    p=head;\n    while(p)    // link everyone and fill the random field\n    {\n        m[p]->next=m[p->next];\n        m[p]->random=m[p->random];\n        p=p->next;\n    }\n    return m[head];\n}\n```","tags":["链表"],"categories":["OJ"]},{"title":"[leetcode]144.二叉树的前序遍历","url":"%2Fposts%2F11169%2F","content":"{% asset_img 453425-20190501113746021-447522744.png %}\n前往二叉树的: [**前序**](../11169),[**中序**](../40851),[**后序**](../34771) 遍历算法\n### 方法一:递归\n```cpp\nvector<int> res;\nvector<int> preorderTraversal(TreeNode* root) \n{\n    if (!root) return res;\n    res.push_back(root->val);\n    if (root->left) preorderTraversal(root->left);\n    if (root->right) preorderTraversal(root->right);\n    return res;\n}\n```\n### 方法二:非递归\n```cpp\nvector<int> preorderTraversal(TreeNode* root) \n{\n    vector<int> res;\n    if (!root) return res;\n    stack<TreeNode*> S;\n    TreeNode* p = root;\n    while(p||!S.empty())\n    {\n        if (p)  // 访问左子树\n        {\n            res.push_back(p->val);\n            S.push(p);\n            p=p->left;\n        }\n        else    // 访问右子树\n        {\n            p=S.top();\n            S.pop();\n            p=p->right;\n        }\n    }\n    return res;\n}\n```\n### 方法三:非递归(该方法可用于后序遍历,需要修改几处代码)\n```cpp\nvector<int> res;\nvector<int> preorderTraversal(TreeNode* root) \n{\n    if (!root) return res;\n    stack<TreeNode*> S;\n    S.push(root);\n    while (!S.empty())\n    {\n        root=S.top();\n        S.pop();\n        if (root->right) S.push(root->right);  // 要实现后序遍历,需要以下两行调换\n        if (root->left) S.push(root->left);\n        res.push_back(root->val);   // res.insert(0,root->val)即为后序遍历\n    }\n    return res;\n}\n```\n结论:\n- 方法三这种形式只适合前序和后序遍历,不适合中序遍历,中序遍历较为麻烦\n- 方法二这种形式只适合前序和中序遍历,不适合后序遍历,后序遍历较为麻烦","tags":["递归"],"categories":["OJ"]},{"url":"%2Fposts%2F0%2F"},{"title":"[leetcode]145.二叉树的后序遍历","url":"%2Fposts%2F34771%2F","content":"{% asset_img 453425-20190501122117295-85727166.png %}\n前往二叉树的: [**前序**](../11169),[**中序**](../40851),[**后序**](../34771) 遍历算法\n### 方法一:递归\n```cpp\nvector<int> res;\nvector<int> postorderTraversal(TreeNode* root) \n{\n    if (!root) return res;\n    if (root->left) postorderTraversal(root->left);\n    if (root->right) postorderTraversal(root->right);\n    res.push_back(root->val);\n    return res;\n}\n```\n### 方法二:非递归\n```cpp\nvector<int> postorderTraversal(TreeNode* root) \n{\n    vector<int> res;\n    if (!root) return res;\n    stack<TreeNode*> S;\n    TreeNode* p=root, *r=nullptr;\n    while (p||!S.empty())\n    {\n        if (p)\n        {\n            S.push(p);\n            p=p->left;\n        }\n        else\n        {\n            p=S.top();\n            if (p->right&&p->right!=r)\n                p=p->right;\n            else\n            {\n                S.pop();\n                res.push_back(p->val);\n                r=p;\n                p=nullptr;\n            }\n        }\n    }\n    return res;\n    }\n```\n### 方法三:非递归\n```cpp\nvector<int> postorderTraversal(TreeNode* root) \n{\n    vector<int> res;\n    if (!root) return res;\n    stack<TreeNode*> S;\n    TreeNode* p=root;\n    S.push(p);\n    while (!S.empty())\n    {\n        p=S.top();\n        S.pop();\n        if (p->left) S.push(p->left);\n        if (p->right) S.push(p->right);\n        res.insert(res.begin(),p->val);\n    }\n    return res;\n}\n```","tags":["递归"],"categories":["OJ"]},{"title":"[leetcode]968.监控二叉树","url":"%2Fposts%2F45343%2F","content":"{% asset_img 453425-20190501230642291-677698823.png %}\n解题思路: 由于叶子节点一定不要安装监视器,这样才能使总监视器数量比较少,因此需要从下往上进行判断当前节点的状态(共:3种状态):\n- 0: 当前节点安装了监视器\n- 1: 当前节点可观,但没有安装监视器\n- 2: 当前节点不可观\n对于空节点,我们认为是可观,但没有安装监视器,因此,叶子节点就为不可观的了,设想一个节点的左右孩子(为空)都可观且没有安装监视器,那该节点必然是不可观即2\n\n有了以上对空节点和叶子节点的处理,我们再来正式分析非终端节点:\n\n- 若一个节点的左孩子或右孩子不可观,那么该节点必然不可观,需要安装监视器,因此返回0状态\n- 若一个节点的左孩子或右孩子都可观且至少有一个安装了监视器,那么该节点必然是可观的,返回1状态\n- 若一个节点的左右孩子都可观且没安装监视器,那么该节点必然是不可观的,返回2状态\n记住,我们以上的分析都是基于从整个二叉树的叶子节点往根部,即从下往上进行,而且要做的就是将不可观的节点变得可观才行(因此要根据左右孩子的节点的状态来判断当前节点状态并做出调整)\n\n这里可能会有疑惑,以上的第一条得出当前节点不可观,然后安装了监视器,而第三条也得出当前节点不可观,但却没有安装监视器,而是直接返回的2状态(当前节点不可观).这是为什么?\n\n因为,对于第一条,因为左右孩子都不可观,为了让左右孩子都可观,则必须给当前节点安装监视器才行,而第三条中,左右孩子都是可观的(没有安装监视器),当前节点的可以直接返回不可观状态,因为后面可以由他的父节点进行摄像头安装,使其变得可观.\n\n### 方法一:递归\n```cpp\n// 0：该节点安装了监视器 1：该节点可观，但没有安装监视器 2：该节点不可观\nint monitor = 0;\nint state(TreeNode* node)\n{\n    if (node == nullptr) return 1;\n    int left  = state(node->left);\n    int right = state(node->right);\n    // 该节点为0的情况\n    if (left == 2 || right == 2)\n    {\n        monitor++;  // 由于左或右节点不可观,则需要给当前节点安装监视器,为0状态\n        return 0;\n    } // 为1的情况\n    else if (left == 0 || right == 0)\n        return 1;   // 当(left!=2&&right!=2)时,才会进行该判断,也就是左右节点一定是可观的,再判断是否有一个安装了监视器,如有安装,则当前节点就不需要安装监视器也可观了,为1状态\n    // 为2的情况\n    else    // 其他:党(left!=2&&right!=2)&&(left!=0&&right!=0),即left==1&&right==1时,左右节点都可观,但没有监视器,当前节点不可观,为2状态\n        return 2;\n}\nint minCameraCover(TreeNode *root)\n{\n    if (root == nullptr) return 0;\n    if (state(root) == 2) monitor++;    // 如果根节点为2的状态,需要加一个监视器\n    return monitor;\n}\n```\n注意:这里的if,else if,else的顺序是不能变的,先判断左右都是不可观的,再就是都可观,左或右至少有一个为监视器,最后才是都可观都无监视器.","tags":["二叉树"],"categories":["OJ"]},{"title":"[leetcode]106.从中序与后序遍历序列构造二叉树","url":"%2Fposts%2F49088%2F","content":"{% asset_img 453425-20190502213938801-1912607084.png %}\n前往 [**中序,后序遍历构造二叉树**](https://brianyi.github.io/posts/49088), [**中序,前序遍历构造二叉树**](https://brianyi.github.io/posts/8827)\n```cpp\nTreeNode* build(vector<int>& inorder, int l1, int r1, vector<int>&postorder, int l2, int r2)\n{\n    if (l1>r1) return nullptr;\n    int x = postorder[r2], i = 0;   // 确定当前根节点\n    for (i = l1; i <= r1 && inorder[i] != x; ++i);  // 在中序遍历序列中找到当前根节点位置(该位置可以划分出左右两个分支)\n    int llen = i - l1;  // 左子树结点数量\n    int rlen = r1 - i;  // 右子树结点数量\n    TreeNode* p = new TreeNode(x);  // 建立根节点\n    p->left = build(inorder, l1, l1 + llen - 1, postorder, l2, l2 + llen - 1);  // 递归建立左子树,-1,-1是把当前根节点位置去掉\n    p->right = build(inorder, r1 - rlen + 1, r1, postorder, r2 - rlen, r2 - 1); // 递归建立右子树,+1,-1是把当前根节点位置去掉\n    return p;\n}\nTreeNode* buildTree(vector<int>& inorder, vector<int>& postorder) {\n    if (inorder.empty()||postorder.empty()) return nullptr;\n    return build(inorder, 0, inorder.size() - 1, postorder, 0, postorder.size() - 1);\n}```","tags":["递归"],"categories":["OJ"]},{"title":"[leetcode]105.从前序与中序遍历序列构造二叉树","url":"%2Fposts%2F8827%2F","content":"{% asset_img 453425-20190502215802448-663626721.png %}\n前往 [**中序,后序遍历构造二叉树**](https://brianyi.github.io/post/49088), [**中序,前序遍历构造二叉树**](https://brianyi.github.io/posts/8827)\n``` cpp\nTreeNode* build(vector<int>& preorder, int l1, int r1, vector<int>& inorder, int l2, int r2)\n{\n    if (l1>r1) return nullptr;\n    int x=preorder[l1], i=0;    // 确定当前根节点\n    for(i=l2;inorder[i]!=x&&i<r2;++i);  // 在中序遍历序列中找到当前根节点位置(该位置可以划分出左右两个分支)\n    int llen=i-l2;  // 左子树结点数量\n    int rlen=r2-i;  // 右子树结点数量\n    TreeNode *p = new TreeNode(x);  // 建立根节点\n    p->left = build(preorder, l1+1, l1+llen, inorder, l2, l2+llen-1);   // 递归建立左子树,+1,-1是把当前根节点位置去掉\n    p->right= build(preorder, r1-rlen+1, r1, inorder, r2-rlen+1, r2);   // 递归建立右子树,+1,+1是把当前根节点位置去掉\n    return p;\n}\nTreeNode* buildTree(vector<int>& preorder, vector<int>& inorder) \n{\n    return build(preorder, 0, preorder.size()-1, inorder, 0, inorder.size()-1);\n}\n```","tags":["递归"],"categories":["OJ"]},{"title":"[leetcode]114.二叉树展开为链表","url":"%2Fposts%2F52504%2F","content":"{% asset_img 453425-20190502231706813-609020347.png %}\n思路:递归,将左子树变成单链表形式,再将右子树变成单链表形式,最后将左子树单链表的末端连接到右子树单链表表头,将根节点的左孩子置空\n### 方法一: 递归\n``` cpp\nvoid flatten(TreeNode* root) \n{\n    if (root==nullptr) return;\n    flatten(root->left);    // 将左子树变成单链表形式\n    flatten(root->right);   // 将右子树变成单链表形式\n    if (root->left) // 将左子树单链表的末端连接到右子树单链表表头\n    {\n        TreeNode* p=root->left;\n        while(p->right) p=p->right;\n        p->right=root->right;\n        root->right=root->left;\n        root->left=nullptr;\n    }\n}\n```","tags":["递归"],"categories":["OJ"]},{"title":"[leetcode]239.滑动窗口最大值","url":"%2Fposts%2F45513%2F","content":"{% asset_img 453425-20190503094513334-1691572073.png %}\n思路:滑动窗口的思想,只要是求连续子序列或者子串问题,都可用滑动窗口的思想\n### 方法一: 滑动窗口\n``` cpp\nvector<int> maxSlidingWindow(vector<int>& nums, int k) \n{\n    vector<int> res;\n    if (nums.size()==0) return res;\n    int i=0;\n    deque<int> dq;　　\n    for (i=0;i<nums.size();++i)\n    {\n        while(!dq.empty()&&nums[i]>nums[dq.back()]) //在尾部添加元素，并保证左边元素都比尾部大\n            dq.pop_back();\n        dq.push_back(i);\n        if (i-k==dq.front())    //在头部移除元素\n            dq.pop_front();\n        if (i>=k-1)\n            res.push_back(nums[dq.front()]);    // 存放每次窗口内的最大值\n    }\n    return res;\n}\n```","tags":["滑动窗口"],"categories":["OJ"]},{"title":"[leetcode]76.最小覆盖子串","url":"%2Fposts%2F39410%2F","content":"{% asset_img 453425-20190503105807149-1455260747.png %}\n思路:滑动窗口思想\n### 方法一:滑动窗口\n``` cpp\nstring minWindow(string s, string t) {\n    // 1.tdict记录T中每个字母与字母个数\n    // 2.维护一个滑动窗口字母的计数表sdict,计数当前窗口内T中字母出现的次数\n    // 3.当窗口内T中字母出现的次数大于等于T中每个字母出现的次数一样,这时第一个最短子串出现,再逐步从左边缩短窗口,\n    // 直到不满足上述条件,然后再从右边扩大窗口,直到满足条件时,再进行最短子串长度对比,一直更新最短长度子串直到结束\n    string minWindow(string s, string t) {\n        if (s.size()<t.size()||s.size()==0) return \"\";\n        string res=\"\";\n        map<char,int> dict_s,dict_t;\n        for(auto item:t)    // 填充T的字母与字母计数表\n            dict_t[item]++;\n        for(int l=0,r=0,k=t.size();r<s.size();++r)\n        {\n            if (dict_t[s[r]])   // 有字符,则进行记录\n            {\n                dict_s[s[r]]++;\n                if (dict_s[s[r]]<=dict_t[s[r]])\n                    --k;\n            }\n            while(k==0) // 满足条件,滑动窗口从左边逐步缩短,直到剔除第一个属于T中的字符为止\n            {\n                if (res.empty()||res.size()>r-l+1) // 最短子串更新\n                    res=s.substr(l,r-l+1);\n                if (dict_t[s[l]])\n                {   dict_s[s[l]]--;\n                    if (dict_s[s[l]]<dict_t[s[l]])\n                        k++;\n                }\n                l++;\n            }\n        }\n        return res;\n    }\n}\n```","tags":["滑动窗口"],"categories":["OJ"]},{"title":"[leetcode]3.无重复字符的最长子串","url":"%2Fposts%2F37922%2F","content":"{% asset_img 453425-20190503112154412-1836563639.png %}\n思路:滑动窗口的思想\n### 方法一:滑动窗口\n``` cpp\nint lengthOfLongestSubstring(string s) \n{\n    /*\n        控制一个滑动窗口,窗口内的字符都是不重复的,通过set可以做到判断字符是否重复\n    */\n    unordered_set<char> set;\n    size_t maxL=0;\n    for(int l=0,r=0;r<s.size();++r)\n    {\n        while(set.count(s[r]))  // 从左缩短窗口,直到剔除当前判断的元素为止\n            set.erase(s[l++]);\n        set.insert(s[r]);   // 将当前判断元素放入到滑动窗口中\n        maxL=max(maxL,set.size());  // 更新无重复字符的最长子串\n    }\n    return maxL;\n}\n```\n","tags":["滑动窗口"],"categories":["OJ"]},{"title":"[leetcode]1028.从先序遍历还原二叉树","url":"%2Fposts%2F38760%2F","content":"{% asset_img 453425-20190503131537591-466226739.png %}\n思路:用一个栈来管理树的层次关系,索引代表节点的深度\n### 方法一:\n``` cpp\nTreeNode* recoverFromPreorder(string S) \n{\n    /*\n        由题意知,最上层节点深度为0(数字前面0条横线),而第二层节点前有1条横线,表示深度为1\n        树的前序遍历: 根-左-右\n        因此,\n    */\n    if (S.empty()) return nullptr;\n    vector<TreeNode*> stack;  // 结果栈\n    for(int i=0,depth=0,val=0;i<S.size();)\n    {\n        for(depth=0;i<S.size()&&S[i]=='-';++i)  // 计算节点的深度\n            depth++;\n        for(val=0;i<S.size()&&S[i]!='-';++i)    // 计算数值\n            val=val*10+S[i]-'0';\n        while (stack.size()>depth)    // 若当前栈的长度(树的高度)大于节点的深度,则可以把栈中最后几个节点pop掉(这些节点各已经成为完整的子树,可以pop掉了)\n            stack.pop_back();\n        TreeNode* node=new TreeNode(val);   // 新建节点用于存放当前深度的结点\n        if (!stack.empty()) // 节点间关联\n        {\n            if (!stack.back()->left)      stack.back()->left=node;\n            else if(!stack.back()->right) stack.back()->right=node;\n        }\n        stack.push_back(node);\n    }\n    return stack[0];\n}\n```\n","tags":["二叉树"],"categories":["OJ"]},{"title":"[leetcode]5040.边框着色","url":"%2Fposts%2F3454%2F","content":"{% asset_img 453425-20190430173552247-46622747.png %}\n### 方法一：dfs的非递归形式\n``` cpp\nusing ll=long long;\nconst ll MAXN=50LL;\nunordered_set<ll> vis,mark;\nvector<vector<int>> colorBorder(vector<vector<int>>& G, int r0, int c0, int color) {\n    queue<ll> Q;\n    Q.push(r0*MAXN+c0);\n    int c=G[r0][c0];\n    int dx[]={-1,1,0,0},dy[]={0,0,-1,1};\n    while (!Q.empty())\n    {\n        int x=Q.front()/MAXN;\n        int y=Q.front()%MAXN;\n        Q.pop();\n        vis.insert(x*MAXN+y);\n        if (x==0||x==G.size()-1||y==0||y==G[0].size()-1)    // 边界方块可变色\n            mark.insert(x*MAXN+y);\n        else if (G[x-1][y]!=c||G[x+1][y]!=c||G[x][y-1]!=c||G[x][y+1]!=c)    // 四个方向中,任意一个方块颜色不同,则可变色\n            mark.insert(x*MAXN+y);\n        for (int d=0;d<4;d++)   // 放入连通分量的所有方块\n        {\n            int nx=x+dx[d],ny=y+dy[d];\n            if (0<=nx&&nx<G.size()&&0<=ny&&ny<G[0].size()&&!vis.count(nx*MAXN+ny)&&G[nx][ny]==c)\n                Q.push(nx*MAXN+ny);\n        }\n    }\n    for (auto it:mark)\n        G[it/MAXN][it%MAXN]=color;\n    return G;\n}\n```\n思路:用vis记录访问过的方块,mark标记连通分量中需要修改颜色的方块,并非连通分量中所有的方块都要修改颜色,比如:一个方块如果四周(四个方向邻接的)都是相同颜色,那么只需要修改四周方块的颜色,而自己颜色不变(开始的时候没理解题意,以为只要是连通分量内的方块颜色都需要改变)\n\n### 方法二: dfs递归形式,只不过把上面的非递归改为递归了\n``` cpp\nusing ll=long long;\nconst ll MAXN=50LL;\nunordered_set<ll> vis,mark;\nvoid dfs(vector<vector<int>>& G, int x, int y, int c)\n{\n    int dx[]={-1,1,0,0},dy[]={0,0,-1,1};\n    vis.insert(x*MAXN+y);\n    if (x==0||x==G.size()-1||y==0||y==G[0].size()-1)    // 边界方块可变色\n        mark.insert(x*MAXN+y);\n    else if (G[x-1][y]!=c||G[x+1][y]!=c||G[x][y-1]!=c||G[x][y+1]!=c)    // 四个方向中,任意一个方块颜色不同,则可变色\n        mark.insert(x*MAXN+y);\n    for (int d=0;d<4;d++)   // 放入连通分量的所有方块\n    {\n        int nx=x+dx[d],ny=y+dy[d];\n        if (0<=nx&&nx<G.size()&&0<=ny&&ny<G[0].size()&&!vis.count(nx*MAXN+ny)&&G[nx][ny]==c)\n            dfs(G,nx,ny,c);\n    }\n}\nvector<vector<int>> colorBorder(vector<vector<int>>& G, int r0, int c0, int color) {\n    dfs(G,r0,c0,G[r0][c0]);\n    for (auto it:mark)\n        G[it/MAXN][it%MAXN]=color;\n    return G;\n}\n```\n### 方法三:dfs递归,但通过修改G中的数据,来记录是否访问过,和是否需要修改颜色,国外的一个[大佬](https://leetcode.com/problems/coloring-a-border/discuss/282847/C%2B%2B-with-picture-DFS)写的\nFrom an initial point, perform DFS and flip the cell color to negative to track visited cells.\nAfter DFS is complete for the cell, check if this cell is inside. If so, flip its color back to the positive.\nIn the end, cells with the negative color are on the border. Change their color to the target color.\n{% asset_img image_1556425139.png %}\n``` cpp\nvoid dfs(vector<vector<int>>& g, int r, int c, int cl) {\n    if (r < 0 || c < 0 || r >= g.size() || c >= g[r].size() || g[r][c] != cl) return;    // 剪枝(越界,非着色块)\n    g[r][c] = -cl;    // 着色\n    dfs(g, r - 1, c, cl), dfs(g, r + 1, c, cl), dfs(g, r, c - 1, cl), dfs(g, r, c + 1, cl);\n    if (r > 0 && r < g.size() - 1 && c > 0 && c < g[r].size() - 1 && cl == abs(g[r - 1][c]) &&\n        cl == abs(g[r + 1][c]) && cl == abs(g[r][c - 1]) && cl == abs(g[r][c + 1]))    // 将原四周同色的块,颜色还原\n        g[r][c] = cl;\n}\nvector<vector<int>> colorBorder(vector<vector<int>>& grid, int r0, int c0, int color) {\n    dfs(grid, r0, c0, grid[r0][c0]);\n    for (auto i = 0; i < grid.size(); ++i)    // 根据dfs标记(负数)过的方块进行着色\n        for (auto j = 0; j < grid[i].size(); ++j) grid[i][j] = grid[i][j] < 0 ? color : grid[i][j];\n    return grid;\n}\n```\n结论: 无论是递归还是非递归,先标记(标记vis),再遍历\n","tags":["图"],"categories":["OJ"]},{"title":"[leetcode]5.最长回文子串","url":"%2Fposts%2F21336%2F","content":"{% asset_img 453425-20190505125039549-1852205917.png %}\n### 方法一:中心扩展算法\n\n解题思路:从左到右每一个字符都作为中心轴,然后逐渐往两边扩展,只要发现有不相等的字符,则确定了以该字符为轴的最长回文串,但需要考虑长度为奇数和偶数的不同情况的处理(长度为偶数时轴心为中间两个数的中心,长度为奇数时轴心为中间那个数)\n\n算法时间复杂度: $O(n^{2})$\n```cpp\nstring longestPalindrome(string s) \n{        \n    int idx = 0, maxL = 0;\n    for (int i = 0; i < s.size(); ++i)　　// i为轴的位置,j为回文串半径\n    {\n        for (int j = 0; i - j >= 0 && i + j < s.size(); ++j)    // 奇数\n        {\n            if (s[i - j] != s[i + j])\n                break;\n            if (2 * j + 1 > maxL)\n            {\n                maxL = 2 * j + 1;\n                idx = i - j;\n            }\n        }\n        for (int j = 0; i - j >= 0 && i + j + 1 < s.size(); ++j)    // 偶数\n        {\n            if (s[i-j]!=s[i+j+1])\n                break;\n            if (2 * j + 2 > maxL)\n            {\n                maxL = 2 * j + 2;\n                idx = i - j;\n            }\n        }\n    }\n    return s.substr(idx, maxL);\n}\n```\n---\n### 方法二: manacher(马拉车法)\n解题思路:详见[P3805【模板】manacher算法][1]\n为了使奇数串和偶数串一致性处理,首先进行字符填充,使其成为奇数串,即在每个字符的前后填充字符,例如:\n原串: ABCCBA\n填充后: ~#A#B#C#C#B#A#\n原始串在数组中的位置:\n\n|   A   | B    |   C   |   C   |   B   |   A   |\n| :---: | :--- | :---: | :---: | :---: | :---: |\n|   0   | 1    |   2   |   3   |   4   |   5   |\n\n填充串在数组中的位置\n\n|   ~   |   #   |   A   |   #   |   B   |   #   |   C   |   #   |   C   |   #   |   B   |   #   |   A   |   #   |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|   0   |   1   |   2   |   3   |   4   |   5   |   6   |   7   |   8   |   9   |  10   |  11   |  12   |  13   |\n\n首先说明: \n- 奇数+偶数=奇数, 因此,奇数串填充偶数个#后为奇数串,偶数串填充奇数个#后为奇数串\n- ~字符用来作为边界,用处在于进行两边扩展时做为结束条件\n- 填充串中字符的最大回文半径 - 1 = 原字符串中该字符的回文串长度\n\n关于上述第3条我需要解释一下:\n\n| s_copy |   ~   |   #   |   A   |   #   |   B   |   #   |   C   |   #   |   C   |   #   |   B   |   #   |   A   |   #   |\n| :----: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| index  |   0   |   1   |   2   |   3   |   4   |   5   |   6   |   7   |   8   |   9   |  10   |  11   |  12   |  13   |\n|  pos   |   0   |   0   |   0   |   0   |   1   |   1   |   2   |   2   |   3   |   3   |   4   |   4   |   5   |   5   |\n|   p    |   0   |   1   |   2   |   1   |   2   |   1   |   2   |   7   |   2   |   1   |   2   |   1   |   2   |   1   |\n\n可以看到index=7的位置,#对应的p为7(即最大回文半径),即在C与C之间,表示原字符串中该字符的回文长度为6,那么原字符串ABCCBA的前面3个字符ABC构成的回文串长度为6\n\n>该题思路:\n>1. 字符串填充统一为奇数串\n>2. Manacher法,从左到右遍历每个字符\n>   1. 记录每个字符的最大回文半径\n>   2. 确定已经记录的最大回文串右边界r,和中间轴m\n>   3. 当前字符s[i]是否能关于m找到一个对称点,即要满足:m<=i<=r\n>      1. 能:则得到一个有可能的最大回文半径,并从该半径开始扩展\n>      2. 否:则从新计算最大回文半径\n\n算法时间复杂度为: $O(n)$\n```cpp\nint pos[2005],p[2005];  // pos用于记录填充串与原始字串的位置关系,p用于记录填充串当前字符的最大回文半径\nstring longestPalindrome(string s) \n{        \n    /* 填充字符,统一为奇数串 */\n    string s_new=\"~\";\n    for (int i=0,k=1;i<s.size();++i)\n    {\n        s_new+=\"#\";\n        s_new+=s[i];\n        pos[k++]=i;\n        pos[k++]=i;\n    }\n    s_new+=\"#\";\n    \n    /* manacher */\n    int m=0,r=0,maxL=0,idx=0;\n    for (int i=1;i<s_new.size();++i)\n    {\n        // 获取已知的s_new的最大回文半径,p[i]用于记录填充串对应字符的最大回文半径\n        if (i<r)    // m左边的最大回文半径都已经求出来了\n            p[i]=min(p[2*m-i],r-i); // 当m<=i<=r时,i关于m中心轴对称的点为2*m-i,而p[2*m-i]是一定已经知道的,取min是为了保证一定要在最大回文半径范围内,这样能保证一定是回文串,有可能p[2*m-i]超出了最大回文半径范围,这样会延伸到r的右边,不能保证一定为回文串\n        else\n            p[i]=1; // 如果i超出了已知的最大回文右边界,则比如不能找到关于m对称的点,只能重新计算最大回文半径\n        // 暴力拓展左右两侧,计算当前的最大回文半径\n        while (s_new[i-p[i]]==s_new[i+p[i]])\n            p[i]++;\n        // 新的回文半径比较大,则更新\n        if (r-i<p[i])\n        {\n            m=i;\n            r=i+p[i];\n        }\n        // 更新回文长度(原始字串的回文长度为新字串回文半径-1)\n        if (p[i]-1>=maxL)\n        {\n            maxL=p[i]-1;\n            idx=pos[i]-maxL/2;  // 更新原始回文字串的起始位置\n        }\n    }\n    return s.substr(idx,maxL);\n}\n```\n\n[1]: https://www.luogu.org/problemnew/solution/P3805","tags":["回文串"],"categories":["OJ"]}]
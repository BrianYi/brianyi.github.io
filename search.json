[{"title":"[leetcode]686.重复叠加字符串匹配","url":"%2Fposts%2F136ccaf5%2F","content":"{% asset_img 2019060414363024.png %}\n\n### 解题思路 \n这道题关键在于明白最后求出的`A重复叠加后的长度<len(B)+2*len(A)`,然后便是常规KMP算法了,为啥这道题要记录,等会会说明原因\nKMP算法是固定格式,这里直接求出模式串B的nextval\n然后就是模式串匹配了,即主串A不断倍增,直到模式串匹配为止\n`代码1`:\n```cpp\nfor(i=0,j=0;i<A.size()&&j<B.size()&&A.size()>=B.size();)\n{\n    if (j==-1||A[i]==B[j])\n        i++,j++;\n    else\n        j=nextval[j];\n}\n```\n大家看代码1有什么问题吗?很有趣的一点是,第一遍提交`代码1`时结果返回不对,后来在调试代码,j=-1的时候无缘无故跳出了循环,后面想到size()返回的是size_t格式,是无符号整形,那么-1也被转为无符号了,反而成为了最大值,下次一定要注意,当然即使这里改成代码2也会有一定问题,只有size()超过了有符号范围,那么必然溢出,不过还好题目要求的数量只有1~10000区间内\n`代码2`:\n```cpp\nfor(i=0,j=0;i<(int)A.size()&&j<(int)B.size()&&A.size()>=B.size();)\n{\n    if (j==-1||A[i]==B[j])\n        i++,j++;\n    else\n        j=nextval[j];\n}\n```\n\n\n### 代码实现 \n```cpp\nint nextval[40001];\nvoid get_nextval(string pat)\n{\n    int i=0,j=-1,len=pat.size();\n    nextval[0]=-1;\n    for(;i<len&&j<len;)\n    {\n        if (j==-1||pat[i]==pat[j])\n        {\n            i++,j++;\n            if (pat[i]==pat[j])\n                nextval[i]=nextval[j];\n            else\n                nextval[i]=j;\n        } else j=nextval[j];\n    }\n}\nint repeatedStringMatch(string A, string B) {\n    int maxLen=2*A.size()+B.size(),times=1,i=0,j=0;\n    string tmp=A;\n    get_nextval(B);\n    while(A.size()<maxLen)\n    {\n        for(i=0,j=0;i<(int)A.size()&&j<(int)B.size()&&A.size()>=B.size();)\n        {\n            if (j==-1||A[i]==B[j])\n                i++,j++;\n            else\n                j=nextval[j];\n        }\n        if (j==B.size())\n            return times;\n        A+=tmp;\n        ++times;\n    }\n    return -1;\n}\n```\n","tags":["KMP"],"categories":["OJ"]},{"title":"[leetcode]459.重复的子字符串","url":"%2Fposts%2F6d263cd4%2F","content":"{% asset_img 2019060413565823.png %}\n\n### 解题思路 \n周期串为`s`,那么设定`t`表示周期,值在`[1,len(s)-1]`的范围,那么剩下的就是依次对每个`t`值进行遍历看是否真的满足周期为`t`,数学中周期的表达式是\n$$\\begin{aligned}\n    f(x+t)=f(x)\n\\end{aligned}$$\n那么代码中可以这样写`s[i%t]==s[i]`\n\n### 代码实现 \n```cpp\nbool repeatedSubstringPattern(string s) {\n    int len=s.size(),i=0,t=0;\n    for(t=1;t<=len/2;++t)\n    {\n        if (len%t) continue;    // 有余数,一定不为周期串\n        for (i=t;i<len&&s[i%t]==s[i];++i);\n        if (i==len) return true;\n    }\n    return false;\n}\n```\n\n### 参考\n1. [<<算法竞赛入门经典>>](https://book.douban.com/subject/4138920/)","tags":["周期串"],"categories":["OJ"]},{"title":"[高等数学]第4章 向量代数与空间解析几何","url":"%2Fposts%2Fcb4e5d34%2F","content":"## 向量代数\n### 向量的运算及性质\n- 数量积(点积,内积)\n  - 几何表示: $\\mathbf{a\\cdot b}=\\mathbf{\\vert{a}\\vert\\vert{b}\\vert}\\cos\\theta$,其中$\\theta=\\lang{\\mathbf{a,b}}\\rang$\n  - 代数表示: $\\mathbf{a}=\\{a_x,a_y,a_z\\},\\mathbf{b}=\\{b_x,b_y,b_z\\}$,则$\\mathbf{a\\cdot b}=a_xb_x+a_yb_y+a_zb_z$\n- 向量积(叉积,外积)\n  - 几何表示: $\\mathbf{a\\times b}$是一向量\n  - 模: $\\vert\\mathbf{a\\times b}\\vert=|\\mathbf a||\\mathbf b|\\sin\\theta$,其中$\\theta=\\lang\\mathbf{a,b}\\rang$\n  - 方向: $\\mathbf{a\\times b}$同时垂直于$\\mathbf a$和$\\mathbf b$,且符合右手法则.\n  - 代数表示:\n    $$\\mathbf{a\\times b}=\\begin{vmatrix}\n        \\mathbf{i}&\\mathbf{j}&\\mathbf{k}\\\\\n        a_x&a_y&a_z\\\\\n        b_x&b_y&b_z\n    \\end{vmatrix}$$\n  - 运算规律:\n    - $\\mathbf{b\\times a}=-(\\mathbf{a\\times b})$\n    - 分配律: $\\mathbf{a\\times(b+c)}=\\mathbf{a\\times b+a\\times c}$\n    - 与数乘的结合律: $\\mathbf{(\\lambda{a})\\times b}=\\mathbf{a\\times(\\lambda{b})}=\\mathbf{\\lambda(a\\times b)}$\n  - 向量积在几何上的应用\n    - 求同时垂直于$\\mathbf a$和$\\mathbf b$的向量:$\\mathbf{a\\times b}$\n    - 求以$\\mathbf a$和$\\mathbf b$为邻边的平行四边形面积: $\\mathbf{S=|a\\times b|}$\n    - 判定两向量平行: $\\mathbf{a//b}\\Lrarr\\mathbf{a\\times b}=0$\n- 混合积\n  - 定义:称$\\mathbf{(a\\times b)\\cdot c}$为三和矢量$\\mathbf{a,b,c}$的混合积.本书将混合积记为$\\mathbf{(abc)}$,即$\\mathbf{(abc)}=\\mathbf{(a\\times b)\\cdot c}$,有的书将它记为$[\\mathbf{abc}]$.\n    设$\\mathbf a=\\{a_x,a_y,a_z\\},\\mathbf b=\\{b_x,b_y,b_z\\},\\mathbf c=\\{c_x,c_y,c_z\\},$则\n    $$\\mathbf{(abc)}=\\begin{vmatrix}\n        a_x&a_y&a_z\\\\\n        b_x&b_y&b_z\\\\\n        c_x&c_y&c_z\n    \\end{vmatrix}$$ \n  - 运算规律\n    - 轮换对称性: $\\mathbf{(abc)=(bca)=(cab)}$\n    - 两向量互换,混合积变号: $\\mathbf{(abc)=-(acb)=-(cba)=-(bac)}$\n  - 混合积在几何上的应用\n    求以$\\mathbf{a,b,c}$为棱的平行六面体体积:$V_{six}=|\\mathbf{(abc)}|$\n    判定三向量共面: $\\mathbf{a,b,c}$共面$\\Lrarr\\mathbf{(abc)}=0$\n### 例题\n{% asset_img 2019060416012625.png %}\n{% asset_img 2019060416033926.png %}\n{% asset_img 2019060416041027.png %}\n{% asset_img 2019060416042728.png %}\n### 参考\n1. [复习全书](https://book.douban.com/subject/6101482/)\n","tags":["高等数学"],"categories":["高等数学"]},{"title":"[高等数学]第5章 多元函数微分学","url":"%2Fposts%2F4c20fdbb%2F","content":"## 极值与最值\n### 无条件极值\n- 多元函数极值和极值点的定义\n  - 定义: 若存在$M_0(x_0,y_0)$点的某邻域$U_\\delta(M_0)$,使得$f(x,y)\\le f(x_0,y_0)$或$(f(x,y)\\ge f(x_0,y_0)),\\forall{(x,y)\\in{U_\\delta(M_0)}}$,则称$f(x,y)$在点$M_0(x_0,y_0)$取得极大值(极小值)$f(x_0,y_0)$,极大值与极小值统称为极值.点$M_0(x_0,y_0)$称为$f(x,y)$的极值点.\n- 多元函数驻点的定义\n  - 定义: 凡能使$f'_x(x,y)=0,f'_y(x,y)=0$同时成立的点$(x,y)$称为函数$f(x,y)$的驻点.==(驻点$\\nleftrightarrow$极值点,多元函数中,极值点与驻点没有任何关系)==\n- 多元函数取得极值的必要条件 ==(与一元函数极值必要条件相联系:点极值,导存在,点导0)==\n  - 定理: 设函数$f(x,y)$在点$M_0(x_0,y_0)$的一阶偏导数存在,且在$(x_0,y_0)$取得极值,则由此可见`具有一阶偏导数的函数的极值点一定是驻点,但驻点不一定是极值点.`\n- 二元函数取得极值的充分条件(下述定理仅适用于二元函数)\n  - 定理: 设函数$z=f(x,y)$在点$(x_0,y_0)$的某邻域内有连续的二阶偏导数,且$f'_x(x_0,y_0)=0,f'_y(x_0,y_0)=0$.令$f''_{xx}(x_0,y_0)=A,f''_{xy}(x_0,y_0)=B,f''_{yy}(x_0,y_0)=C$,则\n    (1).$AC-B^2>0$时,$f(x,y)$在点$(x_0,y_0)$取极值,且$\\begin{cases}\n        minimal\\ value& A<0\\\\\n        maximal\\ value& A>0\n    \\end{cases}$\n    (2).$AC-B^2<0$时,$f(x,y)$在点$(x_0,y_0)$无极值.==(内部取不到极值,看边界情况即条件极值)==\n    (3).$AC-B^2=0$时,不能确定$f(x,y)$在点$(x_0,y_0)$是否有极值,还需进一步讨论(一般用极值定义).==(用极大值,极小值定义)==\n### 条件极值(拉格朗日乘子法)\n- 函数$f(x,y)$在条件$\\varphi(x,y)=0$下的极值的必要条件\n  解决此类问题的一般方法是拉格朗日乘数法:\n  先构造拉格朗日函数$F(x,y,\\lambda)=f(x,y)+\\lambda\\varphi(x,y)$,然后解方程组\n  $$\\begin{cases}\n      \\frac{\\partial F}{\\partial x}=\\frac{\\partial f}{\\partial x}+\\lambda\\frac{\\partial\\varphi}{\\partial x}=0,\\\\\n      \\frac{\\partial F}{\\partial y}=\\frac{\\partial f}{\\partial y}+\\lambda\\frac{\\partial\\varphi}{\\partial y}=0,\\\\\n      \\frac{\\partial F}{\\partial\\lambda}=\\varphi(x,y)=0,\n  \\end{cases}$$\n  所有满足此方程的解$(x,y,\\lambda)$中$(x,y)$是函数$f(x,y)$在条件$\\varphi(x,y)=0$下的可能的极值点.\n- 函数$f(x,y,z)$在条件$\\varphi(x,y,z)=0,\\psi(x,y,z)=0$下的极值的必要条件\n  与上一条情况类似,构造拉格朗日函数\n  $$\\begin{aligned}\n      F(x,y,z,\\lambda,\\mu)=f(x,y,z)+\\lambda\\varphi(x,y,z)+\\mu\\psi(x,y,z),\n  \\end{aligned}$$\n  以下与上一条情况类似(略).\n- 拉格朗日乘子法的几何意义\n    举个2维的例子说明,假设有自变量$x$和$y$,给定约束条件$g(x,y)=c$,要求$f(x,y)$在约束条件$g(x,y)=c$下的极值.\n    我们可以画出$f(x,y)$的等高线图,如图.\n    {% asset_img 2019060417254130.png %}\n    此时,约束$g(x,y)=c$由于只有一个自由度,因此也是图中的一条曲线(红色).显然,当约束曲线$g(x,y)=c$与某一条等高线$f(x,y)=d_1$相切时,函数$f(x,y)$取得极值.两曲线相切等价于两曲线在切点处拥有共线的法向量.因此可得函数$f(x,y)$与$g(x,y)=c$在切点处的梯度成正比$\\lambda$,令$\\lambda>0$.假设在$(x_0,y_0)$处取得极值,即\n    $$\\begin{aligned}\n        \\nabla{f(x_0,y_0)}=-\\lambda\\nabla{g(x_0,y_0)}\\\\\n        f'_x(x_0,y_0)+\\lambda{g'_x(x_0,y_0)}=0\\\\\n        f'_y(x_0,y_0)+\\lambda{g'_y(x_0,y_0)}=0\n    \\end{aligned}$$\n    加上约束条件$g(x_0,y_0)=c$即构成了方程组.\n    问:为什么说相切才是最好的值？\n    解答:因为相切说明g在相切点的左右领域都是\"大值\"(或者\"小值\"),这就满足了极值的定义.如果不是相切,那么就是相交,相交的话,左右领域就存在\"大值\"和\"小值\",这不满足极值的定义.相离说明不满足约束条件,就不需要考虑了.\n### 扩展\n- 不等式约束优化\n  当约束加上不等式之后,情况变得更加复杂,首先来看一个简单的情况,给定如下不等式约束问题\n  $$\\begin{aligned}\n      &\\min_xf(x)\\\\\n      s.t.&\\quad g(x)\\le 0\n  \\end{aligned}$$\n  对应的Lagrangian与图形分别如下所示\n  $$\\begin{aligned}\n      L(x,\\lambda)=f(x)+\\lambda{g(x)}\n  \\end{aligned}$$\n  这时可行解必须落在约束区域$g(x)$之内,下图给出了目标函数的等高线与约束\n  {% asset_img 2019060418075931.png %}\n  由图可见可行解$x$只能在$g(x)<0$或者$g(x)=0$的区域里取得\n  - 当可行解$x$落在$g(x)<0$的区域内,此时直接极小化$f(x)$即可\n  - 当可行解$x$落在$g(x)=0$即边界上,此时等价于等式约束优化问题(用拉格朗日乘子法即可)\n  当约束区域包含目标函数原有的可行解时,此时加上约束,可行解仍落在约束区域内部,对应$g(x)<0$的情况,这时约束条件不起作用;\n  当约束区域不包含目标函数原有的可行解时,此时加上约束,可行解落在边界$g(x)=0$上.\n  下图分别描述了上面两种情况,右图表示加上约束,可行解会落在约束区域的边界上.\n  {% asset_img 2019060418131432.png %}\n  以上两种情况就是说,要么可行解落在约束边界上,即得$g(x)=0$,要么可行解落在约束区域内部,此时约束不起作用,另$\\lambda=0$消去约束即可,所以无论哪种情况都会得到\n  $$\\begin{aligned}\n      \\lambda{g(x)}=0\n  \\end{aligned}$$\n  还有一个问题是$\\lambda$的取值,在等式约束优化中,约束函数与目标函数的梯度只要满足平行即可,而在不等式约束中则不然,若$\\lambda\\ne 0$,这便说明可行解$x$是落在约束区域的边界上的,这时可行解应尽量靠近无约束时的解,所以在约束边界上,目标函数的负梯度方向应该远离约束区域朝向无约束时的解,此时正好可得约束函数的梯度方向与目标函数的负梯度方向应相同\n  $$\\begin{aligned}\n      -\\nabla_xf(x)=\\lambda\\nabla_xg(x)\n  \\end{aligned}$$\n  上式需要满足的要求是拉格朗日乘子$\\lambda>0$,这个问题可以举一个形象的例子,假设你去爬山,目标是山顶,但有一个障碍挡住了通向山顶的路,所以只能沿着障碍爬到尽可能靠近山顶的位置,然后望着山顶叹叹气,这里山顶便是目标函数的可行解,障碍便是约束函数的边界,此时的梯度方向一定是指向山顶的,与障碍的梯度同向,如下图\n  {% asset_img 2019060418214333.png %}\n  可见,`对于不等式约束,只要满足一定的条件,依然可以使用拉格朗日乘子法解决,这里的条件便是KKT条件`.\n  接下来给出形式化的KKT条件,首先给出形式化的不等式约束优化问题\n  $$\\begin{aligned}\n      &\\min_xf(x)\\\\\n      s.t.\\quad &h_i(x)=0,\\quad i=1,2,\\cdots,m\\\\\n      &g_j(x)\\le 0,\\quad j=1,2,\\cdots,n\n  \\end{aligned}$$\n  列出Lagrangian得到无约束优化问题\n  $$\\begin{aligned}\n      L(x,\\alpha,\\beta)=f(x)+\\sum_{i=1}^m{\\alpha_ih_i(x)}+\\sum_{j=1}^n\\beta_jg_j(x)\n  \\end{aligned}$$\n  经过之前的分析,便得知加上不等式约束后,可行解$x$需要满足的就是以下$KKT$条件:\n  $$\\begin{aligned}\n      \\nabla_xL(x,\\alpha,\\beta)&=0\\quad\\quad&(1)\\\\\n      \\beta_jg_j(x)&=0,\\quad j=1,2,\\cdots,n&(2)\\\\\n      h_i(x)&=0,\\quad i=1,2,\\cdots,m&(3)\\\\\n      g_j(x)&\\le 0,\\quad j=1,2,\\cdots,n&(4)\\\\\n      \\beta_j&\\ge 0,\\quad j=1,2,\\cdots,n&(5)\n  \\end{aligned}$$\n  满足KKT条件后极小化Lagrangian即可得到在不等式约束条件下的可行解. KKT条件看起来很多,其实很好理解:\n  (1): 拉格朗日取得可行解的必要条件\n  (2): 称为松弛互补条件\n  (3)~(4): 初始的约束条件\n  (5): 不等式约束的Lagrange Multiplier需要满足的条件\n  主要的KKT条件便是(3)和(5),只要满足这两个条件便可直接使用拉格朗日乘子法\n## 方向导数和梯度\n### 方向导数\n偏导数反映的是函数沿坐标轴方向的`变化率`.但许多物理现象告诉我们,只考虑函数沿坐标轴方向的变化率是不够的.例如,热空气要向冷的地方流动,气象学中就要确定大气温度,气压沿着某些方向的变化率.因此我们有必要来讨论函数沿任一指定方向的变化率问题.\n设$l$是$xOy$平面上以$P_0(x_0,y_0)$为始点的一条射线,$\\mathbf e_l=(\\cos\\alpha,\\cos\\beta)$是与$l$同方向的单位向量(图9-9).\n{% asset_img 2019060410134116.png %}\n射线$l$的参数方程为\n$$\\begin{aligned}\n    x&=x_0+t\\cos\\alpha,(t\\ge 0)\\\\\n    y&=y_0+t\\cos\\beta\n\\end{aligned}$$\n设函数$z=f(x,y)$在点$P_0(x_0,y_0)$的某个邻域$U(P_0)$内有定义,$P(x_0+t\\cos\\alpha,y_0+t\\cos\\beta)$为$l$上另一点,且$P\\in{U(P_0)}$.如果函数增量$f(x_0+t\\cos\\alpha,y_0+t\\cos\\beta)-f(x_0,y_0)$与$P$到$P_0$的距离$|PP_0|=t$的比值\n$$\\begin{aligned}\n    \\frac{f(x_0+t\\cos\\alpha,y_0+t\\cos\\beta)-f(x_0,y_0)}{t}\n\\end{aligned}$$\n当$P$沿着$l$趋于$P_0$(即$t\\to 0^+)$时的极限存在,则称此极限为函数$f(x,y)$在点$P_0$沿方向$l$的`方向导数`,记作$\\frac{\\partial{f}}{\\partial{l}}\\vert_{(x_0,y_0)}$,即\n$$\\begin{aligned}\n    \\frac{\\partial f}{\\partial l}\\vert_{(x_0,y_0)}=\\lim_{t\\to{0^+}}\\frac{f(x_0+t\\cos\\alpha,y_0+t\\cos\\beta)-f(x_0,y_0)}{t}\\tag{1}\n\\end{aligned}$$\n从方向导数的定义可知,方向导数$\\frac{\\partial f}{\\partial l}\\vert_{(x_0,y_0)}$就是函数$f(x,y)$在点$P_0(x_0,y_0)$处沿方向$l$的变化率.若函数$f(x,y)$在点$P_0(x_0,y_0)$的偏导数存在,$\\mathbf e_l=\\mathbf i=(1,0)$,则\n$$\\begin{aligned}\n    \\frac{\\partial f}{\\partial l}\\vert_{(x_0,y_0)}=\\lim_{t\\to{0^+}}\\frac{f(x_0+t,y_0)-f(x_0,y_0)}{t}=f_x(x_0,y_0)\n\\end{aligned}$$\n又若$\\mathbf e_l=\\mathbf j=(0,1)$,则\n$$\\begin{aligned}\n    \\frac{\\partial f}{\\partial l}\\vert_{(x_0,y_0)}=\\lim_{t\\to{0^+}}\\frac{f(x_0,y_0+t)-f(x_0,y_0)}{t}=f_y(x_0,y_0)\n\\end{aligned}$$\n但反之,若$\\mathbf e_l=\\mathbf i,\\frac{\\partial z}{\\partial l}\\vert_{(x_0,y_0)}$存在,则$\\frac{\\partial z}{\\partial x}\\vert_{(x_0,y_0)}$未必存在.例如,$z=\\sqrt{x^2+y^2}$在点$O(0,0)$处沿$l=\\mathbf i$方向的方向导数$\\frac{\\partial z}{\\partial l}\\vert_{(0,0)}=1$,而偏导数$\\frac{\\partial z}{\\partial x}\\vert_{(0,0)}$不存在.==(方向导数存在,偏导数未必存在)==\n关于方向导数的存在及计算,我们有以下定理.\n- <b>定理:如果函数$f(x,y)$在点$P_0(x_0,y_0)$可微分,那么函数在该点沿任一方向$l$的方向导数存在,且有</b>\n  $$\\begin{aligned}\n    \\frac{\\partial f}{\\partial l}\\vert_{(x_0,y_0)}=f_x(x_0,y_0)\\cos\\alpha+f_y(x_0,y_0)\\cos\\beta\\tag{2}\n  \\end{aligned}$$\n<b>其中$\\cos\\alpha,\\cos\\beta$是方向余弦.</b>\n> 证: 由假设,$f(x,y)$在点$(x_0,y_0)$可微分,故有\n> $$\\begin{aligned}\n    &f(x_0+\\Delta{x},y_0+\\Delta{y})-f(x_0,y_0)\\\\\n    =&f_x(x_0,y_0)\\Delta{x}+f_y(x_0,y_0)\\Delta{y}+o(\\sqrt{(\\Delta{x})^2+(\\Delta{y})^2})\n\\end{aligned}$$\n> 但点$(x_0+\\Delta{x},y_0+\\Delta{y})$在以$(x_0,y_0)$为始点的射线$l$上时,应有$\\Delta{x}=t\\cos\\alpha,\\Delta{y}=t\\cos\\beta,\\sqrt{(\\Delta{x})^2+(\\Delta{y})^2}=t$.所以\n> $$\\begin{aligned}\n    &\\lim_{t\\to{0^+}}\\frac{f(x_0+t\\cos\\alpha,y_0+t\\cos\\beta)-f(x_0,y_0)}{t}\\\\\n    =&f_x(x_0,y_0)\\cos\\alpha+f_y(x_0,y_0)\\cos\\beta\n\\end{aligned}$$\n> 这就证明了方向导数的存在,且其值为\n> $$\\begin{aligned}\n    \\frac{\\partial f}{\\partial l}\\vert_{(x_0,y_0)}=f_x(x_0,y_0)\\cos\\alpha+f_y(x_0,y_0)\\cos\\beta\n\\end{aligned}$$\n{% asset_img 2019060411312219.png %}\n{% asset_img 2019060411314420.png %}\n### 梯度\n与方向导数有关联的一个概念是函数的梯度.在二元函数的情形,设函数$f(x,y)$在平面区域$D$内具有一阶连续偏导数,则对于每一点$P_0(x_0,y_0)\\in{D}$,都可定出一个向量\n$$\\begin{aligned}\n    f_x(x_0,y_0)\\mathbf i+f_y(x_0,y_0)\\mathbf j\n\\end{aligned}$$\n这向量称为函数$f(x,y)$在点$P_0(x_0,y_0)$的梯度,记作$\\mathbf{grad}f(x_0,y_0)$或$\\nabla{f(x_0,y_0)}$,即\n$$\\begin{aligned}\n    \\mathbf{grad}f(x_0,y_0)=\\nabla{f(x_0,y_0)}=f_x(x_0,y_0)\\mathbf i+f_y(x_0,y_0)\\mathbf j\n\\end{aligned}$$\n其中$\\nabla=\\frac{\\partial}{\\partial x}\\mathbf i+\\frac{\\partial}{\\partial y}\\mathbf j$称为(二维的)向量微分算子或Nabla算子,$\\nabla{f}=\\frac{\\partial f}{\\partial x}\\mathbf i+\\frac{\\partial f}{\\partial y}\\mathbf j$.\n如果函数$f(x,y)$在点$P_0(x_0,y_0)$可微分,$\\mathbf e_l=(\\cos\\alpha,\\cos\\beta)$是与方向$l$同向的单位向量,则\n$$\\begin{aligned}\n    \\frac{\\partial f}{\\partial l}\\vert_{(x_0,y_0)}&=f_x(x_0,y_0)\\cos\\alpha+f_y(x_0,y_0)\\cos\\beta\\\\\n    &=\\mathbf{grad}f(x_0,y_0)\\cdot\\mathbf e_l=\\vert\\mathbf{grad}f(x_0,y_0)\\vert\\cos\\theta\n\\end{aligned}$$\n其中$\\theta=(\\mathbf{grad}\\widehat{f(x_0,y_0),\\mathbf e_l})$.\n这一关系式表明了函数在一点的梯度与函数在这点的方向导数间的关系.\n特别,由这关系可知:\n(1).当$\\theta=0$,即方向$\\mathbf e_l$与梯度$\\mathbf{grad}f(x_0,y_0)$的方向相同时,函数$f(x,y)$增加最快.此时,函数在这个方向的方向导数达到最大值,这个最大值就是梯度$\\mathbf{grad}f(x_0,y_0)$的模,即\n$$\\begin{aligned}\n    \\frac{\\partial f}{\\partial l}\\vert_{(x_0,y_0)}=\\vert\\mathbf{grad}f(x_0,y_0)\\vert\n\\end{aligned}$$\n这个结果也表示:函数$f(x,y)$在一点的梯度$\\mathbf{grad}f$是这样一个向量,它的方向是函数在这点的方向导数取得最大值的方向,它的模就等于方向导数的最大值.\n(2).当$\\theta=\\pi$,即方向$\\mathbf e_l$与梯度$\\mathbf{grad}f(x_0,y_0)$的方向相反时,函数$f(x,y)$减少最快,函数在这个方向的方向导数达到最小值,即\n$$\\begin{aligned}\n    \\frac{\\partial f}{\\partial l}\\vert_{(x_0,y_0)}=-\\vert\\mathbf{grad}f(x_0,y_0)\\vert\n\\end{aligned}$$\n(3).当$\\theta=\\frac{\\pi}{2}$,即方向$\\mathbf e_l$与梯度$\\mathbf{grad}f(x_0,y_0)$的方向正交时,函数的变化率为零,即\n$$\\begin{aligned}\n    \\frac{\\partial f}{\\partial l}\\vert_{(x_0,y_0)}=\\vert\\mathbf{grad}f(x_0,y_0)\\vert\\cos\\theta=0\n\\end{aligned}$$\n我们知道,一般说来二元函数$z=f(x,y)$在几何上表示一个曲面,这曲面被平面$z=c$(c是常数)所截得的曲线$L$的方程为\n$$\\begin{cases}\n    z=f(x,y)\\\\\n    z=c\n\\end{cases}$$\n这条曲线$L$在$xOy$面上的投影是一条平面曲线$L^*$(图9-10),\n{% asset_img 2019060410451818.png %}\n它在$xOy$平面直角坐标系中的方程为\n$$\\begin{aligned}\n    f(x,y)=c\n\\end{aligned}$$\n对于曲线$L^*$上的一切点,已给函数的函数值都是$c$,所以我们称平面曲线$L^*$为函数$z=f(x,y)$的`等值线`.\n若$f_x,f_y$不同时为零,则等值线$f(x,y)=c$上任一点$P_0(x_0,y_0)$处的一个单位法向量为\n$$\\begin{aligned}\n    \\mathbf n&=\\frac{1}{\\sqrt{f_x^2(x_0,y_0)+f_y^2(x_0,y_0)}}(f_x(x_0,y_0),f_y(x_0,y_0))\\\\\n    &=\\frac{\\nabla f(x_0,y_0)}{\\vert\\nabla{f(x_0,y_0)}\\vert}\n\\end{aligned}$$\n这表明函数$f(x,y)$在一点$(x_0,y_0)$的梯度$\\nabla f(x_0,y_0)$的方向就是等值线$f(x,y)=c$在这点的法线方向$\\mathbf n$,而梯度的模$\\vert\\nabla{f(x_0,y_0)}\\vert$就是沿这个法线方向的方向导数$\\frac{\\partial f}{\\partial n}$,于是有\n$$\\begin{aligned}\n    \\nabla{f(x_0,y_0)}=\\frac{\\partial f}{\\partial n}\\mathbf n\n\\end{aligned}$$\n上面讨论的梯度概念可以类似地推广到三元函数的情形.设函数$f(x,y,z)$在空间区域$G$内具有一阶连续偏导数,则对于每一点$P_0(x_0,y_0,z_0)\\in{G}$,都可定出一个向量\n$$\\begin{aligned}\n    f_x(x_0,y_0,z_0)\\mathbf{i}+f_y(x_0,y_0,z_0)\\mathbf{j}+f_z(x_0,y_0,z_0)\\mathbf{k}\n\\end{aligned}$$\n这向量称为函数$f(x,y,z)$在点$P_0(x_0,y_0,z_0)$的`梯度`,将它记作$\\mathbf{grad}f(x_0,y_0,z_0)$或$\\nabla{f(x_0,y_0,z_0)}$,即\n$$\\begin{aligned}\n    &\\mathbf{grad}f(x_0,y_0,z_0)\\\\\n    =&\\nabla{f(x_0,y_0,z_0)}\\\\\n    =&f_x(x_0,y_0,z_0)\\mathbf{i}+f_y(x_0,y_0,z_0)\\mathbf{j}+f_z(x_0,y_0,z_0)\\mathbf{k}\n\\end{aligned}$$\n其中$\\nabla=\\frac{\\partial}{\\partial x}\\mathbf{i}+\\frac{\\partial}{\\partial y}\\mathbf{j}+\\frac{\\partial}{\\partial z}\\mathbf{k}$称为(三维的)`向量微分算子`或`Nabla算子`,$\\nabla{f=\\frac{\\partial f}{\\partial x}\\mathbf{i}+\\frac{\\partial f}{\\partial y}\\mathbf{j}+\\frac{\\partial f}{\\partial z}\\mathbf{k}}$.\n经过与二元函数的情形完全类似的讨论可知,三元函数$f(x,y,z)$在一点的梯度$\\nabla{f}$是这样一个向量,它的方向是函数$f(x,y,z)$在这点的方向导数取得最大值的方向,它的模就等于方向导数的最大值.\n如果我们引进曲面\n$$\\begin{aligned}\n    f(x,y,z)=c\n\\end{aligned}$$\n为函数$f(x,y,z)$的`等值面`的概念,则可得函数$f(x,y,z)$在一点$(x_0,y_0,z_0)$的梯度$\\nabla{f(x_0,y_0,z_0)}$的方向就是等值面$f(x,y,z)=c$在这点的法线方向$\\mathbf n$,而梯度的模$\\vert\\nabla{f(x_0,y_0,z_0)}\\vert$就是函数沿这个法线方向的方向导数$\\frac{\\partial f}{\\partial n}$.\n{% asset_img 2019060411330421.png %}\n{% asset_img 2019060411333722.png %}\n### 参考\n1. [复习全书](https://book.douban.com/subject/6101482/)\n2. [同济 高等数学 第六版](https://book.douban.com/subject/2195654/)\n3. [如何理解拉格朗日乘子法？ - 卢健龙的回答 - 知乎](https://www.zhihu.com/question/38586401/answer/105273125)\n4. [约束优化方法之拉格朗日乘子法与KKT条件 - ooon - 博客园](https://www.cnblogs.com/ooon/p/5721119.html)\n","tags":["高等数学"],"categories":["高等数学"]},{"title":"[leetcode]37.解数独","url":"%2Fposts%2F9eebc96a%2F","content":"{% asset_img 201906032135188.png %}\n\n### 解题思路(约束法+回溯法) \n\n**约束法**:基本的意思是在放置每个数字时都设置约束。在数独上放置一个数字后立即 排除当前**行,列和子方块**对该数字的使用。这会传播 约束条件 并有利于减少需要考虑组合的个数。\n{% asset_img 201906032135189.png %}\n\n**回溯法**:根据已有的约束数组来判断当前可以放什么元素,然后不断尝试(1~9),如果找到可以放的元素,则将其记录到栈,如果当前(1~9)都不能放,则进行回溯(即倒退),将栈顶元素出栈,在上一步进行新的尝试.\n{% asset_img 201906032135190.png %}\n\n**枚举子方块**:使用`方块索引= (行 / 3) * 3 + 列 / 3 其中 / 表示整数除法`。\n{% asset_img 201906032135191.png %}\n\n### 代码实现 \n```cpp\n// 行约束,列约束,块约束\nint row_map[9][10] = { 0 }, col_map[9][10] = { 0 }, box_map[9][10] = { 0 };\n// 增加约束\nvoid constrain(int i, int j, int v)\n{\n    row_map[i][v]++;\n    col_map[j][v]++;\n    box_map[i / 3 * 3 + j / 3][v]++;\n}\n// 取消约束\nvoid deconstrain(int i, int j, int v)\n{\n    row_map[i][v]--;\n    col_map[j][v]--;\n    box_map[i / 3 * 3 + j / 3][v]--;\n}\n// 计算数独\nvoid solveSudoku(vector<vector<char>>& board) {\n    int m = board.size(), n = board[0].size();\n    stack<pair<int,int>> stk;\n    int i = 0, j = 0;\n    // 通过已知的值,计算已知的约束,避免多做无用功\n    for (i = 0; i < m; ++i)\n    {\n        for (j = 0; j < n; ++j)\n        {\n            if (board[i][j] != '.')\n                constrain(i, j, board[i][j] - '0');\n        }\n    }\n    // 计算未知的值\n    for (i=0,j=0;i < m;)\n    {\n        if (board[i][j] == '.')\n        {\n            board[i][j] = '0';\n            stk.push({ i * 9 + j,board[i][j] });\n            int flag = 0;\n            // 找到下一个可用值,没有的话则回溯\n            while (!stk.empty())\n            {\n                pair<int, int> e = stk.top();\n                stk.pop();\n                i = e.first / 9;\n                j = e.first % 9;\n                int v = e.second-'0';\n                flag = 0;\n                deconstrain(i, j, v);\n                for (v = v + 1; v <= 9; ++v)\n                {\n                    if (row_map[i][v] || col_map[j][v] || box_map[i / 3 * 3 + j / 3][v])\n                        continue;\n                    board[i][j] = v + '0';\n                    constrain(i, j, v);\n                    stk.push({ i * 9 + j,board[i][j] });\n                    flag = 1;\n                    break;\n                }\n                if (flag == 1)\n                    break;\n                board[i][j] = '.';\n            }\n        }\n        j++;\n        i = j / 9 ? i + 1 : i;\n        j = j / 9 ? j % 9 : j;\n    }\n}\n```\n\n### 参考\n1. [力扣（LeetCode）37. 解数独](https://leetcode-cn.com/problems/two-sum/solution/jie-shu-du-by-leetcode/)","tags":["数独"],"categories":["OJ"]},{"title":"[matplotlib]notes","url":"%2Fposts%2Fbbaeb0e0%2F","content":"### Matplotlib\n#### pyplot\n- **figure(num=None, figsize=None, dpi=None, facecolor=None, edgecolor=None, frameon=True, FigureClass=<class 'matplotlib.figure.Figure'>, clear=False, \\*\\*kwargs)**:创建一个figure\n  - **num**:可以选数字(类似于标记)或字符串(作为标题),默认None\n  - **figsize**:(float,float),宽高inches,默认rcParams[\"figure.figsize\"]=[6.4,4.8]\n  - **dpi**:分辨率,默认rcParams[\"figure.dpi\"]=100\n  - **facecolor**:背景色,默认rcParams[\"figure.facecolor\"]='w'\n  - **edgecolor**:边框色,默认rcParams[\"figure.edgecolor\"]='w'\n- **gca(\\*\\*kwargs)**:在当前figure中根据关键字获取或创建坐标轴\n#### RcParams(*args,**kwargs):字典结构,存储rc参数","tags":["notes"],"categories":["matplotlib"]},{"title":"[leetcode]303.区域和检索-数组不可变","url":"%2Fposts%2F6c5b9a98%2F","content":"{% asset_img 201906021606251.png %}\n\n### 解题思路 \n`思路一:DP`\ndp[i]表示从1~i的累加和,那么要求从i~j(包含端点)累加和即为\n$$\\begin{aligned}\n    sum_{i,j}=dp[j]-dp[i-1]\n\\end{aligned}$$\n`思路二:树状数组`\n实际树状数组的优势在于查询和修改都是$O(logn)$,本题只是为了熟悉一下这个数据结构,详细请参考[树状数组彻底入门](https://blog.csdn.net/Small_Orange_glory/article/details/81290634)\n核心就是以下两个公式:\n$$\\begin{aligned}\n    C[i]=A[i-lowbit(i)+1]+A[i-lowbit(i)+2]...+A[i]\\tag{1}\n\\end{aligned}$$\n```cpp\nfor(int i=1;i<=m;++i)\n    for(int j=i-lowbit(i)+1;j<=i;++j)\n        C[j]+=A[j];\n```\n\n$$\\begin{aligned}\n    sum[15]&=sum[1111]=C[1111]+C[1110]+C[1100]+C[1000]\\\\\n    &=C[15]+C[14]+C[12]+C[8]\\tag{2}\n\\end{aligned}$$\n```cpp\nsum=0;\nfor(i=x;i>0;i-=lowbit(i))\n    sum+=C[i];\n```\n\n### 代码实现 \n`思路一:DP`\n```cpp\n    vector<int> dp;\n    NumArray(vector<int>& nums) {\n        int m=nums.size();\n        dp.resize(m,0);\n        for(int i=0;i<m;++i)\n            dp[i]=dp[i-1>=0?i-1:0]+nums[i];\n    }\n    \n    int sumRange(int i, int j) {\n        int x=min(i,j),y=max(i,j);\n        return dp[y]-(x-1>=0?dp[x-1]:0);\n    }\n```\n时间复杂度为$O(n)$\n\n`思路二:树状数组`\n```cpp\n    vector<int> c;\n    NumArray(vector<int>& nums) {//树状数组,查询/更新 o(logn)\n        c = vector<int>(nums.size() + 1,0);\n        for(int i = 1;i <= nums.size();i++)\n            for(int j = i-lowbit(i)+1; j<=i; ++j)\n                c[i]+=nums[j-1];\n    }\n    int lowbit(int x){\n        return x & -x;\n    }\n    int sum(int i){\n        int res = 0;\n        for(; i > 0;i -= lowbit(i)) res += c[i];\n        return res;\n    }\n    int sumRange(int i, int j) {\n        return sum(j + 1) - sum(i);\n    }\n```\n### 参考\n1. [树状数组彻底入门](https://blog.csdn.net/Small_Orange_glory/article/details/81290634)","tags":["树状数组"],"categories":["OJ"]},{"title":"[高等数学]第1章 函数,极限,连续","url":"%2Fposts%2Fa86313a0%2F","content":"### 1.函数\n- 定义1.1.1(邻域):设$\\delta\\gt 0$,实数集$U_\\delta(x_0)=\\{x||x-x_0|<\\delta\\}$称为$x_0$的$\\delta$邻域,如果不必说及邻域半径$\\delta$的大小,则简记为$U(x_0)$,称为$x_0$的某邻域.\n  $\\mathring{U}_\\delta(x_0)=\\{x|0\\lt|x-x_0|\\lt\\delta\\}$称为$x_0$的去心$\\delta$邻域,类似地有记号$\\mathring{U}(x_0)$及相应的名称.\n- 定义1.1.3(隐函数):设$x$在某数集$X$内每取一个值时,由方程$F(x,y)=0$可唯一\n###\n### 参考\n1. [复习全书](https://book.douban.com/subject/6101482/)\n","tags":["高等数学"],"categories":["高等数学"]},{"title":"[概率与统计]第6章 数理统计的基本概念","url":"%2Fposts%2F4eb1bf74%2F","content":"### 一.总体,样本,统计量和样本数字特征\n#### 1.总体\n- 定义:数理统计中所研究对象的某项数量指标$X$的全体称为总体.\n> 注:$X$是一个随机变量,称$X$的概率分布为总体分布,$X$的数字特征为总体数字特征,总体中的每个元素称为个体.\n\n#### 2.样本\n- 定义:如果$X_1,X_2,\\cdots,X_n$相互独立且都与总体$X$同分布,则称$X_1,X_2,\\cdots,X_n$为来自总体的简单随机样本,简称为样本.$n$为样本容量,样本的具体观测值$x_1,x_2,\\cdots,x_n$称为样本值,或称总体$X$的$n$个独立观测值.\n> 注:如果总体$X$的分布为$F(x)$,则样本$X_1,X_2,\\cdots,X_n$的分布为\n> $$\\begin{aligned}\n    F_n(x_1,x_2,\\cdots,x_n)=\\prod_{i=1}^n{F(x_i)}.\n\\end{aligned}$$\n> 如果总体$X$有概率密度$f(x)$,则样本$X_1,X_2,\\cdots,X_n$的概率密度为\n> $$\\begin{aligned}\n    f_n(x_1,x_2,\\cdots,x_n)=\\prod_{i=1}^n{f(x_i)}.\n\\end{aligned}$$\n> 如果总体$X$有概率分布$P\\{X=a_j\\}=p_j,j=1,2,\\cdots,$则样本$X_1,X_2,\\cdots,X_n$的概率分布为\n> $$\\begin{aligned}\n    P\\{X_1=x_1,X_2=x_2,\\cdots,X_n=x_n\\}=\\prod_{i=1}^n{P{X_i=x_i}},\n\\end{aligned}$$\n> 其中$x_i$取$a_1,a_2,\\cdots$中的某一个数.\n#### 3.统计量\n- 定义:样本$X_1,X_2,\\cdots,X_n$的不含未知参数的函数$T=T(X_1,X_2,\\cdots,X_n)$称为统计量.\n  > 注:作为随机样本的函数,统计量本身也是一个随机变量.\n  如果$x_1,x_2,\\cdots,x_n$是样本$X_1,X_2,\\cdots,X_n$的样本值,则数值$T(x_1,x_2,\\cdots,x_n)$为统计量$T(X_1,X_2,\\cdots,X_n)$的观测值.\n#### 4.样本数字特征\n设$X_1,X_2,\\cdots,X_n$是来自总体$X$的样本,则称\n(1) 样本均值$\\overline{X}=\\frac{1}{n}\\sum_{i=1}^n{X_i}$\n(2) 样本方差$S^2=\\frac{1}{n-1}\\sum_{i=1}^n{(X_i-\\overline{X})^2}$,样本标准差$S=\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n{(X_i-\\overline{X})^2}}$\n(3) 样本$k$阶原点矩$A_k=\\frac{1}{n}\\sum_{i=1}^n{X_i^k},k=1,2,A_1=\\overline{X}$\n(4) 样本$k$阶中心矩$B_k=\\frac{1}{n}\\sum_{i=1}^n{(X_i-\\overline{X})^k},k=1,2,B_2=\\frac{n-1}{n}S^2\\neq S^2$\n> 补充:总体期望$E(X)$,总体均值$\\mu$,总体方差$D(X)$,总体标准差$\\sigma$\n\n#### 5.样本数字特征的性质\n(1) 如果总体$X$具有数学期望$E(X)=\\mu,$则\n$$\\begin{aligned}\n    E(\\overline{X})=E(X)=\\mu\n\\end{aligned}$$\n> 证:$E(\\overline{X})=\\mu$\n> $$\\begin{aligned}\n    E(\\overline{X})=E(\\frac{1}{n}\\sum_{i=1}^n{X_i})=\\frac{1}{n}\\sum_{i=1}^nE(X_i)=\\frac{1}{n}\\cdot n\\mu=\\mu\n\\end{aligned}$$\n\n(2) 如果总体$X$具有方差$D(X)=\\sigma^2$,则\n$$\\begin{aligned}\n    D(\\overline{X})&=\\frac{1}{n}D(X)=\\frac{\\sigma^2}{n}\\\\\n    E(S^2)&=D(X)=\\sigma^2\n\\end{aligned}$$\n\n> 证:$D(\\overline{X})=\\frac{\\sigma^2}{n}$\n> $$\\begin{aligned}\n    D(\\overline{X})=D(\\frac{1}{n}\\sum_{i=1}^n{X_i})=\\frac{1}{n^2}\\sum_{i=1}^n{D(X_i)}=\\frac{1}{n^2}\\cdot n\\sigma^2=\\frac{\\sigma^2}{n}\n\\end{aligned}$$\n> 证:$E(S^2)=\\sigma^2$\n> $$\\begin{aligned}\n    E(S^2)&=E\\left[\\frac{1}{n-1}\\sum_{i=1}^n{(X_i-\\overline{X})^2}\\right]=\\frac{1}{n-1}E(\\sum_{i=1}^n{X_i^2-n\\overline{X}^2})\\\\\n    &=\\frac{1}{n-1}\\left[\\sum_{i=1}^n{E(X_i^2)}-nE(\\overline{X}^2)\\right]\\\\\n    &=\\frac{1}{n-1}\\left[\\sum_{i=1}^n(\\sigma^2+\\mu^2)-n(\\frac{\\sigma^2}{n}+\\mu^2)\\right]\\\\\n    &=\\frac{1}{n-1}(n\\sigma^2+n\\mu^2-\\sigma^2-n\\mu^2)=\\sigma^2\n\\end{aligned}$$\n\n(3) 如果总体$X$的$k$阶原点矩$E(X^k)=\\mu_k,k=1,2,\\cdots$存在,则当$n\\to\\infty$时\n$$\\begin{aligned}\n    \\frac{1}{n}\\sum_{i=1}^n{X_i^k}\\xrightarrow{P}\\mu_k,\\ k=1,2,\\cdots\n\\end{aligned}$$\n\n补充:\n(1). $\\sum_{i=1}^n(X_i-\\overline{X})^2=\\sum_{i=1}^n{X_i}^2-n\\overline{X}^2$\n(2). $\\sum_{i=1}^n(X_i-\\mu)^2=\\sum_{i=1}^n(X_i-\\overline{X})^2+n(\\overline{X}-\\mu)^2$\n\n> 证(1):\n> $$\\begin{aligned}\n    \\sum_{i=1}^n(X_i-\\overline{X})^2&=\\sum_{i=1}^n(X_i^2-2X_i\\overline{X}+\\overline{X}^2)=\\sum_{i=1}^nX_i^2-2n\\overline{X}^2+n\\overline{X}^2\\\\\n    &=\\sum_{i=1}^n{X_i^2-n\\overline{X}^2}\n\\end{aligned}$$\n> 证(2): \n> $$\\begin{aligned}\n    \\sum_{i=1}^n(X_i-\\mu)^2&=\\sum_{i=1}^n[(X_i-\\overline{X})+(\\overline{X}-\\mu)]^2\\\\\n    &=\\sum_{i=1}^n(X_i-\\overline{X})^2+2\\sum_{i=1}^n(X_i-\\overline{X})(\\overline{X}-\\mu)+\\sum_{i=1}^n(\\overline{X}-\\mu)^2\\\\\n    &=\\sum_{i=1}^n(X_i-\\overline{X})^2+2\\sum_{i=1}^n(X_i\\overline{X}-\\mu{X_i}-\\overline{X}^2+\\mu\\overline{X})+\\sum_{i=1}^n{(\\overline{X}-\\mu)^2}\\\\\n    &=\\sum_{i=1}^n(X_i-\\overline{X})^2+2(n\\overline{X}^2-n\\mu\\overline{X}-n\\overline{X}^2+n\\mu\\overline{X}^2)+\\sum_{i=1}^n{(\\overline{X}-\\mu)^2}\n\\end{aligned}$$\n\n### 参考\n1. [复习全书](https://book.douban.com/subject/6101482/)\n","tags":["概率与统计"],"categories":["概率与统计"]},{"title":"[概率与统计]第5章 大数定律和中心极限定理","url":"%2Fposts%2F19f6d8b2%2F","content":"\n### 切比雪夫不等式\n- 定义:设随机变量$X$的数学期望$E(X)$和方差$D(X)$存在,则对任意$\\epsilon\\gt 0$,总有\n$$\\begin{aligned}\n    P\\{\\vert X-E(X)\\vert \\ge \\epsilon\\}\\le \\frac{D(X)}{\\epsilon^2}\n\\end{aligned}$$\n\n### <span id='jump-0'>依概率收敛</span>\n- 定义:设$X_1,X_2,\\cdots,X_n,\\cdots$是一个随机变量序列,$A$是一个常数,如果对任意$\\epsilon\\gt 0$,有\n$$\\begin{aligned}\n    \\lim_{n\\to+\\infin}P\\{|X_n-A|\\lt \\epsilon\\}=1,\n\\end{aligned}$$\n则称随机变量序列$X_1,X_2,\\cdots,X_n,\\cdots$依概率收敛于常数$A$,记作$X_n\\xrightarrow{P}A$\n\n### 切比雪夫大数定律\n- 定义:设$X_1,X_2,\\cdots,X_n,\\cdots$为<span class='my-color-b'>两两不相关</span>的随机变量序列,存在常数$C$,使<span class='my-color-b'>$D(X_i)\\le C(i=1,2,\\cdots)$</span>,则对任意$\\epsilon\\gt 0$,有\n$$\\begin{aligned}\n    \\lim_{n\\to\\infin}P=\\left\\{\\left|\\frac{1}{n}\\sum_{i=1}^n{X_i}-\\frac{1}{n}\\sum_{i=1}^n{E(X_i)}\\right|\\lt \\epsilon\\right\\}=1\n\\end{aligned}$$\n\n### 伯努利大数定律\n- 定义:设随机变量$X_n\\sim B(n,p),n=1,2,\\cdots,$则对于任意$\\epsilon\\gt 0$,有\n  $$\\begin{aligned}\n      \\lim_{n\\to +\\infin}P\\left\\{\\left|\\frac{X_n}{n}-p\\right|\\lt\\epsilon\\right\\}=1\n  \\end{aligned}$$\n\n### 辛钦大数定律\n- 定义:设随机变量$X_1,X_2,\\cdots,X_n,\\cdots$<span class='my-color-b'>独立同分布</span>,具有<span class='my-color-b'>数学期望$E(X_i)=\\mu,i=1,2,\\cdots,$</span>则对任意$\\epsilon\\gt 0$有\n$$\\begin{aligned}\n    \\lim_{n\\to +\\infin}P\\left\\{\\left|\\frac{1}{n}\\sum_{i=1}^n{X_i-\\mu}\\right|\\lt\\epsilon\\right\\}=1\n\\end{aligned}$$\n\n### 棣莫弗-拉普拉斯中心极限定理\n- 定义:设随机变量$X_n\\sim B(n,p)(n=1,2,\\cdots)$,则对于任意实数$x$,有\n  $$\\begin{aligned}\n      \\lim_{n\\to +\\infin}P\\left\\{\\frac{X_n-np}{\\sqrt{np(1-p)}}\\le x\\right\\}=\\Phi(x)\n  \\end{aligned}$$\n其中$\\Phi(x)$是标准正太的分布函数\n> 注:定理表面当$n$充分大时,服从$B(n,p)$ 的随机变量$X_n$经标准化后得$\\frac{X_n-np}{\\sqrt{np(1-p)}}$近似服从标准正态分布$N(0,1)$,或者说$X_n$近似地服从$N(np,np(1-p))$\n\n### 列维-林德伯格中心极限定理\n- 定义:设随机变量$X_1,X_2,\\cdots,X_n,\\cdots$独立同分布,具有数学期望与方差,$E(X_n)=\\mu,D(X_n)=\\sigma^2,n=1,2,\\cdots,$则对于任意实数$x$,有\n$$\\begin{aligned}\n    \\lim_{n\\to\\infin}P\\left\\{\\frac{\\sum_{i=1}^n{X_i-n\\mu}}{\\sqrt{n}\\sigma}\\le x\\right\\}=\\Phi(x)\n\\end{aligned}$$\n> 注:定理表明当$n$充分大时$\\sum_{i=1}^n{X_i}$的标准化$\\frac{\\sum_{i=1}^n{X_i-n\\mu}}{\\sqrt{n}\\sigma}$近似服从标准正态分布$N(0,1)$,或者说$\\sum_{i=1}^n{X_i}$近似地服从$N(n\\mu,n\\sigma^2)$\n### 参考\n1. [复习全书](https://book.douban.com/subject/6101482/)\n","tags":["概率与统计"],"categories":["概率与统计"]},{"title":"[概率与统计]第7章 参数估计","url":"%2Fposts%2F19f58936%2F","content":"### 一.点估计\n#### 1.点估计\n- 定义:用样本$X_1,X_2,\\cdots,X_n$构造的统计量$\\hat{\\theta}(X_1,X_2,\\cdots,X_n)$来估计未知参数$\\theta$称为点估计.统计量$\\hat{\\theta}(X_1,X_2,\\cdots,X_n)$称为估计量.\n> 注: 估计量是随机变量,它所取得的观测值$\\hat\\theta(x_1,x_2,\\cdots,x_n)$称为估计值.有时将$\\theta$的估计量和估计值统称为$\\theta$的估计.\n\n#### 2.无偏估计量\n- 定义:设$\\hat\\theta$是$\\theta$的估计量,如果$E(\\hat\\theta)=\\theta$,则称$\\hat\\theta=\\hat\\theta(X_1,X_2,\\cdots,X_n)$是未知参数$\\theta$的无偏估计量.\n> 证1:$\\overline{X}$是$\\mu$的无偏估计量\n> $$\\begin{aligned}\n    E(\\overline{X})=E\\left(\\frac{1}{n}\\sum_{i=1}^n{X_i}\\right)=\\frac{1}{n}\\sum_{i=1}^n{E(X_i)}=\\mu\n\\end{aligned}$$\n> 证2:$S^2$是$\\sigma^2$的无偏估计量\n> $$\\begin{aligned}\n    E(S^2)&=\\frac{1}{n-1}E\\left[\\sum_{i=1}^n(X_i-\\overline{X})^2\\right]=\\frac{1}{n-1}E\\left[\\sum_{i=1}^n(X_i^2-2X_i\\overline{X}+\\overline{X}^2)\\right]\\\\\n    &=\\frac{1}{n-1}E\\left[\\sum_{i=1}^nX_i^2-2\\overline{X}\\sum_{i=1}^n{X_i}+n\\overline{X}^2\\right]=\\frac{1}{n-1}\\left[\\sum_{i=1}^nX_i^2-n\\overline{X}^2\\right]\\\\\n    &=\\frac{1}{n-1}\\left[n(\\sigma^2+\\mu^2)-n[D(\\overline{X})+(E\\overline{X})^2]\\right]=\\frac{1}{n-1}\\left[(\\sigma^2+\\mu^2)-\\left(\\frac{\\sigma^2}{n}+\\mu^2\\right)\\right]\\\\\n    &=\\sigma^2\n\\end{aligned}$$\n\n#### 3.更有效估计量\n- 定义:设$\\hat\\theta_1$和$\\hat\\theta_2$都是$\\theta$的无偏估计量,且$D(\\hat\\theta_1)\\le D(\\hat\\theta_2)$,则称$\\hat\\theta_1$比$\\hat\\theta_2$更有效,或$\\hat\\theta_1$比$\\hat\\theta_2$更有效估计量.\n\n#### 4.一致估计量\n- 定义:设$\\hat\\theta(X_1,X_2,\\cdots,X_n)$是$\\theta$的估计值,如果$\\hat\\theta$[依概率收敛](../19f6d8b2/#jump-0)于$\\theta$,则称$\\hat\\theta(X_1,X_2,\\cdots,X_n)$为$\\theta$的一致估计量.\n\n### 二.估计量的求法和区间估计\n#### 1.矩估计法\n- 定义:用样本估计相应的总体矩,用样本矩的函数估计总体矩相应的函数,然后求出要估计的参数,称这种估计法为矩估计法.\n#### 2.矩估计法步骤\n- 设总体X的分布含有未知参数$\\theta_1,\\theta_2,\\cdots,\\theta_k,\\alpha_l=E(X^l)$存在,显然它是$\\theta_1,\\theta_2,\\cdots,\\theta_k$的函数,记作$\\alpha_l(\\theta_1,\\theta_2,\\cdots,\\theta_k),l=1,2,\\cdots,k$.样本的$l$阶原点矩为$A_l=\\frac{1}{n}\\sum_{i=1}^n{X_i^l}$.令\n$$\\begin{aligned}\n    \\alpha_l(\\theta_1,\\theta_2,\\cdots,\\theta_k)=A_l,l=1,2,\\cdots,k.\n\\end{aligned}$$\n从这$k$个方程组中,可以解得$\\theta_1,\\theta_2,\\cdots,\\theta_k$.\n矩估计法不需要知道总体的具体分布数学形式,只要知道各阶矩存在.\n如果不用原点矩,而用中心矩也可以求解:用样本中心矩等于总体中心矩来建立方程组.\n求$k$个参数的估计一般就列出一阶矩到$k$阶矩的方程.\n设$g(\\alpha_1,\\alpha_2)$是一阶矩$\\alpha_1$和二阶矩$\\alpha_2$的函数,而$\\hat\\alpha_1$和$\\hat\\alpha_2$分别为$\\alpha_1$和$\\alpha_2$的矩估计,则$g(\\hat\\alpha_1,\\hat\\alpha_2)$的矩估计.\n#### 3.最大似然估计法\n设$X_1,X_2,\\cdots,X_n$是来自总体$X$的样本,$x_1,x_2,\\cdots,x_n$是样本值,$\\theta$是待估参数.\n- **似然函数**\n    定义:对于离散型总体$X$,设其概率分布为$P{X=a_i}=p(a_i;\\theta),i=1,2,\\cdots,$称函数\n    $$\\begin{aligned}\n        L(\\theta)=L(X_1,X_2,\\cdots,X_n;\\theta)=\\prod_{i=1}^n{p(X_i;\\theta)}\n    \\end{aligned}$$\n    为参数$\\theta$的似然函数.\n    对于连续型总体$X$,概率密度为$f(x;\\theta)$,则称函数\n    $$\\begin{aligned}\n        L(\\theta)=L(X_1,X_2,\\cdots,X_n;\\theta)=\\prod_{i=1}^n{f(X_i;\\theta)}\n    \\end{aligned}$$\n    为参数$\\theta$的似然函数.\n- **最大似然估计法**\n    定义:对于给定的样本值$(x_1,x_2,\\cdots,x_n)$,使似然函数$L(x_1,x_2,\\cdots,x_n;\\theta)$达到最大值的参数值$\\hat\\theta=\\hat\\theta(x_1,x_2,\\cdots,x_n)$称为未知参数$\\theta$的最大似然估计值,相应的使似然函数$L(X_1,X_2,\\cdots,X_n;\\theta)$达到最大值的参数值$\\hat\\theta=\\hat\\theta(X_1,X_2,\\cdots,X_n)$称为$\\theta$的最大似然估计量.一般统称为$\\theta$的最大似然估计.称这种估计法为最大似然估计法.\n- **最大似然估计法步骤**\n    如果$L(\\theta)$或$lnL(\\theta)$关于$\\theta$可微,值$\\hat\\theta$往往可以从方程\n    $$\\begin{aligned}\n        \\frac{dL(\\theta)}{d\\theta}=0\\ or\\ \\frac{dln(\\theta)}{d\\theta}=0\n    \\end{aligned}$$\n    中求解,称这两个方程为似然方程.\n    如果要估计的参数是两个$\\theta_1$和$\\theta_2$,则得似然方程组\n    $$\\begin{cases}\n        \\displaystyle\\frac{\\partial L(\\theta)}{\\partial \\theta_1}=0,\\\\\n        \\displaystyle\\frac{\\partial L(\\theta)}{\\partial \\theta_2}=0\n    \\end{cases}\n    or\n    \\begin{cases}\n        \\displaystyle\\frac{\\partial \\ln L(\\theta)}{\\partial \\theta_1}=0,\\\\        \n        \\displaystyle\\frac{\\partial \\ln L(\\theta)}{\\partial \\theta_2}=0      \n    \\end{cases}$$\n    解这两个方程组,可以得到$\\hat\\theta_1$和$\\hat\\theta_2$.\n    有时,使$L(\\theta)$或$\\ln L(\\theta)$达到最大值的$\\hat\\theta$不一定是$L(\\theta)$或$\\ln L(\\theta)$驻点,这时不能用似然方程来求解,应采用其他方法求最大似然估计.\n\n### 参考\n1. [复习全书](https://book.douban.com/subject/6101482/)\n","tags":["概率与统计"],"categories":["概率与统计"]},{"title":"[leetcode]221.最大正方形","url":"%2Fposts%2Feefa3a9f%2F","content":"{% asset_img 2019053113453649.png %}\n\n### 解题思路 \n首先当然是对图形进行处理,将其看作是柱状图,如何看呢?\n即,这样看类似于`自顶向下`,先房顶,再房身,最后看地基\ni=0\n`1 0 1 0 0`\ni=1\n`1 0 1 0 0`\n`2 0 2 1 1`\ni=2\n`1 0 1 0 0`\n`2 0 2 1 1`\n`3 1 3 2 2`\ni=3\n`1 0 1 0 0`\n`2 0 2 1 1`\n`3 1 3 2 2`\n`4 0 0 3 0`\n\n`思路一(单调栈)`: 跟之前[[leetcode]84.柱状图中最大的矩形](../f814d224)思路一样,只不过在选最大矩形的时候变成了最大正方形\n`思路二(DP)`: 把dp[i][j]看作是以(i,j)为结尾的最大正方形,那么需要确定上边长,与左边长,取较小者\nmin(dp[i-1][j],dp[i][j-1])\n然后再与dp[i-1][j-1]比较,取最小,那么最后的式子为\n$$\\begin{aligned}\n    dp[i][j]=min(dp[i-1][j-1],min(dp[i-1][j],dp[i][j-1]))    \n\\end{aligned}$$\n\n### 代码实现 \n`思路一(单调栈)`:\n```cpp\n// 求出直方图中最大矩形\nint maxSquare(vector<int>& heights)\n{\n    heights.push_back(0);\n    stack<int> stk;\n    int len=heights.size(),maxArea=0;\n    for(int i=0;i<len;++i)\n    {\n        while(!stk.empty()&&heights[i]<heights[stk.top()])\n        {\n            int h=heights[stk.top()];\n            stk.pop();\n            int l=(stk.empty()?0:stk.top()+1),r=i;\n            h=min(h,r-l);\n            maxArea=max(maxArea,h*h);\n        }\n        stk.push(i);\n    }\n    return maxArea;\n}\n// 所有直方图中最大矩形\nint maximalSquare(vector<vector<char>>& matrix) {\n    if (matrix.empty()) return 0;\n    int m=matrix.size(),n=matrix[0].size();\n    vector<vector<int>> M(m,vector<int>(n,0));\n    for(int i=0;i<m;++i)\n    {\n        for(int j=0;j<n;++j)\n        {\n            if (i==0)\n                M[i][j]=matrix[i][j]-'0';\n            else if (matrix[i][j]!='0')\n                M[i][j]=M[i-1][j]+matrix[i][j]-'0';\n        }\n    }\n    int maxArea=0;\n    for(int i=0;i<m;++i)\n        maxArea=max(maxArea,maxSquare(M[i]));\n    return maxArea;\n}\n```\n\n`思路二(DP)`:\n```cpp\nint maximalSquare(vector<vector<char>>& matrix) {\n    if (matrix.empty()) return 0;\n    int m=matrix.size(),n=matrix[0].size(),maxL=0;\n    vector<vector<int>> dp(m,vector<int>(n,0));\n    for(int i=0;i<m;++i)\n    {\n        for(int j=0;j<n;++j)\n        {\n            if (i==0||j==0)\n                dp[i][j]=matrix[i][j]-'0';\n            else if (matrix[i][j]!='0')\n                dp[i][j]=min(dp[i-1][j-1],min(dp[i-1][j],dp[i][j-1]))+1;\n            maxL=max(maxL,dp[i][j]);\n        }\n    }\n    return maxL*maxL;\n}\n```","tags":["单调栈"],"categories":["OJ"]},{"title":"[life]notes","url":"%2Fposts%2Fb320b84b%2F","content":"### 学习\n- 先看视频,再看书更有效率\n- 心不要急,稳扎稳打,书读百遍,其义自见\n- 看纸质书比电子书效率高,且不伤眼睛\n### 当前\n- [ ] 支持向量机\n- [ ] EM算法\n- [ ] 图聚类算法\n- [ ] 流形学习\n- [ ] 稀疏分解算法\n- [ ] 矩阵低秩分解算法\n### 计划(一年)\n- 6月\n  - [ ] 机器学习实战(第三部分)\n  - [ ] 矩阵分析与应用\n  - [ ] 矩阵低秩分解\n- 7月\n  - [ ] 深度学习(迁移学习 行人再识别)\n  - [ ] PyTorch\n  - [ ] 矩阵分析与应用\n- 业余时间\n  - [ ] 复习总结\n  - [ ] 博客完善\n  - [ ] 凸优化\n### 了解一下\n- 概率图模型，近似推断，采样方法，自回归模型，高斯过程，强化学习\n### 参考","tags":["life"],"categories":["life"]},{"title":"[book]The Matrix Cookbook","url":"%2Fposts%2F3c112665%2F","content":"> 书籍[下载](matrixcookbook.pdf)\n\n\n# 标记与命名法\n|              标记              |                                   命名                                   |\n| :----------------------------: | :----------------------------------------------------------------------: |\n|         $\\mathbf A^+$          |                            矩阵$A$的伪逆矩阵                             |\n|    $\\mathbf{A}^\\frac{1}{2}$    |                          $A$的平方根(非元素级)                           |\n| $(\\mathbf A)_{ij}$或$A_{i,j}$  |                          矩阵$A$的第$(i,j)$元素                          |\n|       $[\\mathbf A]_{ij}$       |                         删除$i$行,$j$列后的矩阵A                         |\n|          $\\mathbf a$           |                                  列向量                                  |\n|           $\\real_z$            |                                 标量实部                                 |\n|       $\\real_\\mathbf z$        |                                 向量实部                                 |\n|       $\\real_\\mathbf Z$        |                                 矩阵实部                                 |\n|           $\\image_z$           |                                 标量虚部                                 |\n|       $\\image_\\mathbf z$       |                                 向量虚部                                 |\n|       $\\image_\\mathbf Z$       |                                 矩阵虚部                                 |\n|   $\\mathrm{det}(\\mathbf A)$    |                              $A$的行列式值                               |\n|    $\\mathrm{Tr}(\\mathbf A)$    |                                 $A$的迹                                  |\n|   $\\mathrm{diag}(\\mathbf A)$   | 矩阵$A$的对角阵,比如:$(\\mathrm{diag}(\\mathbf A))_{ij}=\\delta_{ij}A_{ij}$ |\n|   $\\mathrm{eig}(\\mathbf A)$    |                             矩阵$A$的特征值                              |\n|   $\\mathrm{vec}(\\mathbf A)$    |                             矩阵$A$的向量版                              |\n|         $\\mathrm{sup}$         |                                集合上确界                                |\n|     $\\Vert \\mathbf A\\Vert$     |                                矩阵的norm                                |\n|        $\\mathbf A^\\ast$        |                                复共轭矩阵                                |\n|         $\\mathbf A^H$          |                       转置和复共轭矩阵(Hermitian)                        |\n|   $\\mathbf A\\circ \\mathbf B$   |                                元素级乘积                                |\n| $\\mathbf A\\bigotimes\\mathbf B$ |                                  张量积                                  |\n|          $\\mathbf 0$           |                                  空矩阵                                  |\n|          $\\mathbf I$           |                                  单位阵                                  |\n|        $\\mathbf J^{ij}$        |                    单元素矩阵,$(i,j)$位置为1,其余全0                     |\n|        $\\mathbf\\Sigma$         |                                 正定矩阵                                 |\n|        $\\mathbf\\Lambda$        |                                 对角矩阵                                 |\n\n# 1.基础\n{% asset_img 2019053020463546.png %}\n## 1.1迹\n{% asset_img 2019053020472847.png %}\n## 1.2行列式\n$A$为$n\\times n$大小的矩阵\n$$\n\\begin{aligned}\n    \\mathrm{det}(\\mathbf{I+uv}^T)=1+\\mathbf u^T\\mathbf v\n\\end{aligned}\n$$\n当$n=2$时:\n$$\\begin{aligned}\n    \\mathrm{det}\\mathbf{(I+A)}=1+\\mathrm{det}(\\mathbf A)+\\mathrm{Tr}(\\mathbf A)\n\\end{aligned}\n$$\n当$n=3$时:\n$$\\begin{aligned}\n    \\mathrm{det}(\\mathbf{I+A})=1+\\mathrm{det(\\mathbf A)}+\\mathrm{Tr(\\mathbf A)}+\\frac{1}{2}\\mathrm{Tr(\\mathbf A)^2}-\\frac{1}{2}\\mathrm{Tr(\\mathbf A^2)}\n\\end{aligned}\n$$\n当$n=4$时:\n$$\\begin{aligned}\n    \\mathrm{det(\\mathbf{I+A})}=&1+\\mathrm{det(\\mathbf A)}+\\mathrm{Tr(\\mathbf A)}+\\frac{1}{2}\\\\\n    &+\\mathrm{Tr(\\mathbf A)^2}-\\frac{1}{2}\\mathrm{Tr(\\mathbf A^2)}\\\\\n    &+\\frac{1}{6}\\mathrm{Tr(\\mathbf A)^3}-\\frac{1}{2}\\mathrm{Tr(\\mathbf A)}\\mathrm{Tr(\\mathbf A^2)}+\\frac{1}{3}\\mathrm{Tr(\\mathbf A^3)}\n\\end{aligned}\n$$\n##1.3 2x2的特例\n{%asset_img 2019053021131548.png%}\n\n### 10.2.2 向量操作符\n$\\mathrm{vec}(A)$将矩阵$A$按列堆叠成向量\n{% asset_img 2019053020081545.png %}\n\n### 参考","tags":["The Matrix Cookbook"],"categories":["book"]},{"title":"[机器学习实战]第6章 支持向量机(SVM)","url":"%2Fposts%2F80b6a5fe%2F","content":"{% asset_img 2019053016131743.png %}\n### 了解\n\n### 点导超平面S的距离\n输入空间中任意$x_0$到超平面$S$的距离为:\n$$\\begin{aligned}\n    d=\\frac{|\\mathbf {w\\cdot x_0}+b|}{||\\mathbf{w}||}\n\\end{aligned}\n$$\n$\\mathbf {w,x_0,x}$都为$N$维向量,点代表内积,$\\mathbf {w\\cdot x_0}$为一个数\n推导: 点$\\mathbf x_0$到超平面$S: \\mathbf{w\\cdot x}+b=0$距离$d$的计算过程如下:\n1. 设点$x_0$在平面$S$上的投影为$x_1$,则$\\mathbf{w\\cdot x_1}+b=0$\n2. 由于向量$\\overrightarrow{\\mathbf{x_0x_1}}$与$S$平面的法向量$\\mathbf w$平行,所以\n$$\\begin{aligned}\n    |\\mathbf{w\\cdot\\overrightarrow{x_0x_1}}|=|\\mathbf w|\\cdot|\\overrightarrow{\\mathbf{x_0x_1}}|=\\sqrt{(w^1)^2+\\cdots+(w^N)^2}\\cdot d=||\\mathbf w||\\cdot d\n\\end{aligned}\n$$\n又\n$$\\begin{aligned}  \n\\mathbf w\\cdot \\overrightarrow{\\mathbf{x_0x_1}}&=w^1(x_0^1-x_1^1)+w^2(x_0^2-x_1^2)+\\cdots+w^N(x_0^N-x_1^N)\\\\\n&=w^1x_0^1+w^2x_0^2+\\cdots+w^Nx_0^N-(w^1x_1^1+w^2x_1^2+\\cdots+w^Nx_1^N)\\\\\n&=w^1x_0^1+w^2x_0^2+\\cdots+w^Nx_0^N-(-b)\\\\\n&=|\\mathbf{w\\cdot x_0}+b|\n\\end{aligned}\n$$\n因此$||\\mathbf w||\\cdot d=|\\mathbf{w\\cdot x_0}+b|$,推出$d=\\frac{\\displaystyle|\\mathbf {w\\cdot x_0}+b|}{\\displaystyle||\\mathbf{w}||}$\n### 相关\n$$\\begin{aligned}\n    u^Tv=p\\cdot\\Vert{u}\\Vert\n\\end{aligned}$$\nSVM有三宝,间隔对偶和核技巧\nSVM分为:\n- hard-margin svm\n- soft-margin svm\n- kernel svm\n判别模型\n$$\\begin{aligned}\n    f(w)=sign(w^Tx+b)\n\\end{aligned}$$\n最大间隔分类器\n$$\\begin{aligned}\n    \\max margin(w,b)\\\\\n    s.t. y_i\n\\end{aligned}$$\n### 参考","tags":["机器学习实战"],"categories":["机器学习"]},{"title":"[leetcode]84.柱状图中最大的矩形","url":"%2Fposts%2Ff814d224%2F","content":"{% asset_img 2019053014352639.png %}\n\n### 了解\n单调栈(Monotone Stack)是一种特殊的栈,特殊之处在于栈内的元素都保持一个单调性,可能`单调递减`(如:5 4 4 3 2 1),也可能`单调递增`(如: 1 2 2 3 4)\n\n### 解题思路 \n单调栈用来存放柱子下标,只保存高度是`单调递增`的柱子,对于每一个柱子,我们都求出以该柱子为高的矩形最大面积\n- 能完全覆盖第0个柱子的最大矩形\n    {% asset_img 20190328100531585.png %}\n- 能完全覆盖第1个柱子的最大矩形\n    {% asset_img 20190328100617165.png %}\n- 能完全覆盖第2个柱子的最大矩形\n    {% asset_img 20190328100648369.png %}\n- 能完全覆盖第3个柱子的最大矩形\n    {% asset_img 20190328100625809.png %}\n- 能完全覆盖第4个柱子的最大矩形\n    {% asset_img 20190328100655726.png %}\n- 能完全覆盖第5个柱子的最大矩形\n    {% asset_img 20190328100702406.png %}\n首先,我们将其看成下图形式\n{% asset_img 2019053015105342.png %}\n柱子中间数字表示编号,下面数字即为边界,每一个柱子都有左边下标和右边下标,例如第0个柱子,左边为0,右边为1\n<span class='my-color-b'>注意：基于各个高度的最大矩形是在出栈的时候计算的，因此必须要让所有高度都出栈。这里是利用单调栈的性质让其全部出栈，即在原始数组后添一个0.</span>\n那么,为了确定每个柱子为高的最大矩形面积,我们就需要找到最近的与之相邻的比它小的柱子(即确定左边界和右边界)\n如:\n第2根柱子,右边界即4,左边界即2,那么该柱子确定的矩形面积为5*(4-2)=10\n第3根柱子,右边界即4,左边界即3,那么该柱子确定的矩形面积为6*(4-3)=6\n- 确定左边界\n    该柱子左边第一个比当前柱子高度小的柱子,它的右下标\n    栈顶元素作为要确定的柱子,因此左边比它高度的小柱子必然是该元素抛出后的下一个栈顶元素的右边下标,就确定了左边界\n- 确定右边界\n    该柱子右边第一个比当前柱子高度小的柱子,它的左下标\n    即单调栈的栈顶必然为当前最高的柱子,只要遍历时发现有比栈顶高度小的柱子的左边下标,就确定了右边界\n### 代码实现 \n```cpp\nint largestRectangleArea(vector<int>& heights)\n{\n    heights.push_back(0);\n    stack<int> stk;\n    int maxArea=0,len=heights.size();\n    for(int i=0;i<len;++i)\n    {\n        while(!stk.empty()&&heights[i]<heights[stk.top()])\n        {\n            int h=heights[stk.top()]; // 要确定的柱子的高度\n            stk.pop();  // 为了得到下一个栈顶元素\n            int l=(stk.empty()?0:stk.top()+1),r=i; // 如果栈空,就说明左边元素都比它大,则左边界为0\n            maxArea=max(maxArea,h*(r-l)); // 选最大的矩形面积\n        }\n        stk.push(i);\n    }\n    return maxArea;\n}\n```\n时间复杂度$O(n)$\n### 参考\n1. [](https://blog.csdn.net/Zolewit/article/details/88863970)","tags":["单调栈"],"categories":["OJ"]},{"title":"[leetcode]31.下一个排列","url":"%2Fposts%2F428400a2%2F","content":"{% asset_img 2019052912303931.png %}\n\n### 解题思路 \n可以看到,`只要不是单调递减的序列,下一个排列的数一定是比当前数大的`,但`同时也应当是比当前数大的所有数当中最小的`\n因此,可分为两种情况:\n- **不单调**: 则找到下一个比它大的数,且这个数是所有排列中比当前数大的最小的一个\n    例如: 找`1 5 8 4 7 6 5 3 1`的下一个排列\n    方法是: 低位到高位找到第一个不是单调递减的数,这里是`4`,`4`后面的数都满足单调递减,则从后面单调递减的数中找到一个比4大的最小的数进行交换,然后把`4`后的数进行逆序即可\n    如果不理解,我们不妨这样看,单独看`4 7 6 5 3 1`,它后面一个排列应该是什么呢?由于`7 6 5 3 1`是单调递减,已经达到了该排列的最大值,因此`4 7 6 5 3 1`中只能更换`4`的值,当然是比`4`大的数中尽可能的小的数,那么就是`5`,交换后是这样`5 7 6 4 3 1`(交换后必然还是后面的数单调递减(即是以`5`开头的最大的排列)),这样就将后面的数逆序,即可保障是`5`开头的最小的一个排列`5 1 3 4 6 7`\n- **单调递减**: 即所有排列中最大的数,那么它的下一个排列必然是所有排列中最小的数,那么只需逆序即可\n    例如: `7 6 5 4 3 2 1`逆序为`1 2 3 4 5 6 7`\n### 代码实现 \n\n```cpp\n    void nextPermutation(vector<int>& nums) {\n        if (nums.size()<=1) return ;\n        int i=nums.size()-2,j=nums.size()-1;\n        for(;i>=0&&nums[i]>=nums[i+1];--i);\n        if (i>=0) // i<0,则说明为单调递减,i>=0则不单调\n        {\n            for(;j>=0&&nums[i]>=nums[j];--j);\n            swap(nums[i],nums[j]);\n        }\n        reverse(nums.begin()+i+1,nums.end());\n    }\n```\n算法时间复杂度$O(n)$","tags":["排列"],"categories":["OJ"]},{"title":"[python]notes","url":"%2Fposts%2Ffd60ea9c%2F","content":"### Python3\n#### 循环\n{% asset_img 2019052911252030.png %}","tags":["python"],"categories":["python"]},{"title":"[leetcode]32.最长有效括号","url":"%2Fposts%2Ff8bebee9%2F","content":"{% asset_img 201905261449501.png %}\n\n### 题目含义\n\n### 解题思路\nDP思想:\n初始dp数组为0,dp[i]表示以下标为i的字符结尾的最长有效字符串的长度.\n那么很明显,有效字符串一定以')'结尾,进一步得出以'('结尾的字符串对应的dp数组位置上都为0,则\n1. $s[i]=')'$且$s[i-1]='('$,即形如$\"...()\"$,可以推出:\n    $$\\begin{aligned}\n        i\\lt 2&\\;\\Rightarrow\\;dp[i]=2\\\\\n        i\\ge 2&\\;\\Rightarrow\\;dp[i]=dp[i-2]+2\n    \\end{aligned}\n    $$\n    即在上一个有效字符串长度之上增加了2(增加的是最后一个()长度)\n2. $s[i]=')'$且$s[i-1]=')'$,即形如$\"...))\"$,可以推出:\n    $$\\begin{aligned}\n        i-dp[i-1]-2<0&\\;\\Rightarrow\\;dp[i]=dp[i-1]+2\\\\\n        i-dp[i-1]-2\\ge 0&\\;\\Rightarrow\\;dp[i]=dp[i-1]+2+dp[i-dp[i-1]-2]\n    \\end{aligned}\n    $$\n    这个式子的推导可以用下图表示\n    $$\\begin{aligned}\n    \\underbrace{(\\ )\\ (\\ )}_{dp[i-dp[i-1]-2]}\\ (\\ \\underbrace{(\\ (\\ )\\ (\\ )\\ )}_{dp[i-1]}\\underbrace{\\ )}_i\n    \\end{aligned}\n    $$\n    可以这样理解,$\"()()()\"$表示同级括号(1级),则最长为6,$\"((()))\"$表示不同级括号,1级为最外层,2级往内一层,3级最里层\n    那么,求dp[i]也就是找以i结尾的有效1级括号的长度,那么我们可以先得到内部一级也就是dp[i-1]的长度,然后加2就为当前以i结尾的一个括号的长度,然而还得求所有有效的同级括号,因此找到前面也就是$\"\\underbrace{(\\cdots)(\\cdots)}_{many\\ brackets}\\underbrace{(\\cdots)}_{final\\ bracket}\"$,即最后那个括号的前面所有同级括号长度$dp[i-dp[i-1]-2]$,由于要找最长的有效括号长度,因此需要不断更新最大值\n### 代码实现 \n- **思路清晰版**\n    ```cpp\n    int longestValidParentheses(string s) {\n        if (s.empty()) return 0;\n        vector<int> dp(s.size(),0);\n        int maxn=0;\n        for(int i=1;i<s.size();++i)\n        {\n            if (s[i]==')')\n            {\n                if (s[i-1]=='(')    \\\\ 形如\"...()\"\n                {\n                    if (i>=2)   \\\\ 形如\"...()\"\n                        dp[i]=dp[i-2]+2;\n                    else    \\\\ \"()\"\n                        dp[i]=2;\n                }\n                else if(i-dp[i-1]-1>=0&&s[i-dp[i-1]-1]=='(')    \\\\ 形如\"...))\"\n                {\n                    if (i-dp[i-1]-2>=0) \\\\ 形如\"...(...)((...))\",实际等价于\"...(...)(...)\"\n                        dp[i]=dp[i-1]+2+dp[i-dp[i-1]-2];\n                    else    \\\\ 形如\"((...))\"\n                        dp[i]=dp[i-1]+2;\n                }\n            }\n            maxn=max(maxn,dp[i]);   \\\\ 求出最长有效括号长度\n        }\n        return maxn;\n    }\n    ```\n    算法时间复杂度$O(n)$\n- **精简版**\n    ```cpp\n    int longestValidParentheses(string s) {\n        if (s.empty()) return 0;\n        vector<int> dp(s.size()+1,0);\n        int maxn=0;\n        for(int i=1;i<s.size();++i)\n        {\n            if (s[i]==')')\n            {\n                if (s[i-1]=='(')\n                    dp[i]=(i>=2?dp[i-2]:0)+2;\n                else if(i-dp[i-1]>=1&&s[i-dp[i-1]-1]=='(')\n                    dp[i]=dp[i-1]+2+dp[i-dp[i-1]>=2?i-dp[i-1]-2:0];\n                maxn=max(maxn,dp[i]);\n            }\n        }\n        return maxn;\n    }\n    ```\n    算法时间复杂度$O(n)$","tags":["dp"],"categories":["OJ"]},{"title":"[mathjax]MathJax符号整理","url":"%2Fposts%2F682ccd58%2F","content":"### 字母对应表\n|         文本         |              显示               |    文本     |      显示       |     文本     |        显示         |      文本       |        显示         |\n| :------------------: | :-----------------------------: | :---------: | :-------------: | :----------: | :-----------------: | :-------------: | :-----------------: |\n|        \\alpha        |         $\\alpha,\\Alpha$         |    \\beta    |  $\\beta,\\Beta$  |     \\nu      |      $\\nu,\\Nu$      |       \\xi       |      $\\xi,\\Xi$      |\n|        \\gamma        |         $\\gamma,\\Gamma$         |   \\delta    | $\\delta,\\Delta$ |      o       |        $o,O$        |       \\pi       |      $\\pi,\\Pi$      |\n| \\epsilon,\\varepsilon | $\\epsilon,\\Epsilon,\\varepsilon$ |    \\zeta    |  $\\zeta,\\Zeta$  |     \\rho     |     $\\rho,\\Rho$     |     \\sigma      |   $\\sigma,\\Sigma$   |\n|         \\eta         |           $\\eta,\\Eta$           |   \\theta    | $\\theta,\\Theta$ |     \\tau     |     $\\tau,\\Tau$     |    \\upsilon     | $\\upsilon,\\Upsilon$ |\n|        \\iota         |          $\\iota,\\Iota$          |   \\kappa    | $\\kappa,\\Kappa$ | \\phi,\\varphi | $\\phi,\\Phi,\\varphi$ |      \\chi       |     $\\chi,\\Chi$     |\n|       \\lambda        |        $\\lambda,\\Lambda$        |     \\mu     |    $\\mu,\\Mu$    |     \\psi     |     $\\psi,\\Psi$     |     \\omega      |   $\\omega,\\Omega$   |\n|        \\nabla        |            $\\nabla$             |             |                 |              |                     |                 |                     |\n|         \\pm          |              $\\pm$              |   \\times    |    $\\times$     |     \\div     |       $\\div$        |      \\mid       |       $\\mid$        |\n|        \\nmid         |             $\\nmid$             |    \\cdot    |     $\\cdot$     |    \\circ     |       $\\circ$       |      \\ast       |       $\\ast$        |\n|       \\bigodot       |           $\\bigodot$            | \\bigotimes  |  $\\bigotimes$   |  \\bigoplus   |     $\\bigoplus$     |      \\leq       |       $\\leq$        |\n|        \\vert         |          $\\vert,\\Vert$          |    \\sim     |     $\\sim$      | \\overline{X} |   $\\overline{X}$    | \\xrightarrow{P} |  $\\xrightarrow{P}$  |\n|      \\mathring       |         $\\mathring{U}$          | \\lang,\\rang |  $\\lang,\\rang$  |    \\Lrarr    |      $\\Lrarr$       |                 |                     |\n|                      |                                 |             |                 |              |                     |                 |                     |\n|                      |                                 |             |                 |              |                     |                 |                     |\n|                      |                                 |             |                 |              |                     |                 |                     |\n|                      |                                 |             |                 |              |                     |                 |                     |\n|                      |                                 |             |                 |              |                     |                 |                     |\n|                      |                                 |             |                 |              |                     |                 |                     |\n|                      |                                 |             |                 |              |                     |                 |                     |\n|                      |                                 |             |                 |              |                     |                 |                     |\n|                      |                                 |             |                 |              |                     |                 |                     |\n\n### 参考\n1. [MathJax basic tutorial and quick reference](MathJax basic tutorial and quick reference - Mathematics Meta Stack Exchange.html)\n2. [mathjax语法介绍](https://www.jianshu.com/p/a7fa1ed4ca20)","tags":["mathjax"],"categories":["mathjax"]},{"title":"[杂谈]亲戚关系图谱","url":"%2Fposts%2F9c4487da%2F","content":"### 前言\n你是否经常在亲戚朋友聚会时会碰到喊不出称呼的尴尬场景,下面这张`亲戚关系图谱`将帮你好好理清一下亲戚关系.\n{% asset_img 0bf9c8fa0ec09fac7aa.jpg %}\n\n### 亲戚关系图谱\n根据上图做的部分修改,适用于南方这边\n```mermaid\ngraph TD\n    waigong[\"外公,外婆\"]---jiujiu[\"<b>舅舅</b>,舅妈\"]\n    waigong---yifu[\"<b>姨妈</b>,姨父\"]\n    waigong---baba[\"<b>妈妈,爸爸</b>\"]\n    yeye[\"爷爷,奶奶\"]---baba\n    yeye---bobo[\"<b>伯伯</b>,伯妈\"]\n    yeye---gudie[\"姑爹,<b>姑妈</b>\"]\n    jiujiu---biaojiemei[\"表姐妹\"]\n    jiujiu---biaoxiongdi1[\"表兄弟\"]\n    yifu---yibiaozimei[\"姨表姊妹\"]\n    yifu---yibiaoxiongdi[\"姨表兄弟\"]\n    baba---zimeizhangdi[\"姊妹丈弟,<b>姊妹妹妹</b>\"]\n    baba---laogong[\"<b>老婆,老公(你在这)</b>\"]\n    baba---xiongdi[\"<b>兄弟兄弟</b>,兄嫂弟妹\"]\n    bobo---tangxiongdi[\"堂兄弟,堂姐妹\"]\n    gudie---biaoxiongdi2[\"表兄弟\"]\n    zimeizhangdi---waisheng[\"外甥女,外甥\"]\n    laogong---nver[\"儿婿,<b>女儿</b>\"]\n    laogong---erzi[\"<b>儿子</b>,儿媳妇\"]\n    xiongdi---zhizi[\"侄女,侄子\"]\n    nver---sunzi1[\"孙女,孙子\"]\n    erzi---sunzi2[\"孙女,孙子\"]\n```\n> 1. **堂表之分**: 堂兄弟是爸爸的兄弟的儿子,表兄弟是爸爸的姐妹的儿子和妈妈的兄弟姐妹的儿子,堂姐妹和表姐妹也一样区分.简而言之,一个姓的都是堂\n> 2. **加深颜色**: 图中加深颜色表示与父节点有**直接血缘关系**\n\n","tags":["杂谈"],"categories":["杂谈"]},{"title":"[paper 2]Semi-supervised Feature Selection via Rescaled Linear Regression Xiaojun","url":"%2Fposts%2F531010d9%2F","content":"\n### Abstract\n- 基于特征选择的线性回归方法通常学习一个[投影矩阵(projection matrix)](../e772abbc)并通过它来评估特征的重要性,然而这些方法不能找到**投影矩阵的全局和稀疏解**,新的方法通过用一系列的缩放因子重新调节最小二乘法回归模型的系数,本文将给出理论诠释为何可以使用投影矩阵来给特征排序\n\n### Introduction\n- 标记信息能否获得,决定了特征选择能否在监督学习和无监督学习环境下进行\n  - 在监督特征选择中,特征的相关性可以通过特征和类别的关系来评估\n  - 无监督特征选择中,没有标签信息,特征的相关性可以通过特征的依赖性或相似性来评估\n  - 通常我们只能得到一小部分带有标记的数据集和大部分的未标记的数据集,基于这两部分数据集来进行特征选择,这种方法叫做\"半监督特征选择\"\n  > 附加:无标签数据一般是有标签数据中的某一个类别的（不要不属于的，也不要属于多个类别的）\n  有标签数据的标签应该都是对的\n  无标签数据一般是类别平衡的（即每一类的样本数差不多）\n  无标签数据的分布应该和有标签的相同或类似 等等。\n  ","tags":["paper"],"categories":["paper"]},{"title":"[机器学习实战] 第5章 Logistic回归","url":"%2Fposts%2Ff0799167%2F","content":"### 最大似然估计法\n- **定义[离散型]**: 若总体$X$属离散型,其分布律$P\\{X=x\\}=p(x;\\theta),\\theta\\isin\\Theta$的形式为已    知,$\\theta$为待估参数,$\\Theta$是$\\theta$可能取值的范围.设$X_1,X_2,\\cdots,X_n$是来自$X$的样本,则$X_1,X_2,\\cdots,X_n$的联合分布律为\n    $$\\begin{aligned}\n        \\prod_{i=1}^n{p(x_i;\\theta)}\n    \\end{aligned}\n    $$\n    又设$x_1,x_2,\\cdots,x_n$是相应与$X_1,X_2,\\cdots,X_n$的一个样本值.易知样本$X_1,X_2,\\cdots,X_n$取到观察值$x_1,x_2,\\cdots,x_n$的概率,亦即事件$\\{X_1=x_1,X_2=x_2,\\cdots,X_n=x_n\\}$发生的概率为\n    $$\\begin{aligned}\n        L(\\theta)=L(x_1,x_2,\\cdots,x_n;\\theta)=\\prod_{i=1}^n{p(x_i;\\theta)},\\theta\\isin\\Theta\\quad (1.1)\n    \\end{aligned}\n    $$\n    这一概率随$\\theta$的取值而变化,它是$\\theta$的函数,$L(\\theta)$成为样本的 **似然函数** (注意,这里$x_1,x_2,\\cdots,x_n$是已知的样本值,它们都是常数)\n    由费希尔(R.A.Fisher)引进的最大似然估计法,就是固定样本观测值$x_1,x_2,\\cdots,x_n,$在$\\theta$取值的可能范围$\\Theta$内挑选使似然函数$L(x_1,x_2,\\cdots,x_n;\\theta)$达到最大的参数值$\\hat\\theta$,作为参数$\\theta$的估计值.即取$\\hat\\theta$使\n    $$\\begin{aligned}\n        L(x_1,x_2,\\cdots,x_n;\\hat\\theta)=\\max_{\\theta\\isin\\Theta}{L(x_1,x_2,\\cdots,x_n;\\theta)}\\quad (1.2)\n    \\end{aligned}\n    $$\n    这样得到的$\\hat\\theta$与样本值$x_1,x_2,\\cdots,x_n$有关,常记为$\\hat\\theta(x_1,x_2,\\cdots,x_n)$,成为参数$\\theta$的 **最大似然估计值** ,而相应的统计量$\\hat\\theta(X_1,X_2,\\cdots,X_n)$称为参数$\\theta$的 **最大似然估计量**.\n- **定义[连续型]**: 若总体$X$属连续型,其概率密度$f(x;\\theta),\\theta\\isin\\Theta$的形式已知,$\\theta$为待估参数,$\\Theta$是$\\theta$可能取值的范围.设$X_1,X_2,\\cdot,X_n$是来自$X$的样本,则$X_1,X_2,\\cdots,X_n$的联合密度为\n    $$\\begin{aligned}\n        \\prod_{i=1}^n{f(x_i,\\theta)}\n    \\end{aligned}\n    $$\n    设$x_1,x_2,\\cdots,x_n$是相应于样本$X_1,X_2,\\cdots,X_n$的一个样本值,则随机点$(X_1,X_2,\\cdots,X_n)$落在点$(x_1,x_2,\\cdots,x_n)$的邻域(边长分别为$dx_1,dx_2,\\cdots,dx_n$的$n$维立方体)内的概率近似地为\n    $$\\begin{aligned}\n        \\prod_{i=1}^n{f(x_i;\\theta)dx_i}\\quad (1.3)\n    \\end{aligned}\n    $$\n    其值随$\\theta$的取值而变化.与离散型的情况一样,我们取$\\theta$的估计值$\\hat\\theta$使概率(1.3)取到最大值,但因子$\\prod_{i=1}^n{dx_i}$不随$\\theta$而变,故只需考虑函数\n    $$\\begin{aligned}\n        L(\\theta)=L(x_1,x_2,\\cdots,x_n;\\theta)=\\prod_{i=1}^n{f(x_i;\\theta)}\\quad (1.4)\n    \\end{aligned}\n    $$\n    的最大值.这里$L(\\theta)$称为样本的 **似然函数**.若\n    $$\\begin{aligned}\n        L(x_1,x_2,\\cdots,x_n;\\hat\\theta)=\\max_{\\theta\\isin\\Theta}{L(x_1,x_2,\\cdots,x_n;\\theta)},\n    \\end{aligned}\n    $$\n    则称$\\hat\\theta(x_1,x_2,\\cdots,x_n)$为$\\theta$的 **最大似然估计值**,称$\\hat\\theta(X_1,X_2,\\cdots,X_n)$为$\\theta$的 **最大似然估计量**.\n    > 在很多情况下,$p(x;\\theta)$和$f(x;\\theta)$关于$\\theta$可微,这时$\\hat\\theta$常可从方程$\\frac{dL(\\theta)}{d\\theta}=0$解得,又因$L(\\theta)$与$lnL(\\theta)$在同一$\\theta$处取到极值,因此,$\\theta$的最大似然估计$\\theta$也可以从方程$\\frac{dlnL(\\theta)}{d\\theta}=0$求得,而从后一方程求解往往比较方便.该方程被称为 **对数似然方程**.\n\n### Sigmoid函数\n  - **Sigmoid函数定义**: **S函数**得名因其形状像S字母,是一种常见的**逻辑函数(logistic function)**\n    $$\\begin{aligned}\n        S(t)=\\frac{1}{1+e^{-t}}\\quad (1)\n    \\end{aligned}\n    $$\n    其级数展开为:\n    $$\\begin{aligned}\n        S(t)=\\frac{1}{2}+\\frac{1}{4}t-\\frac{1}{48}t^3+\\frac{1}{480}t^5-\\frac{17}{80640}t^7+\\frac{31}{1451520}t^9-\\frac{691}{319334400}t^{11}+O(t^{12})\n    \\end{aligned}\n    $$\n    {% asset_img 201905250800123432.png %}\n    <p style=\"text-align: center;\" >图5-1 Sigmoid图像</p>\n    公式(1)自变量取值为任意实数,值域为$[0,1]$\n    将任意的输入映射到$[0,1]$区间,我们在线性回归中可以得到一个预测值,再将该值映射到Sigmoid函数中这样就完成了由值到概率的转换,也就是分类任务\n    python代码实现\n    \n    ```python\n    def sigmoid(z):\n        return 1 / (1+np.exp(-z))\n    ```\n\n### Logistic回归模型\n1. Logistic分布\n    - **定义(李航书):** 设$X$是连续随机变量,$X$服从Logistic分布是指$X$具有下列分布函数和密度函数:\n    $$\\begin{aligned}\n    F(x)=P(X\\le x)=\\frac{1}{1+e^{-\\frac{x-\\mu}{\\gamma}}}&\\quad (2)\\\\\n    f(x)=F'(x)=\\frac{e^{-\\frac{x-\\mu}{\\gamma}}}{\\gamma(1+e^{-\\frac{x-\\mu}{\\gamma}})^2}&\\quad (3)\n    \\end{aligned}\n    $$\n    式中,$\\mu$为位置参数,$\\gamma\\gt 0$为形状参数\n    - **说明**: 逻辑回归算法是分类算法,我们将它作为分类算法使用.有时候可能因为这个算法的名字出现了\"回归\"使你感到困惑,但逻辑回归算法实际上是一种分类算法,它适用于标签$y$取值离散的情况,如:$1\\ 0\\ 0\\ 1$\n    - **假设函数**: $h_\\theta(x)=g(\\theta x)=S(\\theta x)$,其中h表示假设函数(hypothesis function),g表示逻辑函数(logistic function),S表示Sigmoid函数(Sigmoid function)\n    - **决策边界()**: Sigmoid函数是常用的一个逻辑函数,我们通过将输入(样本)$x$映射到Sigmoid函数,从而把输出控制在[0,1]范围,对于分类问题,我们需要输出0或1,我们可以预测:\n      - 当$h_\\theta(x)\\ge 0.5$时,预测$y=1$\n      - 当$h_\\theta(x)\\lt 0.5$时,预测$y=0$\n        {% asset_img 2019052814183916.png %}\n        决策边界(decision boundary)代码实现如下\n        ```python\n        def decision_boundary(prob):\n            return prob > 0.5*np.ones(len(prob))\n        ```\n\n### 对比线性回归和Logistic回归模型\n1. 线性回归的方法:\n   1. 假设函数$h_\\theta(x)=\\theta x$(这样多项式的形式可以表示所有的值,$\\theta$为未知参数,我们要求出一个$\\theta$,对于每一个样本$x$放进去$h_\\theta(x)$得出的预测值都可以尽可能的匹配观测值)\n   2. 线性回归的代价函数时所有模型误差的平方和(这样可以保障是凸函数),代价函数为\n    $$\\begin{aligned}\n        J(\\theta)=\\frac{1}{2m}\\sum_{i=0}^m(h_\\theta(x^{(i)})-y^{(i)})^2\n    \\end{aligned}$$\n   3. 为使预测值与观测值最接近,从而应当使代价函数$J(\\theta)$最小,虽然可以采用求导的方法求最值,但求导属于一元函数(即1维空间,1个特征值),但是现实生活中每个样本的特征值远远超过1个,因此要采用偏导数(适用于多元函数)的方法的方法求最值,梯度是指向值变化最大的方向,用于求多元函数的最值再适合不过,因此采用梯度下降的方法来求得代价函数的最小值,由于梯度方向只是指向值变化最大的方向,而我们仍需要尽可能沿着观测的值走(不能一步太大跑偏了),需要在到达最小值的过程中不断的重新计算梯度方向,因此引入$\\alpha$作为学习率(learning rate),决定了我们沿着能让代价函数下降程度最大的方向迈出的步子有多大,这个过程便是梯度下降的过程,梯度下降有几种实现方式:\n         1. 批量梯度下降(BGD: Batch Gradient Descent):即每一次都同时让所有的参数减去学习速率乘以代价函数$J(\\theta)$的偏导数\n            $$\\begin{aligned}\n                &J(\\theta)=\\frac{1}{2m}\\sum_{i=0}^m(h_\\theta(x^{(i)})-y^{(i)})^2\\\\\n                &\\frac{\\partial}{\\partial\\theta_i}{J(\\theta)}=\\frac{1}{m}\\sum_{j=0}^m(h_\\theta(x^{(j)})-y^{(j)})x_i^{(j)}\\\\\n            &\\theta_i=\\theta_i-\\alpha\\frac{\\partial}{\\partial\\theta_i}{J(\\theta)}=\\theta_i-\\alpha\\sum_{j=0}^m(h_\\theta(x^{(j)})-y^{(j)})x_i^{(j)}\n            \\end{aligned}$$\n            <span class='my-color-b'>梯度下降法把$\\frac{1}{m}$放到了学习率$\\alpha$中,因此最后只需保留$\\alpha$即可</span>\n         2. 随机梯度下降(SGD: Stochastic Gradient Descent):与批量梯度下降的区别在于,随机梯度下降仅仅选取一个样本j来求梯度.公式为:\n            $$\\begin{aligned}\n                \\theta_i=\\theta_i-\\alpha(h_\\theta(x^{j})-y^{(j)})x_i^{(j)}\n            \\end{aligned}\n            $$\n2. Logistic回归的方法:\n   1. 假设函数$h_\\theta(x)=g(\\theta x)=\\frac{1}{1+e^{-\\theta x}}$\n        假设函数$h_\\theta(x)$代码实现如下\n        ```python\n        def hypo(theta, x):\n            \"\"\"假设函数(hypothesis function)\n\n            Arguments:\n                theta {array} -- 参数值\n                x {array} -- 一个数据或数据集\n\n            Returns:\n                array -- 通过假设函数得到的预测值\n            \"\"\"\n            return sigmoid(np.dot(x, theta))\n        ```\n   2. 我们不能将$h_\\theta(x)$带入线性回归的代价函数(即所有模型误差的平方和)中,因为这样得到的代价函数会是一个非凸函数,意味着我们的代价函数有许多局部最小值,这回影响梯度下降算法寻找全局最小值,Logistic的代价函数定义为\n        $$\\begin{aligned}\n            J(\\theta)=-\\frac{1}{m}\\sum_{i=0}^m[y^{(i)}log(h_\\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\\theta(x^{(i)}))]\n        \\end{aligned}\n        $$\n        代价函数$J(\\theta)$实现代码如下\n        ```python\n        def jcost(theta, X, y):\n            \"\"\"J代价函数\n\n            Arguments:\n                theta {array} -- 参数值\n                X {array} -- 数据集\n                y {array} -- 标签集\n\n            Returns:\n                array -- 代价\n            \"\"\"\n            hX = hypo(theta, X)\n            # 元素级乘法获取第一项的值(数组)\n            first = np.multiply(y, np.log(hX))\n            # 元素级乘法获取第二项的值(数组)\n            second = np.multiply(np.ones(len(y))-y, np.log(np.ones(len(hX))-hX))\n            return -(first+second)/len(X)\n        ```\n   3. 同线性回归方法一样,为了使代价函数$J(\\theta)$最小,采用梯度下降方式\n      1. 批量梯度下降(BGD: Batch Gradient Descent):即每一次都同时让所有的参数减去学习速率乘以代价函数$J(\\theta)$的偏导数\n        $$\\begin{aligned}\n            &J(\\theta)=-\\frac{1}{m}\\sum_{i=0}^m[y^{(i)}log(h_\\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\\theta(x^{(i)}))]\\\\\n            &\\frac{\\partial}{\\partial \\theta_i}J(\\theta)=\\frac{1}{m}\\sum_{j=0}^m(h_\\theta(x^{(j)})-y^{(j)})x_i^{(j)}=\\frac{1}{m}\\sum_{j=0}^m(S(\\theta x^{(j)})-y^{(j)})x_i^{(j)}\\\\\n            &\\theta_i=\\theta_i-\\alpha\\frac{\\partial}{\\partial\\theta_i}J(\\theta)=\\theta_i-\\alpha\\sum_{j=0}^m(h_\\theta(x^{(j)})-y^{(j)})x_i^{(j)}=\\theta_i-\\alpha\\sum_{j=0}^m(S(\\theta x^{(j)})-y^{(j)})x_i^{(j)}\n        \\end{aligned}\n        $$\n        <span class='my-color-b'>梯度下降法把$\\frac{1}{m}$放到了学习率$\\alpha$中,因此最后只需保留$\\alpha$即可\n        虽然Logistic回归和线性回归的代价函数的偏导数都一样,但并非所有代价函数最终求导都为同一形式</span>\n        我们来推导一下Logistic回归的代价函数的偏导数的式子是如何求出的\n        首先\n        $$\\begin{aligned}\n            &y^{(j)}log(h_\\theta(x^{(j)}))+(1-y^{(j)})log(1-h_\\theta(x^{(j)}))\\\\\n            =&y^{(j)}log(S(\\theta x^{(j)}))+(1-y^{(j)})log(1-S(\\theta x^{(j)}))\\\\\n            =&y^{(j)}log(\\frac{1}{1+exp(-\\theta x^{(j)})})+(1-y^{(j)})log(1-\\frac{1}{1+exp(-\\theta x^{(j)})})\\\\\n            =&-y^{(j)}log(1+exp(-\\theta x^{(j)}))-(1-y^{(j)})log(1+exp(\\theta x^{(j)}))\n        \\end{aligned}$$\n        所以\n        $$\\begin{aligned}\n            \\frac{\\partial}{\\partial\\theta_i}J(\\theta)&=\\frac{\\partial}{\\partial\\theta_i}[-\\frac{1}{m}\\sum_{j=0}^m[-y^{(j)}log(1+e^{-\\theta x^{(j)}})-(1-y^{(j)})log(1+exp(\\theta x^{(j)}))]]\\\\\n            &=-\\frac{1}{m}\\sum_{j=0}^m[-y^{(j)}\\frac{-x_i^{(j)}e^{-\\theta x^{(j)}}}{1+e^{-\\theta x^{(j)}}}-(1-y^{(j)})\\frac{x_i^{(j)}e^{\\theta x^{(j)}}}{1+e^{\\theta x^{(j)}}}]\\\\\n            &=-\\frac{1}{m}\\sum_{j=0}^m[y^{(j)}\\frac{x_i^{(j)}}{1+e^{\\theta x^{(j)}}}-(1-y^{(j)})\\frac{x_i^{(j)}e^{\\theta x^{(j)}}}{1+e^{\\theta x^{(j)}}}]\\\\\n            &=-\\frac{1}{m}\\sum_{j=0}^m[\\frac{y^{(j)}x_i^{(j)}-x_i^{(j)}e^{\\theta x^{(j)}}+y^{(j)}x_i^{(j)}e^{\\theta x^{(j)}}}{1+e^{\\theta x^{(j)}}}]\\\\\n            &=-\\frac{1}{m}\\sum_{j=0}^m[\\frac{y^{(j)}(1+e^{\\theta x^{(j)}})-e^{\\theta x^{(j)}}}{1+e^{\\theta x^{(j)}}}]x_i^{(j)}\\\\\n            &=-\\frac{1}{m}\\sum_{j=0}^m[y^{(j)}-\\frac{e^{\\theta x^{(j)}}}{1+e^{\\theta x^{(j)}}}]x_i^{(j)}\\\\\n            &=-\\frac{1}{m}\\sum_{j=0}^m[y^{(j)}-\\frac{1}{1+e^{-\\theta x^{(j)}}}]x_i^{(j)}\\\\\n            &=-\\frac{1}{m}\\sum_{j=0}^m[y^{(j)}-S(\\theta x^{(j)})]x_i^{(j)}\\\\\n            &=-\\frac{1}{m}\\sum_{j=0}^m[y^{(j)}-h_\\theta(x^{(j)})]x_i^{(j)}\\\\\n            &=\\frac{1}{m}\\sum_{j=0}^m[h_\\theta(x^{(j)})-y^{(j)}]x_i^{(j)}\n        \\end{aligned}$$\n        证毕\n        <span class='my-color-b'>注:虽然得到的梯度下降算法表面上看上去与线性回归的梯度下降算法一样,但是这里的$h_\\theta(x)=g(\\theta x)$与线性回归中不同,所以实际上是不一样的.另外,在运行梯度下降算法之前,进行[特征缩放(特征归一化)](../75b8f8af#jump-autonorm)依旧是非常必要的.</span>\n        批量梯度代码实现如下\n        ```python\n        def partial_jcost(theta, X, y, x):\n            \"\"\"J代价函数关于theta的偏导数\n\n            Arguments:\n                theta {array} -- 参数值\n                X {array} -- 数据集\n                y {array} -- 标签集\n                x {array} -- 一个数据或数据集\n\n            Returns:\n                array -- 偏导值\n            \"\"\"\n            return np.dot(hypo(theta, X)-y, x)\n\n        def batch_gradient_desc(X, y, alpha=0.01, numIterations=500):\n            \"\"\"BGD下降法(批量梯度下降法)\n\n            Arguments:\n                X {array} -- 数据集\n                y {array} -- 标签集\n\n            Keyword Arguments:\n                alpha {float} -- 步长(学习率) (default: {0.01})\n                numIterations {int} -- 迭代次数 (default: {500})\n\n            Returns:\n                array -- 返回参数值\n            \"\"\"\n            # 获取特征数n\n            n = len(X[0])\n            theta = np.ones(n)\n            # 批量梯度下降法\n            for i in range(numIterations):\n                theta = theta-alpha*partial_jcost(theta, X, y, X)\n            return theta\n        ```\n\n        1. 随机梯度下降(SGD: Stochastic Gradient Descent):与批量梯度下降的区别在于,随机梯度下降仅仅选取一个样本j来求梯度.公式为:\n        $$\\begin{aligned}\n            \\theta_i=\\theta_i-\\alpha(h_\\theta(x^{j})-y^{(j)})x_i^{(j)}=\\theta_i-\\alpha(S(\\theta x^{(j)})-y^{(j)})x_i^{(j)}\n        \\end{aligned}\n        $$\n        随机梯度下降法代码实现如下\n        ```python\n        def stochastic_gradient_desc(X, y, alpha=0.01, numIterations=100):\n            \"\"\"SGD下降法(随机梯度下降法)\n\n            Arguments:\n                X {array} -- 数据集\n                y {array} -- 标签集\n\n            Keyword Arguments:\n                alpha {float} -- 步长(学习率) (default: {0.01})\n                numIterations {int} -- 迭代次数 (default: {100})\n\n            Returns:\n                array -- 返回参数值\n            \"\"\"\n            # 获取样本数m,特征数n\n            m, n = len(X), len(X[0])\n            theta = np.ones(n)\n            # 随机梯度下降法\n            for k in range(numIterations):\n                dataIndex = range(m)\n                for i in range(m):\n                    alpha = 4/(1.0+k+i)+0.01\n                    randIndex = int(np.random.uniform(0,len(dataIndex)))\n                    theta=theta-alpha*np.multiply(hypo(X[randIndex], theta)-y[randIndex],X[randIndex])\n            return theta\n        ```\n\n### 最优化理论中常用的优化算法\n#### 梯度下降法和梯度上升法\n- **梯度下降法wiki**: 如果$F(x)$在点$a$附近的邻域是有定义且可微的,那么函数$F(x)$在点$a$沿着负梯度方向值减少得最快(下降得最快),遵循下列公式\n    $$\\begin{aligned}\n        a_{n+1}=a_n-\\gamma{\\nabla{F(a_n)}}\\tag 7\n    \\end{aligned}\n    $$\n    对于$\\gamma\\isin\\Reals_+$为一个足够小的数值时成立,那么$F(a)\\ge F(b)$\n    考虑到这一点,我们可以从函数$F$的局部极小值的初始估计$x_0$出发,并考虑如下序列$x_0,x_1,\\cdots$使得\n    $$\\begin{aligned}\n        x_{n+1}=x_n-\\gamma_n\\nabla{F(x_n)},\\quad n\\ge 0\\tag 8\n    \\end{aligned}\n    $$\n    因此可以得到\n    $$\\begin{aligned}\n        F(x_0)\\ge F(x_1)\\ge F(x_2)\\ge\\cdots,\n    \\end{aligned}\n    $$\n    如果顺利的话序列$(X_n)$将收敛到期望的局部极小值.注意:每次迭代过程中$\\gamma$值都可以改变.\n    {%asset_img 2019052618559123.png %}\n    当函数$F$是凸函数,所有的局部最小值也是全局最小值,因此,上面这幅图梯度下降可以收敛到全局解(即最小值).上图中,$F$被定义在一个\"碗\"状图形上.蓝色的为轮廓线,轮廓线上$F$的值是不变的.红色箭头表示的方向是箭头原点的负梯度方向,该方向与该原点所在的轮廓线相互正交.图中梯度下降法最终引向了\"碗底\",该点便是$F$的最小值点.\n- **梯度下降的相关概念**\n    {% asset_img v2-77ab0b5fab73c_r.jpg %}\n    1. **步长(learning rate)**:步长决定了在梯度下降迭代的过程中，每一步沿梯度负方向前进的长度。用上面下山的例子，步长就是在当前这一步所在位置沿着最陡峭最易下山的位置走的那一步的长度。\n    2. **特征(feature)**:指的是样本中输入部分，比如2个单特征的样本$(x^{(0)},y^{(0)}),(x^{(1)},y^{(1)})$,则第一个样本特征为$x^{(0)}$，第一个样本输出为$y^{(0)}$。\n    3. **假设函数(hypothesis function)**:在监督学习中，为了拟合输入样本，而使用的假设函数，记为$h_\\theta(x)$。比如对于单个特征的$m$个样本$(x^{(i)},y^{(i)})(i=1,2,...m)$,可以采用拟合函数如下： $h_\\theta(x_1)=\\theta_0+\\theta_1{x_1}$,为了更好的表示,则增加$x_0=1$,则$h_\\theta(x)=h_\\theta(x_0,x_1)=\\theta_0x_0+\\theta_1x_1$\n    4. **损失函数或代价函数(loss function或cost function)**：为了评估模型拟合的好坏，通常用代价函数来度量拟合的程度。代价函数极小化，意味着拟合程度最好，对应的模型参数即为最优参数。在线性回归中，代价函数通常为预测值与观测值的差取平方。比如对于$m$个样本$(x^{(i)},y^{(i)})(i=1,2,...m)$,采用线性回归，代价函数为：\n    $J(\\theta_0,\\theta_1)=\\frac{1}{2m}\\sum_{i=0}^m(h_\\theta(x^{(i)})-y^{(i)})^2$\n    之所以从$i=0$而不是$i=1$开始,是增加了$x^{(0)}=1$,为了方便运算,其中$x^{(i)}$表示第$i$个样本特征,$y^{(i)}$表示第$i$个样本的观测值,$h_\\theta(x^{(i)})$为假设函数,表示第$i$个样本的预测值\n- **梯度下降法的代数方式描述**\n    1. **先决条件**： 确认优化模型的假设函数和代价函数。\n        比如对于线性回归，假设函数表示为 $h_θ(x_1,x_2,...x_n)=θ_0+θ_1x_1+...+θ_nx_n$, 其中$θ_i(i = 0,1,2...n)$为模型参数，$x_i(i = 0,1,2...n)$为每个样本的$n$个特征值。这个表示可以简化，我们增加一个特征$x_0=1$ ，这样$h_\\theta(x_0,x_1,...x_n)=\\displaystyle\\sum_{i=0}^n{\\theta_ix_i}$\n        同样是线性回归，对应于上面的假设函数，代价函数为：\n        $$\\begin{aligned}\n            J(\\theta_0,\\theta_1...,\\theta_n)=\\frac{1}{2m}\\sum_{j=0}^m(h_\\theta(x_0^{(j)},x_1^{(j)},...x_n^{(j)})−y^{(j)})^2\n        \\end{aligned}$$\n        <span class='my-color-b'>比如说,$x=(x_0,x_1,x_2,\\cdots,x_n)$表示一个样本$x$有n($x_0=1$)个特征值,$h_\\theta(x)=x\\theta=θ_0x_0+θ_1x_1+...+θ_nx_n$,中间$x\\theta$表示**向量点积**,在$x$中增加$x_0$(值为1)是为了方便向量点积运算(必须长度相同),且不会产生任何影响,$\\theta=(\\theta_0,\\theta_1,\\cdots,\\theta_n)$</span>\n    2. **算法相关参数初始化**：主要是初始化$\\theta_0,\\theta_1...,\\theta_n$,算法终止距离$\\epsilon$以及步长$\\alpha$。在没有任何先验知识的时候，我喜欢将所有的$\\theta$初始化为1， 将步长初始化为0.01。在调优的时候再 优化。\n    3. **算法过程(批量梯度下降法)**：\n       1). 确定当前位置的代价函数的梯度，对于$\\theta_i$,其梯度表达式如下：\n       $$\\begin{aligned}\n           \\frac{\\partial}{\\partial\\theta_i}{J(\\theta_0,\\theta_1,\\cdots,\\theta_n)}\n       \\end{aligned}\n       $$\n       2). 用步长乘以代价函数的梯度，得到当前位置下降的距离，即$\\alpha\\frac{\\partial}{\\partial\\theta_i}{J(\\theta_0,\\theta_1,\\cdots,\\theta_n)}$对应于前面登山例子中的某一步。\n       3). 确定是否所有的$\\theta_i$,梯度下降的距离都小于$\\epsilon$，如果小于$\\epsilon$则算法终止，当前所有的$θ_i(i=0,1,...n)$即为最终结果。否则进入步骤4\n       4). 更新所有的$\\theta$，对于$\\theta_i$，其更新表达式如下。更新完毕后继续转入步骤1.\n        $$\\begin{aligned}\n            \\theta_i=\\theta_i-\\alpha\\frac{\\partial}{\\partial\\theta_i}{J(\\theta_0,\\theta_1,\\cdots,\\theta_n)}\n        \\end{aligned}\n        $$\n        下面用线性回归的例子来具体描述梯度下降。假设我们的样本是\n        $(x_1^{(0)},x_2^{(0)},\\cdots,x_n^{(0)},y_0),(x_1^{(1)},x_2^{(1)},\\cdots,x_n^{(1)},y_1),\\cdots,(x_1^{(m)},x_2^{(m)},\\cdots,x_n^{(m)},y_m)$,代价函数如前面先决条件所述：\n        $$\\begin{aligned}\n            J(\\theta_0,\\theta_1,\\cdots,\\theta_n)=\\frac{1}{2m}\\sum_{j=0}^m(h_\\theta(x_0^{(j)},x_1^{(j)},\\cdots,x_n^{(j)})-y^{(j)})^2\n        \\end{aligned}\n        $$\n        则在算法过程步骤1中对于$\\theta_i$ 的偏导数计算如下： \n        $$\\begin{aligned}\n            \\frac{\\partial}{\\partial\\theta_i}{J(\\theta_0,\\theta_1,\\cdots,\\theta_n)}=\\frac{1}{m}\\sum_{j=0}^m(h_\\theta(x_0^{(j)},x_1^{(j)},\\cdots,x_n^{(j)})-y^{(j)})x_i^{(j)}\n        \\end{aligned}\n        $$\n        由于样本中没有$x_0$上式中令所有的$x_0^{(j)}$为1.\n        步骤4中$\\theta_i$的更新表达式如下：\n        $$\\begin{aligned}\n            \\theta_i=\\theta_i-\\alpha\\frac{1}{m}\\sum_{j=0}^m(h_\\theta(x_0^{(j)},x_1^{(j)},\\cdots,x_n^{(j)})-y^{(j)})x_i^{(j)}\n        \\end{aligned}\n        $$\n        从这个例子可以看出当前点的梯度方向是由所有的样本决定的，加$\\frac{1}{m}$ 是为了好理解。由于步长也为常数，他们的乘积也为常数，所以<span class='my-color-b'>这里$\\alpha\\frac{1}{m}$可以用一个常数来处理,一般在程序中的处理是只保留$\\alpha$</span>。\n        在下面会详细讲到的梯度下降法的变种，他们主要的区别就是对样本的采用方法不同。这里我们采用的是用所有样本(即批量梯度下降法)。\n- **梯度下降法的矩阵方式描述**\n    1. **先决条件**： 和上面类似， 需要确认优化模型的假设函数和代价函数。对于线性回归，假设函数$h_θ(x_1,x_2,...x_n)=θ_0+θ_1x_1+...+θ_nx_n$的矩阵表达方式为：\n        $$\\begin{aligned}\n            h_\\theta(X)=X\\theta=(x_1,x_2,\\cdots,x_m)\\theta,\\quad\\theta=(\\theta_0, \\theta_1,\\cdots,\\theta_n)\n        \\end{aligned}\n        $$\n        其中， 假设函数$h_\\theta(X)$为mx1的向量,$\\theta$为(n+1)x1的向量，里面有n+1个代数法的模型参数。$X$为mx(n+1)维的矩阵。m代表样本的个数，n+1代表样本的特征数。\n        代价函数的表达式为：$J(\\theta)=\\frac{1}{2}(X\\theta−Y)^T(X\\theta−Y)$, 其中$Y$是样本的输出向量，维度为mx1.\n        <span class='my-color-b'>这里$X=(x_1,x_2,\\cdots,x_m)$,表示m个样本的集合(每个样本有n+1个特征值$x_0,x_1,\\cdots,x_n$,多的1为增加的$x0=1$)</span>\n    2. **算法相关参数初始化**: $\\theta$向量可以初始化为默认值，或者调优后的值。算法终止距离$\\epsilon$，步长$\\alpha$和代数形式比没有变化。\n    3. **算法过程(批量梯度下降法)**:\n        1). 确定当前位置的代价函数的梯度，对于$\\theta$向量,其梯度表达式如下：$\\frac{\\partial}{\\partial\\theta}J(\\theta)$\n        2). 用步长乘以代价函数的梯度，得到当前位置下降的距离，即$\\alpha\\frac{\\partial}{\\partial\\theta}J(\\theta)$对应于前面登山例子中的某一步。\n        3). 确定$\\theta$向量里面的每个值,梯度下降的距离都小于$\\epsilon$，如果小于$\\epsilon$则算法终止，当前$\\epsilon$向量即为最终结果。否则进入步骤4.\n        4). 更新$\\theta$向量，其更新表达式如下。更新完毕后继续转入步骤1.\n        $$\\begin{aligned}\n            \\theta=\\theta-\\alpha\\frac{\\partial}{\\partial\\theta}J(\\theta)\n        \\end{aligned}\n        $$\n        还是用线性回归的例子来描述具体的算法过程。\n        代价函数对于$\\theta$向量的偏导数计算如下：\n        $$\\begin{aligned}\n          \\frac{\\partial}{\\partial\\theta}J(\\theta)=X^T(X\\theta-Y)  \n        \\end{aligned}\n        $$\n        步骤4中θ向量的更新表达式如下：$\\theta=\\theta-\\alpha{X^T}(X\\theta-Y)$\n        对于之前的代数法,可以看到矩阵法要简洁很多。这里面用到了矩阵求导链式法则，和两个矩阵求导的公式。\n        这里面用到了矩阵求导链式法则，和两个个矩阵求导的公式。\n        公式1: $\\frac{\\partial}{\\partial x}(x^Tx)=2x$,$x$为向量\n        公式2: $\\nabla_X{f(AX+B)}=A^T\\nabla_Y{f},\\ Y=AX+B$, $f(Y)$为标量\n        如果需要熟悉矩阵求导建议参考张贤达的《矩阵分析与应用》一书。\n- **程序演示**\n    我们通过构造函数$f(x)=(x-1)(x-2)(x-3)=x^3-6x^2+11x-6$来展示二维空间(平面)上的梯度上升和梯度下降过程\n    {% asset_img 20190526192019192.png %}\n    上图中,左图为二维空间的梯度上升示意图,红色点即为每次梯度上升迭代的过程,可以看到点与点之间的距离是逐步缩小的,只到最后收敛为0,即点取得极大值(注意:只是极大值(局部解),不是最大值,如果是凸函数,则为最大值(全局解)),右图为二维空间的梯度下降示意图,蓝色点即为每次梯度下降迭代的过程,同样最终收敛点取得极小值(局部解) \n#### 批量梯度下降法(BGD: Batch Gradient Descent)\n批量梯度下降法，是梯度下降法最常用的形式，具体做法也就是在更新参数时使用所有的样本来进行更新，这个方法对应于前面代数形式的线性回归的梯度下降算法，也就是说3.3.1的梯度下降算法就是批量梯度下降法。　\n$$\\begin{aligned}\n    \\theta_i=\\theta_i-\\alpha\\sum_{j=0}^m(h_\\theta(x_0^{(j)},x_1^{(j)},\\cdots,x_n^{(j)})-y^{(j)})x_i^{(j)}\n\\end{aligned}\n$$　\n或\n{% asset_img v2-5809743fd06c4ff8.jpg %}\n<span class='my-color-b'>一般记住第一个,用预测值减观测值$\\sum_{j=0}^m(h_\\theta(x_0^{(j)},x_1^{(j)},\\cdots,x_n^{(j)})-y^{(j)})$,由于是梯度下降,因此是减$\\alpha$(虽然第二福图是加,我们这样记只是为了方便记忆)</span>\n由于我们有m个样本，这里求梯度的时候就用了所有m个样本的梯度数据。\n\n#### 随机梯度下降法(SGD: Stochastic Gradient Descent)\n随机梯度下降法，其实和批量梯度下降法原理类似，区别在与求梯度时没有用所有的m个样本的数据，而是仅仅选取一个样本j来求梯度。对应的更新公式是：\n$$\\begin{aligned}\n    \\theta_i=\\theta_i-\\alpha(h_\\theta(x_0^{(j)},x_1^{(j)},\\cdots,x_n^{(j)})-y^{(j)})x_i^{(j)}\n\\end{aligned}\n$$\n或\n{% asset_img v2-b3f14a09ad27df9_r.jpg %}\n<span class='my-color-b'>一般记住第一个,用预测值减观测值$\\sum_{j=0}^m(h_\\theta(x_0^{(j)},x_1^{(j)},\\cdots,x_n^{(j)})-y^{(j)})$,由于是梯度下降,因此是减$\\alpha$(虽然第二福图是加,我们这样记只是为了方便记忆)</span>\n随机梯度下降法，上面的批量梯度下降法是两个极端，一个采用所有数据来梯度下降，一个用一个样本来梯度下降。自然各自的优缺点都非常突出。对于训练速度来说，随机梯度下降法由于每次仅仅采用一个样本来迭代，训练速度很快，而批量梯度下降法在样本量很大的时候，训练速度不能让人满意。对于准确度来说，随机梯度下降法用于仅仅用一个样本决定梯度方向，导致解很有可能不是最优。对于收敛速度来说，由于随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解。\n那么，有没有一个中庸的办法能够结合两种方法的优点呢？有！这就是小批量梯度下降法。\n#### 小批量随机梯度下降法(MBGD: Mini-batch Gradient Descent)\n小批量梯度下降法是批量梯度下降法和随机梯度下降法的折衷，也就是对于m个样本，我们采用x个样子来迭代，$1\\lt x\\lt m$。一般可以取x=10，当然根据样本的数据，可以调整这个x的值。对应的更新公式是：\n$$\\begin{aligned}\n    \\theta_i=\\theta_i-\\alpha\\sum_{j=t}^{t+x-1}(h_\\theta(x_0^{(j)},x_1^{(j)},\\cdots,x_n^{(j)})-y^{(j)})x_i^{(j)}\n\\end{aligned}\n$$　\n或\n{% asset_img v2-96181aa7bcd.jpg %}\n#### 牛顿法和拟牛顿法(Quasi-Newton Methods)\n#### DFP算法\n#### 局部优化法(BFGS)\n#### 有限内存局部优化法(L-BFGS)\n#### 共轭梯度法(Conjugate Gradient)\n#### 拉格朗日乘数法\n#### 启发式优化算法-智能算法\n1. 人工神经网络\n2. 模拟退火算法\n3. 禁忌搜索算法\n4. 粒子群算法\n5. 蚁群算法\n6. 鱼群算法\n7. 布谷鸟算法\n8. 遗传算法\n9. 免疫算法\n10. 进化多目标算法\n### Logistic二分类程序运行结果\n{% asset_img 2019052817502312.png %}\n可以看出程序预测结果还是比较准确的$(0.98,1.00)$,由于每次运行程序都会重新打乱样本,则训练样本和测试样本会有所不同,因此多次运行会得到不同的精确度,上图是先将特征值进行了归一化的结果,将特征值的范围都限定在了$[0,1]$之间可以更好的把握特征的分布,不至于图像过大(由于有些特征之间值的差距过大),数据点分布过于稀疏\n### Logistic二分类代码实现\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\n\n\ndef loadDataSet_iris():\n    \"\"\"数据集生成\n\n    Returns:\n        array -- 数据集\n        array -- 标签集\n    \"\"\"\n    dataMat, labelMat = load_iris(return_X_y=True)\n    dataMat, labelMat = dataMat[:100,:2], labelMat[:100]\n    return dataMat, labelMat\n\n\ndef sigmoid(z):\n    return 1 / (1+np.exp(-z))\n\n\ndef hypo(x, theta):\n    \"\"\"假设函数(hypothesis function)\n\n    Arguments:\n        theta {array} -- 参数值\n        x {array} -- 一个数据或数据集\n\n    Returns:\n        array -- 通过假设函数得到的预测值\n    \"\"\"\n    return sigmoid(np.dot(x, theta))\n\n\ndef jcost(theta, X, y):\n    \"\"\"J代价函数\n\n    Arguments:\n        theta {array} -- 参数值\n        X {array} -- 数据集\n        y {array} -- 标签集\n\n    Returns:\n        array -- 代价\n    \"\"\"\n    hX = hypo(X, theta)\n    # 元素级乘法获取第一项的值(数组)\n    first = np.multiply(y, np.log(hX))\n    # 元素级乘法获取第二项的值(数组)\n    second = np.multiply(np.ones(len(y))-y, np.log(np.ones(len(hX))-hX))\n    return -(first+second)/len(X)\n\n\ndef partial_jcost(theta, X, y, x):\n    \"\"\"J代价函数关于theta的偏导数\n\n    Arguments:\n        theta {array} -- 参数值\n        X {array} -- 数据集\n        y {array} -- 标签集\n        x {array} -- 一个数据或数据集\n\n    Returns:\n        array -- 偏导值\n    \"\"\"\n    return np.dot(hypo(X, theta)-y, x)\n\n\ndef decision_boundary(prob):\n    \"\"\"决策边界:\n           概率>=0.5时,为1\n           概率<0.5时,为0\n\n    Arguments:\n        prob {array} -- 一组概率值\n\n    Returns:\n        array -- 一组类别值\n    \"\"\"\n    return prob >= 0.5*np.ones(len(prob))\n\n\ndef accuracy_rate(X, y, theta):\n    \"\"\"计算预测准确率\n\n    Arguments:\n        X {array} -- 预测数据集\n        y {array} -- 已知观测值\n        theta {array} -- 参数值\n\n    Returns:\n        float -- 返回预测准确率\n    \"\"\"\n    y_predict = classify(hypo(X, theta))\n    trueCount = np.sum(y_predict == y)\n    return float(trueCount)/len(y)\n\n\ndef batch_gradient_desc(X, y, alpha=0.01, numIterations=500):\n    \"\"\"BGD下降法(批量梯度下降法)\n\n    Arguments:\n        X {array} -- 数据集\n        y {array} -- 标签集\n\n    Keyword Arguments:\n        alpha {float} -- 步长(学习率) (default: {0.01})\n        numIterations {int} -- 迭代次数 (default: {500})\n\n    Returns:\n        array -- 返回参数值\n    \"\"\"\n    # 获取特征数n\n    n = len(X[0])\n    theta = np.ones(n)\n    # 批量梯度下降法\n    for i in range(numIterations):\n        theta = theta-alpha*partial_jcost(theta, X, y, X)\n    return theta\n\n\ndef stochastic_gradient_desc(X, y, alpha=0.01, numIterations=100):\n    \"\"\"SGD下降法(随机梯度下降法)\n\n    Arguments:\n        X {array} -- 数据集\n        y {array} -- 标签集\n\n    Keyword Arguments:\n        alpha {float} -- 步长(学习率) (default: {0.01})\n        numIterations {int} -- 迭代次数 (default: {100})\n\n    Returns:\n        array -- 返回参数值\n    \"\"\"\n    # 获取样本数m,特征数n\n    m, n = len(X), len(X[0])\n    theta = np.ones(n)\n    # 随机梯度下降法\n    for k in range(numIterations):\n        for i in range(m):\n            theta=theta-alpha*np.multiply(hypo(X[i], theta)-y[i],X[i])\n    return theta\n\n\ndef classify(prob):\n    \"\"\"由概率返回分类类别\n\n    Arguments:\n        prob {array} -- 每个样本的概率\n\n    Returns:\n        array -- 返回分类类别\n    \"\"\"\n    return decision_boundary(prob)\n\n\ndef auto_norm(X):\n    \"\"\"特征归一化(或特征缩放)\n\n    Arguments:\n        X {array} -- 数据集\n\n    Returns:\n        array -- 返回归一化后的数据集\n    \"\"\"\n    X = np.array(X)\n    n = len(X[0])\n    minVals = X.min(0)\n    maxVals = X.max(0)\n    newVals = (X-minVals)/(maxVals-minVals)\n    return newVals\n\n\ndef plotBestFit(theta, dataMat, labelMat, title='Gradient Descent', subplt=111):\n    \"\"\"绘制图像\n\n    Arguments:\n        theta {array} -- 参数列表\n        dataMat {array} -- 样本集\n        labelMat {array} -- 标签集\n\n    Keyword Arguments:\n        title {str} -- 图像标题 (default: {'Gradient Descent'})\n        subplt {int} -- 子图 (default: {111})\n    \"\"\"\n    # 存储分类,所有标签值0的为一类,标签值1的为一类\n    xcord0 = []\n    ycord0 = []\n    xcord1 = []\n    ycord1 = []\n    # 分类\n    for i in range(len(dataMat)):\n        if labelMat[i] == 1:\n            xcord1.append(dataMat[i][1])\n            ycord1.append(dataMat[i][2])\n        else:\n            xcord0.append(dataMat[i][1])\n            ycord0.append(dataMat[i][2])\n    plt.subplot(subplt)\n    plt.scatter(xcord0, ycord0, s=30, c='red', marker='s', label='failed')\n    plt.scatter(xcord1, ycord1, s=30, c='green', label='success')\n    x0_min = dataMat[:, 1].min()\n    x0_max = dataMat[:, 1].max()\n    x = np.arange(x0_min, x0_max, 0.1)\n    y = (-theta[0]-theta[1]*x)/theta[2]\n    plt.plot(x, y)\n    plt.xlabel('X1')\n    plt.ylabel('X2')\n    plt.title(title)\n    plt.legend()\n\n\nif __name__ == \"__main__\":\n    # 读取数据集,标签集\n    dataMat, labelMat = loadDataSet_iris()\n    m = len(dataMat)\n    # 特征归一化(特征缩放)\n    dataMat[:, :] = auto_norm(dataMat[:, :])\n    # 所有数据的特征增加一列x0为1\n    dataMat = np.column_stack((np.ones(m), dataMat))\n    # 交叉验证:将数据打乱\n    rndidx = np.arange(m)\n    np.random.shuffle(rndidx)\n    shuffledX = []\n    shuffledy = []\n    for i in range(m):\n        shuffledX.append(dataMat[rndidx[i]])\n        shuffledy.append(labelMat[rndidx[i]])\n    dataMat, labelMat = np.array(shuffledX), np.array(shuffledy)\n    X, y = np.array(dataMat), np.array(labelMat)\n    mTrain = int(0.75*m)\n    mTest = m-mTrain\n    # 获取前mTrain个数据做训练数据,用于训练模型\n    Xtrain, ytrain = np.array(dataMat[:mTrain]), np.array(labelMat[:mTrain])\n    # 获取后mTest个数据做测试数据,用于测试预测准确率\n    Xtest, ytest = np.array(dataMat[-mTest:]), np.array(labelMat[-mTest:])\n    # 画布大小\n    plt.figure(figsize=(13, 6))\n    # 使用BGD批量梯度下降法\n    theta = batch_gradient_desc(Xtrain, ytrain)\n    # 拿测试数据放入模型,计算预测准确率\n    bgd_acc = accuracy_rate(Xtest, ytest, theta)\n    # 绘图\n    plotBestFit(theta, X, y, 'BGD(Batch Gradient Descent) accuracy %.2f' %\n                bgd_acc, 121)\n    # 使用SGD随机梯度下降法\n    theta = stochastic_gradient_desc(Xtrain, ytrain)\n    # 拿测试数据放入模型,计算预测准确率\n    sgd_acc = accuracy_rate(Xtest, ytest, theta)\n    # 绘图\n    plotBestFit(theta, X, y, 'SGD(Stochastic Gradient Descent) accuracy %.2f' %\n                sgd_acc, 122)\n    plt.show()\n\n```\n上述代码文件[下载](LogisticRegression.py)\n### Logistic回归用于多分类(one-vs-all)介绍\n这个思想很容易理解,我们假设现在是三分类问题,如下图\n{% asset_img 2019052823285119.png %}\n现在有一个数据集,有3个类别,我们要做的是选定其中一个类别数据作为正类$y=1$,其余类别作为负类,这样就转换为二分类问题了,这正是Logistic回归擅长处理的问题,得到这个模型,记为$h_\\theta^{(1)}(x)$,接着选另一类作为正类$(y=1)$,将其余类作为负类,同样用Logistic回归处理,得到模型,记为$h_\\theta^{(2)}(x)$,最后同样处理得到$h_\\theta^{(3)}(x)$\n这样,我们就得到了一系列模型简记为:$h_\\theta^{(i)}=p(y=i|x;\\theta)$其中$i=(1,2,\\cdots,k)$(k表示共有k个类)\n{% asset_img 2019052823372620.png %}\n如上图所示,我们每幅图都选中了一个类别作为正类(为该类原本的形状),其余类别作为负类(为圆形)\n现在要做的就是在这些分类器里,输入$x$,然后选择一个让$h_\\theta^{(i)}(x)$最大的$i$,即$max_i{h_\\theta^{(i)}}\n$\n### Logistic回归用于多分类(one-vs-all)程序运行结果\n以下是自己写的logistic回归实现的多分类方法和使用`sklearn.linear_model`中的`LogisticRegression`来进行对比\n{% asset_img 2019052917235832.png %}\n左边为原始数据展示,中间为自己实现的one-vs-all多分类测试结果,右边为sklearn测试结果,预测率一模一样,在调试过程中,两个预测准确率的精度连小数都是一模一样,有时候自己的反而更高,说明方法应该没有差错\n### Logistic回归用于多分类(one-vs-all)代码实现\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import make_classification\n\ndef loadDataSet_iris():\n    \"\"\"数据集生成\n\n    Returns:\n        array -- 数据集\n        array -- 标签集\n    \"\"\"\n    dataMat, labelMat = load_iris(return_X_y=True)  # 多分类(3类)\n    #dataMat, labelMat = dataMat[:100],labelMat[:100] # 二分类\n    n = len(dataMat[0])\n    return dataMat, labelMat\n\n\ndef sigmoid(z):\n    return 1 / (1+np.exp(-z))\n\n\ndef hypo(x, theta):\n    \"\"\"假设函数(hypothesis function)\n\n    Arguments:\n        theta {array} -- 参数值\n        x {array} -- 一个数据或数据集\n\n    Returns:\n        array -- 通过假设函数得到的预测值\n    \"\"\"\n    return sigmoid(np.dot(x, theta))\n\n\ndef partial_jcost(theta, X, y, x):\n    \"\"\"J代价函数关于theta的偏导数\n\n    Arguments:\n        theta {array} -- 参数值\n        X {array} -- 数据集\n        y {array} -- 标签集\n        x {array} -- 一个数据或数据集\n\n    Returns:\n        array -- 偏导值\n    \"\"\"\n    return np.dot((hypo(X, theta)-y), x)\n\n\ndef predict(X, hDict):\n    \"\"\"预测结果\n\n    Arguments:\n        X {array} -- 测试数据集\n        hDict {dict} -- 存储每个类别的参数值\n\n    Returns:\n        array -- 返回预测类别结果\n    \"\"\"\n    m = len(X)\n    y_predict = []\n    for i in range(m):\n        maxPro = -1.0\n        maxLabel = 0\n        for key in hDict.keys():\n            theta = hDict[key]\n            pro = hypo(X[i], theta)\n            if pro > maxPro:\n                maxPro = pro\n                maxLabel = key\n        y_predict.append(maxLabel)\n    return y_predict\n\n\ndef accuracy_rate(X, y, hDict):\n    \"\"\"计算预测准确率\n\n    Arguments:\n        X {array} -- 测试数据集\n        y {array} -- 已知测试结果\n        hDict {dict} -- 存储每个类别的参数值\n\n    Returns:\n        float -- 准确率\n    \"\"\"\n    m, n = len(X), len(X[0])\n    y_predict = predict(X, hDict)\n    true_count = np.sum(y_predict == y)\n    return float(true_count)/m\n\n\ndef batch_gradient_desc(X, y, alpha=0.01, numIterations=500):\n    \"\"\"BGD下降法(批量梯度下降法)\n\n    Arguments:\n        X {array} -- 数据集\n        y {array} -- 标签集\n\n    Keyword Arguments:\n        alpha {float} -- 步长(学习率) (default: {0.01})\n        numIterations {int} -- 迭代次数 (default: {500})\n\n    Returns:\n        array -- 返回参数值\n    \"\"\"\n    # 获取特征数n\n    n = len(X[0])\n    theta = np.ones(n)\n    # 批量梯度下降法\n    for i in range(numIterations):\n        theta = theta-alpha*partial_jcost(theta, X, y, X)\n    return theta\n\n\ndef stochastic_gradient_desc(X, y, alpha=0.01, numIterations=100):\n    \"\"\"SGD下降法(随机梯度下降法)\n\n    Arguments:\n        X {array} -- 数据集\n        y {array} -- 标签集\n\n    Keyword Arguments:\n        alpha {float} -- 步长(学习率) (default: {0.01})\n        numIterations {int} -- 迭代次数 (default: {100})\n\n    Returns:\n        array -- 返回参数值\n    \"\"\"\n    # 获取样本数m,特征数n\n    m, n = len(X), len(X[0])\n    theta = np.ones(n)\n    # 随机梯度下降法\n    for k in range(numIterations):\n        dataIndex = range(m)\n        for i in range(m):\n            alpha = 4/(1.0+k+i)+0.01\n            randIndex = int(np.random.uniform(0,len(dataIndex)))\n            theta=theta-alpha*np.multiply(hypo(X[randIndex], theta)-y[randIndex],X[randIndex])\n    return theta\n\n\ndef auto_norm(X):\n    \"\"\"特征归一化(或特征缩放)\n\n    Arguments:\n        X {array} -- 数据集\n\n    Returns:\n        array -- 返回归一化后的数据集\n    \"\"\"\n    X = np.array(X)\n    n = len(X[0])\n    minVals = X.min(0)\n    maxVals = X.max(0)\n    newVals = (X-minVals)/(maxVals-minVals)\n    return newVals\n\n\ndef plotBestFit(dataMat, labelMat, title='Gradient Descent', subplt=111):\n    \"\"\"绘制图像\n\n    Arguments:\n        theta {array} -- 参数列表\n        dataMat {array} -- 样本集\n        labelMat {array} -- 标签集\n\n    Keyword Arguments:\n        title {str} -- 图像标题 (default: {'Gradient Descent'})\n        subplt {int} -- 子图 (default: {111})\n    \"\"\"\n    dataDict = {}\n    # 分类绘图\n    for i in range(len(dataMat)):\n        if labelMat[i] not in dataDict:\n            dataDict[labelMat[i]]=[]\n        dataDict[labelMat[i]].append(dataMat[i])\n    plt.subplot(subplt)\n    for y in dataDict.keys():\n        X=np.array(dataDict[y])\n        plt.scatter(X[:,1], X[:,2], s=30, label=y)\n    plt.xlabel('X1')\n    plt.ylabel('X2')\n    plt.title(title)\n    plt.legend()\n\n\ndef train(X, y):\n    \"\"\"训练模型,获取每个类别的参数值\n    \n    Arguments:\n        X {array} -- 输入训练数据集\n        y {array} -- 输入训练标签集\n    \n    Returns:\n        dict -- 返回每个类别的参数值\n    \"\"\"\n    dataDict = {}\n    m = len(y)\n    for i in range(m):\n        if y[i] not in dataDict.keys():\n            dataDict[y[i]] = []\n        dataDict[y[i]].append(X[i])\n    hDict = {}\n\n    for key in dataDict.keys():\n        dataX = []\n        dataY = []\n        for k, v in dataDict.items():\n            dataX.extend(v)\n            if k == key:\n                dataY.extend(np.ones(len(v)))\n            else:\n                dataY.extend(np.zeros(len(v)))\n        theta = batch_gradient_desc(dataX, dataY)\n        hDict[key] = theta\n    return hDict\n\n\nif __name__ == \"__main__\":\n    # 读取数据集,标签集\n    dataMat, labelMat = loadDataSet_iris()\n    m = len(dataMat)\n    # 特征归一化(特征缩放)\n    dataMat[:, :] = auto_norm(dataMat[:, :])\n    # 所有数据的特征增加一列x0为1\n    dataMat = np.column_stack((np.ones(m), dataMat))\n    # 交叉验证:将数据打乱\n    rndidx = np.arange(m)\n    np.random.shuffle(rndidx)\n    shuffledX = []\n    shuffledy = []\n    for i in range(m):\n        shuffledX.append(dataMat[rndidx[i]])\n        shuffledy.append(labelMat[rndidx[i]])\n    dataMat, labelMat = np.array(shuffledX), np.array(shuffledy)\n    X, y = np.array(dataMat), np.array(labelMat)\n    mTrain = int(0.75*m)\n    mTest = m-mTrain\n    # 获取前mTrain个数据做训练数据,用于训练模型\n    Xtrain, ytrain = np.array(dataMat[:mTrain]), np.array(labelMat[:mTrain])\n    # 获取后mTest个数据做测试数据,用于测试预测准确率\n    Xtest, ytest = np.array(dataMat[-mTest:]), np.array(labelMat[-mTest:])\n    # 画布大小\n    plt.figure(figsize=(13, 6))\n\n    # 显示原始数据\n    plotBestFit(X, y, 'Original Data', 131)\n\n    # 自己实现的LogisticRegression预测\n    hDict = train(Xtrain, ytrain)\n    y_predict = predict(Xtest, hDict)\n    sgd_acc = accuracy_rate(Xtest, ytest, hDict)\n    plotBestFit(Xtest, y_predict, 'Prediction Data (accuracy %.2f)' %\n                sgd_acc, 132)\n\n    # sklearn LogisticRegression预测\n    clf = LogisticRegression(random_state=0, solver='lbfgs',\n                             multi_class='multinomial').fit(X, y)\n    clf.fit(Xtrain[:, 1:], ytrain)   # sklearn不需要前面的x0,直接放入特征即可\n    y_predict = clf.predict(Xtest[:, 1:])\n    sgd_acc = np.sum(y_predict == ytest)/len(ytest) # 直接用clf.score()也可以\n    plotBestFit(Xtest, y_predict,\n                'Sklearn Prediction Data (accuracy {0:.2f})'.format(sgd_acc), 133)\n\n    plt.show()\n\n```\n\n代码文件[下载](LogisticRegression(one-vs-all).py)\n### 数学补充\n- **梯度**\n    **定义**:一个标量函数$f(x_1,x_2,\\cdots,x_n)$的梯度(或梯度向量场)被表示为$\\nabla f$或者$\\overrightarrow{\\nabla}f$,$\\nabla$表示 **微分算子**.标记$gradf$也被称为梯度.$f$的梯度被定义为唯一的向量场，它与任意单位向量$\\mathbf e$在每一点$x$上的点积是$f$沿着$\\mathbf e$的方向导数,即\n    $$\\begin{aligned}\n        (\\nabla{f(x)})\\cdot\\mathbf v=D_{\\mathbf v}{f(x)}\\quad (4)\\\\\n    \\end{aligned}\n    $$\n    > **注释**: $D_\\mathbf{v}{f(x)}$表示$f(x)$沿着向量\n    > $\\mathbf v$方向的方向导数,例如:\n    > 假设在三维空间$\\mathbf v=(\\cos\\alpha,\\cos\\beta,\\cos\\gamma)$,则(4)为\n    >$$\\begin{aligned}\n    >    (\\nabla{f(x)})\\cdot\\mathbf v&=(\\frac{\\partial f}{\\partial x},\\frac{\\partial f}{\\partial y},\\frac{\\partial f}{\\partial z})\\cdot{(\\cos\\alpha,\\cos\\beta,\\cos\\gamma)}\\\\\n    >    &=\\frac{\\partial f}{\\partial x}\\cos\\alpha+\\frac{\\partial f}{\\partial y}\\cos\\beta+\\frac{\\partial f}{\\partial z}\\cos\\gamma\\\\\n    >    &=D_{\\mathbf v}f(x)\n    >\\end{aligned}\n    >$$\n    **常用**:\n    1. $\\nabla f=gradf=\\frac{\\partial f}{\\partial x}\\overrightarrow{\\mathbf{e_x}}+\\frac{\\partial f}{\\partial y}\\overrightarrow{\\mathbf{e_y}}+\\frac{\\partial f}{\\partial z}\\overrightarrow{\\mathbf{e_z}}$\n    2. 线性特性:\n       1. $\\nabla (\\alpha f+\\beta g)(a)=\\alpha \\nabla f(a)+\\beta \\nabla g(a)$\n    3. 乘积特性:\n       1. $\\nabla (fg)(a)=f(a)\\nabla g(a)+g(a)\\nabla f(a)$\n\n- **向量微分算子**\n    **定义**: $\\nabla$被称为向量微分算子,在坐标$(x_1,x_2,\\cdots,x_n)$和标准基为$\\{\\overrightarrow{\\mathbf e_1},\\cdots,\\overrightarrow{\\mathbf e_n}\\}$的笛卡尔坐标系$\\mathbf R^n$中,$\\nabla$被按照偏导数的方式定义为\n    $$\\begin{aligned}\n        \\nabla=\\sum_{i=1}^n{\\overrightarrow{\\mathbf e_i}\\frac{\\partial}{\\partial x_i}=(\\frac{\\partial}{\\partial{x_1}},\\cdots,\\frac{\\partial}{\\partial{x_n}})}\\tag 5\n    \\end{aligned}\n    $$\n    在坐标$(x,y,z)$和标准基为$\\{\\overrightarrow{\\mathbf{e_x}},\\overrightarrow{\\mathbf{e_y}},\\overrightarrow{\\mathbf{e_z}}\\}$的三维笛卡尔坐标系$\\mathbf R^3$下,$\\nabla$被写成\n    $$\\begin{aligned}\n        \\nabla=\\overrightarrow{\\mathbf e_x}\\frac{\\partial}{\\partial{x}}+\\overrightarrow{\\mathbf e_y}\\frac{\\partial}{\\partial{y}}+\\overrightarrow{\\mathbf e_z}\\frac{\\partial}{\\partial{z}}=(\\frac{\\partial}{\\partial x},\\frac{\\partial}{\\partial y},\\frac{\\partial}{\\partial z})\\tag 6\n    \\end{aligned}\n    $$\n    **常用**:\n    1. Laplacian算子$\\Delta$(标量操作符): $\\Delta=\\frac{\\partial^2}{\\partial x^2}+\\frac{\\partial^2}{\\partial y^2}+\\frac{\\partial^2}{\\partial z^2}=\\nabla\\cdot\\nabla=\\nabla^2$\n    2. 乘积规则:\n        {% asset_img 201905252315313.png %}\n\n### 参考\n1. [<<概率论与数理统计>>第7章 参数估计](https://book.douban.com/subject/3165271/)\n2. [Python3《机器学习实战》学习笔记（六）：Logistic回归基础篇之梯度上升算法](https://blog.csdn.net/c406495762/article/details/77723333)\n3. [wiki Gradient](https://en.wikipedia.org/wiki/Gradient)\n4. [梯度上升与梯度下降](https://www.cnblogs.com/hitwhhw09/p/4715030.html#commentform)\n5. [梯度下降（Gradient Descent）小结](https://www.cnblogs.com/pinard/p/5970503.html)\n6. [吴恩达机器学习个人笔记](#)","tags":["机器学习实战"],"categories":["机器学习"]},{"title":"[hdoj]1010.Tempter of the Bone","url":"%2Fposts%2F5533e030%2F","content":"{% asset_img 2019052403501231.png %}\n\n### 题目含义\n在$N$行$M$列的迷宫中,若能从起点到达终点(不能重复走已经走过的路)并恰好用$T$步到达,则输出$YES$,否则$NO$.($1\\lt N,M\\lt 7;\\ 0\\lt T\\lt 50$)\n'$X$':墙,不能通过, '$S$':起点, '$D$':终点, '.':空地\n### 解题思路(dfs(回溯)+剪枝)\n剪枝:即把不会产生答案的,或不必要的枝条\"剪掉\"\n\n剪枝(1):确定上下界,可以走的最少步>T||可以走的最少步<T,这两种情况都不可能,需要剪掉\n剪枝(2):有限步内,其它走法(绕道)到达终点要比曼哈顿距离多走偶数步\n\n\n> **曼哈顿距离** :图中红线代表曼哈顿距离，绿色代表欧氏距离，也就是直线距离，而蓝色和黄色代表等价的曼哈顿距离。\n> {% asset_img 2019052404001232.jpg %}\n\n\n### 代码实现 \n```cpp\n#include <cstdio>\n#include <cstdlib>\n#include <cstring>\n#define MAXN 10\nchar maze[MAXN][MAXN];\nint dx[4] = { -1,1,0,0 }, dy[4] = { 0,0,-1,1 };\nint N, M, T, sx, sy, ex, ey;\nbool dfs(int x, int y, int t)\n{\n    int nx, ny;\n    if (x < 0 || x >= N || y < 0 || y >= M) return false;\n    if (t == T && x == ex && y == ey)\n        return true;\n    int tmp = (T - t) - abs(x - ex) + abs(y - ey);\n    if (tmp < 0 || tmp&1)// 剪枝(1):可以走的最少步<T,剪枝(2):其它走法(绕道)到达终点要比曼哈顿距离多走偶数步\n        return false;\n    for (int i = 0; i < 4; i++)\n    {\n        nx = x + dx[i];\n        ny = y + dy[i];\n        if (maze[nx][ny] != 'X')\n        {\n            maze[nx][ny] = 'X';\n            if (dfs(nx, ny, t + 1)) return true;\n            maze[nx][ny] = '.';\n        }\n    }\n    return false;\n}\nint main()\n{\n#ifdef _L\n    read_from_file\n#endif\n    while (~scanf(\"%d%d%d\", &N, &M, &T))\n    {\n        getchar();\n        if (!N && !M && !T) break;\n        int wall = 0;\n        memset(maze, 0, sizeof(maze));\n        for (int i = 0; i < N; i++)\n        {\n            for (int j = 0; j < M; j++)\n            {\n                scanf(\"%c\", &maze[i][j]);\n                if (maze[i][j] == 'S')\n                    sx = i, sy = j;\n                else if (maze[i][j] == 'X')\n                    wall++;\n                else if (maze[i][j] == 'D')\n                    ex = i, ey = j;\n            }\n            getchar();\n        }\n        if (N*M - wall <= T)    // 剪枝(1),可以走的最多步都<T\n            printf(\"NO\\n\");\n        else\n        {\n            maze[sx][sy] = 'X';\n            if (dfs(sx, sy, 0))\n                printf(\"YES\\n\");\n            else printf(\"NO\\n\");\n        }\n    }\n    return 0;\n}\n```","tags":["回溯"],"categories":["OJ"]},{"title":"[algorithm]常用基础","url":"%2Fposts%2F72dd6e90%2F","content":"{% asset_img  %}\n\n### 数论\n1. **质数**\n   - 素数判断\n        ```cpp\n        bool is_prime(int n)\n        {\n            if (n==1) return 0;\n            int sqt = sqrt(n);\n            for (i=2;i<=sqt;++i)\n                if (n%i == 0) return 0;\n            return 1;\n        }\n        ```\n   - 获取所有n以内的素数(Eratosthenes筛选法求解质数)\n        ```cpp\n        vector<int> allPrimes(int n)\n        {\n            vector<int> primes(n+1,1);\n            int sqt=sqrt(n),i=0,j=0;\n            for(i=2;i<=sqt;++i)\n            {\n                if (primes[i])\n                {\n                    for(j=2*i;j<=n;j+=i)\n                        primes[j]=0;\n                }\n            }\n            return primes;\n        }\n        ```\n        下图为Eratosthenes筛选法求解质数的过程\n        {% asset_img 20190108153428899.gif %}","tags":["基础算法"],"categories":["OJ"]},{"title":"[leetcode]279.完全平方数","url":"%2Fposts%2F9176b286%2F","content":"{% asset_img 2019052413473237.png %}\n\n### 解题思路 \n假设dp[i]表示和为i的最少完全平方数.\n那么就有:\n$$\\begin{aligned}\n    dp[i]=min(dp[i],dp[i-j*j]+1), \\quad j=1,2,\\cdots,i-j*j\n\\end{aligned}\n$$\n$min$中的+1表示要加最后$j*j$这个数,也就是$i-j*j+j*j=i$,完全平方数的和才能为$i$\n这个式子是什么意思呢?\n比如n=10,那么\ndp[10]=min(dp[10],dp[10-1]+1), j=1, 要求出dp[9]\ndp[10]=min(dp[10],dp[10-4]+1), j=2, 要求出dp[6]\ndp[10]=min(dp[10],dp[10-9]+1), j=3, 要求出dp[1]\n求dp[9]=min(dp[9],dp[9-1]+1), j=1, 要求出dp[8]\n求dp[9]=min(dp[9],dp[9-4]+1), j=2, 要求出dp[5]\n求dp[9]=min(dp[9],dp[9-9]+1), j=3, 要求出dp[0]\n同样求dp[8],dp[5],dp[0]...\n因此,我们可以顺序求出dp[0],dp[1],...,dp[n]即可\n初始条件:dp[i]=i,每个数都可以化为i个1相加,这样组成和的完全平方数的个数最多\n边界条件:dp[0]=0,0没有完全平方数,所以个数是0\n\n### 代码实现 \n```cpp\n    int numSquares(int n) {\n        //利用动态规划 定义长度为n+1的数组 对应索引所对应的数装最少的完全平方数\n        vector<int> dp(n+1,0);\n        dp[0] = 0;  // 边界条件,0没有完全平方数\n        for (int i=1;i<=n;++i)\n        {\n            dp[i]=i;    // i个1,即初始为最多完全平方数个数\n            for (int j=1; i-j*j>=0;++j)\n                dp[i]=min(dp[i],dp[i-j*j]+1);\n        }\n        return dp[n];\n```\n时间复杂度$O(n^2)$","tags":["dp"],"categories":["OJ"]},{"title":"[leetcode]174.地下城游戏","url":"%2Fposts%2F523ea935%2F","content":"{% asset_img 2019052120381514.png %}\n\n### 题目含义\n玩过魔塔游戏的人都知道,假设你是个`肉盾`(不具备攻击力),如果你的`生命值`$HP$比恶魔的`攻击力`$DPS$高,那么必然可以扛过去,那么最后剩余的$HP_{remain}=HP-DPS$,只有当$HP_{remain}>0$时,你才能继续游戏.不过地图中也有`补给`(血瓶)可以加血,可以给自己补充`生命值`(活着才能喝)\n那么这道题的意思就是让你计算你在起点处(左上角第一个方格)的初始生命值最少为多少时,可以一路 `砍怪升(tao)级(ming)` 不**Game Over**($HP\\le 0$)的情况下找到公主(右下角第一个方格).\n\n### 解题思路 \n经过上面的抽象解释含义,大家应该对题目有所理解了\n- **首先,我们考虑从`左上至右下`的方法**\n    如果用dp[i][j]来表示骑士从起点(0,0)到达(i,j)所需的最少血量的话,那么我们就需要掌握两个因素,一个是骑士到达每一格时的血量是否比这一路来的最少血量还要少,因此要记录两个量,比较麻烦,比如dp[1][1]的最少血量如何计算呢?得知道上方一格dp[0][1]的值和最少血量,还得知道左边一个dp[1][0]的值和最少血量,然后算算min(dp[0][1]+dp[1][1],dp[0][1]处的最少血量),min(dp[1][0]+dp[1][1],dp[1][0]处的最少血量)...**我们换种思路(做题时如果发现一种思路不行一定要换个方向思考)**\n- **我们考虑从`右下至左上`的方法**\n    如果用dp[i][j]来表示骑士从点(i,j)到达终点(m-1,n-1)所需的最少血量的话,那么我们就可以倒推了,那么我们假设骑士已经到达了终点(m-1,n-1),我们来计算骑士在终点时所需的血量dp[m-1][n-1]=max(1,1-dungeon[m-1][n-1]),我们来详细解释一下这个公式的含义:\n    $$dp[m-1][n-1]=max(1,1-dungeon[m-1][n-1])\\quad (1)$$\n    骑士到达终点时,如果有恶魔那么血量就得为$HP=1-dungeon[m-1][n-1]$才能救出公主,如果没有恶魔那么骑士在终点处只要活着就行($HP=1$),再说其他情况,先说骑士不在右边缘和下边缘的情况,当骑士在(i,j)时,那么所需的血量为$dp[i][j]=max(1,min(dp[i+1][j],dp[i][j+1])-dungeon[i][j])$,这个式子有点长,我们分解一下:\n    $$\n    \\begin{aligned}\n        need &= min(dp[i+1][j],dp[i][j+1])-dungeon[i][j]\\quad &(2)\\\\\n        dp[i][j] &= max(1,need)\\quad                          &(3)\n    \\end{aligned}\n    $$\n    公式(1)中$need$表示所需的血量,要想知道骑士在$(i,j)$位置的血量,就需要知道骑士在$(i+1,j)$和$(i,j+1)$位置处的血量为多少,然后选择最小的来减去(i,j)处的值,选最小的血量(正)是为了减去$dungeon[i][j]$(有恶魔为负)保证计算得到一个较小的血量,这样就可以保证最后计算到起点时是最少的血量,而公式(2)是为了保证骑士必须活着($HP\\ge 1$),你可能会想:难道$need\\le 0$吗?是的,如果$dungeon[i][j]$处不是恶魔,而是血瓶($HP+9999999$)那么得到的$need$必然为一个负值,因此,这样一来就把骑士从$(i,j)$到终点$(m-1,n-1)$处所需的血量计算出来了,只需一步一步倒退到起点$(0,0)$即可计算出$dp[0][0]$的值,也就是骑士在$(0,0)$到终点$(m-1,n-1)$所需的最少血量.\n    哦!对了,差点忘了,还有边缘没分析,边缘情况与上面情况相同仍然可以用上面的公式(2)(3),但有一些特殊,如果骑士在右边缘,那么(2)的$min(dp[i+1][j],dp[i][j+1])$就要改为$min(dp[i+1][j],MAX\\_INT)$了,因为骑士如果越界那么必然不可能,那么可以默认骑士在墙处的生命值为极大$MAX\\_INT$,公式即为\n    $$\n    \\begin{aligned}\n        need &= min(dp[i+1][j],INT\\_MAX)-dungeon[i][j]\\quad &(4)\\\\\n        dp[i][j] &= max(1,need)\\quad                       &(5)\n    \\end{aligned}\n    $$\n    同样,如果骑士在下边缘,公式即为\n    $$\n    \\begin{aligned}\n        need &= min(dp[i][j+1],INT\\_MAX)-dungeon[i][j]\\quad &(6)\\\\\n        dp[i][j] &= max(1,need)\\quad                       &(7)\n    \\end{aligned}\n    $$\n    经过以上分析,我们就可以开始写代码了\n### 代码实现 \n- **步骤清晰的代码**\n  ```cpp\n    int calculateMinimumHP(vector<vector<int>>& dungeon) {\n        int m=dungeon.size(),n=dungeon[0].size(),i=0,j=0,need=0;\n        vector<vector<int>> dp(m,vector<int>(n,0));\n        // 计算骑士在终点时的HP\n        dp[m-1][n-1]=max(1,1-dungeon[m-1][n-1]);\n        // 计算骑士在右边缘时的HP\n        for(i=m-2,j=n-1;i>=0;--i)\n        {\n            need=min(dp[i+1][j],INT_MAX)-dungeon[i][j];\n            dp[i][j]=max(1,need);\n        }\n        // 计算骑士在下边缘时的HP\n        for(i=m-1,j=n-2;j>=0;--j)\n        {\n            need=min(dp[i][j+1],INT_MAX)-dungeon[i][j];\n            dp[i][j]=max(1,need);\n        }\n        // 计算骑士在内部(包括左边缘和上边缘)时的HP\n        for(i=m-2;i>=0;--i)\n        {\n            for(j=n-2;j>=0;--j)\n            {\n                need=min(dp[i+1][j],dp[i][j+1])-dungeon[i][j];\n                dp[i][j]=max(1,need);\n            }\n        }\n        return dp[0][0];\n    }\n  ```\n- **精炼后的代码**\n  ```cpp\n    int calculateMinimumHP(vector<vector<int>>& dungeon) {\n        // dp[i][j]表示从(i,j)到终点(m-1,n-1)需要的最少生命值\n        // 骑士在外圈时需要生命值INT_MAX(因为越界)\n        // 倒推,从终点到起点来算,每一个位置上,骑士需要的最少生命值为min(下方位置最少生命值,右方位置最少生命值)-当前位置要消耗多少生命值\n        int m=dungeon.size(),n=dungeon[0].size(),i=0,j=0;\n        vector<vector<int>> dp(m+1,vector<int>(n+1,INT_MAX));\n        dp[m-1][n]=dp[m][n-1]=1;\n        for(i=m-1;i>=0;--i)\n        {\n            for(j=n-1;j>=0;--j)\n            {\n                int need = min(dp[i+1][j],dp[i][j+1]) - dungeon[i][j];    // 计算骑士从(i,j)处到终点(m-1,n-1)所需的最少血量\n                dp[i][j] = max(1,need); // 如果所需need为负表示血量远远足够,那么必须保证骑士在该点处时的生命值为1(活着)\n            }\n        }\n        return dp[0][0];\n    }\n  ```\n时间复杂度$O(m*n)$\n### 结论\n`动态规划`真(cao)棒(dan)!","tags":["dp"],"categories":["OJ"]},{"title":"[leetcode]动态规划Top down & Bottom up","url":"%2Fposts%2F25b36638%2F","content":"### 介绍\n**动态规划(DP:dynamic programming)**与**分治法**相似,都是通过组合子问题来求解原问题,但两者是有区别的\n- **分治法:** 将问题划分为互不相交的子问题,递归的求解子问题(过程中会反复进行计算,做许多不必要的工作),再把它们的解组合起来,求出原问题的解.\n- **动态规划:** 将问题划分为重叠的子问题,即不同的子问题具有公共的子子问题,对每个子子问题只求解一次(记忆性),不再进行重复的计算,避免了不必要的计算工作(核心:问题是否能用动态规划解决取决于这些`子问题`会不会被重复调用。)\n动态规划方法通常用来求解最优化问题(optimization problem).这类问题可以有许多可行解,每个解都有一个值,我们希望寻找最优值(最小值或最大值)的解,这样的解称为**一个最优解**,因为最优解可能有多个.\n\n### 动态规划算法基本步骤\n1. 刻画一个最优解特征的模型\n2. 递归定义最优解的值\n3. 计算一个最优解的值,一般用bottom up的方法\n4. 从已经计算完的信息中构造最优解\n\n### 经典的Rod Cutting问题\n有一根长度为$n$的铁棒和一个已知的铁棒长度对应价格表,现让你将铁棒进行切割,问可以获得的最大收益为多少?\n$$\n\\begin{array}{c|lcr}\n    length\\ i &1 &2 &3 &4 &5 &6 &7 &8 &9 &10\\\\\n    \\hline\n    price\\ p_i &1 &5 &8 &9 &10 &17 &17 &20 &24 &30 \n\\end{array}\n$$\n这里$i$表示铁棒长度,$p_i$表示对应长度的价格\n- 题目分析\n    由于铁棒价格表只有长度为1~10的铁棒价格,也就是说,如果n=11,那么直接卖是行的,要切割为n=1+10(其中一种方法),那么收益为1+30元\n    假设n=4,则可以划分的方式和收益如下图所示:\n    {% asset_img 2019052211443317.png %}\n    可以看出$p_2+p_2=5+5=10$这种划分方法收益最大(是一个最优解)\n    如果铁棒长度为$n$,那么我们可以划分铁棒的方法就有$2^{n-1}$种,你可能会惊讶怎么会有这么多种?因为对于铁棒的每一处位置我们都有2种选择,切割或不切割\n- 代码实现\n  - 法一: 递归 $O(^2)$\n    ```cpp\n    int CutRod(vector<int>& p, int n)\n    {\n        if (n == 0) return 0;\n        int maxR = INT_MIN;\n        for (int i = 1; i <= min(n,10); ++i)\n            maxR = max(maxR, p[i] + CutRod(p, n - i));\n        return maxR;\n    }\n    ```\n  - 法二: top-down(记忆化递归)\n    ```cpp\n    int MemoizedCutRodAux(vector<int>& p, int n, vector<int>& r)\n    {\n        int maxR = 0;\n        if (r[n] >= 0)\n            return r[n];\n        if (n == 0)\n            maxR = 0;\n        else\n        {\n            maxR = INT_MIN;\n            for (int i = 1; i <= min(n,10); ++i)\n                maxR = max(maxR, p[i] + MemoizedCutRodAux(p, n - i, r));\n        }\n        r[n] = maxR;\n        return maxR;\n    }\n    ```\n  - 法三: bottom-up $O(n^2)$\n    ```cpp\n    int BottomUpCutRod(vector<int>& p, int n)\n    {\n        vector<int> r(n + 1, 0);\n        for (int i = 1; i <= n; ++i)\n        {\n            int maxR = INT_MIN;\n            for (int j = 1; j <= i; ++j)\n                maxR = max(maxR, p[(j-1)%10+1] + r[i - j]);\n            r[i] = maxR;\n        }\n        return r[n];\n    }\n    ```\n### 相关\n[[leetcode]动态规划(Dynamic Programming)](https://brianyi.github.io/posts/26386)\n### 扩展阅读\n[常见的动态规划问题分析与求解](https://www.cnblogs.com/wuyuegb2312/p/3281264.html)\n[漫画：什么是动态规划？](http://www.sohu.com/a/153858619_466939)\n","tags":["dp"],"categories":["OJ"]},{"title":"[机器学习实战]第4章 基于概率论的分类方法:朴素贝叶斯","url":"%2Fposts%2F28f8d320%2F","content":"### 了解\nKNN分类算法和决策树分类算法最终都是预测出实例的确定的分类结果，但是，有时候分类器会产生错误结果；本章要学的朴素贝叶斯分类算法则是给出一个最优的猜测结果，同时给出猜测的概率估计值。\n### 先验概率,后验概率,全概率公式和贝叶斯公式\n> 一种癌症,得了这种癌症的人被检测出阳性的几率为90%,未得这种癌症的人被检测出阴性的几率为90%,而人群中得这种癌症的几率为1%,一个人被检测出阳性,问这个人得癌症的几率为多少?\n\n我们用$A$表示事件\"测出为阳性\",用$B_1$表示\"得癌症\",$B_2$表示\"未得癌症\".\n则可以得到:\n$$\n\\begin{aligned}\n&P(A|B_1)=0.9,\\ P(A|B_2)=1-0.9=0.1,\\ P(B_1)=0.01,\\ P(B_2)=1-0.01=0.99\\\\\n&P(A,B_1)=P(B_1)\\cdot P(A|B_1)=0.01\\times 0.9=0.009\\\\\n&P(A,B_2)=P(B_2)\\cdot P(A|B_2)=0.99\\times 0.1=0.099\n\\end{aligned}\n$$\n一个人被检测出阳性,且这个人得癌症的几率可被表示为$P(B_1|A)$,那么根据以上结果我们来进行计算:\n$$\n\\begin{aligned}\n    P(B_1|A)&=\\frac{P(B_1)\\cdot P(A|B_1)}{P(A)}=\\frac{P(B_1)\\cdot P(A|B_1)}{P(A,B_1)+P(A,B_2)}\\quad (1)\\\\\n    &=\\frac{0.01\\times 0.9}{0.009+0.099}=\\frac{0.009}{0.108}\\approx 0.083\n\\end{aligned}\n$$\n当然也很容易求得阳性未得癌症的概率$P(B_2|A)=1-P(B_1|A)=1-0.083=0.917$\n这里$P(B_1|A),P(B_2|A)$中都多了一竖线,被称为 **后验概率** ,即在$A$的情况下$B_1$(或$B_2$)发生的概率,后验概率就是一种**条件概率**(但反之不成立), 而$P(A),P(B_1),P(B_2)$就是 **先验概率** ,上述公式(1)中,分母$P(A)=P(A,B_1)+P(A,B_2)$为 **全概率公式**,整个公式(1)就是 **贝叶斯公式**\n- **后验概率与条件概率的区别**\n    后验概率就是一种条件概率,但是与其他条件概率的不同之处在于,它限定了目标事件为隐变量取值,而其中的条件为观测结果\n    一般的条件概率,条件和事件都可以是任意的\n    举例说明:\n    1). 如果我们出门之前我们听到新闻说今天路上出了个交通事故，那么我们想算一下堵车的概率，这个就叫做条件概率 。也就是P(堵车|交通事故)。这是==有因求果==\n    2). 如果我们已经出了门，然后遇到了堵车，那么我们想算一下堵车时由交通事故引起的概率有多大，那这个就叫做后验概率 （其实也是条件概率，但是通常习惯这么说） 。也就是P(交通事故|堵车)。这是==有果求因==\n- **全概率公式**\n    设$B_1,B_2,\\cdots,B_n$满足$\\displaystyle\\bigcup_{i=1}^n B_i=\\Omega,B_iB_j=\\empty(i\\neq j)$且$P(B_k)>0,k=1,2,\\cdots,n,$则对任意事件$A$有\n    $$\\begin{aligned}\n        P(A)=\\sum_{i=1}^n{P(A,B_i)}=\\sum_{i=1}^n{P(B_i)\\cdot P(A|B_i)}\\quad (2)\n    \\end{aligned}\n    $$\n    称满足$\\displaystyle\\bigcup_{i=1}^n{B_i}=\\Omega$和$B_iB_j=\\empty(i\\neq j)$的$B_1,B_2,\\cdots,B_n$为$\\Omega$的一个完备事件组.\n- **贝叶斯公式**\n    设$B_1,B_2,\\cdots,B_n$满足$\\displaystyle\\bigcup_{i=1}^n B_i=\\Omega,B_iB_j=\\empty(i\\neq j)$且$P(A)>0,P(B_k)>0,k=1,2,\\cdots,n,$则\n    $$\\begin{aligned}\n        P(B_j|A)=\\frac{P(B_j)\\cdot P(A|B_j)}{\\sum_{i=1}^n P(B_i)\\cdot P(A|B_i)}, j=1,2,\\cdots,n.\\quad (3)\n    \\end{aligned}\n    $$\n### 朴素贝叶斯算法\n- **介绍**\n    朴素贝叶斯算法是基于贝叶斯定理与特征条件独立假设的分类方法.最为广泛的两种分类模型是决策树模型(Decision Tree)和朴素贝叶斯模型(Naive Bayesian Model,NBM).\n    朴素贝叶斯算法的使用的就是贝叶斯公式,我们将贝叶斯公式应用到机器学习领域即为:\n    $$\\begin{aligned}\n        P(class|feature)=\\frac{P(feature|class)\\cdot P(class)}{P(feature)}\\quad (4)\n    \\end{aligned}\n    $$\n    我们只需求出$P(class|feature)$来加以比较选出最大的即可\n    **朴素贝叶斯算法的朴素一词解释**\n    朴素(naive)也就是简单(天真)的意思,朴素贝叶斯算法的朴素:\n    1.假设各个特征之间相互独立\n    2.假设各个特征同等重要\n    那么就会有\n    $$\\begin{aligned}\n        P(A=a_i,B=b_i,C=c_i|D=d_i)&=P(A=a_i|D=d_i)\\cdot P(B=b_i|D=d_i)\\cdot P(C=c_i|D=d_i)\\quad &(5)\\\\\n        &\\neq P(A=a_i)\\cdot P(B=b_i)\\cdot P(C=c_i)\\quad &(6)\n    \\end{aligned}$$\n    注意:只是说特征$A,B,C$之间相互独立,而不包括D,那么公式(6)就不成立\n    如果没有朴素二字,那么特征之间不一定相互独立,则问题就会变得复杂起来,因为这3个特征的联合概率分布是3维空间,\n    {% asset_img 2019052217342720.png %}\n    现实生活中特征个数是很多的,每一个特征可以取的值也非常多,如果不假设特征之间相互独立的话,那么通过统计来估计概率的值变得几乎不可做(计算量太大)\n\n    ---\n    对于公式(4)在机器学习领域中如何应用,我们来看一个例子,如<span id='jump4-1'>图4-1</span>所示\n    {% asset_img v2-8b7031854b7c8r.jpg %}\n    <p style='text-align:center'>图4-1</p>\n    \n    **现在给我们的问题是，如果一对男女朋友，男生想女生求婚，男生的四个特点分别是不帅，性格不好，身高矮，不上进，请你判断一下女生是嫁还是不嫁？**\n    这是一个典型的分类问题，**转为数学问题就是比较p(嫁|(不帅、性格不好、身高矮、不上进))与p(不嫁|(不帅、性格不好、身高矮、不上进))的概率**，谁的概率大，就能给出嫁或者不嫁的预测\n    这里我们联系到朴素贝叶斯公式:\n    {% asset_img 62ef7babb177ea_r.jpg %}\n    那么我们需要将等式右边分子的后验概率,先验概率和分母的先验概率求出来\n    {% asset_img 2019052218075422.png %}\n    \n\n\n- **朴素贝叶斯算法定义**\n    输入:训练数据$T={(\\mathbf x_1,\\mathbf y_1),(\\mathbf x_2,\\mathbf y_2),\\cdots,(\\mathbf x_N,\\mathbf y_N)}$,其中$\\mathbf x_i=(x_i^{(1)},x_i^{(2)},\\cdots,x_i^{(n)})^T$,$x_i^{(j)}$是第$i$个样本的第$j$个特征,$x_i^{(j)}\\isin{\\{a_{j1},a_{j2},\\cdots,a_{js_j}\\}}$,$a_{jl}$是第$j$个特征可能取的第$l$个值,$j=1,2,\\cdots,n,\\ l=1,2,\\cdots,S_j,\\ y_i\\isin{\\{c_1,c_2,\\cdots,c_K\\}}$;\n    输出:实例$x$的分类.\n    (1).计算先验概率及条件概率\n    $$\\begin{aligned}\n        &P(Y=c_k)=\\frac{\\sum_{i=1}^N{I(y_i=c_k)}}{N},\\ k=1,2,\\cdots,K&\\quad (7)\\\\\n        &P(X^{(j)}=a_{jl}|Y=c_k)=\\frac{\\sum_{i=1}^N{I(x_i^{(j)}=a_{jl},y_i=c_k)}}{\\sum_{i=1}^N{I(y_i=c_k)}}&\\quad (8)\\\\\n        &j=1,2,\\cdots,n;\\ l=1,2,\\cdots,S_j;\\ k=1,2,\\cdots,K\n    \\end{aligned}\n    $$\n    > 注释: $\\sum_{i=1}^N{I(y_i=c_k)}$表示统计一下类别为$c_k$的个数\n    > $\\sum_{i=1}^N{I(x_i^{(j)}=a_{jl},y_i=c_k)}$表示统计一下类别为$c_k$且第$j$个特征的特征值为$a_{jl}$的个数\n\n    (2).对于给定实例$\\mathbf x=(x^{(1)},x^{(2)},\\cdots,x^{(n)})^T$,计算\n    $$\\begin{aligned}\n        P(Y=c_k)\\prod_{j=1}^n{P(X^{(j)}=x^{(j)}|Y=c_k)},\\ k=1,2,\\cdots,K&\\quad (9)\n    \\end{aligned}\n    $$\n    (3).确定实例$\\mathbf x$的类别\n    $$\\begin{aligned}\n        y=\\argmax_{c_k}{P(Y=c_k)}\\prod_{j=1}^n{P(X^{(j)}=x^{(j)}|Y=c_k)}&&\\quad (10)\n    \\end{aligned}\n    $$\n    > 1. 大写字母表示矩阵(如$T$),小写字母粗体表示向量(如$\\mathbf {x_i}$),常规表示数值(如$x_i^{(1)},a_{j1},y_i,c_1$)\n    > 2. $\\mathbf x=(x_1,x_2,\\cdots,x_n)^T$,表示$x$为一个含有$n$个值的列向量,线性代数中一般如果没有特殊说明,都默认为是列向量\n    > 3. $\\mathbf {x_i}$表示第i个样本,每个样本都有n个特征,那么为了方便表示第几个样本的第几个特征,我们采用上标表特征,下标表样本的形式,即$x_i^{(5)}$表示第i个样本的第5个特征的值或特征值,这个$x_i^{(5)}$为什么不是粗体了呢?是因为它是一个数值而不是一个向量\n    > 4. 指示函数$I_A{(x)}$:指示函数是定义在某集合$X$上的函数,表示其中有哪些元素属于某一子集$A$,常用在集合论中.\n    集$X$的子集$A$的指示函数是函数$1_A: X\\to\\{0,1\\}$,定义为\n    $1_A(x)=\\begin{cases}\n        1 &if\\ x\\isin A\\\\\n        0 &if\\ x\\notin A\n    \\end{cases}$\n\n    ---\n    公式解析: \n    我们设$Y$为输出随机变量,表示类别的随机变量,$X$为输入随机变量,表示样本的随机变量,实际求的是$P(Y=c_k|X=x)$,即输入随机变量为样本$\\mathbf x$时,求出类别为$c_k$的概率,利用熟知的朴素贝叶斯公式即\n    $$\\begin{aligned}\n        P(Y=c_k|X=x)&=\\frac{P(X=x|Y=c_k)\\cdot P(Y=c_k)}{P(X=x)}&\\quad (11)\\\\\n        &=\\frac{P(Y=c_k)\\cdot \\prod_{j=1}^n{P(X^{(j)}=x^{(j)}|Y=c_k)}}{P(X=x)}&\\quad (12)\n    \\end{aligned}\n    $$\n    > 公式(12)成立的前提是朴素贝叶斯公式,也即各个特征之间相互独立\n\n    我们通过改变$Y$的值(即改变类别)来找到最大的概率,由于分母不随$Y$而变化,因此可以省去计算分母的过程,直接变为求下面公式(即上面公式(10))\n    $$\\begin{aligned}\n        y=\\argmax_{c_k}P(Y=c_k)\\prod_{j=1}^n{P(X^{(j)}=x^{(j)}|Y=c_k)}\n    \\end{aligned}\n    $$\n\n- **引入**\n    假设现在我们有一个数据集,由两类数据组成,如<span id='jump-4-2'>图4-2</span>所示\n    {% asset_img 2019052215350118.png %}\n    \n    <p style='text-align:center'>图4-2</p>\n    \n    我们用$p_1(x,y)$表示数据点$(x,y)$属于类别1(图中的圆)的概率,$p_2(x,y)$表示数据点$(x,y)$属于类别2(图中的三角形)的概率,现在有一个数据为(x,y),可以用下面规则来判断:\n    如果$p_1(x,y)>p_2(x,y)$,那么类别为1\n    如果$p_2(x,y)>p_1(x,y)$,那么类别为2\n    这就是贝叶斯决策理论的核心思想,即`具有最高概率的决策`\n\n### 实践1:使用朴素贝叶斯进行文档分类\n机器学习的一个重要应用就是文档的自动分类.在文档分类中整个文档(如一封电子邮件)是 **一个实例** ,而文档中的 **某些元素则构成特征** ,我们观察文档中出现的词,并把每个词的出现或者不出现作为一个特征,这样得到的特征数目就会跟词汇表中的词目一样多.`朴素贝叶斯是介绍贝叶斯分类器的一个扩展,是用于文档分类的常用算法`\n由统计学知,如果每个特征需要N个样本,那么10个特征将需要$N^{10}$个样本,对于包含1000个特征的词汇表将需要$N^{1000}$个样本,所需要的样本会随着特征数目增大而指数上升,如果特征之间相互独立,那么样本数就可以从$N^{1000}$减少到$1000\\times N$\n根据前面已有的知识,我们可以开始编写一个可以自动将侮辱性言论分类的贝叶斯分类器\n> 以在线社区为例,了不影响社区发展,我们要屏蔽侮辱性的言论,所以要构建一个快速过滤器,如果某条留言使用了负面或者侮辱性的语言,那么就将该留言标识为内容不当.过滤这类内容是一个很常见的需求,对此问题建立两个类别:侮辱类和费侮辱类,使用1和0分别标识\n\n如何判断一个文档是否是侮辱类的文档呢?\n1. **拆分文本，准备数据**\n    要从文本中获取特征，显然我们需要先拆分文本，这里的文本指的是来自文本的词条，每个词条是字符的任意组合。词条可以为单词，当然也可以是URL，IP地址或者其他任意字符串。将文本按照词条进行拆分，根据词条是否在词汇列表中出现，将文档组成成词条向量，向量的每个值为1或者0，其中1表示出现，0表示未出现。\n    接下来，以在线社区的留言为例。对于每一条留言进行预测分类，类别两种，侮辱性和非侮辱性，预测完成后，根据预测结果考虑屏蔽侮辱性言论，从而不影响社区发展。\n    ```python\n    #---------------------------从文本中构建词条向量-------------------------\n    #1 要从文本中获取特征，需要先拆分文本，这里特征是指来自文本的词条，每个词\n    #条是字符的任意组合。词条可以理解为单词，当然也可以是非单词词条，比如URL\n    #IP地址或者其他任意字符串 \n    #  将文本拆分成词条向量后，将每一个文本片段表示为一个词条向量，值为1表示出现\n    #在文档中，值为0表示词条未出现\n\n\n    #导入numpy\n    from numpy import *\n\n    def loadDataSet():\n    #词条切分后的文档集合，列表每一行代表一个文档\n        postingList=[['my','dog','has','flea',\\\n                    'problems','help','please'],\n                    ['maybe','not','take','him',\\\n                    'to','dog','park','stupid'],\n                    ['my','dalmation','is','so','cute',\n                    'I','love','him'],\n                    ['stop','posting','stupid','worthless','garbage'],\n                    ['my','licks','ate','my','steak','how',\\\n                    'to','stop','him'],\n                    ['quit','buying','worthless','dog','food','stupid']]\n        #由人工标注的每篇文档的类标签\n        classVec=[0,1,0,1,0,1]\n        return postingList,classVec\n\n    #统计所有文档中出现的词条列表    \n    def createVocabList(dataSet):\n        #新建一个存放词条的集合\n        vocabSet=set([])\n        #遍历文档集合中的每一篇文档\n        for document in dataSet:\n            #将文档列表转为集合的形式，保证每个词条的唯一性\n            #然后与vocabSet取并集，向vocabSet中添加没有出现\n            #的新的词条        \n            vocabSet=vocabSet|set(document)\n        #再将集合转化为列表，便于接下来的处理\n        return list(vocabSet)\n\n    #根据词条列表中的词条是否在文档中出现(出现1，未出现0)，将文档转化为词条向量    \n    def setOfWords2Vec(vocabSet,inputSet):\n        #新建一个长度为vocabSet的列表，并且各维度元素初始化为0\n        returnVec=[0]*len(vocabSet)\n        #遍历文档中的每一个词条\n        for word in inputSet:\n            #如果词条在词条列表中出现\n            if word in vocabSet:\n                #通过列表获取当前word的索引(下标)\n                #将词条向量中的对应下标的项由0改为1\n                returnVec[vocabSet.index(word)]=1\n            else: print('the word: %s is not in my vocabulary! '%'word')\n        #返回inputet转化后的词条向量\n        return returnVec\n    ```\n    需要说明的是，上面函数creatVocabList得到的是所有文档中出现的词汇列表，列表中没有重复的单词，每个词是唯一的。\n\n2. **由词向量计算朴素贝叶斯用到的概率值**\n    这里，如果我们将之前的点$(x,y)$换成词条向量$\\mathbf w$(各维度的值由特征是否出现的0或1组成)，在这里词条向量的维度和词汇表长度相同。\n    $$p(c_i|\\mathbf w)=\\frac{p(\\mathbf w|c_i)\\cdot p(c_i)}{p(\\mathbf w)}$$\n    使用该公式计算文档词条向量属于各个类的概率,然后比较概率大小,从而预测出分类结果。\n    具体地，首先，可以通过统计各个类别的文档数目除以总得文档数目，计算出相应的$p(c_i)$；然后，基于条件独立性假设，将$\\mathbf w$展开为一个个的独立特征，那么就可以将上述公式写为$p(\\mathbf w|c_i)=p(w_0|c_i)\\cdot p(w_1|c_i)\\cdots p(w_N|c_i)$,这样就很容易计算，从而极大地简化了计算过程。\n    函数的伪代码为：\n    ```python\n    计算每个类别文档的数目\n    计算每个类别占总文档数目的比例\n    对每一篇文档：\n    　　对每一个类别：\n    　　　　如果词条出现在文档中->增加该词条的计数值 #统计每个类别中出现的词条的次数\n    　　　　增加所有词条的计数值 #统计每个类别的文档中出现的词条总数 \n    　　对每个类别：\n    　　　　将各个词条出现的次数除以类别中出现的总词条数目得到条件概率\n    返回每个类别各个词条的条件概率和每个类别所占的比例\n    ```\n    代码如下:\n    ```python\n    #训练算法，从词向量计算概率p(w0|ci)...及p(ci)\n    #@trainMatrix：由每篇文档的词条向量组成的文档矩阵\n    #@trainCategory:每篇文档的类标签组成的向量\n    def trainNB0(trainMatrix,trainCategory):\n        #获取文档矩阵中文档的数目\n        numTrainDocs=len(trainMatrix)\n        #获取词条向量的长度\n        numWords=len(trainMatrix[0])\n        #所有文档中属于类1所占的比例p(c=1)\n        pAbusive=sum(trainCategory)/float(numTrainDocs)\n        #创建一个长度为词条向量等长的列表\n        p0Num=zeros(numWords);p1Num=zeros(numWords)\n        p0Denom=0.0;p1Denom=0.0\n        #遍历每一篇文档的词条向量\n        for i in range(numTrainDocs):\n            #如果该词条向量对应的标签为1\n            if trainCategory[i]==1:\n                #统计所有类别为1的词条向量中各个词条出现的次数\n                p1Num+=trainMatrix[i]\n                #统计类别为1的词条向量中出现的所有词条的总数\n                #即统计类1所有文档中出现单词的数目\n                p1Denom+=sum(trainMatrix[i])\n            else:\n                #统计所有类别为0的词条向量中各个词条出现的次数\n                p0Num+=trainMatrix[i]\n                #统计类别为0的词条向量中出现的所有词条的总数\n                #即统计类0所有文档中出现单词的数目\n                p0Denom+=sum(trainMatrix[i])\n        #利用NumPy数组计算p(wi|c1)\n        p1Vect=p1Num/p1Denom  #为避免下溢出问题，后面会改为log()\n        #利用NumPy数组计算p(wi|c0)\n        p0Vect=p0Num/p0Denom  #为避免下溢出问题，后面会改为log()\n        return p0Vect,p1Vect,pAbusive\n    ```\n3. **针对算法的部分改进**\n    1. 计算概率时，需要计算多个概率的乘积以获得文档属于某个类别的概率，即计算$p(w_0|c_i)\\cdot p(w_1|c_i)\\cdots p(w_N|c_i)$，然后当其中任意一项的值为0，那么最后的乘积也为0.为降低这种影响，采用拉普拉斯平滑，在分子上添加a(一般为1)，分母上添加ka(k表示类别总数)，即在这里将所有词的出现数初始化为1，并将分母初始化为2*1=2\n        > 拉普拉斯平滑: 其实就是计算概率的时候，对于分子+1，避免出现概率为0。这样乘起来的时候，不至于因为某个量x，在观察样本库（训练集）中没有出现过，会导致整个实例的概率结果是0。在文本分类的问题中，当一个词语没有在训练样本中出现，该词语概率为0，使用连乘计算文本出现概率时也为0。这是不合理的，不能因为一个事件没有观察到就武断的认为该事件的概率是0。\n\n        ```python\n        p0Num=ones(numWords);p1Num=ones(numWords)\n        p0Denom=2.0;p1Denom=2.0\n        ```\n    2. 解决下溢出问题\n        正如上面所述，由于有太多很小的数相乘。计算概率时，由于大部分因子都非常小，最后相乘的结果四舍五入为0,造成下溢出或者得不到准确的结果，所以，我们可以对乘积取自然对数，在代数中有$ln(a\\cdot b)=lna+lnb$,于是通过求对数可以避免下溢出或者浮点数舍入导致的错误。同时采用自然对数处理不会有任何损失。\n        {% asset_img 2019052408342026.png %}\n        于是可以将原代码改为下面代码\n        ```python\n        p0Vect=log(p0Num/p0Denom);p1Vect=log(p1Num/p1Denom)\n        ```\n    下面是朴素贝叶斯分类函数的代码：\n    ```python\n    #朴素贝叶斯分类函数\n    #@vec2Classify:待测试分类的词条向量\n    #@p0Vec:类别0所有文档中各个词条出现的频数p(wi|c0)\n    #@p0Vec:类别1所有文档中各个词条出现的频数p(wi|c1)\n    #@pClass1:类别为1的文档占文档总数比例\n    def classifyNB(vec2Classify,p0Vec,p1Vec,pClass1):\n        #根据朴素贝叶斯分类函数分别计算待分类文档属于类1和类0的概率\n        p1=sum(vec2Classify*p1Vec)+log(pClass1)\n        p0=sum(vec2Classify*p0Vec)+log(1.0-pClass1)\n        if p1>p0:\n            return 1\n        else:\n            return 0\n\n    #分类测试整体函数        \n    def testingNB():\n        #由数据集获取文档矩阵和类标签向量\n        listOPosts,listClasses=loadDataSet()\n        #统计所有文档中出现的词条，存入词条列表\n        myVocabList=createVocabList(listOPosts)\n        #创建新的列表\n        trainMat=[]\n        for postinDoc in listOPosts:\n            #将每篇文档利用words2Vec函数转为词条向量，存入文档矩阵中\n            trainMat.append(setOfWords2Vec(myVocabList,postinDoc))\\\n        #将文档矩阵和类标签向量转为NumPy的数组形式，方便接下来的概率计算\n        #调用训练函数，得到相应概率值\n        p0V,p1V,pAb=trainNB0(array(trainMat),array(listClasses))\n        #测试文档\n        testEntry=['love','my','dalmation']\n        #将测试文档转为词条向量，并转为NumPy数组的形式\n        thisDoc=array(setOfWords2Vec(myVocabList,testEntry))\n        #利用贝叶斯分类函数对测试文档进行分类并打印\n        print(testEntry,'classified as:',classifyNB(thisDoc,p0V,p1V,pAb))\n        #第二个测试文档\n        testEntry1=['stupid','garbage']\n        #同样转为词条向量，并转为NumPy数组的形式\n        thisDoc1=array(setOfWords2Vec(myVocabList,testEntry1))\n        print(testEntry1,'classified as:',classifyNB(thisDoc1,p0V,p1V,pAb))\n    ```\n    这里需要补充一点，上面也提到了关于如何选取文档特征的方法，上面用到的是==词集模型==，`即对于一篇文档，将文档中是否出现某一词条作为特征，即特征只能为0不出现或者1出现`；然后，`一篇文档中词条的出现次数也可能具有重要的信息，于是我们可以采用`==词袋模型==，`在词袋向量中每个词可以出现多次，这样，在将文档转为向量时，每当遇到一个单词时，它会增加词向量中的对应值`\n    > **词集模型(Set Of Words)**： 单词构成的集合，集合自然每个元素都只有一个，也即词集中的每个单词都只有一个。\n    **词袋模型(Bag Of Words)**： 如果一个单词在文档中出现不止一次，并统计其出现的次数（频数）。\n\n    具体将文档转为词袋向量的代码为:\n    ```python\n    def bagOfWords2VecMN(vocabList,inputSet):\n    #词袋向量\n    returnVec=[0]*len(vocabList)\n    for word in inputSet:\n        if word in vocabList:\n            #某词每出现一次，次数加1\n            returnVec[vocabList.index(word)]+=1\n    return returnVec\n    ```\n    利用`词集模型`或`词袋模型`的程序运行结果(本例结果一样):\n    {% asset_img 2019052408541227.png %}\n\n### 实践2:使用朴素贝叶斯过滤垃圾邮件\n{% asset_img 2019052410014929.png %}\n- 准备数据:切分文本\n    由于前面讲述的例子中词向量是预先给定的(通过loadDataSet()),下面介绍如何从文本文档中构建自己的词列表\n    我们使用python中的正则表达式语法和内嵌语法,来实现一封完整的电子邮件的处理,数据集[下载email.zip](email.zip)\n    {% asset_img 2019052410581331.png %}\n    \n- 测试算法:使用朴素贝叶斯进行交叉验证\n  > **交叉验证**: 交叉验证是在机器学习建立模型和验证模型参数时常用的办法。交叉验证，顾名思义，就是重复的使用数据，把得到的样本数据进行切分，组合为不同的训练集和测试集，用训练集来训练模型，用测试集来评估模型预测的好坏。在此基础上可以得到多组不同的训练集和测试集，某次训练集中的某样本在下次可能成为测试集中的样本，即所谓“交叉”。　\n\n    下面将文本解析器集成到一个完整分类器中\n    ```python\n    def textParse(bigString):\n    \"\"\"解析邮件为单词列表\n    \n    Arguments:\n        bigString {str} -- 一封邮件内容\n    \"\"\"\n    # 导入正则表达式\n    import re\n    # 使用正则表达式\n    listOfTokens = re.split(r'\\W+', bigString)\n    # 将所有长度>2的单词小写并构成一个单词列表\n    return [tok.lower() for tok in listOfTokens if len(tok) > 2]\n\n    def spamTest():\n        \"\"\"\n        对贝叶斯垃圾邮件分类器进行自动化处理\n        选取40封邮件做训练数据集,10封邮件做测试数据集,计算分类器的错误率\n        \"\"\"\n        import random\n        docList = []; classList = []; fullText = []\n        # 遍历垃圾邮件,选取50封邮件(垃圾邮件,正常邮件各25封),穿插进行文本提取,标记类别\n        for i in range(1,26):\n            # 解析垃圾邮件为单词列表\n            wordList = textParse(open('email/spam/%d.txt' % i).read())\n            # 加入文档列表集中  \n            docList.append(wordList) \n            # 扩展到全文中   \n            fullText.extend(wordList)   \n            # 标记类别:垃圾邮件\n            classList.append(1)\n            # 解析正常邮件为单词列表\n            wordList = textParse(open('email/ham/%d.txt' % i, encoding='utf-8', errors='ignore').read())\n            # 加入文档列表集中\n            docList.append(wordList)\n            # 扩展到全文中\n            fullText.extend(wordList)\n            # 标记类别:正常邮件\n            classList.append(0)\n        # 创建词汇表\n        vocabList = createVocabList(docList)\n        # 随机生成50个数的列表\n        trainingSet = list(range(50)); testSet = []\n        # 随机选取10个邮件作为测试数据集\n        for i in range(10):\n            randIndex = int(random.uniform(0, len(trainingSet)))\n            testSet.append(trainingSet[randIndex])\n            del(trainingSet[randIndex])\n        trainMat = []; trainClasses = []\n        # 选取剩下的40个邮件作为训练数据集\n        for docIndex in trainingSet:\n            # 邮件转为词向量并追加到训练数据集\n            trainMat.append(setOfWords2Vec(vocabList, docList[docIndex]))\n            # 已知的邮件类别追加到训练数据集\n            trainClasses.append(classList[docIndex])\n        # 获取p(w|c=0),p(w|c=1),p(c=1)\n        # p(c|w)=p(w|c)*p(c)/p(w),由于p(w)不变,可以直接比较p(w|c)*p(c)/p(w)\n        p0V,p1V,pSpam = trainNB0(np.array(trainMat), np.array(trainClasses))\n        errorCount = 0\n        for docIndex in testSet:\n            # 邮件转为词向量\n            wordVector = setOfWords2Vec(vocabList, docList[docIndex])\n            # 计算判错类别的个数\n            if classifyNB(np.array(wordVector), p0V, p1V, pSpam) != classList[docIndex]:\n                errorCount += 1\n                print('classification error ', docList[docIndex])\n        # 显示分类器的错误率\n        print('the error rate is: ', float(errorCount)/len(testSet))\n    ```\n    程序运行结果如下:\n    {% asset_img 2019052412390434.png %}\n    {% asset_img 2019052412395035.png %}\n    spamTest()会输出在10封随机选择的电子邮件上的分类错误率,因为电子邮件是随机选择的,所以每次输出结果可能有些差别.如果发现错误的话,函数会输出错分文档的词表,这样就可以了解到底是哪篇文档发生了错误,总体来说错误率还是相当低的,维持在4%以下(自己测了多次)\n\n### 总结\n- 我们的目的是计算$P(Y=y_j|X=x),j=1,2,\\cdots,n$,求出$\\argmax_{y_j}P(Y=y_j|X=x)$,然而:\n    $$\\begin{aligned}\n        P(Y=y_j|X=x)&=\\frac{P(Y=y_j)\\cdot{P(X=x|Y=y_j)}}{P(X=x)}\\quad j=1,2,\\cdots,n\\\\\n        &=\\frac{P(Y=y_j)\\cdot\\prod_{i=1}^n{P(X^{(i)}=x^{(i)}|Y=y_j)}}{P(X=x)}\\quad i,j=1,2,\\cdots,n\n    \\end{aligned}\n    $$\n    $$\\begin{aligned}\n    \\argmax_{y_j}{P(Y=y_j|X=x)}&=\\argmax_{y_j}\\frac{P(Y=y_j)\\cdot\\prod_{i=1}^n{P(X^{(i)}=x^{(i)}|Y=y_j)}}{P(X=x)}\\\\&=\\argmax_{y_j} P(Y=y_j)\\cdot\\prod_{i=1}^n{P(X^{(i)}=x^{(i)}|Y=y_j)}\\quad i,j=1,2,\\cdots,n\\quad (13)\n    \\end{aligned}$$\n    最终所求即为(13)式(各特征之间相互独立为前提)\n- 对于分类而言,使用概率有时要比使用硬规则更为有效,贝叶斯概率及贝叶斯准则提供了一种利用已知值来估计未知概率的有效方法\n- 之所以被称为朴素贝叶斯,是因为该理论假设各特征之间相互独立(独立性假设),因此降低了对数据量的需求,独立性假设是指一个词的出现概率并不依赖于文档中的其他词,尽管条件独立性假设并不正确,但是朴素贝叶斯仍然是一种有效的分类器\n\n### 参考\n1. [你对贝叶斯统计都有怎样的理解？](https://www.zhihu.com/question/21134457/answer/169523403)\n2. [<<2019考研数学复习全书(数学一)>>](https://book.douban.com/subject/26354557/)\n3. [后验概率与条件概率区别](https://www.cnblogs.com/gczr/p/10154451.html)\n4. [<<机器学习实战>>第4章 朴素贝叶斯](https://book.douban.com/subject/24703171/)\n5. [<<统计学习方法>>第4章 朴素贝叶斯法](https://book.douban.com/subject/10590856/)\n6. [带你理解朴素贝叶斯分类算法](https://zhuanlan.zhihu.com/p/26262151)\n7. [机器学习实战之朴素贝叶斯](https://www.cnblogs.com/zy230530/p/6847243.html)\n","tags":["机器学习实战"],"categories":["机器学习"]},{"title":"[math]投影矩阵","url":"%2Fposts%2Fe772abbc%2F","content":"> From: [什么是projection matrix （投影矩阵）](https://www.zhihu.com/question/62801807/answer/341425237)\n\n那从 **投影(Projection)** 说起吧，假设我们有一条直线 determined by vector $a$，还有一个 vector $b$，想把 $b$ 投影到 $a$ 上，得到 $p$：\n{% asset_img v2-408fdce89f9c9b.jpg %}\n要想得到 $p$，线性代数的办法是：因为 $p$ 在 $a$ 上，所以 $p=xa$ , 再定义 $e = b - p$ （从 $p$ 指向 $b$), 那么 $a \\perp e$，所以：\n$$\n\\begin{aligned}\n    a^T(b-xa)&=0\\\\\n    xa^Ta&=a^Tb\\\\\nx&=\\frac{\\displaystyle a^Tb}{\\displaystyle a^Ta}\n\\end{aligned}\n$$\n得到 $p=ax=a\\frac{\\displaystyle a^Tb}{\\displaystyle a^Ta}$ ，进一步再把这个投影写成 **投影矩阵(Projection Matrix)** 的形式：\n$p=Pb$,其中 P 是矩阵，那么： \n$$p=ax=\\frac{\\displaystyle aa^Tb}{\\displaystyle a^Ta}\\\\\nP=\\frac{\\displaystyle aa^T}{\\displaystyle a^Ta}$$\n关键性质大概有三条： \n1. $Rank(P)=1$。想想$P=\\frac{\\displaystyle aa^T}{\\displaystyle a^Ta}$ 的column space，分母是个 scalar，分子是个矩阵并且每一列是 $a$ 向量本身的常数倍，$Rank$ 是 1 没毛病。所以任何 vector $b$ 乘以 $P$，会落在 $a$ 所在直线上。接着从线性代数的角度考虑，$Pb$ 相当于在 $P$ 的 column space 里搞事情，$P$ 的 column space 就是 $a$ 那条直线，随便怎么搞都在 $a$ 上。\n2. $P^T=P$ 。因为$P^T=\\frac{\\displaystyle (aa^T)^T}{\\displaystyle a^Ta}=\\frac{\\displaystyle aa^T}{\\displaystyle a^Ta}=P$\n3. $P^2=P$ 。因为$P^2=\\frac{\\displaystyle aa^T}{\\displaystyle a^Ta}\\frac{\\displaystyle aa^T}{\\displaystyle a^Ta}=\\frac{\\displaystyle a(a^Ta)a^T}{\\displaystyle (a^Ta)a^Ta}=\\frac{\\displaystyle aa^T}{\\displaystyle a^Ta}=P$  。所以，随便你用多少次~\n","tags":["matrix"],"categories":["math"]},{"title":"[leetcode]63.不同路径II","url":"%2Fposts%2F6e2a1502%2F","content":"{% asset_img 201905201512007.png %}\n\n### 解题思路 \n请先参考上一题 [**[leetcode]62.不同路径**](https://brianyi.github.io/posts/ef733dfb) 的做法\n本题在上一题的基础上增加了限制条件,地图上多了障碍物.但仍然是dp的思路,dp[i][j]用来存储从$(0,0)$到$(i,j)$的路径数\n因为给了地图数组,且障碍物为1,空地为0,那么正好可以利用这个地图数组作为dp状态\n那么仍然先考虑特例,边缘情况,如果边缘没有障碍物,那么$dp[i][0]$或$dp[0][j]$为1,如果边缘有障碍物,那么$dp[i][0]$或$dp[0][j]$为0(错!我开始就是这么简单考虑,后来想到...)\n实际,如果边缘某处有障碍物,那么从该处开始的后面都应当认为无法到达,即$dp[i][0]$或$dp[0][j]$为0\n再考虑不在边缘的情况,没有障碍物时\n$$\ndp[i][j]=dp[i-1][j]+dp[i][j-1]\n$$\n有障碍物时直接赋值,$dp[i][j]=0$\n`本来用的原地图数组来做dp,结果在提交时有一个特例竟然说int无法表示,即溢出了,那么就改成了unsinged long long`\n### 代码实现 \n```cpp\n    using ull = unsigned long long;\n    int uniquePathsWithObstacles(vector<vector<int>>& s) {\n        vector<vector<ull>> dp;\n        for(auto it:s)\n        {\n            vector<ull> t;\n            for(auto it2:it)\n                t.push_back(it2);\n            dp.push_back(t);\n        }\n        int m=dp.size(),n=dp[0].size();\n        /* 如果为障碍1,那么取反为0,表示有0条路径到达\n         * 如果为空地0,那么取反为1,表示有1条路径到达 */\n        dp[0][0]=!dp[0][0]; \n        for(int i=1;i<m;++i) // 左边缘:空地为则dp为1,遇到第一个障碍物,则当前及后面方格dp都为0\n        {\n            dp[i][0]=!dp[i][0];\n            if (!dp[i-1][0])\n                dp[i][0]=0;\n        }\n        for(int j=1;j<n;++j) // 上边缘:空地为则dp为1,遇到第一个障碍物,则当前及后面方格dp都为0\n        {\n            dp[0][j]=!dp[0][j];\n            if (!dp[0][j-1])\n                dp[0][j]=0;\n        }\n        for (int i=1;i<m;++i) // 内部包含下,右边缘:障碍物则dp为0,否则为左边方格可达的路径数与上边方格可达的路径数之和\n        {\n            for(int j=1;j<n;++j)\n            {\n                if (dp[i][j])\n                    dp[i][j]=0;\n                else\n                    dp[i][j]=dp[i-1][j]+dp[i][j-1];\n            }\n        }\n        return dp[m-1][n-1];\n    }\n```\n算法时间复杂度$O(m*n)$\n","tags":["dp"],"categories":["OJ"]},{"title":"[leetcode]62.不同路径","url":"%2Fposts%2Fef733dfb%2F","content":"{% asset_img 201905201405595.png %}\n### 解题思路 \n看到这道题我以为是迷宫算法呢,像以前一样用dfs结果超时,后来看题解才明白可以用 **排列组合** 或 **动态规划(DP)** 来做\n### 思路一: 排列组合\n因为机器人只能向右或者向下走,那么最终肯定只能有m+n-2步到达终点,且这m+n-2步中必然有m-1步向下走,n-1步向右走,那么就简单了,求总共有多少不同路径也就是求在这m+n-2步中m-1步向下走的走法有多少种(或n-1步向右走的走法有多少种)?\n{% asset_img 201905201414106.png %}\n那么答案公式即为\n$$\nC_{m+n-2}^{m-1}=\\frac{\\displaystyle A_{m+n-2}^{m-1}}{\\displaystyle (m-1)!}\n$$\n`代码实现`\n```cpp\nusing ld = long double;\nint uniquePaths(int m, int n) {\n    ld a = 1, b = 1;\n    for(int i=n,k=m-1;i<=m+n-2;++i)\n    {\n        a*=i;\n        b*=k;\n        k=k==1?1:k-1;\n    }\n    return a/b+0.5; // 四舍五入\n}\n```\n时间复杂度为$O(n)$\n### 思路二: DP\n用$dp[i][j]$表示从点$(0,0)$到点$(i,j)$可以有多少种走法.\n先看特例:\n因为只能向下和向右,那么靠边界的一边必然走法只有一种即\n$$dp[i][0]=1(0\\le i\\le m-1)$$\n$$dp[0][j]=1(0\\le j\\le n-1)$$\n再看其他位置,如果当前在$(i,j)$位置$(i\\neq 0,j\\neq 0)$,则上一步位置可以是左边也可以是上边方格,那么当前到达位置的走法必然为上边方格走法与左边方格走法之和\n$$dp[i][j]=dp[i-1][j]+dp[i][j-1]$$\n`代码实现`\n```cpp\nint uniquePaths(int m, int n) {\n    vector<vector<int>> dp(m,vector<int>(n, 0));\n    for(int i=0;i<m;++i)\n        dp[i][0]=1;\n    for(int j=0;j<n;++j)\n        dp[0][j]=1;\n    for(int i=1;i<m;++i)\n        for(int j=1;j<n;++j)\n            dp[i][j]=dp[i-1][j]+dp[i][j-1];\n    return dp[m-1][n-1];\n}\n```\n时间复杂度为$O(m*n)$\n\n  ","tags":["dp"],"categories":["OJ"]},{"title":"[leetcode]34. 在排序数组中查找元素的第一个和最后一个位置","url":"%2Fposts%2Fe8124129%2F","content":"{% asset_img 2019051919134438.png %}\n\n### 解题思路 \n已知题目数组为有序,且要求算法时间复杂度$O(log_2n)$那么明显是二分查找\n首先,进行二分查找,找到元素后进行左遍历找到目标值的开始位置,右遍历找到目标值的结束位置\n### 代码实现 \n```cpp\nvector<int> searchRange(vector<int>& nums, int target) {\n    int len=nums.size();\n    int l=0,r=len-1,idx=-1;\n    while(l<=r)\n    {\n        int m=(l+r)>>1;\n        if (nums[m]==target)\n        {\n            idx=m;\n            break;\n        }\n        else if (nums[m]<target)\n            l=m+1;\n        else\n            r=m-1;\n    }\n    if (idx==-1)\n        return vector<int>{-1,-1};\n    l=r=idx;\n    while(l-1>=0&&nums[l-1]==target) --l;   // 这里比较巧妙,先判断越界,再判断值相等,最后进行移位\n    while(r+1<len&&nums[r+1]==target) ++r;\n    return vector<int>{l,r};\n}\n```\n### 结论\n题目比较简单,记录一下","tags":["二分查找"],"categories":["OJ"]},{"title":"[leetcode]10. 正则表达式匹配","url":"%2Fposts%2F886a4a97%2F","content":"{% asset_img 2019051917403936.png %}\n### 解题思路 \n{% asset_img 2019051917424837.png %}\n算法时间复杂度$O(m*n)$\n### 代码实现 \n```cpp\nint dp[500][500];\nbool isMatch(string s, string p) {\n\ts.insert(s.begin(), '#');\n\tp.insert(p.begin(), '#');\n\tif (p.size()==1 && s.size()==1) return true;\n\tint len_s = s.size()-1;\n\tint len_p = p.size()-1;\n\n\t// s为空,p为#*#*#*... ,匹配\n\tdp[0][0] = 1;   // s=空,p=空,必然匹配\n\tfor (int j = 1; j <= len_p; ++j)\n\t\tif (p[j] == '*'&&dp[0][j - 2])\n\t\t\tdp[0][j] = 1;\n\n\n\tfor (int i = 1; i <= len_s; ++i)\n\t{\n\t\tfor (int j = 1; j <= len_p; ++j)\n\t\t{\n\t\t\tif (p[j] == s[i] || p[j] == '.')  // p[j]是字符且能匹配s[i],则结果取决于前面j-1个长度串\n\t\t\t\tdp[i][j] = dp[i - 1][j - 1];\n\t\t\tif (p[j] == '*')\n\t\t\t{\n\t\t\t\tif (p[j - 1] != s[i] && p[j - 1] != '.')\t// 前面为字母且不等于s[i],那么#*就算作空,则匹配长度由dp[i][j-2]决定\n\t\t\t\t\tdp[i][j] = dp[i][j - 2];\n\t\t\t\telse // 前面字符(可以为.)与s[i]相匹配,则*表示至少一次(>=1),这里不明白(硬记吧)\n\t\t\t\t\tdp[i][j] = (dp[i][j - 1] || dp[i][j - 2] || dp[i - 1][j]);\n\t\t\t}\n\t\t}\n\t}\n\treturn dp[len_s][len_p];\n}\n```\n### 结论\n这道题有些难,不愧是困难级别的,平时正则也用得多,但没想到实现起来如此难","tags":["dp"],"categories":["OJ"]},{"title":"[english]idiomatic expression","url":"%2Fposts%2F381dde6d%2F","content":"### The birds and the bees\n\"The birds and the bees\" is an English-language idiomatic expression and euphemism that refers to courtship and sexual intercourse. The \"Birds and the Bees talk\" is generally the event in most children's lives in which the parents explain what sexual relationships are.\n### Casabianca\n\"Casabianca\" is a poem by the English poet Felicia Dorothea Hemans, first published in The Monthly Magazine, Vol 2, August 1826.[1]\nThe poem starts:\nThe boy stood on the burning deck\nWhence all but he had fled;\nThe flame that lit the battle's wreck\nShone round him o'er the dead.\nThe poem commemorates an actual incident that occurred in 1798 during the Battle of the Nile aboard the French ship Orient. The young son Giocante (his age is variously given as ten, twelve and thirteen) of commander Louis de Casabianca remained at his post and perished when the flames caused the magazine to explode.\n### Waste not, want not\n精打细算,不愁吃穿\n","tags":["idiomatic expression"],"categories":["english"]},{"title":"[机器学习实战]第3章 决策树","url":"%2Fposts%2F49e197e8%2F","content":"### 了解\n如果已知一个结论:一个动物如果不浮出水面,有鱼鳍,那么它一定是鱼.那么现在你有很多的动物,你怎么判断哪些是鱼类哪些不是呢?\n我们可以通过将文字转换为数值的形式来表示,即:\n`不浮出水面用1表示(0则表示浮出水面),有鱼鳍用1表示(0则表示没有鱼鳍),是鱼类用1表示(0则表示非鱼类)`\n**特征标签(即属性):** 不浮出水面,有鱼鳍\n**特征值(特征的数字表示):** 0或1\n**类别标签(即类别名):** 鱼类\n**类别值(即类别的数字表示):** 0或1\n\n**例如:** \n```python\nfeatLabels = ['不浮出水面','有鱼鳍']\nclassLabels = ['鱼类']\n```\n一般,一个已知的数据集都是以向量形式表示为 **[特征值1,特征值2,...,特征值n,类别值]** ,而要预测的数据集则为 **[特征值1,特征值2,...,特征值n]** ,如果你有一个动物的数据为 **[1,1]** ,根据开头已知的结论可以得出 **[1(不浮出水面),1(有鱼鳍)]** 一定是鱼\n\n### 决策树的含义\n顾名思义,用于进行决策的一种树状结构\n{% asset_img 2019051819365621.png %}\n\n### 决策树创建的伪代码\n```python\n    def createBranch():\n        检测数据集中的每个子项是否属于同一分类:\n            If so return 类标签\n            Else\n                寻找划分数据集的最好特征\n                划分数据集\n                创建分支节点\n                    for每个划分的子集\n                        调用函数createBranch并增加返回结果到分直节点中\n                return 分支结点\n```\n\n### 信息增益\n- **自信息**: 在信息论中,自信息(英语：self-information),由**克劳德·香农**提出,是与概率空间中的单一事件或离散随机变量的值相关的信息量的量度.自信息的期望值就是信息论中的**熵**,它反映了随机变量采样时的平均不确定程度.\n  如果待分类的事物可能划分在多个分类之中,则符号$x_i$的自信息定义为\n    $$I(X)=-log{p(x_i)}$$\n    $p(x_i)$是选择该分类的概率\n- 熵或香农熵(entropy): 信息熵是考虑该随机变量的所有可能取值，即所有可能发生事件所带来的信息量的期望。熵定义为自信息的期望值,即度量数据的无序(混乱)程度,**越乱则熵越大,越纯则熵越小**,公式为 \n  $$H(X)=\\sum_{i=1}^n p(x_i)I(x_i)=-\\sum_{i=1}^n p(x_i)log p(x_i)$$\n  熵越大,随机变量的不确定性就越大.从定义可验证\n  $$0\\leq H(p)\\leq logn$$\n  当随机变量只取两个值,例如1,0时,即$X$的分布为\n  $$P(X=1)=p,\\ P(X=0)=1-p,\\ 0\\leq p\\leq 1$$\n  熵为\n  $$H(p)=-plog_2p-(1-p)log_2{(1-p)}$$\n  这时,熵$H(p)$随概率$p$变化的曲线如下图所示\n    {% asset_img Binary_entropy_plot.svg.png %}\n- 条件熵: 条件熵$H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性。条件熵$H(Y|X)$定义为$X$给定条件下$Y$的条件概率分布的熵对$X$的数学期望：\n  {% asset_img 130152886.png %}\n  \n  > **注意**: \n  > 1. $X$表示变量,变量$X$中的每个值都会取,而$x$表示具体某个值,因此$H(Y|X=x)=-\\sum_{y}p(y|x)logp(y|x)$\n  > 2. $\\sum_{x,y}=\\sum_x\\sum_y$(两个累加可以写成一个),$\\sum{a_i}=\\sum_{i=1}^n{a_i}$(非正式写法会省略下面的$i$)\n  > 3. $p(X)=\\sum_xp(x)=\\sum_{i=1}^np(x_i)$\n\n  条件熵$H(Y|X)$相当于联合熵$H(X,Y)$减去单独的熵$H(X)$,即\n  $H(Y|X)=H(X,Y)−H(X)$,证明如下：\n  {% asset_img 1537760199.png %}\n\n- 计算数据的香农熵\n  首先导入必要的包\n    `import numpy as np`\n    `import math`\n    `import operator`\n    ```python\n    def calcShannonEnt(dataSet):\n        '''\n        :param dataSet: 数据集\n        :return shannonEnt: 香农熵值\n        '''\n        numEntries = len(dataSet)   # 获取数据集总数\n        classValCounts = {}    # 字典存放每个类别对应数据集的数量\n        for featVec in dataSet:\n            classVal = featVec[-1]  # 获取类别值\n            if classVal not in classValCounts.keys():\n                classValCounts[classVal] = 0\n            classValCounts[classVal] += 1\n        shannonEnt = 0.0\n        for key in classValCounts:\n            prob = float(classValCounts[key])/numEntries\n            shannonEnt += -prob * math.log(prob,2)\n        return shannonEnt\n    ```\n- 实验\n  我们实验一下是否数据集越纯,熵值越小,数据集越乱,熵值越大\n  ```python\n  def createDataSet():\n    dataSet = [[1, 1, 'yes'],\n              [1, 1, 'yes'],\n              [1, 0, 'no'],\n              [0, 1, 'no'],\n              [0, 1, 'no']]\n    featLabels = ['no surfacing','flippers']\n    return dataSet, featLabels\n    \n    >>> dataSet,featLabels = createDataSet()    # 获取数据集和特征标签\n    >>> calcShannonEnt(dataSet) # 计算香农熵\n    0.9709505944546686\n  ```\n  我们试试如果让数据集变纯会怎样?\n  ```python\n    >>> dataSet[0][-1]=dataSet[1][-1]='no';dataSet\n    [[1, 1, 'no'], [1, 1, 'no'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]\n    >>> calcShannonEnt(dataSet) # 计算香农熵\n    0.0\n  ```\n  果然变小了,再试试数据集变乱?\n  ```python\n    >>> dataSet[1][-1]='yes';dataSet[2][-1]='possible';dataSet[3][-1]='impossible';dataSet[4][-1]='may';dataSet\n    [[1, 1, 'no'],\n    [1, 1, 'yes'],\n    [1, 0, 'possible'],\n    [0, 1, 'impossible'],\n    [0, 1, 'may']]\n    >>> calcShannonEnt(dataSet) # 计算香农熵\n    2.321928094887362\n  ```\n  变得非常大了\n### 划分数据集\n- 根据给定特征划分数据集\n  当我们按照某个特征划分数据集时,就需要将所有符合要求的元素抽取出来\n  ```python\n    def splitDataSet(dataSet, axis, value):\n        '''\n        :param dataSet: 待划分的数据集\n        :param axis: 划分数据集的特征\n        :param value: 特征值\n        :return retDataSet: 返回划分后的数据集\n        '''\n        retDataSet = []\n        for featVec in dataSet:\n            if featVec[axis] == value:  # 只选择包含选定特征值的数据集\n                reducedFeatVec = featVec[:axis] # 该行及下一行将选定数据集的选定特征值去掉(即用于下轮其他特征值的划分)\n                reducedFeatVec.extend(featVec[axis+1:])\n                retDataSet.append(reducedFeatVec)\n        return retDataSet\n  ```\n  下面用<span id='jump-3-2'>图3-2</span>来说明上面函数**splitDataSet**的用途\n  {% asset_img 2019051911283026.png %}\n  这是一个根据特征(是否浮出水面(No sufacting), 是否有鳍(Filppers))来判断是否是鱼类(Fish)的决策图,已有5个数据集数据,我们来将数据进行划分,看是否符合图示\n  首先构造图中数据集\n  {% asset_img 2019051911325428.png %}\n  根据**No surfacing**特征来划分数据\n  - **No(0)**\n    {% asset_img 2019051911385529.png %}\n    可以发现分类结果都为no,即不是鱼,因此纯度已经很高,无需再划分了\n  - **Yes(1)**\n    {% asset_img 2019051911392930.png %}\n    结果中仍然有不同类别,有yes,有no,因此还可以根据下一个特征继续划分\n    从数据集中排除已经划分过的特征,从剩下的数据集中的特征继续划分\n    {% asset_img 2019051911410131.png %}\n    根据Flippers特征又可以继续划分数据\n    - **No(0)**\n    {% asset_img 2019051911412932.png %}\n    - **Yes(1)**\n    {% asset_img 2019051911415033.png %}\n- 选择最好的数据集划分方式\n  每一次划分都选择当前信息增益最大的特征进行划分\n  ```python\n    def chooseBestFeatureToSplit(dataSet):\n        '''\n        :param dataSet: 数据集\n        :return bestFeature: 返回获取信息增益最大的特征索引\n        '''\n        numFeatures = len(dataSet[0]) - 1 # 获取总特征数量(减1是去掉尾部的分类标签值)\n        baseEntropy = calcShannonEnt(dataSet)   # 计算未划分数据时的香农熵(或熵)\n        bestInfoGain = 0.0\n        bestFeature = -1\n        for i in range(numFeatures):    # 计算按每一个特征进行划分时,所得到的信息增益,并选取能得到最大信息增益的特征\n            featList = [example[i] for example in dataSet]  # 获取第i个特征的所有特征值\n            uniqueVals = set(featList)  # 特征值去重\n            newEntropy = 0.0\n            for value in uniqueVals:    # 计算按该特征划分后的香农熵\n                subDataSet = splitDataSet(dataSet, i, value)\n                prob = len(subDataSet)/float(len(dataSet))\n                newEntropy += prob * calcShannonEnt(subDataSet)\n            infoGain = baseEntropy - newEntropy # 计算信息增益\n            if infoGain > bestInfoGain: # 选取最大的信息增益,和特征\n                bestInfoGain = infoGain\n                bestFeature = i\n        return bestFeature\n  ```\n### 递归构造决策树\n- 介绍\n  从原始数据集中,基于最好的特征(即**信息增益最大,从混乱到越纯的程度越高**)划分数据集,每一个分支都是根据一个特征的数据划分,在该分支下的子分支都将不再包含该特征(因为前门已经根据该特征划分过了),每次划分,子分支也可能有多个($\\ge 2$),划分的数据将传递到子分支上(利用信息增益作为特征划分的依据,越划分则下面分支上的数据越少也就越纯),因此我们可以采用递归的原则处理数据集\n  **递归结束的条件**(满足**任意一个**):\n    - 程序遍历完所有划分数据集的特征(若叶节点不纯则采用多数表决策略)    $\\quad (1)$\n    - 每个分支下的数据都是相同的分类(比如上图示的叶节点)    $\\quad (2)$\n- 多数表决策略\n  有时候,我们在划分数据时,已经遍历完了所有特征,但叶节点上的类标签依然不是唯一的,此时我们可以采取多数表决的策略,也就是少数服从多数,选择类别较多的那个类标签作为该叶节点的分类\n  > 上述情况易发生在特征少,而数据量较多的情况\n  ```python\n    def majorityCnt(classValList):\n        '''\n        :param classValList: 数据集类别值列表\n        :return sortedClassCount[0][0]: 返回出现次数最多的类别值\n        '''\n        classValCount = {}\n        for vote in classValList:  # 字典存储每个类别的数据集数量\n            if vote in classValCount.keys():\n                classValCount[vote] = 0\n            classValCount[vote] += 1\n        sortedClassValCount = sorted(classValCount.items(), key=operator.itemgetter(1), reverse=True) # 根据类别值出现次数来排序(逆序),出现次数最多类别值的为第一个\n        return sortedClassValCount[0][0]   # 返回出现次数最多的类别值\n  ```\n- 递归构造决策树代码(这里采用字典结构来存储树,当然实际也可创建数据结构来存储)\n  ```python\n    def createTree(dataSet, featureLabels):\n        '''\n        :param dataSet: 数据集\n        :param featureLabels: 特征标签\n        :return myTree: 构造好的决策树\n        '''\n        featLabels = featureLabels[:]\n        classValList = [example[-1] for example in dataSet]    # 获取数据集对应的类别\n        if classValList.count(classValList[0]) == len(classValList): # 检测到数据集已为同一类别,满足递归结束的条件$(2)$,终止递归\n            return classValList[0]\n        if len(dataSet[0]) == 1:    # 遍历完所有特征,满足递归结束的条件$(1)$,终止递归\n            return majorityCnt(classValList)\n        bestFeat = chooseBestFeatureToSplit(dataSet)    # 选择信息增益最大的特征\n        bestFeatLabel = featLabels[bestFeat]    # 获取特征的名称\n        myTree = {bestFeatLabel:{}} # 在该分支结点注明特征名称\n        del(featLabels[bestFeat])   # 已经遍历过的特征从特征标签中去掉\n        featValues = [example[bestFeat] for example in dataSet] # 获取该特征的所有值\n        uniqueVals = set(featValues)    # 去重\n        for value in uniqueVals:\n            subFeatLabels = featLabels[:]   # 获取剩下的特征标签(还未进行划分的特征)\n            myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subFeatLabels)   # 为该特征下的每个分支创建子树(递归进行)\n        return myTree\n  ```\n  来进行测试一下\n  {% asset_img 201905201643038.png %}\n  果然,决策树跟[图3-2](#jump-3-2)中一样\n\n  > 步骤\n  > 1. 写好递归结束条件\n  > 2. 选择最好特征(信息增益最大),创建该特征结点\n  > 3. 根据该特征进行划分数据集,并递归建树\n\n### 测试算法: 使用决策树执行分类\n- 使用决策树来预测数据类别\n  经过以上部分,将已知类别的训练数据进行训练后,生成的决策树就可以用来进行实际数据的分类了.现在来编写使用决策树预测类别的函数\n- 决策树预测函数的代码\n  ```python\n    def classify(decisionTree, featLabels, featVec):\n        '''\n        :param decisionTree: 决策树,根据样本特征来预测样本的类别\n        :param featLabels: 特征标签\n        :param featVec: 样本的特征向量\n        '''\n        firstStr = list(decisionTree.keys())[0]   # 获取当前用于决策的特征标签\n        secondDict = decisionTree[firstStr]    # 获取该特征标签下的所有不同特征值分支(所有子树)\n        featIndex = featLabels.index(firstStr)  # 获取该特征标签对应的特征索引\n        for featVal in secondDict.keys():   # 遍历特征标签下的所有不同特征值\n            if featVec[featIndex] == featVal:   # 若当样本的特征值相同则进入下一个分支\n                if type(secondDict[featVal]).__name__ == 'dict':    # 下一个分支是否是子树\n                    classLabel = classify(secondDict[featVal], featLabels, featVec) # 在子树中继续递归决策(选择分支)\n                else: classLabel = secondDict[featVal]  # 下一个分支是叶子节点,那么可以直接确定类别了\n        return classLabel   # 返回预测的类别标签\n  ```\n  来进行测试一下鱼类样本 **[1,1]**\n  {% asset_img 201905202100249.png %}\n  果然,结果为`yes`,判断正确!不过我们用的训练数据只有5条,所以差异性也有可能存在,在实际应用中要尽量采用较多的训练数据来训练模型,从而达到较高的预测准确率.\n\n### 使用算法: 决策树的存储\n- 介绍\n  由于每次构造决策树都要耗费大量时间,那么可以在最开始构造好决策树后将它序列化存储到磁盘,并在需要的时候读出来\n- 使用 **pickle模块** 存储和读取决策树\n  - **存储决策树** 到磁盘\n从```python\n    def saveTree(decisionTree, fileName):\n        '''\n        :param decisionTree: 决策树\n        :param fileName: 要存储的文件名\n        '''\n        import pickle\n        fw = open(fileName, 'wb')\n        pickle.dump(decisionTree, fw)\n        fw.close()\n  ```\n  - **读取决策树** 到内存\n  ```python\n    def readTree(fileName):\n        '''\n        :param fileName: 要读取的文件名\n        :return decisionTree: 返回决策树\n        '''\n        import pickle\n        fr = open(fileName, 'rb')\n        return pickle.load(fr)\n  ```\n  来进行测试一下\n  {% asset_img 2019052021361210.png %}\n  还是很方便,其实 **存储决策树** 就是把数据从`内存`中拷贝到了`硬盘`而已(以二进制的形式), **读取决策树** 就是将数据从`硬盘`读到`内存`,二进制只占了很小的硬盘空间,数据文件如下图`红圈`所示`decision.data`.\n  {% asset_img 2019052021460911.png %}\n  可以发现,一棵决策树只占了`84字节`,相比每次都要构造决策树来说,从内存中读取决策树将大大节省了时间消耗\n### 总结\n- 信息公式$I(x_i)=-logp(x_i)$\n- 熵\n  信息熵,香农熵(因为是香农发明的),熵都是一个东西,用于表示数据的混乱程度,**越混乱,熵越大,越纯,熵越小**\n  公式 $H(X)=\\sum_x{p(x)I(x)}=-\\sum_x{p(x)logp(x)}$\n- 信息增益\n  也就是熵的变化,信息增益越大就表明变得纯度越高,信息增益越小就表明变得纯度稍低\n  而决策树中一般选择使得信息增益最大的特征作为分类的特征\n- 决策树预测\n  数据实例最终会被明确划分到某个分类中\n\n### 参考\n1. [详解机器学习中的熵、条件熵、相对熵和交叉熵](http://www.cnblogs.com/kyrieng/p/8694705.html)\n2. [<<机器学习实战>>第3章 决策树](https://book.douban.com/subject/24703171/)\n3. [<<统计学习方法>>第5章 决策树](https://book.douban.com/subject/10590856/)","tags":["机器学习实战"],"categories":["机器学习"]},{"title":"[numpy]notes","url":"%2Fposts%2Ffd60ea9b%2F","content":"\n### Numpy\n#### **np**\n- **array(object, dtype=None, copy=True, order='K', subok=False, ndmin=0)**: 创建一个数组\n    {% asset_img 2019052717422813.png %}\n- **dot(a,b,out=None)**: 两个数组点积\n  - 如果a,b都为1维数组,直接向量的内积\n  - 如果a,b都为$\\ge$2维数组,使用矩阵乘法,但使用的是`np.matmul`或`a@b`(矩阵乘法)而不是`np.multiply(a,b)`或`a*b`(二者为元素级乘法)\n    {% asset_img 201905271213131.png %}\n    - 计算$\\mathbf {ab}=\\sum_{i=0}^m{ab}=a_1b_1+a_2b_2+\\cdots+a_mb_m$,可以分开为$a$和$b$,则可以直接写成`np.dot(a,b)`或`a@b`\n    - 计算$\\sum_{j=0}^n(\\theta x-y)x$,可以分开为$(\\theta x-y)$和$x$,则可以直接写成`np.dot(θx-y,x)`或`(θx-y)@x`\n    - 计算$\\mathbf {ab}$(元素级)=$[a_1b_1,a_2b_2,\\cdots,a_nb_n]$,可以直接写成`np.multiply(a,b)`或`a*b`\n    - 计算$a_i-a_ib_i,i=0,1,2,\\cdots,n$,可以直接写成`a-np.multiply(a,b)`或`a-a*b`\n  - 如果a或b为0维(也就是标量数值),等价于使用`multiply`,可以用`np.multiply(a,b)`或`a*b`(二者为元素级乘法)\n    {% asset_img 201905271214352.png %}\n  - 如果a为N维,b为1维数组,则是最后一个axis的a与b的积之和\n    {% asset_img 201905271621045.png %}\n    {% asset_img 201905271217153.png %}\n- **full(shape, fill_value, dtype=None, order='C')**: 返回一个给定`shape`和`type`,填充`fill_value`的数组\n    {% asset_img 2019052717324610.png %}\n- **full_like(a, fill_value, dtype=None, order='K', subok=True, shape=None)**: 返回一个与`a`相同`shape`且填充`fill_value`的数组\n    {% asset_img 2019052717344811.png %}\n- **ones(shape, dtype=None, order='C')**: 返回一个`shape`大小的`全1数组`\n- **ones_like(a, dtype=None, order='K', subok=True, shape=None)**: 返回一个与`a`相同`shape`大小的`全1数组`\n- **row_stack**: 将1D数组作为2D数组的行,等价于`vstack`\n- **column_stack**: 将1D数组作为2D数组的列等价于\n  - 1D情况\n    {% asset_img 2019052909404427.png %}\n  - 2D情况\n    {% asset_img 2019052909501128.png %}\n- **vstack**: 并列扩展,等价于`row_stack`\n- **hstack**: 并行扩展\n    {% asset_img 2019052909522629.png %}\n- **r_**: 并行连接   \n- **c_**: 并列连接\n    {% asset_img 2019052909242822.png %}\n    `r_`,`c_`都可通过沿一个轴叠加数字来创建数组\n    {% asset_img 2019052909285523.png %}\n- **meshgrid(\\*xi, \\*\\*kwargs)**:由坐标向量返回坐标矩阵\n  即,我只需给横坐标数组x,纵坐标数组y,就可构成一张网\n  - **indexing**:{'xy','ij'},可选.表示输出矩阵时,索引选用Cartesian(默认为'xy')或matrix('ij')\n  当选用Cartesian(默认为'xy')时,输入长度为$M,N$,则输出为$(N,M)$矩阵,如下图\n  {% asset_img 201906031709414.png %}\n  当选用matrix('ij')时,输入长度为$M,N$,则输出为$(M,N)$矩阵,如下图\n  {% asset_img 201906031747526.png %}\n  上图每个点是由$(x,y)_{ij}$构成,如果我们一个一个坐标输入,要输入$3\\times 10$个点,要做很多重复工作,因此这个函数只需提供x轴和y轴坐标的坐标数组即可,值得注意的是,画线方式默认是按**二维数组的列**(不是图像的列)进行\n  用于画3d图像时给定2维坐标\n  {% asset_img 201906031815497.png %}\n#### **np.random**\n- **rand(d0, d1, …, dn)** 生成shape大小的随机数(0~1)\n  ```python\n  >>> np.random.rand(3,2)\n  array([[ 0.14022471,  0.96360618],  #random\n          [ 0.37601032,  0.25528411],  #random\n          [ 0.49313049,  0.94909878]]) #random\n  ```\n- **randn(d0, d1, …, dn)** 生成shape大小的标准正态分布\n  要表示正太分布 $N(\\mu, \\sigma^2)$, 用下面形式:\n  `sigma * np.random.randn(...) + mu`\n  根据N(3, 6.25)随机生成2x4数组大小的样例:\n  ```python\n  >>> 3 + 2.5 * np.random.randn(2, 4)\n  array([[-4.49401501,  4.00950034, -1.81814867,  7.29718677],   # random\n     [ 0.39924804,  4.68456316,  4.99394529,  4.84057254]])  # random\n  ```\n- **randint(low[, high, size, dtype])**\t从小(包含)到大(不包含)返回随机整数.\n  - **Parameters**:\t\n    - **low :** int\n      Lowest (signed) integer to be drawn from the distribution (unless high=None, in which case this parameter is one above the highest such integer).\n    - **high** : int, optional\n      If provided, one above the largest (signed) integer to be drawn from the distribution (see above for behavior if high=None).\n    - **size** : int or tuple of ints, optional\n      Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. Default is None, in which case a single value is returned.\n    - **dtype** : dtype, optional\n      Desired dtype of the result. All dtypes are determined by their name, i.e., ‘int64’, ‘int’, etc, so byteorder is not available and a specific precision may have different C types depending on the platform. The default value is ‘np.int’.\n      New in version 1.11.0.    \n  - **Returns:**\t\n      **out** : int or ndarray of ints\n      size-shaped array of random integers from the appropriate distribution, or a single such random int if size not provided.\n  - **Examples**\n    ```python\n      >>> np.random.randint(5, size=(2, 4))\n      array([[4, 0, 2, 1], # random\n              [3, 2, 2, 0]])\n    ```\n- **shuffle(x)** 随机重排序列\n  - **Parameters:**\t\n      x : 要被重排的array或list\n  - **Examples**\n      {% asset_img 201905251718352.png %}\n      多维数组只会沿第一个轴进行重排\n      {% asset_img 201905251719553.png %}\n#### **np.linspace**\n- **linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None, axis=0)**:\n    按指定间隔值返回均匀的数\n    返回`num`个均匀分布的数,间隔通过`[start,stop]`计算出\n    间隔最后的点可以选择是否包含在内\n    {% asset_img 201905271228234.png %}\n#### **np.matmul**\n- **matmul()**: 等价于`np.dot(a,b)`,`a@b`(数组的矩阵乘法)\n#### **np.matrix**\n- **Attributes**\n  - A: 返回ndarray(维数不变)\n    {% asset_img 201905271719369.png %}\n  - A1: 返回ndarray(一维)\n    {% asset_img 201905271719048.png %}\n  - H: 返回(复数)共轭转置[(complex)conjugate transpose]\n  - I: 返回逆矩阵\n  - T: 返回转置矩阵\n  - flags: 内存布局\n  - size: 返回总元素个数\n- **getA(self)**\n  - 也可直接`self.A`,将自己作为ndarray返回,等价于`np.asarry(self)`\n  - {% asset_img 2019051819061420.png %}\n#### **np.ndarray(为数组array的最原始类型)**\n- **argmax(axis=None, out=None)**: 返回沿给定轴的最大值的索引\n- **argmin(axis=None, out=None)**: 返回沿给定轴的最小值的索引\n- **min(axis=None, out=None, keepdims=False, initial=<no value>, where=True)** 返回沿给定轴的最小值\n  - **Examples**\n    axis=0时,在列中选取最小值,axis=1时,在行中选取最小值\n  {% asset_img 2019051819061419.png %}","tags":["numpy"],"categories":["numpy"]},{"title":"[机器学习实战]第2章 k-近邻算法","url":"%2Fposts%2F75b8f8af%2F","content":"### k-近邻算法概述\nk-近邻算法采用测量不同特征值之间距离进行分类\n- 优点: 精度高,对异常值不敏感,无数据输入假定\n- 缺点: 计算复杂度高,空间复杂度高\n- 使用数据范围: 数值型和标称型\n\n### k-近邻算法的伪代码\n对位未知类别属性的数据集中的每个点依次执行以下操作:\n1. 计算已知类别数据集中的点与当前点之间的距离\n2. 按照距离递增次序排序\n3. 选取与当前点距离最小的k个点\n4. 确定前k个点所在类别的出现频率\n5. 返回前k个点出现频率最高的类别作为当前点的预测分类\n  \n### k-近邻算法核心代码\n```python\ndef classify0(inX, dataSet, labels, k):\n  '''\n  :param inX: 预测数据\n  :param dataSet: 已知数据集\n  :param labels: 已知数据所属标签值\n  :param k: 取前k个点出现频率最高的类别作为当前点的预测分类\n  :return:  \n  '''\n  dataSetSize = dataSet.shape[0]  # 获取一维长度(即样本数量)\n  diffMat = tile(inX, (dataSetSize, 1)) - dataSet # 获取差异矩阵\n  sqDiffMat = diffMat**2  # 取平方\n  sqDistances = sqDiffMat.sum(axis=1) # 二维数据累加,即计算每个点与预测点距离的平方和\n  distances = sqDistances**0.5  # 开根号\n  sortedDistIndicies = distances.argsort()  # 返回排序后的索引值(数据顺序不变)\n  classCount = {}\n  for i in range(k):\n    voteILabel = labels[sortedDistIndicies[i]]\n    classCount[voteILabel] = classCount.get(voteILabel, 0) + 1  #get(key, default): Return the value for key if key is in the dictionary, else default.\n  sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse=True) # 逆序后选择发生频率最高的标签\n  return sortedClassCount[0][0]\n```\n\n### <span id='jump-autonorm'>特征值归一化(特征缩放)</span>\n特征中,有些特征的数字差值对计算结果影响较大,而实际每个特征应当是同等重要的,因此需要进行归一化处理,将数值都转为比重(0.0~1.0)\n公式: $$newValue = \\frac{oldValue-minValue}{maxValue-minValue}$$\n```python\ndef auto_norm(X):\n    \"\"\"特征归一化(或特征缩放)\n    \n    Arguments:\n        X {array} -- 数据集\n    \n    Returns:\n        array -- 返回归一化后的数据集\n    \"\"\"\n    X=np.array(X)   \n    n=len(X[0])\n    # 获取各column最小值\n    minVals=X.min(0)    \n    # 获取各column最大值    \n    maxVals=X.max(0)    \n    # 计算每个数据所在比重(0.0~1.0)\n    newVals=(X-minVals)/(maxVals-minVals)\n    return newVals\n```\n\n### 小结\nk-近邻算法对于数据量不是很大的情况还是比较适宜的,因为效率不高,不过可以采用建立k-tree的方法来提高搜索效率","tags":["机器学习实战"],"categories":["机器学习"]},{"title":"[机器学习实战]第1章 机器学习基础","url":"%2Fposts%2F60021%2F","content":"### 机器学习主要任务\n- 监督学习主要任务: \n  - 分类: 将实例数据划分到合适的类别\n  - 回归: 用于预测数值型数据\n  - 分类和回归属于监督学习,之所以为监督学习,因为算法必须知道预测什么(即已经知道了分类信息)\n- 无监督学习主要任务:\n  - 聚类: 此时数据没有类别信息,也不会给定目标值,将数据集合分成类似的对象组成的多个类的过程称为聚类\n  - 密度估计: 将寻找描述数据统计值的过程称为密度估计\n### 吴恩达视频表示方法<span class='my-color-b'>(博客采用)</span>\n$m$表示训练集中实例的数量,$n$表示样本的特征数\n$x$代表特征/输入变量\n$y$代表目标变量/输出变量\n$(x,y)$代表训练集中的实例\n<b>$(x^{(i)},y^{(i)})$代表第$i$个观测实例</b>\n<b>$x_j^{(i)}$代表矩阵第$i$行的第$j$个特征(或第$i$个样本的第$j$个特征)</b>\n<b>$x_i$代表第$i$个样本</b>\n<b>$(x_i,y_i)$训练集中第i个实例</b>\n$h$代表学习算法的解决方案或函数也称为假设(hypothesis)\n### 李航书表示方法\n$x$代表输入,实例\n$y$代表输出,标记\n$(x_i,y_i)$第i个样本的训练数据点\n$x=(x^{(1)},x^{(2)},\\cdots,x^{(n)})$输入向量,$n$维实数向量\n$x_i^{(j)}$输入向量$x_i$的第$j$分量\n> 注意:吴恩达老师和李航老师,两者在数据表示方法时有不同,比如$x_i^{(j)}$意思完全相反\n> <span class='my-color-b'>本博客将采用吴恩达老师的表示方法</span>\n\n### 开发机器学习应用程序的步骤\n1. 收集数据: 利用各种方法收集样本数据\n2. 准备输入数据: 得到收集的数据后,必须确保数据格式符合要求\n3. 分析输入数据: 主要是人工分析以前得到的数据. 主要作用是确保数据集中没有垃圾数据(数据为空,或者与其他数据存在明显的差异)\n4. 训练算法: 机器学习算法从这一步才真正开始学习. 如果使用无监督学习算法,由于不存在目标变量值,也就不需要训练算法,则直接到第5步\n5. 测试算法: 对于监督学习,必须已知用于评估算法的目标变量值. 对于无监督学习,必须用其他的评测手段来检验算法的成功率. 不满意算法的输出结果则回到4步改正再测试.\n6. 使用算法: 将机器学习算法转为应用程序.\n\n### 总结\n这一章只是大概讲了一下机器学习的主要任务,即监督学习,无监督学习(应当还有半监督学习),然后就是实施步骤","tags":["机器学习实战"],"categories":["机器学习"]},{"title":"[leetcode]双指针法","url":"%2Fposts%2F54332%2F","content":"### 实战1\n{% asset_img 2019051715574813.png %}\n\n- **解题思路** \n    此题首先想到的当然是暴力破解,需要$O(n^2)$,如果采用双指针法,则需要$O(n)$即可\n    \n- **代码实现** \n    ```cpp\n    int maxArea(vector<int>& height) {\n        int maxarea = 0;\n        int i=0,j=height.size()-1;\n        while(i<j)\n        {\n            maxarea=max(maxarea,min(height[i],height[j])*(j-i));\n            if (height[i]<height[j])    // 高度较小的指针进行移动\n                i++;\n            else\n                j--;\n        }\n        return maxarea;\n    }\n    ```\n    **疑惑**: 高度较小的指针进行移动可以保证答案最大吗,难度不会漏掉其他有可能最大的结果吗?\n    **解答**: 可以用草稿纸画一下,由题意可以知道,面积是由底边宽和最短指针高度决定的,那么如果移动较高指针,情况只可能有两种:\n    - 较高指针的高度变得比较矮指针的高度还小(或等于),那么面积必然比原来小\n    - 较高指针的高度变得比较矮指针的高度大,由于高度还是由较矮指针决定,而底边却比原来短了1,那么面积必然比原来小\n    \n    因此,综上两种情况,只能移动较矮指针才有可能获取到更大的面积\n\n### 实战2\n{% asset_img 2019051716023414.png %}\n\n- **解题思路**\n  先排序$O(nlog_2n)$,然后采用$i\\lt left\\lt right$的方式,固定$i$,然后$left$和$right$进行双指针筛选\n- **代码实现**\n    ```cpp\n    int threeSumClosest(vector<int>& nums, int target) {\n        sort(nums.begin(),nums.end());  // 可以用堆排序或其他$(nlog_2n)$的算法\n        int minVal = 0,len=nums.size(),delt=999999;\n        for(int i=0;i<len-2;++i)    // 固定第一个数,后面的数双指针法进行筛选\n        {\n            while(i>0&&i<len-2&&nums[i]==nums[i-1]) ++i;    // i去重\n            int l=i+1,r=len-1;\n            while(l<r)\n            {\n                int sum=nums[i]+nums[l]+nums[r];\n                if (abs(sum-target)<delt)\n                    minVal = sum, delt=abs(sum-target);\n                else\n                {\n                    if (sum-target>0)\n                    {\n                        --r;\n                        while(l<r&&nums[r]==nums[r+1]) --r; // r去重\n                    }\n                    else\n                    {\n                        --l;\n                        while(l<r&&nums[l]==nums[l-1]) ++l; // l去重\n                    }\n                }\n            }\n        }\n        return minVal;\n    }\n    ```\n\n### 实战3\n{% asset_img 2019051719562317.png %}\n\n- **解题思路**\n  先排序$O(nlog_2n)$,然后按照$i\\le j\\le left\\le right$的方法,对$i,j,left,right$进行去重(去重时,应该先加完,判断,再去重),`三数之和,两数之和同理`\n- **代码实现**\n    ```cpp\n        vector<vector<int>> fourSum(vector<int>& nums, int target) {\n        vector<vector<int>> ans;\n        int len=nums.size();\n        sort(nums.begin(),nums.end());\n        for(int i=0;i<len-3;++i)\n        {\n            while(i>0&&i<len-3&&nums[i]==nums[i-1]) i++;    // i去重\n            for (int j=i+1;j<len-2;++j)\n            {\n                while(j>i+1&&j<len-2&&nums[j]==nums[j-1]) j++;  // j去重\n                int l=j+1,r=len-1,sum=target-(nums[i]+nums[j]);\n                while(l<r)\n                {\n                    int diff=nums[l]+nums[r]-sum;\n                    if (diff==0)\n                    {\n                        ans.push_back(vector<int>{nums[i],nums[j],nums[l],nums[r]});\n                        l++,r--;\n                        while(l<r&&nums[l]==nums[l-1]) l++; // l去重\n                        while(l<r&&nums[r]==nums[r+1]) r--; // r去重\n                    }\n                    else if (diff<0)\n                    {\n                        l++;\n                        while(l<r&&nums[l]==nums[l-1]) l++; // l去重\n                    }\n                    else\n                    {\n                        r--;\n                        while(l<r&&nums[r]==nums[r+1]) r--; // r去重\n                    }\n                }\n            }\n        }\n        return ans;\n    }\n    ```","tags":["双指针法"],"categories":["OJ"]},{"title":"[paper 1]Generalized Uncorrelated Regression with Adaptive Graph for Unsupervised Feature Selection","url":"%2Fposts%2F443%2F","content":"\n\n### unknown knowledge\n1. Stiefel manifold\n2. conventional ridge regression model\n3. uncorrelated constraint\n4. closed-form solution\n5. graph regularization term\n6. Generalized uncorrelated constraint\n7. manifold learning\n8. sparsity regularization models\n9. geometrical manifold learning\n10. spectral regression models\n11. Spectral feature selection\n12. l1-regularized regression model\n13. robust nonnegative matrix factorization\n14. local learning\n15. robust feature learning\n16. ridge regression\n17. dimensionality reduction\n18. indicator matrix\n19. subspace learning\n20. embedding-based feature selection method\n21. Graph Regularization\n22. closed-form solution\n23. clustering accuracy\n24. normalized mutual information\n\n### notation\n- **RBF kernel(Radial basis function kernel)**\n  - introduction\n  In machine learning, the radial basis function kernel, or **RBF kernel**, is a popular kernel function used in various kernelized learning algorithms. In particular, it is commonly used in **support vector machine classification**.\n  The **RBF kernel** on two samples x and x', represented as feature vectors in some input space, is defined as $K(\\mathbf x,\\mathbf x')=e^{-\\frac{||\\mathbf x-\\mathbf x'||^2}{2\\sigma^2}}$\n  $||\\mathbf x-\\mathbf x'||^2$ may be recognized as the squared Euclidean distance between the two feature vectors. $\\sigma$ is a free parameter. An equivalent definition involves a parameter $\\gamma ={\\tfrac {1}{2\\sigma ^{2}}}$: $K(\\mathbf {x} ,\\mathbf {x'} )=e^{-\\gamma \\|\\mathbf {x} -\\mathbf {x'} \\|^{2}}$\n  Since the value of the **RBF kernel** decreases with distance and ranges between **zero** (in the limit) and **one** (when $\\mathbf {x = x'}$), it has a ready interpretation as a similarity measure.\n  The feature space of the kernel has an infinite number of dimensions; for $\\sigma =1$, its expansion is:\n  {% asset_img c8d959c116119a.svg %}\n\n- **trivial solution**\n  A solution or example that is ridiculously simple and of little interest. Often, solutions or examples involving the number 0 are considered trivial. Nonzero solutions or examples are considered nontrivial.\n  For example, the equation x + 5y = 0 has the trivial solution x = 0, y = 0. Nontrivial solutions include x = 5, y = –1 and x = –2, y = 0.4.\n\n\n> Abstract\n\nby virtue of a generalized uncorrelated constraint, present an improved sparse regression model for seeking the uncorrelated yet discriminative features.\n\n> Introduction\n\nIt is natural that features always have many different clusters and each cluster has a mass of features for high-dimensional data. For example, there are eyes, nose, and mouth features in a face image. Therefore, the selected features with high redundancy lose their diversity and degrade their performance in clustering or classification tasks.\nTo address this problem, we present a generalized uncorre-lated regression model (GURM). Subsequently, an Uncorrelated Regression with Adaptive graph for unsupervised Feature Selection (URAFS) is proposed. The main contributions of this brief are summarized as follows.\n\n\n\n\n> References\n\n## feature selection\n\n### clustering\n[1] F. Nie, X. Wang, and H. Huang, “Clustering and projected clustering\nwith adaptive neighbors,” in Proc. ACM SIGKDD Int. Conf. Knowl.\nDiscovery Data Mining, 2014, pp. 977–986.\n[2] R. Zhang, F. Nie, and X. Li, “Projected clustering via robust orthogonal\nleast square regression with optimal scaling,” in Proc. Int. Joint Conf.\nNeural. Netw., 2017, pp. 2784–2791.\n\n### classification\n[3] J. Gui, T. Liu, D. Tao, Z. Sun, and T. Tan, “Representative vector\nmachines: A unified framework for classical classifiers,” IEEE Trans.\nCybern., vol. 46, no. 8, pp. 1877–1888, Aug. 2016.\n[4] J. Gui, D. Tao, Z. Sun, Y. Luo, X. You, and Y. Y. Tang, “Group\nsparse multiview patch alignment framework with view consistency\nfor image classification,” IEEE Trans. Image Process., vol. 23, no. 7,\npp. 3126–3132, Jul. 2014.\n\n### face recognition\n[5] C. Y. Lu, H. Min, J. Gui, L. Zhu, and Y. K. Lei, “Face recognition\nvia weighted sparse representation,” J. Vis. Commun. Image Represent.,\nvol. 24, no. 2, pp. 111–116, Feb. 2013.\n[6] J.-X. Mi, D. Lei, and J. Gui, “A novel method for recognizing face with\npartial occlusion via sparse representation,” Optik—Int. J. Light Electron\nOpt., vol. 124, no. 24, pp. 6786–6789, 2013.\n\n---\n\n## unsupervised feature selection\n\n### filter-based methods\n[7] X. He, D. Cai, and P. Niyogi, “Laplacian score for feature selection,”\nin Proc. 19th Annu. Conf. Neural Inf. Process. Syst., Vancouver, BC,\nCanada, Dec. 2005, pp. 507–514.\n[8] Z. Zhao and H. Liu, “Spectral feature selection for supervised\nand unsupervised learning,” in Proc. Conf. Mach. Learn., 2007,\npp. 1151–1157.\n[9] M. Qian and C. Zhai, “Robust unsupervised feature selection,” in Proc.\nIJCAI, 2013, pp. 1621–1627.\n\n### wrapper-based methods\n[10] S. Tabakhi, P. Moradi, and F. Akhlaghian, “An unsupervised feature\nselection algorithm based on ant colony optimization,” Eng. Appl. Artif.\nIntell., vol. 32, pp. 112–123, Jun. 2014.\n\n### embedding-based methods(most popular these years) 11-15\n### conventional spectral-based feature selection methods 11-14,17,18,35\n[11] D. Cai, C. Zhang, and X. He, “Unsupervised feature selection for multi￾cluster data,” in Proc. ACM SIGKDD Int. Conf. Knowl. Discovery Data\nMining, 2010, pp. 333–342.\n[12] Z. Zhao, L. Wang, and H. Liu, “Efficient spectral feature selection\nwith minimum redundancy,” in Proc. Nat. Conf. Artif. Intell., 2010,\npp. 673–678.\n[13] C. Hou, F. Nie, X. Li, D. Yi, and Y. Wu, “Joint embedding learning\nand sparse regression: A framework for unsupervised feature selection,”\nIEEE Trans. Cybern., vol. 44, no. 6, pp. 793–804, Jun. 2014.\n[14] Z. Li, Y. Yang, J. Liu, X. Zhou, and H. Lu, “Unsupervised feature\nselection using nonnegative spectral analysis,” in Proc. Nat. Conf. Artif.\nIntell., 2012, pp. 1026–1032.\n[15] S. Wang, J. Tang, and H. Liu, “Embedded unsupervised feature selec￾tion,” in Proc. Nat. Conf. Artif. Intell., 2015, pp. 470–476.\n\n### an overview of recent structured sparsity-inducing feature selection methods(another expression of embedding-based methods)\n[16] J. Gui, Z. Sun, S. Ji, D. Tao, and T. Tan, “Feature selection based on\nstructured sparsity: A comprehensive study,” IEEE Trans. Neural Netw.\nLearn. Syst., vol. 28, no. 7, pp. 1490–1507, Jul. 2017.\n[17] Q. Gu, Z. Li, and J. Han, “Joint feature selection and subspace learning,”\nin Proc. Nat. Conf. Artif. Intell., 2011, pp. 1294–1299.\n[18] H. Liu, M. Shao, and Y. Fu, “Consensus guided unsupervised feature\nselection,” in Proc. Nat. Conf. Artif. Intell., 2012, pp. 1874–1880.\n[19] F. Nie, W. Zhu, and X. Li, “Unsupervised feature selection with\nstructured graph optimization,” in Proc. Nat. Conf. Artif. Intell., 2016,\npp. 1302–1308.\n[20] Y. Yang, H. T. Shen, Z. Ma, Z. Huang, and X. Zhou, “2, 1-norm\nregularized discriminative feature selection for unsupervised learning,”\nin Proc. IJCAI, vol. 22, 2011, p. 1589.\n[21] L. Shi, L. Du, and Y.-D. Shen, “Robust spectral learning for unsuper￾vised feature selection,” in Proc. IEEE Conf. Data Mining, Dec. 2014,\npp. 977–982.\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 1595\n[22] J. Gui, Z. Sun, W. Jia, R. Hu, Y. Lei, and S. Ji, “Discrimi\u0002nant sparse neighborhood preserving embedding for face recognition,”\nPattern Recognit., vol. 45, no. 8, pp. 2884–2893, 2012.\n[23] W. Karush, “Minima of functions of several variables with inequalities\nas side constraints,” M.S. thesis, Dept. Math., Univ. Chicago, Ghicago,\nIL, USA, 1939.\n[24] J. Huang, F. Nie, H. Huang, and C. Ding, “Robust manifold nonnegative\nmatrix factorization,” ACM Trans. Knowl. Discovery Data, vol. 8, no. 3,\n2014, Art. no. 11.\n[25] F. Nie, R. Zhang, and X. Li, “A generalized power iteration method for\nsolving quadratic problem on the Stiefel manifold,” Sci. China Inf. Sci.,\nvol. 60, no. 11, p. 112101, 2017.\n[26] F. Nie, H. Huang, X. Cai, and C. H. Ding, “Efficient and\nrobust feature selection via joint \u00022, 1-norms minimization,” in Proc.\nAdv. Neural Inf. Process. Syst., Vancouver, BC, Canada, 2010,\npp. 1813–1821.\n[27] J. J. Hull, “A database for handwritten text recognition research,”\nIEEE Trans. Pattern Anal. Mach. Intell., vol. 16, no. 5, pp. 550–554,\nMay 1994.\n[28] C. E. Thomaz and G. A. Giraldi, “A new ranking method for principal\ncomponents analysis and its application to face image analysis,” Image\nVis. Comput., vol. 28, no. 6, pp. 902–913, 2010.\n[29] J. Gui, Z. Sun, G. Hou, and T. Tan, “An optimal set of code words and\ncorrentropy for rotated least squares regression,” in Proc. IEEE Int. Joint\nConf. Biometrics, Sep. 2014, pp. 1–6.\n[30] M. M. Nordstrøm, M. Larsen, J. Sierakowski, and M. B. Stegmann,\n“The IMM face database-an annotated dataset of 240 face images,”\nInform. Math. Modelling, Tech. Univ. Denmark, DTU, Kongens Lyngby,\nKingdom of Denmark, Tech. Rep. DK-2800, May 2004.\n[31] S. A. Nene, S. K. Nayar, and H. Murase, “Columbia object image library\n(COIL-20),” Dept. Comput. Sci., Columbia Univ., New York, NY, USA,\nTech. Rep. CUCS-005-96, Feb. 1996.\n[32] C. H. Papadimitriou and K. Steiglitz, Combinatorial Optimization:\nAlgorithms and Complexity. Upper Saddle River, NJ, USA: Prentice\u0002Hall, 1982.\n[33] A. Strehl and J. Ghosh, “Cluster ensembles—A knowledge reuse frame\u0002work for combining multiple partitions,” J. Mach. Learn. Res., vol. 3,\npp. 583–617, Dec. 2002.\n[34] K. Fan, “On a theorem of weyl concerning eigenvalues of linear trans\u0002formations,” Proc. Nat. Acad. Sci. USA, vol. 35, no. 11, pp. 652–655,\n1949.\n[35] R. Zhang, F. Nie, and X. Li, “Feature selection under regularized orthog\u0002onal least square regression with optimal scaling,” Neurocomputing,\nvol. 273, pp. 547–553, Jan. 2018.","tags":["paper"],"categories":["paper"]},{"title":"[leetcode]动态规划(Dynamic Programming)","url":"%2Fposts%2F26386%2F","content":"\n> 转载自[浅谈什么是动态规划以及相关的「股票」算法题](https://mp.weixin.qq.com/s/p91e-EuSuVK3bfOc7uJplg)\n\n\n### 概念\n- **动态规划**算法是通过拆分问题，定义问题状态和状态之间的关系，使得问题能够以递推（或者说分治）的方式去解决。在学习动态规划之前需要明确掌握几个重要概念。\n- **阶段：** 对于一个完整的问题过程，适当的切分为若干个相互联系的子问题，每次在求解一个子问题，则对应一个阶段，整个问题的求解转化为按照阶段次序去求解。\n- **状态：** 状态表示每个阶段开始时所处的客观条件，即在求解子问题时的已知条件。状态描述了研究的问题过程中的状况。\n- **决策：** 决策表示当求解过程处于某一阶段的某一状态时，可以根据当前条件作出不同的选择，从而确定下一个阶段的状态，这种选择称为决策。\n- **策略：** 由所有阶段的决策组成的决策序列称为全过程策略，简称策略。\n- **最优策略：** 在所有的策略中，找到代价最小，性能最优的策略，此策略称为最优策略。\n- **状态转移方程：** 状态转移方程是确定两个相邻阶段状态的演变过程，描述了状态之间是如何演变的。\n\n### 使用场景\n- 能采用动态规划求解的问题一般要具有3个性质:\n  - **最优化：** 如果问题的最优解所包含的子问题的解也是最优的，就称该问题具有最优子结构，即满足最优化原理。子问题的局部最优将导致整个问题的全局最优。换句话说，就是问题的一个最优解中一定包含子问题的一个最优解。\n    > 最优化原理: 一个过程的最优策略具有这样的性质，即无论其初始状态及初始决策如何，其以后诸决策对以第一个决策所形成的状态作为初始状态的过程而言，必须构成最优策略。\n  - **无后效性：** 即某阶段状态一旦确定，就不受这个状态以后决策的影响。也就是说，某状态以后的过程不会影响以前的状态，只与当前状态有关，与其他阶段的状态无关，特别是与未发生的阶段的状态无关。\n  - **重叠子问题：** 即子问题之间是不独立的，一个子问题在下一阶段决策中可能被多次使用到。（该性质并不是动态规划适用的必要条件，但是如果没有这条性质，动态规划算法同其他算法相比就不具备优势）\n\n### 算法流程\n- **划分阶段：** 按照问题的时间或者空间特征将问题划分为若干个阶段。\n- **确定状态以及状态变量：** 将问题的不同阶段时期的不同状态描述出来。\n- **确定决策并写出状态转移方程：** 根据相邻两个阶段的各个状态之间的关系确定决策。\n- **寻找边界条件：** 一般而言，状态转移方程是递推式，必须有一个递推的边界条件。\n- **设计程序：** 解决问题\n\n### 实战练习\n\n#### 实战1\n- 题目描述\n{% asset_img 201905152254244.png %}\n\n- 题目解析\n  - 状态: 有 **买入(buy)** 和 **卖出(sell)** 两种状态\n  - 转移方程: \n    对于买来说，买之后可以卖出（进入卖状态），也可以不再进行股票交易（保持买状态）。\n    对于卖来说，卖出股票后不在进行股票交易（还在卖状态）。\n    **所以我们只要考虑当天买和之前买哪个花费更低，当天卖和之前卖哪个收益更高。**\n    $buy = min(buy,prices[i])$\n    $sell = max(sell,prices[i]-buy)$\n  - 边界\n    buy = INT_MAX, sell = 0，最后返回 sell 即可。\n- 代码实现\n  - 方法一: DP\n    ```cpp\n    int maxProfit(vector<int>& prices) {\n        int len=prices.size();\n        if (len<=1) return 0;\n        int buy=INT_MAX,sell=0;\n        for (int i=0;i<len;++i)\n        {\n            buy=min(buy,prices[i]);         // 当天买与之前买选取花费最少的\n            sell=max(sell,prices[i]-buy);   // 当天卖与之前卖选取收益最多的\n        }\n        return sell;\n    }\n    ```\n  - 方法二: Greedy\n    ```cpp\n    int maxProfit(vector<int>& prices) {\n        int len=prices.size();\n        if (len<=1) return 0;\n        int maxVal=0;\n        for(int i=1;i<len;++i)  // 增量就是纯收益\n            if (prices[i]>prices[i-1])\n                maxVal+=prices[i]-prices[i-1];\n        return maxVal;\n    }\n    ```\n\n#### 实战2\n- 题目描述\n  {% asset_img 201905152324445.png %}\n\n- 题目解析\n  - 状态: 有 **买入(buy)** 和 **卖出(sell)** 两种状态\n  - 转移方程: \n    对比上题，这里**可以有无限次的买入和卖出**，也就是说**买入状态之前可拥有卖出状态**，所以买入的转移方程需要变化。\n    **所以我们只要考虑当天买和之前买哪个花费更低，当天卖和之前卖哪个收益更高。**\n    $buy = min(buy,prices[i]-sell)$\n    $sell = max(sell,prices[i]-buy)$\n  - 边界\n    buy = INT_MAX, sell = 0，最后返回 sell 即可。\n- 代码实现\n    ```cpp\n    int maxProfit(vector<int>& prices) {\n        int len=prices.size();\n        if (len<=1) return 0;\n        int buy=INT_MAX,sell=0;\n        for (int i=0;i<len;++i)\n        {\n            buy=min(buy,prices[i]-sell);    // 当天买与之前买选取花费最少的\n            sell=max(sell,prices[i]-buy);   // 当天卖与之前卖选取收益最多的\n        }\n        return sell;\n    }\n    ```\n#### 实战3\n- 题目描述\n  {% asset_img 201905161307449.png %}\n  \n- 题目解析\n  - 状态: 有 **第一次买入（fstBuy） 、 第一次卖出（fstSell）、第二次买入（secBuy） 和 第二次卖出（secSell）** 这四种状态。\n  - 转移方程:\n    这里可以有两次的买入和卖出，也就是说 **买入** 状态之前可拥有 **卖出** 状态，所以买入和卖出的转移方程需要变化。\n    $fstBuy=min(fstBuy,prices[i])$\n    $fstSell=max(fstSell,prices[i]-fstBuy)$\n    $secBuy=min(secBuy,prices[i]-fstSell)$\n    $secSell=max(secSell,prices[i]-secBuy)$\n  - 边界\n    fstBuy = INT_MAX, fstSell = 0\n    secBuy = INT_MAX, secSell = 0\n    最后返回 sell 即可\n- 代码实现\n    ```cpp\n    int maxProfit(vector<int>& prices) {\n        int len=prices.size();\n        if (len<=1) return 0;\n        int fstBuy=INT_MAX,fstSell=0,secBuy=INT_MAX,secSell=0;\n        for(int i=0;i<len;++i)\n        {\n            fstBuy=min(fstBuy,prices[i]);           // 第一次买花费的最小成本\n            fstSell=max(fstSell,prices[i]-fstBuy);  // 第一次卖出获取的最大收益\n            secBuy=min(secBuy,prices[i]-fstSell);   // 第二次买花费的最小成本\n            secSell=max(secSell,prices[i]-secBuy);  // 第二次卖出获取的最大收益\n        }\n        return secSell;\n    }\n    ```\n\n#### 实战4\n- 题目描述\n  {% asset_img 2019051613454010.png %}\n\n- 题目解析\n  - 状态: 有 **第一次买入,第n次买入,第n次卖出** 这三种状态,用$dp[i][0]$表示**第i次买入**,$dp[i][1]$表示**第i次卖出**,这里根据题意i从0开始\n  - 转移方程:\n    只有**第一次买入**时没有之前状态,以后的**买入状态之前都是卖出状态,卖出状态之前都是买入状态**\n    $dp[0][0]=min(dp[0][0],prices[i])$\n    $dp[0][1]=max(dp[0][1],prices[i]-dp[0][0])$\n    $1\\le j\\lt k$时,\n    $dp[j][0]=min(dp[j][0],prices[i]-dp[j-1][1])$\n    $dp[j][1]=max(dp[j][1],prices[i]-dp[j][0])$\n  - 边界\n    dp[i][0]=INT_MAX, dp[i][1]=0 ($0\\le i\\lt k$)\n    最后返回dp[k-1][1]即可\n- 代码实现\n    ```cpp\n    int dp[1000][2];\n    int greedy(vector<int>& prices) // greedy algorithm\n    {\n        int maxVal=0,len=prices.size();\n        for(int i=1;i<len;++i)\n            if (prices[i]>prices[i-1])\n                maxVal+=prices[i]-prices[i-1];\n        return maxVal;\n    }\n    int maxProfit(int k, vector<int>& prices) {\n        int len=prices.size();\n        if (len<=1||k==0) return 0;\n        if (k>=len/2) return greedy(prices);\n        \n        // dp(dynamic programming)\n        for(int i=0;i<k&&i<len;++i)\n            dp[i][0]=INT_MAX;\n        for(int i=0;i<len;++i)\n        {\n            dp[0][0]=min(dp[0][0],prices[i]);\n            dp[0][1]=max(dp[0][1],prices[i]-dp[0][0]);\n            for(int j=1;j<=i&&j<k;++j)\n            {\n                dp[j][0]=min(dp[j][0],prices[i]-dp[j-1][1]);\n                dp[j][1]=max(dp[j][1],prices[i]-dp[j][0]);\n            }\n        }\n        return dp[k-1][1];\n    }\n    ```\n\n### 练习题目\n- 题目描述\n  {% asset_img 2019052209205216.png %}\n\n- 题目解析\n  不难发现,这个问题可以被分解为一些包含最优结构的子问题,而这些子问题是叠加的,我们用$dp[i]$来表示到第$i$级阶梯的方法总数,那么在这一级阶梯之前,我们上一步是在哪级阶梯呢?因为一次只能爬1级或2级阶梯,那么上一步就在第$i-1$级或第$i-2$级阶梯,因此,可以得到的转移方程为:\n  $$\n  dp[i]=dp[i-1]+dp[i-2]\\quad (1)\n  $$\n  这里$dp[i-1]$和$dp[i-2]$就是$dp[i]$的最优子结构,即要想知道到第i级阶梯有多少种方法,那么先求出到第i-1级阶梯和第i-2级阶梯有多少种方法,后面的这两个问题就是第一个问题的最优子结构\n  如果一个问题没有边界,那么就无法得到有限的结果,那么我们再来想边界条件:\n  爬到第一级阶梯需要:$dp[1]=1$种方法\n  爬到第二级阶梯需要:$dp[2]=2$种方法\n- 代码实现\n  ```cpp\n    int climbStairs(int n) {\n        vector<int> dp(n+1,0);\n        dp[1]=1;\n        dp[2]=2;\n        for(int i=3;i<=n;++i)\n            dp[i]=dp[i-1]+dp[i-2];\n        return dp[n];\n    }\n  ```\n  算法时间复杂度$O(n)$\n- 进一步思考\n  针对这道题,实际还有更优的算法,熟悉$Fibonacci$数列的同学可能发现了,其实公式(1)就是$Fibonacci$数列公式,而找第$n$个$Fibonacci$数的公式为:\n  $$\\begin{aligned}\n  F_n=\\frac{1}{\\sqrt{5}}\\left[(\\frac{1+\\sqrt{5}}{2})^n-(\\frac{1-\\sqrt{5}}{2})^n\\right]\n  \\end{aligned}$$\n  具体公式是如何推导出来的,具体可以参考[**知乎:斐波那契数列通项公式是怎样推导出来的？**](https://www.zhihu.com/question/25217301)\n- 代码实现\n  ```cpp\n    int climbStairs(int n) {\n        double sqrt5 = sqrt(5.0);\n        double fibn = pow((1.0+sqrt5)/2,n+1)-pow((1.0-sqrt5)/2,n+1);\n        return (int)(fibn/sqrt5);\n    }\n  ```\n  算法时间复杂度$O(1)$\n  当然还有很多的实现方法,比如:暴力破解 $O(2^n)$,记忆化递归 $O(n)$,Binets $O(logn)$就不一一介绍了","tags":["dp"],"categories":["OJ"]},{"title":"[math]notes","url":"%2Fposts%2F56623%2F","content":"### 0. Notation\n\n> From: [List of mathematical symbols](https://en.wikipedia.org/wiki/List_of_mathematical_symbols)\n> notice:\n> - $\\mathbb B$ is equal to $\\mathbf B$, represented boolean domain\n\n|   Symbol    | Name                                      | Explanation                                                                                                                                                                                                                                                                         | Example                                                                                                                      |\n| :---------: | :---------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------- |\n| $\\mathbf B$ | boolean domain                            | $\\mathbf B$ either {0, 1}, {false, true}, {F, T}, or $\\left\\{\\bot ,\\top \\right\\}$                                                                                                                                                                                                   |                                                                                                                              |\n| $\\mathbf C$ | complex numbers                           | $\\mathbf C$ means ${a+b i : a,b ∈ ℝ}$                                                                                                                                                                                                                                               |                                                                                                                              |\n| $\\mathbf I$ | indicator function                        | The indicator function of a subset A of a set X is a function $\\mathbf 1_A\\colon X\\to \\{0,1\\}$ defined as : $\\mathbf 1_A(x):={\\begin{cases}1&{\\text{if }}x\\in A,\\\\0&{\\text{if }}x\\notin A.\\end{cases}}$ <br>Note that the indicator function is also sometimes denoted 1. |                                                                                                                              |\n| $\\mathbf N$ | natural numbers                           | N means either { 0, 1, 2, 3, ...} or { 1, 2, 3, ...}.                                                                                                                                                                                                                               |                                                                                                                              |\n| $\\mathbf Q$ | rational numbers                          | ℚ means {p/q : p ∈ ℤ, q ∈ ℕ}                                                                                                                                                                                                                                                        |                                                                                                                              |\n| $\\mathbf R$ | real numbers                              | ℝ means the set of real numbers.                                                                                                                                                                                                                                                    |                                                                                                                              |\n| $\\mathbf Z$ | integers                                  | ℤ means {..., −3, −2, −1, 0, 1, 2, 3, ...}.<br>ℤ+ or ℤ> means {1, 2, 3, ...}.<br>ℤ≥ means {0, 1, 2, 3, ...}.<br>ℤ* is used by some authors to mean {0, 1, 2, 3, ...} and others to mean {... -2, -1, 1, 2, 3, ... }.                                                                |                                                                                                                              |\n|  $\\times$   | 1.multiplication                          | 3 × 4 means the multiplication of 3 by 4.                                                                                                                                                                                                                                           |                                                                                                                              |\n|             | 2.Cartesian product                       | X × Y means the set of all ordered pairs with the first element of each pair selected from X and the second element selected from Y.                                                                                                                                                | {1,2} × {3,4} =<br>{(1,3),(1,4),(2,3),(2,4)}                                                                                 |\n|             | 3.cross product                           | $\\bm u\\times \\bm v$ means the cross product of vectors $\\bm u$ and $\\bm v$                                                                                                                                                                                                          | (1,2,5) × (3,4,−1) =<br>(−22, 16, − 2)                                                                                       |\n|  $\\otimes$  | tensor product, tensor product of modules | $V\\otimes U$ means the tensor product of V and U. $V\\otimes _{R}U$ means the tensor product of modules $V$ and $U$ over the ring $R$.                                                                                                                                               | $\\{1, 2, 3, 4\\} ⊗ \\{1, 1, 2\\} =\\\\ \\{\\{1, 1, 2\\}, \\{2, 2, 4\\}, \\{3, 3, 6\\}, \\{4, 4, 8\\}\\}$                                    |\n|   $\\circ$   | Hadamard product                          | For two matrices (or vectors) of the same dimensions $A,B\\in {\\mathbb {R} }^{m\\times n}$ the Hadamard product is a matrix of the same dimensions $A\\circ B\\in {\\mathbb {R} }^{m\\times n}$ with elements given by $(A\\circ B)_{i,j}=(A)_{i,j}\\cdot (B)_{i,j}$.                       | $\\begin{bmatrix}1&2\\\\2&4\\\\\\end{bmatrix} \\circ \\begin{bmatrix}1&2\\\\0&0\\\\\\end{bmatrix}=\\begin{bmatrix}1&4\\\\0&0\\\\\\end{bmatrix}$ |\n|             |                                           |                                                                                                                                                                                                                                                                                     |                                                                                                                              |\n|             |                                           |                                                                                                                                                                                                                                                                                     |                                                                                                                              |\n|             |                                           |                                                                                                                                                                                                                                                                                     |                                                                                                                              |\n|             |                                           |                                                                                                                                                                                                                                                                                     |                                                                                                                              |\n|             |                                           |                                                                                                                                                                                                                                                                                     |                                                                                                                              |\n|             |                                           |                                                                                                                                                                                                                                                                                     |                                                                                                                              |\n|             |                                           |                                                                                                                                                                                                                                                                                     |                                                                                                                              |\n|             |                                           |                                                                                                                                                                                                                                                                                     |                                                                                                                              |\n|             |                                           |                                                                                                                                                                                                                                                                                     |                                                                                                                              |\n### 1. Norm\n- **Definition**\n- **Absolute-value norm**\n  $||x||=|x|\\quad (1)$\n  实数或复数构成的一维向量空间的norm\n  (1)也为**L1 norm**\n- **Euclidean norm(欧几里得范数)**\n  - n维欧几里得空间$R^n$中,$\\bm x=(x_1,x_2,...,x_n)$\n    $||\\bm x||_2(or ||\\bm x||):=\\sqrt{x_1^2+\\dots+x_n^2}\\quad (2)$\n    (2)为**Euclidean norm**,给出了原点到$X$点的一般距离\n    **注:** $||\\cdot||_2,||\\cdot||$都表示$L_2$范数\n  - n维附属空间$C^n$,可表示为\n    $||\\bm z||_2(or ||\\bm z||):=\\sqrt{|z_1|^2+\\dots+|z_n|^2}=\\sqrt{z_1\\bar{z_1}+\\dots+z_n\\bar{z_n}}\\quad (3)$\n  - $\\bm x$表示列向量时,$\\bm x=(x_1,x_2,\\dots,x_n)^T$,$\\bm x^*$表示**共轭转置(conjugate transpose)**,可表示为\n    $||\\bm x||:=\\sqrt{x^*x}\\quad (4)$\n  - **Euclidean norm**也被称为**Euclidean length,$L^2$distance,$L^2$norm**\n- **Manhattan norm(Taxicab norm)**\n  - 名字即为出租车从矩形街道原点到$\\bm x$点的距离\n    $\\begin{aligned}\n    ||\\bm x||_1:=\\sum_{i=1}^n|x_i|\\quad (5)\n    \\end{aligned}$\n    (5)也被称为**L1 norm**\n- **p-norm**\n  - $p\\geq 1$的实数,向量$\\bm x=(x_1,\\dots,x_n)^T$的p-norm表示为\n    $\\begin{aligned}\n    ||\\bm x||_p:=(\\sum_{i=1}^n|x_i|^p)^\\frac{1}{p}\\qquad (6)\n    \\end{aligned}$\n    当$p=1$时,为**Manhattan norm**\n    当$p=2$时,为**Euclidean norm**\n- **Matrix norm**\n  - a matrix norm is a vector norm in a vector space whose elements (vectors) are matrices (of given dimensions).\n  - $A_{m\\times n}$,第$i$行表示为$\\bm a^i=(a_{i1},\\dots,a_{in})$,第$j$列表示为$\\bm a_j=(a_{1j},\\dots,a_{mj})$,第$i$行$j$列表示为$a_{ij}$,\n  - 将$m\\times n$的矩阵看作$m$个行向量,每个行向量有$n$个元素,即$A=(\\bm a^1,\\dots,\\bm a^m)^T, \\bm a^1=(a_{11},...,a_{1n})^T$\n    $\\begin{aligned}\n    ||A||_p=||vec(A)||_p=(\\sum_{i=1}^m\\sum_{j=1}^n|a_{ij}|^p)^\\frac{1}{p}\n    \\end{aligned}$\n  - **$L_{2,1}$ and $L_{p,q}$ norms**\n    - $p,q\\ge 1$,$L_{p,q}$ norm表示为\n    $\\begin{aligned}\n    ||A||_{p,q}=(\\sum_{i=1}^{m}||\\bm a^i||_p^q)^\\frac{1}{q}=(\\sum_{i=1}^m(\\sum_{j=1}^n|a_{ij}|^p)^\\frac{q}{p})^\\frac{1}{q}\n    \\end{aligned}$\n    - $L_{2,1}$ norm表示为\n    $\\begin{aligned}\n        ||A||_{2,1}=\\sum_{i=1}^m||\\bm a^i||_2=\\sum_{i=1}^m(\\sum_{j=1}^n|a_{ij}|^2)^\\frac{1}{2}\n    \\end{aligned}$\n- **Frobenius norm(F-norm)**\n  - $A_{m\\times n}$,当$L_{p,q}$中,$p=q=2$时,$L_{2,2}$被称为**Frobenius norm(or Hilbert-Schmidt norm)**,可以表示为\n    $$\\begin{aligned}\n        ||A||_F&=||A||_{2,2}\\\\\n        &=\\sum_{i=1}^m||\\bm a^i||_2^2=(\\sum_{i=1}^m(\\sum_{j=1}^n|a_{ij}|^2)^\\frac{2}{2})^\\frac{1}{2}=\\sqrt{\\sum_{i=1}^m\\sum_{j=1}^n|a_{ij}|^2}\\\\\n        &=\\sqrt{trace(A^TA)}=\\sqrt{\\sum_{i=1}^{min\\{m,n\\}}\\sigma_i^2(A)}\n    \\end{aligned}$$\n\n### 2. Space\n- **Definition**\n  - 一个空间是一个有附加结构的集合(set)\n- **Euclidean spaces**\n  - Definition\n    encompasses **two-dimensional Euclidean plane**,**three-dimensional space of Euclidean geometry**, and similar spaces of **higher dimension**\n  - Euclidean distance\n    vector $\\bm x=(x_1,\\dots,x_n)$\n    $||\\bm x||=\\sqrt{\\bm x\\cdot\\bm x}=\\sqrt{\\sum_{i=1}^n x_i^2}\\quad (1)$\n    上式(1)为内积,表示向量$\\bm x$的长度,同时也满足norm的标准,被称为$R^n$空间的**Euclidean norm**\n    $d(\\bm x,\\bm y)=||\\bm x-\\bm y||=\\sqrt{\\sum_{i=1}^n(x_i-y_i)^2}\\quad (2)$\n    上式(2)为距离函数,被称为**Euclidean distance(Euclidean metric)**,为**Pythagorean theorem**(勾股定理)的一个特例\n  - Squared Euclidean distance(SED)\n    $d^2(\\bm x, \\bm y)=||\\bm x-\\bm y||^2(or ||\\bm x-\\bm y||_2^2)=\\sum_{i=1}^n(x_i-y_i)^2$\n- **Linear spaces**\n- **Topological spaces**\n- **Hilbert spaces**\n\n### 3. Matrix\n- **Toeplitz matrix**，形如\n   {% asset_img 18f00e8851de7fb2e91e743abfb00b41.png %}\n- **Hankel matix**，形如\n   {% asset_img 7a76bd718b5429e22c1f320ebb6400bf.png %}\n   刚好和就是toeplitz的transpose\n- **Degree matrix**，这个和拓扑学有关了，此矩阵只有main diagonal上有非零值，代表的是对应edge(node)所连接的vetices的数量（如果自循环则算两个）\n   $G=(V,E), |V|=n$\n   {% asset_img 2019050920271027.png %}\n- **Adjacency matrix**，也和拓扑学有关，为仅有1或者0的矩阵。\n   如果两个edge之间有vertex相连，则对应位置填1。因为这个性质，此矩阵为symmetric的，main diagonal上的1表示自循环。\n\n|             Labeled graph             |           Degree matrix            |        Adjacency matrix         |          Laplacian matrix           |\n| :-----------------------------------: | :--------------------------------: | :-----------------------------: | :---------------------------------: |\n| {% asset_img 175px-6n-graf.svg.png %} | {% asset_img 0836612538d722.svg %} | {% asset_img 56ef36960e2.svg %} | {% asset_img e495d70fe91b272.svg %} |\n\n- **Laplacian matix**。由上面两位计算得到:\n   $L=D-A$\n\n- **Circulant matrix**, T的变种，如下\n   {% asset_img cb126605ae067e1f4bae13598a2a39f8.png %}\n\n- **Symplectic matrix**\n   指满足这个条件的$M_{2n\\cdot{2n}}$矩阵：$M^T\\Omega M=\\Omega$.\n   其中,另一个矩阵必须是nonsingular, skew-symmetric matrix.，例如选$\\Omega=\\begin{bmatrix}\n   0 & I_n \\\\ -I_n & 0 \\\\    \n   \\end{bmatrix}$ 是一个block matrix,I是单位矩阵(identity matrix)\n\n- **Vandermonde matrix**,形如\n   {% asset_img 642ce6b42c22729068792a6496d81ee7.png %}\n\n-  **Hessenberg matrix**\n    Hessenberg matrix is a special kind of square matrix, one that is \"almost\" triangular. To be exact, an upper Hessenberg matrix has zero entries below the first subdiagonal, and a lower Hessenberg matrix has zero entries above the first superdiagonal\n    例如：upper Hessenberg matrix\n    {% asset_img dd7c78e1ed6e6c999036fed54fe648d0.png %}\n\n- **Hessian matrix**\n    对于实数函数 $f(x_1, x_2, \\dots, x_n)\\,$求二阶偏导（second-order partial derivatives），如下\n    {% asset_img f7296865484b39fcbac598a99b7f3dbb.png %}\n\n- **Idempotent matrix(幂等阵)**\n    Definition: $M^2=M$\n    性质:\n    - $\\lambda$只能为0或1\n    - $A\\cdot(A-E)=0$,$A-E$的每列都为$Ax=0$的解 \n    - $E+A$可逆(因为$E+A$所有$\\lambda$大于0)\n    - $\\lambda_1=1$是$r(A)$重根,$\\lambda_2=0$是$n-r(A)$重根,且有n个线性无关特征向量,即$\\exist$可逆$P$,使$P^{-1}AP=\\begin{bmatrix}\n        E_r & 0 \\\\\n        0   & 0 \\\\\n    \\end{bmatrix}$\n    - 幂等阵必相似于对角阵,$\\exist$可逆$P$,使$P^{-1}AP=\\Lambda=\\begin{bmatrix}\n        E_r&0\\\\\n        0&0\\\\\n    \\end{bmatrix}$\n      > 证明如下:\n      $A^2=A\\implies(E-A)\\cdot{A}=0$,$A$中有$r(A)$个线性无关列向量(即$(E-A)x=0$的解向量)$\\implies\\lambda_1=1$至少是$r(A)$重特征值$\\qquad (1)$\n      $Ax=0\\implies(0\\cdot{E}-A)x=0,\\lambda_2=0$至少是$n-r(A)$重根$\\qquad (2)$\n      由$(1),(2)\\implies\\lambda_1=1$($r(A)$重根),$\\lambda_2=0$($n-r(A)$重根)\n\n- **Orthogonal matrix(正交阵)**\n  - Definition: $A_{n\\times n},A^TA=E(A^{-1}=A^T)$,则称$A$为正交阵\n  - Properties:\n    - $A^T=A^{-1},A^TA=E$\n    - $A$的列向量都是单位向量,且两两正交\n      - 引申:若已知$A$中某$a_{ij}=1$则该$a_{ij}$所在的行与列的其他元素为0\n    - $A\\bm x=\\bm b$有唯一解$\\bm x=A^{-1}\\bm b$\n- **Centering matrix**\n    Definition: $\\bm 1=(1,\\dots,1)^T, C_n=I_n-\\frac{1}{n}\\bm {11}^T$\n    example: \n    $C_1=\\left[0\\right]$\n    $C_2=\n    \\begin{bmatrix}\n        1&0\\\\\n        0&1\\\\\n    \\end{bmatrix}-\\frac{1}{2}\n    \\begin{bmatrix}\n        1&1\\\\\n        1&1\\\\\n    \\end{bmatrix}=\n    \\begin{bmatrix}\n        \\frac{1}{2}&-\\frac{1}{2}\\\\\n        -\\frac{1}{2}&\\frac{1}{2}\\\\\n    \\end{bmatrix}$\n- **Similarity matrix(distance matrix)**\n- **Euclidean distance matrix**\n  - Definition: $\\bm x_1,\\cdots,\\bm x_n$ are defined on **m-dimensional space**,$\\bm x_1=(x_1,\\dots,x_m)$,the elements of $A$ are given by \n  $A=(a_{ij})$\n  $a_{ij}=d_{ij}^2=||\\bm x_i-\\bm x_j||_2^2$\n  where $||\\cdot||_2$ denotes **2-norm** on $\\bm R^m$\n  {% asset_img 79b8411936e4b4f.svg %}\n- **Scatter matrix**\n  - Definition: $n$ samples of m-dimensional data, represented as $m\\times n$ matrix, $X=(\\bm x_1,\\dots,\\bm x_n)$, sample mean is\n  $\\overline \\bm x = \\frac{1}{n}\\sum_{j=1}^n \\bm x_j$\n  where $\\bm x_j=(x_{1j},\\dots,x_{mj})^T$ is j-th column of $X$\n  The **scatter matrix** is $m\\times m$ positive **semi-definite matrix**\n  $S=\\sum_{j=1}^n(\\bm x_j - \\overline \\bm x)(\\bm x_j - \\overline \\bm x)^T=\\sum_{j=1}^n(\\bm x_j - \\overline \\bm x)\\otimes(\\bm x_j - \\overline \\bm x)=(\\sum{j=1}^n\\bm x_j\\bm x_j^T)-n\\overline\\bm x \\overline\\bm x^T$\n  mutiplication is regarded as outer product,the scatter matrix can be also expressed as\n  $S=XC_n X^T$\n  where $C_n$ is $n\\times n$ centering matrix\n- **Projection matrix**\n  \n- **Transformation matrix**\n  - Definition: If $T$ is a linear transformation mapping $\\mathbb {R} ^{n}$ to $\\mathbb {R} ^{m}$ and $\\vec {x}$ is a column vector with $n$ entries, then\n    $T( \\vec x ) = \\mathbf{A} \\vec x$\n    for some $m\\times n$ matrix $A$, called the transformation matrix of $T$. Note that $A$ has $m$ rows and $n$ columns, whereas the transformation $T$ is from $\\mathbb {R} ^{n}$ to $\\mathbb {R} ^{m}$. \n  - Non-linear transformations\n    - affine transformations\n    - projective transformations\n- **Affine transformation matrix**\n- **Projective transformation matrix**\n### 4. Definiteness of a matrix\n- **Definitions for real matrices**\n  A $n\\times n$ symmetric real matrix $M$ is said to be **positive definite** if $\\bm x^TM\\bm x>0$ for all non-zero $\\bm x$ in $\\mathbb {R} ^{n}$. Formally,\n  $$\\boxed{M{\\text{ positive definite}}\\quad \\iff \\quad \\bm x^{\\textsf {T}}M\\bm x>0{\\text{ for all }}\\bm x\\in \\mathbb {R} ^{n}\\setminus \\mathbf {0} }$$\n  \n  A $n\\times n$ symmetric real matrix $M$ is said to be **positive semidefinite** or **non-negative definite** if $\\bm x^{\\textsf {T}}M\\bm x\\geq 0$ for all non-zero $\\bm x$ in $\\mathbb {R} ^{n}$. Formally,\n  $$\\boxed{\\displaystyle M{\\text{ positive semi-definite}}\\quad \\iff \\quad \\bm x^{\\textsf {T}}M\\bm x\\geq 0{\\text{ for all }}\\bm x\\in \\mathbb {R} ^{n}\\setminus \\mathbf {0} }$$\n\n  A $n\\times n$ symmetric real matrix $M$ is said to be **negative definite** if $\\bm x^{\\textsf {T}}M\\bm x<0$ for all non-zero $\\bm x$ in $\\mathbb {R} ^{n}$. Formally,\n  $$\\boxed{\\displaystyle M{\\text{ negative definite}}\\quad \\iff \\quad \\bm x^{\\textsf {T}}M\\bm x<0{\\text{ for all }}\\bm x\\in \\mathbb {R} ^{n}\\setminus \\mathbf {0} }$$\n\n  A $n\\times n$ symmetric real matrix $M$ is said to be **negative semidefinite** or **non-positive definite** if $\\bm x^{\\textsf {T}}M\\bm x\\leq 0$ for all non-zero $\\bm x$ in $\\mathbb {R} ^{n}$. Formally,\n  $$\\boxed{\\displaystyle M{\\text{ negative semi-definite}}\\quad \\iff \\quad \\bm x^{\\textsf {T}}M\\bm x\\leq 0{\\text{ for all }}\\bm x\\in \\mathbb {R} ^{n}\\setminus \\mathbf {0} }$$\n\n  A $n\\times n$ symmetric real matrix which is neither **positive semidefinite** nor **negative semidefinite** is called **indefinite**.","tags":["math"],"categories":["math"]},{"title":"[leetcode]5055.困于环中的机器人","url":"%2Fposts%2F272%2F","content":"{% asset_img 2019051215261229.png %}\n\n### 解题思路\n- 检测最终状态,如果改变了方向(无论过程中移动到了何处),经过了≤4轮,最终一定会回到变为初始状态(回到原处且面向北方),那么必然是无法离开\n- 如果指令结束时回到了原点,那么必然是无法离开\n\n### 代码实现\n\n方法一: 4轮指令过程中一定可以验证是否可以离开\n```cpp\nbool isRobotBounded(string ins) \n{\n    set<tuple<int,int,int,int>> dict;\n    int dx[]={-1,0,1,0},dy[]={0,-1,0,1};\n    int x=0,y=0,len=ins.size(),dir=0;\n    for (int i=0;i<=4;++i)\n    {\n        for (int j=0;j<len;++j)\n        {\n            if (ins[j]=='L')\n                dir++;\n            else if (ins[j]=='R')\n                dir+=3;\n            else if (ins[j]=='G')\n            {\n                x+=dx[dir];\n                y+=dy[dir];\n            }\n            dir%=4;\n            if (dict.count(make_tuple(x,y,j,dir)))\n                return true;\n            dict.insert(make_tuple(x,y,j,dir));\n        }\n    }\n    return false;\n}\n```\n\n方法二: $O(n)$\n```cpp\nbool isRobotBounded(string instructions)\n{\n    int dx[]={0,1,0,-1},dy[]={-1,0,1,0}, x=0,y=0,dir=0; // 左下右上\n    for (int i=0;i<ins.size();++i)\n    {\n        if (ins[i]=='L')\n            dir=(dir+1)%4;\n        else if (ins[i]=='R')\n            dir=(dir+3)%4;\n        else\n        {\n            x+=dx[dir];\n            y+=dy[dir];\n        }\n    }\n    return (x==0&&y==0)||(dir>0);\n}\n```\n\n### 结论\n- 只要最终回到原点,那么一定是bounded\n- 只要最终面向的不是北方,那么将会在剩下1到3轮回到最开始的状态(即指令结束时只要改变了方向,那么每轮都会改变方向,最终回到原点)","tags":["leetcode"],"categories":["OJ"]},{"title":"[schedule]TODO","url":"%2Fposts%2F55269%2F","content":"\n### 1.Extensive reading(One book at least twice for learning)\n| Book                                   | Date | Progress |\n| :------------------------------------- | :--- | :------- |\n| Grade4RS                               | 6/5  | 98-103   |\n| Social Studies For Our Children Book 1 |      |          |\n| Social Studies For Our Children Book 2 |      |          |\n| Social Studies For Our Children Book 3 |      |          |\n| Social Studies For Our Children Book 4 |      |          |\n| Social Studies For Our Children Book 5 |      |          |\n| Social Studies For Our Children Book 6 |      |          |\n\n### 2.Papers\n- 5/20 Semi-supervised feature selection via rescaled linear regression\n- 5/20 Semi-supervised Feature Selection Based on Least Square Regression with Redundancy Minimization\n","tags":["TODO"],"categories":["schedule"]},{"title":"[leetcode]375. Guess Number Higher or Lower II","url":"%2Fposts%2F42608%2F","content":"{% asset_img 2019050820074824.png %}\n\n### 题目含义\n\n给定一个n值为最大值,我从中选出一个数,然后你来猜,猜错了我会告诉你你的数字大了还是小了,并罚你所说数字的钱数,然后再猜,问你最少有多少钱才能保证你一定能猜对?\n\n### 解题思路\n\n没想到是dp的思想,关键在于这个最少是多少钱如何理解.\n假设n=5,你可能会想到,如果你有2+3+4+5=14元,那就一定能猜对,因为我选的数字一定会$\\ge$1,没错,但是这不是最少的钱数,即应该说如果你运气足够差的情况下(即每次都猜错),所花的最少钱数,在leetcode的discuss版块看到的解释:\n\n{% asset_img 2019050820370125.png %}\n\n这道题的意思是,你足够聪明能够选到一种策略,每次都按这个策略来,然后所花费的钱一定会最少,比如:\n\n| n值    | 猜测序列(罚款最多的猜法) | 罚款 |\n| :----- | :----------------------: | :--- |\n| n=1时  |                          | 0    |\n| n=2时  |            1             | 1    |\n| n=3时  |            2             | 2    |\n| n=4时  |           1->3           | 4    |\n| n=5时  |           2->4           | 6    |\n| n=6时  |           3->5           | 8    |\n| n=7时  |           4->6           | 10   |\n| n=8时  |           5->7           | 12   |\n| n=9时  |           6->8           | 14   |\n| n=10时 |           7->9           | 16   |\n\n上述表格的意思,拿n=10来说,我只要有16元,我就一定能够猜对,n=9来说,我只要有14元,我就一定能够猜对,下面来分析,比如n=10时:\n- 如果选的数是8,先猜7,再猜9,最后必然猜对,则花16元,花光了\n- 如果选的数是4,先猜7,而这时你知道是比7小了,那么即n=6了,在按n=6的方法来猜,再猜3,再猜5,最后猜对了, 总共花的钱数为7+3+5=15元,手里还剩1元\n\n如果上面没懂,再来分析下面,即:\n1. 如果n=2\n   - 你猜1(错了),再猜就必然正确,罚1元\n   - 你猜2(错了),再猜就必然正确,罚2元\n   - n=2时,罚款最少只要1元\n2. 如果n=3\n   - 你猜1(错了),再猜2(错了),再猜就必然正确,罚3元\n   - 你猜1(错了),再猜3(错了),再猜就必然正确,罚5元\n   - 你猜2(错了),再猜就必然正确,罚2元\n   - 你猜3(错了),再猜1(错了),再猜就必然正确,罚4元\n   - 你猜3(错了),再猜2(错了),再猜就必然正确,罚5元\n   - n=3时,罚款最少只要2元\n\n通过总结以上规律,假设要猜数的范围在[i,j],每次你猜一个数k,就可以划分出两个区域[i,k-1],[k+1,j],然后就可以确定一个区域必然没有我选的数,因此,需要罚款k元,那么你要付的钱即为:k+max([i,k-1]范围猜对最少要花的钱,[k+1,j]范围猜对最少要花的钱),而k可以取的范围为[i,j],即你可以猜的数为[i,j],那么综合出所有猜的结果,选出最少花费的钱数,核心公式即为:\n\n$$\ndp[i][j]=min(dp[i][j],k+max(dp[i][k-1],dp[k+1][j]))\n$$\n> 提示1: $k+max(dp[i][k-1],dp[k+1][j])$ 即运气足够差的情况下,要花的钱\n> 提示2: $min(dp[i][j],k+max(dp[i][k-1],dp[k+1][j]))$ 即选出所有选择中要花的最少的钱\n\n### 代码实现: DP思想\n时间复杂度$O(n^3)$\n```cpp\nint dp[500][500];\nint getMoneyAmount(int n) {\n    for (int d=0;d<n;++d)\n    {\n        for (int i=1;i+d<=n;++i)\n        {\n            int j=i+d;\n            dp[i][j]=i==j?0:INT_MAX;\n            for (int k=i;k<=j;++k)\n                dp[i][j]=min(dp[i][j],k+max(dp[i][k-1],dp[k+1][j]));\n        }\n    }\n    return dp[1][n];\n}\n```\n\n### 结论\n就像汉诺塔的递归一样,要细抠一层层递归真的很难理解,但是如果记住大体思想就能写出来.这道题也是这样,记住了核心公式的含义就能解出来,即,**假如我当前猜k,没猜对,然后我已经知道了如果要猜的数落在[i,k-1]范围,我要花费多少钱就一定能猜对,如果落在[k+1,j]范围,我要花多少钱就一定能猜对,那么选取这两个中花钱最多的与k相加就是我本轮在先猜k的条件下赢得比赛(最终猜出来)需要要花的最多钱数.**没想到代码这么少,细抠却如此难理解,终于理解为什么这道题有695个鄙视了\n\n---\n\n最后,再给出我是如何得到n=1~10的那张表格的代码部分\n\n```cpp\nint dp[500][500];\nstring dps[500][500];\nint getMoneyAmount(int n) \n{\n    for (int d = 0; d < n; ++d)\n    {\n        for (int i = 1; i + d <= n; ++i)\n        {\n            int j = i + d;\n            dp[i][j] = i==j?0:INT_MAX;\n            for (int k = i; k <= j; ++k)\n            {\n                string str = to_string(k);\n                if (dp[i][k - 1]!=0|| dp[k + 1][j]!=0)\n                {\n                    if (dp[i][k - 1] >= dp[k + 1][j])\n                        str += \"->\" + dps[i][k - 1];\n                    else\n                        str += \"->\" + dps[k + 1][j];\n                }\n                if (dp[i][j] > k + max(dp[i][k - 1], dp[k + 1][j]))\n                    dps[i][j] = str;\n                dp[i][j] = min(dp[i][j], k + max(dp[i][k - 1], dp[k + 1][j]));\n            }\n        }\n    }\n    return dp[1][n];\n}\n\nint main()\n{\n    int n = 10;\n    int a = getMoneyAmount(n);\n    for (int i = 1; i < n; ++i)\n        for (int j = i + 1; j <= n; ++j)\n            printf(\"[%d,%d]: %s\\n\", i, j, dps[i][j].c_str());\n    return 0;\n}\n```\n","tags":["dp"],"categories":["OJ"]},{"title":"[leetcode]312. Burst Balloons","url":"%2Fposts%2F31611%2F","content":"{% asset_img 2019050817052623.png %}\n\n> 题目含义: n个气球,下标从0到n-1,对应于数组nums.每个气球都有一个编号nums[i],每当你扎破一个气球就可以得到$nums[left]\\cdot{nums[i]}\\cdot{nums[right]}$的硬币,这里的left和right是紧挨着气球i的两个气球,当气球i被扎破后,气球left和right就相邻了.请你求出将所有气球扎破后,你能获取的最大的金币数.\n\n注意: 题目要求你假设nums[-1]=nums[n]=1,也就是说当你要扎破第一个气球时,得到的硬币是$nums[-1]\\cdot{nums[0]}\\cdot nums[1]$,扎破最后一个气球时,得到的硬币是$nums[n-2]\\cdot{nums[n-1]}\\cdot{nums[n]}$的,即假设只有一个气球,且编号为3,那么扎破它得到的硬币数为$nums[-1]\\cdot{nums[0]}\\cdot{nums[1]} = 1\\cdot{3}\\cdot{1} = 3$\n\n### 解题思路: DP的思想\n\n该问题与[[leetcode]1039. Minimum Score Triangulation of Polygon](/2019/05/07/leetcode-1039-Minimum-Score-Triangulation-of-Polygon/)相似,DP的思想即将一个大问题,划分为多个小问题来求解,并且大问题和小问题应当存在一种递推(堆叠)的关系.\n在此问题中,我们将$dp[i][j]$用于表示把气球$i$到气球$j$全部扎破后所得到的最大金币数.假设我们最后扎破的气球为$k$,满足$i\\leq{k}\\leq{j}$的关系,那么这里就有一个递推关系,如下图所示\n\n{% asset_img 20190508052601.png %}\n\n> 如何理解上述的递推关系呢?\n\n我们做下处理将原题的下标0~n-1变成1~n(后面会解释原因),假设现在有n个气球,则:\n$1$表示左边界,$n$表示右边界,那么两边确定了,通过枚举最后扎破的气球k来解答.\n假设要扎破气球$1~n$,那么最后扎破的气球可能为$1,...,n-1,n$有如下三种情况\n- $k=1$时,即最后扎破的气球为序号最前的气球,那么最大硬币数为$dp[1][n]=0+dp[k+1][n]+1\\cdot{nums[k]}\\cdot{1}$\n- $k=n$时,即最后扎破的气球为序号最后的气球,那么最大硬币数为$dp[1][n]=dp[1][k-1]+0+1\\cdot{nums[k]}\\cdot{1}$\n- $1\\le{k}\\le{n}$时,即最后扎破的气球为中间的这些气球的一个(不在两端),那么最大硬币数为$dp[1][n]=dp[1][k-1]+dp[k+1][n]+1\\cdot{nums[k]}\\cdot{1}$\n\n需要注意的是,从1到n的总硬币数是要从小到大来计算的,即,上述的d[k+1][n],d[1][k-1],d[k+1][n]都是通过之前的计算算出,那么递推公式如下,假设要扎破气球$i~j(1\\le{i}\\leq{j}\\le{n})$,那么最后扎破的气球可能为$i,...,j$有如下三种情况\n- $k=i$,即最后扎破的气球为序号最前的气球,那么最大硬币数为$dp[i][j]=0+dp[k+1][j]+nums[i-1]\\cdot{nums[k]}\\cdot{nums[k]}$\n- $k=j$,即最后扎破的气球为序号最后的气球,那么最大硬币数为$dp[i][j]=dp[i][k-1]+0+nums[i-1]\\cdot{nums[k]}\\cdot{nums[k]}$\n- $i\\le{k}\\le{j}$,即最后扎破的气球为中间的这些气球的一个(不在两端),那么最大硬币数为$dp[i][j]=dp[i][k-1]+dp[k+1][j]+nums[i-1]\\cdot{nums[k]}\\cdot{nums[k]}$\n\n将以上两种情况都考虑进去(包含两端和不包含两端的),则即为代码部分的特殊处理,代码用A来代表nums了,将A的首部插入一个1,尾部也插入一个1,而气球真正的数量为n个,气球的标号为A[1]~A[n],A[0]和A[1]设置为1,即为题目的条件nums[-1]=nums[n]=1,则计算过程中无论是否是端点的情况,都可以正常计算了,说起来很麻烦,如果文字部分没理解的话可以看看下面的视频讲解\n\n\n下面是一位老哥的视频讲解:\n{% youtube IFNibRVgFBo %}\n\n### 实现代码: DP的思想\n\n```cpp\nint dp[600][600];\nint maxCoins(vector<int>& A) {\n    if (A.empty()) return 0;\n    int len=A.size();\n    A.insert(A.begin(),1);\n    A.push_back(1);\n    for (int d=0;d<len;++d) // d为i与j之间的间隔\n    {\n        for (int i=1;i+d<=len;++i)\n        {\n            int j=i+d;\n            for (int k=i;k<=j;++k)  // 在i与j之间(包含i,j)枚举k\n                // start i, end j, final burst k\n                dp[i][j] = max(dp[i][j], dp[i][k-1]+A[i-1]*A[k]*A[j+1]+dp[k+1][j]);\n        }\n    }\n    return dp[1][len];\n}\n```","tags":["dp"],"categories":["OJ"]},{"title":"[leetcode]1039. Minimum Score Triangulation of Polygon","url":"%2Fposts%2F35653%2F","content":"{% asset_img 20190507422058.png %}\n题目含义: 一个凸多边形由$N$个顶点构成,每个顶点有一个数值,顶点顺时针排列为$A[0],A[1],...,A[N-1]$,假设你将多边形分成$N-2$个三角形,每一个三角形的值为三个顶点值的乘积,多边形的总分数是构成它的$N-2$个三角形的值的总和,求一个$N$多边形的最小总分数.\n\n\n### 解题思路: DP动态规划的思想\n\n首先,$dp[i][j]$表示顺时针从$i$到$j$构成的多边形的最小分数,于是取一点$k$,满足$i<k<j$,再取$d$表示$i$到$j$的距离,进行枚举,在一个大的多边形中,$d=2$先把周边一圈小三角形枚举一边,$d=3$再把周边一圈的四边形枚举一遍(由$d=2$时求出了小三角形的分数计算得出四边形的最小分数),...,$d=N-1$再把$N$边形枚举一边,如下图所示:\n\n{% asset_img 20190508122646.jpg %}\n\n核心公式为: \n\n$$\ndp[i][j] = min(dp[i][j], dp[i][k]+dp[k][j]+A[i]\\cdot{A[k]}\\cdot{A[j]})\n$$\n\n### 代码实现: DP思想\n\n```cpp\nint dp[100][100];\nint minScoreTriangulation(vector<int>& A) \n{\n    int len=A.size();\n    for (int d=2;d<len;++d) // d作为i与j之间的间距\n    {\n        for (int i=0; i+d<len; ++i)\n        {\n            int j=i+d;\n            dp[i][j]=INT_MAX;\n            for (int k=i+1;k<j;++k)\n                dp[i][j]=min(dp[i][j],dp[i][k]+dp[k][j]+A[i]*A[k]*A[j]);\n        }\n    }\n    return dp[0][len-1];\n}\n```","tags":["dp"],"categories":["OJ"]},{"title":"[algorithm]线性表","url":"%2Fposts%2F24346%2F","content":"## 一. 线性表基础算法\n\n### 1.线性表插入操作\n\n线性表插入操作(在第$i(1≤i≤L.length+1)$个位置上插入新元素$elem$)\n\n```cpp\nbool InsertSeq( SeqList& L, int i, ElemType elem )\n{\n    if ( i < 1 || i>L.length + 1 || L.length >= MAXSIZE )\n        return false;\n    for ( j = L.length - 1; j >= i - 1; j-- )\n        L.elem[j + 1] = L.elem[j];\n    L.elem[j + 1] = elem;\n    L.length++;\n    return true;\n}\n```\n\n说明:\n- 插入操作: 可选位置为$1≤i≤L.length+1$\n- 最好情况: 表尾$(i=n+1)$插入, $O(1)$\n- 最坏情况: 表头$(i=1)$插入, $O(n)$\n- 平均情况: 设 $P_i=\\frac{1}{(n+1)}$ 是在第$i$个位置插入一个结点的概率,则在长度为$n$的线性表中插入一个结点所需的移动结点的平均次数为$\\frac {n}{2}$次,即$O(n)$:\n\n$$\n\\sum_{i=1}^{n+1}{P_i}⋅(n+1−i)=\\frac{1}{n+1}\\cdot\\sum_{i=1}^{n+1}(n−i+1)=\\frac{1}{n+1}\\cdot\\frac{n(n+1)}{2}=\\frac{n}{2}\n$$\n\n### 2.线性表删除操作\n\n```cpp\nbool DeleteSeq( SeqList& L, int i, ElemType& elem )\n{\n    for ( i<1 || i>L.length ) return false;\n    elem = L.elem[i - 1];\n    for ( j = i; j < L.length; j++ )\n        L.elem[j - 1] = L.elem[j];\n    L.length--;\n    return true;\n}\n```\n\n说明:\n- 最好情况: 删除表位$(i=n)$,$O(1)$\n- 最坏情况: 删除表头$(i=1)$,$O(n)$\n- 平均情况: 设$P_i=\\frac{1}{n}$是删除第$i$个位置上结点的概率,则在长度为$n$的线性表中删除一个结点所需移动结点的平均次数为$\\frac{n−1}{2}$次,即$O(n)$:\n\n$$\n\\sum_{i=1}^{n}{Pi}\\cdot{(n−i)}=\\frac{1}{n}\\sum_{i=1}^{n}n(n−i)=\\frac{1}{n}\\cdot\\frac{n(n−1)}{2}=\\frac{n−1}{2}\n$$\n\n### 3.线性表查找操作\n\n```cpp\nint LocateSeq( SeqList& L, ElemType elem )\n{\n    for ( i = 0; i < L.length; i++ )\n        if ( L.elem[i].key == elem.key )\n            return i + 1;\n    return 0;\n}\n```\n\n说明:\n- 最好情况: 查找到表头,$O(1)$\n- 最坏情况: 查找到表尾,$O(n)$\n- 平均情况: 设$P_i=\\frac{1}{n}$是查找元素在第$i(1≤i≤L.length)$个位置上的概率,则在长度为$n$的线性表中查找值为$elem$的元素所需比较的平均次数为$\\frac{n+1}{2}$次,$O(n)$:\n\n$$\n\\sum_{i=1}^{n}P_i\\cdot{i}=\\frac{1}{n}\\cdot\\sum_{i=1}^{n}i=\\frac{1}{n}\\cdot\\frac{n(n+1)}{2}=\\frac{n+1}{2}\n$$\n\n## 二.线性表综合应用\n\n### 1.删除线性表中所有值为$x$的数据元素\n\n```cpp\nbool DeleteX( SeqList& L, ElemType x )\n{\n    int k = 1;\n    for ( i = 1; i <= L.length; i++ )\n        if ( L.elem[i].key != x.key )\n            L.elem[k++] = L.elem[i];\n    L.length = k;\n    return true;\n}\n```\n\n### 2.从有序顺序表中删除值在$[s,t]$的所有元素\n\n```cpp\nbool DeleteS2TOrderedSeq( SeqList& L, int s, int t )\n{\n    for ( i = 1; i <= L.length&&L.elem[i].key < s; i++ );    // 找≥s的第一个元素\n    for ( j = i; j <= L.length&&L.elem[j].key <= t; j++ );    // 找>t的第一个元素\n    while ( j <= L.length )\n        L.elem[i++] = L.elem[j++];\n    L.length = i;\n    return true;\n}\n```\n\n### 3.从顺序表中删除值在$[s,t]$的所有元素\n\n```cpp\nbool DeleteS2TSeq( SeqList& L, int s, int t )\n{\n    int k = 1;\n    for ( i = 1; i <= L.length; i++ )\n        if ( L.elem[i].key<s || L.elem[i].key>t )\n            L.elem[k++] = L.elem[i];\n    L.length = k;\n    return true;\n}\n```\n\n### 4.从有序顺序表中删除所有值重复的元素\n\n```cpp\nbool DeleteSameOrderedSeq( SeqList& L )\n{\n    int k = 1;\n    for ( i = 2; i <= L.length; i++ )\n        if ( L.elem[i].key != L.elem[k].key )\n            L.elem[++k] = L.elem[i];\n    L.length = k;\n    return true;\n}\n```\n\n### 5.将两个有序顺序表合并为一个新的有序顺序表\n\n```cpp\nbool Merge( SeqList A, SeqList B, SeqList& C )\n{\n    int i = 1, j = 1, k = 1;\n    while ( i<=A.length&&j<=B.length )\n    {\n        if ( A.elem[i].key <= B.elem[j].key )\n            C.elem[k++] = A.elem[i++];\n        else\n            C.elem[k++] = B.elem[j++];\n    }\n    while ( i <= A.length ) C.elem[k++] = A.elem[i++];\n    while ( j <= B.length ) C.elem[k++] = B.elem[j++];\n    C.length = k - 1;\n    return true;\n}\n```\n\n### 6.原数组$A[m+n]={a_1,a_2,...,a_m,b_1,b_2,...,b_n}$,现要求转变为$A[m+n]={b_1,b_2,...,b_n,a_1,a_2,...,a_m}$\n\n```cpp\n// 元素倒置\nvoid Reverse( ElemType A[], int s, int e )\n{\n    for ( i = s; i < ( s + e ) / 2; i++ )\n        swap( A[i], A[s + e - i - 1] );\n}\n\nvoid ExChange( ElemType A[], int m, int n )\n{\n    Reverse( A, 0, m );\n    Reverse( A, m, m + n );\n    Reverse( A, 0, m + n );\n} \n```\n\n### 7.线性表$(a_1,a_2,...,a_n)$递增有序,设计算法花最少时间找到数值为$x$的元素:\n\n> 1)找到,则与其后继元素位置互换\n> 2)未找到,将其插入表中并使表中元素仍然递增有序\n\n```cpp\n// 使用折半查找的方法\nvoid SearchExchangeInsert( ElemType A[], int n, ElemType x )\n{\n    int low = 1, high = n;\n    while ( low <= high )\n    {\n        mid = ( low + high ) / 2;\n        if ( x.key == A[mid].key )\n        {\n            if ( mid != n )\n                swap( A[mid], A[mid + 1] );\n            return;\n        }\n        else if ( x.key < A[mid].key ) high = mid - 1;\n        else low = mid + 1;\n    }\n    for ( j = n; j >= high + 1; j-- )\n        A[j + 1] = A[j];\n    A[j + 1] = x;\n}\n```\n\n### 8.设计算法将一维数组$R$中的序列循环左移$p(0<p<n)$个位置(算法思想和6.相同)\n\n```cpp\n// 元素倒置\nvoid Reverse( ElemType A[], int s, int e )\n{\n    for ( i = s; i < ( s + e ) / 2; i++ )\n        swap( A[i], A[s + e - i - 1] );\n}\n\nvoid ShiftLeft( ElemType R[], int n, int p )\n{\n    Reverse( R, 0, p );\n    Reverse( R, p, n );\n    Reverse( R, 0, n );\n}\n```\n\n### 9.长度为$L(L≥1)$的升序序列$S$,处在$⌈L2⌉$个位置的数成为$S$的中位数,设计一个在时空都尽量高效的算法找出两个等长序列$A$和$B$的中位数\n\n```cpp\nint FindMidFromABOrderedSeq( int A[], int B[], int n )\n{\n    int s1, s2, e1, e2, m1, m2;\n    s1 = s2 = 0;\n    e1 = e2 = n - 1;\n    while ( s1 != e1 || s2 != e2 )\n    {\n        m1 = ( s1 + e1 ) / 2;\n        m2 = ( s2 + e2 ) / 2;\n        if ( A[m1] == B[m2] )\n            return A[m1];\n        else if ( A[m1] < B[m2] )\n        {\n            if ( !( ( s1 + e1 ) % 2 ) )\n                s1 = m1, e2 = m2;\n            else\n                s1 = m1 + 1, e2 = m2;\n        }\n        else\n        {\n            if ( !( ( s2 + e2 ) % 2 ) )\n                s2 = m2, e1 = m1;\n            else\n                s2 = m2 + 1, e1 = m1;\n        }\n    }\n    return A[s1] < B[s2] ? A[s1] : B[s2];\n}\n```\n\n## 三.线性表的链式表示\n\n### 1.采用头插法建立单链表\n\n```cpp\nLinkList CreateList( LinkList& L )\n{\n    L = ( LinkList ) malloc( sizeof( LNode ) );\n    L->next = NULL;\n    scanf( \"%d\", &x );\n    while ( x != 9999 )\n    {\n        s = ( LNode* ) malloc( sizeof( LNode ) );\n        s->data = x;\n        s->next = L->next;\n        L->next = s;\n        scanf( \"%d\", &x );\n    }\n    return L;\n}\n```\n\n### 2.采用尾插法建立单链表\n\n```cpp\nLinkList CreateList( LinkList& L )\n{\n    L = ( LinkList ) malloc( sizeof( LNode ) );\n    L->next = NULL;\n    r = L;\n    scanf( \"%d\", &x );\n    while ( x != 9999 )\n    {\n        s = ( LNode* ) malloc( sizeof( LNode ) );\n        s->data = x;\n        r->next=s;\n        r = s;\n        scanf( \"%d\", &x );\n    }\n    r->next = NULL;\n    return L;\n}\n```\n\n## 四.线性表相关综合算法\n\n### 1.递归删除不带头结点的单列表$L$中所有值为$x$的结点\n\n```cpp\nvoid DeleteX( LinkList& L, ElemType x )\n{\n    if ( !L ) return;\n    if ( L->data == x )\n    {\n        q = L;\n        L = L->next;\n        free( q );\n        DeleteX( L, x );\n    }\n    else\n        DeleteX( L->next, x );\n} \n```\n\n### 2.删除带头结点的单链表$L$中所有值为$x$的结点\n\n```cpp\nvoid DeleteX( LinkList& L, ElemType x )\n{\n    pre = L;\n    p = L->next;\n    while ( p )\n    {\n        if ( p->data == x )\n        {\n            q = p;\n            pre->next = p->next;\n            p = p->next;\n            free( q );\n        }\n        else\n        {\n            pre = p; p = p->next;\n        }\n    }\n}\n```\n\n### 3.反向输出带头结点的单链表$L$的每个结点的值\n\n```cpp\nvoid PrintX( LinkList L )\n{\n    if ( !L )return;\n    PrintX( L->next );\n    visit( L );\n}\n```\n\n### 4.删除带头结点单链表$L$中最小值结点\n\n```cpp\nLinkList DeleteMin( LinkList& L )\n{\n    LinkList p, s, pre, q;\n    p = s = L->next;\n    pre = q = L;\n    while ( p )\n    {\n        if(p->data<s->data )\n        {\n            s = p; q = pre;\n        }\n        pre = p;\n        p = p->next;\n    }\n    q->next = s->next;\n    free( s );\n    return L;\n}\n```\n\n### 5.将带头结点的单链表就地逆置,\"就地\"指辅助空间复杂度为$O(1)$\n\n```cpp\nLinkList Reverse( LinkList L )\n{\n    LinkList p, q;\n    p = L->next;\n    L->next = NULL;\n    while ( p )\n    {\n        q = p->next;\n        p->next = L->next;\n        L->next = p;\n        p = q;\n    }\n    return L;\n}\n```\n\n### 6.将带头结点的单链表$L$排序,使其递增有序\n\n```cpp\nvoid InsertSort( LinkList& L )\n{\n    LinkList p, pre, r;\n    p = L->next; r = p->next;\n    p->next = NULL; p = r;\n    while ( p )\n    {\n        r = p->next;\n        pre = L;\n        while ( pre->next&&pre->next->data < p->data )\n            pre = pre->next;\n        p->next = pre->next;\n        pre->next = p;\n        p = r;\n    }\n}\n```\n\n### 7.在带头结点的单链表中,删除值介于$(s,t)$之间的元素\n\n```cpp\nvoid DeleteS2T( LinkList& L, int s, int t )\n{\n    LinkList pre, p;\n    pre = L; p = pre->next;\n    while ( p )\n    {\n        if ( p->data > s && p->data < t )\n        {\n            pre->next = p->next;\n            free( p );\n            p = pre->next;\n        }\n        else\n        {\n            pre = p;\n            p = p->next;\n        }\n    }\n}\n```\n\n### 8.找出两个单链表的公共结点\n\n```cpp\nLinkList SearchCommon( LinkList L1, LinkList L2 )\n{\n    LinkList pA, pB;\n    int lenA, lenB, dist;\n    pA = L1->next, pB = L2->next;\n    lenA = lenB = 0;\n    while ( pA ) { pA = pA->next; lenA++; }\n    while ( pB ) { pB = pB->next; lenB++; }\n    pA = L1->next, pB = L2->next;\n    if ( lenA > lenB )\n    {\n        dist = lenA - lenB;\n        while ( dist-- ) pA = pA->next;\n    }\n    else\n    {\n        dist = lenB - lenA;\n        while ( dist-- ) pB = pB->next;\n    }\n    while ( pA )\n    {\n        if ( pA == pB ) return pA;\n        pA = pA->next, pB = pB->next;\n    }\n    return NULL;\n}\n```\n\n### 9.带表头结点的单链表,按递增次序输出单链表中各结点的数据元素,并释放空间\n\n```cpp\nvoid AscDelete( LinkList& L )\n{\n    LinkList p, s, pre, r;\n    while ( L->next )\n    {\n        s = p = L->next; r = pre = L;\n        while ( p )\n        {\n            if ( p->data < s->data )\n            {\n                s = p; r = pre;\n            }\n            pre = p;\n            p = p->next;\n        }\n        r->next = s->next;\n        visit( s );\n        free( s );\n    }\n    free( L );\n}\n```\n\n### 10.将带头结点的单链表$A$分解成两个带头结点的单链表$A$和$B$,$A$中含有奇数序号元素,$B$中含有偶数序号元素且相对位置不变\n\n```cpp\n// 法一\nLinkList Split( LinkList& A )\n{\n    LinkList p, B, rA, rB;\n    int i = 0;\n    p = A->next;\n    B = ( LinkList ) malloc( sizeof( LNode ) );\n    rA = A; A->next = NULL;\n    rB = B; B->next = NULL;\n    while ( p )\n    {\n        i++;\n        if (i%2)\n        {\n            rA->next = p; rA = p;\n        }\n        else\n        {\n            rB->next = p; rB = p;\n        }\n        p = p->next;\n    }\n    rA->next = NULL;\n    rB->next = NULL;\n    return B;\n}\n```\n\n```cpp\n// 法二\nLinkList Split( LinkList& A )\n{\n    LinkList p, B, rB, pre;\n    int i = 0;\n    B = ( LinkList ) malloc( sizeof( LNode ) );\n    rB = B;\n    pre = A; p = pre->next;\n    while ( p )\n    {\n        i++;\n        if ( i % 2 == 0 )\n        {\n            pre->next = p->next;\n            rB->next = p;\n            rB = p;\n            p = pre->next;\n        }\n        else\n        {\n            pre = p;\n            p = p->next;\n        }\n    }\n    return B;\n}\n```\n\n### 11.$C={a_1,b_1,a_2,b_2,...,a_n,b_n}$为线性表,带有头结点,设计一个就地算法将其拆分为两个线性表,使$A={a_1,a_2,...,a_n}$,$B={b_n,...,b_2,b_1}$\n\n```cpp\nLinkList Split( LinkList& A )\n{\n    LinkList B, pre, p;\n    int i = 0;\n    B = ( LinkList ) malloc( sizeof( LNode ) );\n    pre = A; p = pre->next;\n    while ( p )\n    {\n        i++;\n        if ( i % 2 == 0 )\n        {\n            pre->next = p->next;\n            p->next = B->next;\n            B->next = p;\n            p = pre->next;\n        }\n        else\n        {\n            pre = p;\n            p = p->next;\n        }\n    }\n    return B;\n}\n```\n\n### 12.在递增有序的带头结点的单链表中,数值相同的只保留一个,使表中不再有重复的元素\n\n```cpp\nvoid DeleteSame( LinkList& L )\n{\n    LinkList p, q;\n    p = L->next;\n    while ( p )\n    {\n        q = p->next;\n        if ( q&&q->data == p->data )\n        {\n            p->next = q->next;\n            free( q );\n        }\n        else\n            p = p->next;\n    }\n}\n```\n\n### 13.将两个按元素值递增的单链表合并为一个按元素值递减的单链表\n\n```cpp\nvoid MergeList( LinkList& LA, LinkList& LB )\n{\n    LinkList pA, pB, q;\n    pA = LA->next; pB = LB->next;\n    LA->next = NULL;\n    while ( pA&&pB )\n    {\n        if ( pA->data <= pB->data )\n        {\n            q = pA->next;\n            pA->next = LA->next;\n            LA->next = pA;\n            pA = q;\n        }\n        else\n        {\n            q = pB->next;\n            pB->next = LA->next;\n            LA->next = pB;\n            pB = q;\n        }\n    }\n    if ( pA )\n        pB = pA;\n    while(pB )\n    {\n        q = pB->next;\n        pB->next = LA->next;\n        LA->next = pB;\n        pB = q;\n    }\n    free( LB );\n}\n```\n\n### 14.$A,B$为两个元素递增有序的单链表(带头结点),设计算法从$A,B$中公共元素产生单链表$C$,要求\n\n```cpp\nvoid MergeList( LinkList& LA, LinkList& LB )\n{\n    LinkList pA, pB, q;\n    pA = LA->next; pB = LB->next;\n    LA->next = NULL;\n    while ( pA&&pB )\n    {\n        if ( pA->data <= pB->data )\n        {\n            q = pA->next;\n            pA->next = LA->next;\n            LA->next = pA;\n            pA = q;\n        }\n        else\n        {\n            q = pB->next;\n            pB->next = LA->next;\n            LA->next = pB;\n            pB = q;\n        }\n    }\n    if ( pA )\n        pB = pA;\n    while ( pB )\n    {\n        q = pB->next;\n        pB->next = LA->next;\n        LA->next = pB;\n        pB = q;\n    }\n    free( LB );\n}\n```\n\n### 15.求两个元素递增排列的链表(带头结点)$A$和$B$的交集并存放于$A$链表中,并释放其他结点\n\n```cpp\nvoid Intersect( LinkList& LA, LinkList& LB )\n{\n    LinkList pA, pB, r, q;\n    pA = LA->next; pB = LB->next;\n    r = LA; LA->next = NULL;\n    while ( pA&&pB )\n    {\n        if ( pA->data == pB->data )\n        {\n            r->next = pA;\n            r = pA;\n            pA = pA->next;\n            q = pB;\n            pB = pB->next;\n            free( q );\n        }\n        else if ( pA->data < pB->data )\n        {\n            q = pA;\n            pA = pA->next;\n            free( q );\n        }\n        else\n        {\n            q = pB;\n            pB = pB->next;\n            free( q );\n        }\n    }\n    r->next = NULL;\n    while ( pA )\n    {\n        q = pA;\n        pA = pA->next;\n        free( q );\n    }\n    while ( pB )\n    {\n        q = pB;\n        pB = pB->next;\n        free( q );\n    }\n    free( LB );\n}\n```\n\n### 16.判断单链表序列$B$是否是$A$的连续子序列(不带头结点)\n\n```cpp\nbool IsSubsequence( LinkList A, LinkList B )\n{\n    LinkList pA, pB, h;\n    pA = A; pB = B;\n    h = pA;\n    while ( pA&&pB )\n    {\n        if ( pA->data == pB->data )\n        {\n            pA = pA->next;\n            pB = pB->next;\n        }\n        else\n        {\n            h = h->next;\n            pA = h;\n            pB = B;\n        }\n    }\n    if ( pB ) return false;\n    return true;\n}\n```\n\n### 17.判断带头结点的循环双链表是否对称\n\n```cpp\nbool IsSymmetry( DLinkList L )\n{\n    DLinkList p, q;\n    p = L->next; q = L->prior;\n    while ( p != q && q->next != p )\n    {\n        if ( p->data != q->data )\n            return false;\n        p = p->next;\n        q = q->next;\n    }\n    return true;\n}\n```\n\n### 18.将循环单链表$h2$链接到$h1$之后\n\n```cpp\nLinkList Link( LinkList& h1, LinkList& h2 )\n{\n    LinkList p;\n    p = h1;\n    while ( p->next != h1 )p = p->next;\n    p->next = h2;\n    p = h2;\n    while ( p->next != h2 )p = p->next;\n    p->next = h1;\n    return h1;\n}\n```\n\n### 19.带头结点的循环链表,按递增次序输出循环链表中各结点的数据元素,并释放空间\n\n```cpp\nvoid AscDelete( LinkList& L )\n{\n    LinkList p, s, r, pre;\n    while ( L->next != L )\n    {\n        s = p = L->next; r = pre = L;\n        while ( p != L )\n        {\n            if ( p->data < s->data )\n            {\n                s = p; r = pre;\n            }\n            pre = p;\n            p = p->next;\n        }\n        visit( s );\n        r->next = s->next;\n        free( s );\n    }\n    free( L );\n}\n```\n\n### 20.查找单链表(带头结点)中倒数第$k$个位置的结点,成功:则输出并返回$true$,否则只返回$false$\n\n```cpp\nbool SearchBackwardK( LinkList L, int k )\n{\n    LinkList p, q;\n    int count;\n    p = q = L->next;\n    count = 0;\n    while (p)\n    {\n        if ( count < k ) count++;\n        else q = q->next;\n        p = p->next;\n    }\n    if ( count < k ) return false;\n    visit( q );\n    return true;\n}\n```\n\n### 21.链表中$data$绝对值相等的点,只保留第一次出现的结点$(|data|≤n)$\n\n```cpp\nvoid DeleteSameAbs( LinkList L, int n )\n{\n    LinkList pre, p;\n    int *B, pos;\n    B = ( int * ) malloc( sizeof( int )*( n + 1 ) );\n    for ( int i = 0; i < n + 1; i++ )\n        B[i] = 0;\n    pre = L; p = L->next;\n    while ( p )\n    {\n        pos = p->data > 0 ? p->data : -p->data;\n        if ( B[pos] == 0)\n        {\n            B[pos] = 1; pre = p; p = p->next;\n        }\n        else\n        {\n            pre->next = p->next; free( p ); p = pre->next;\n        }\n    }\n    free( B );\n}\n```\n\n### 22.带头结点的循环双链表递增排序\n\n```cpp\nvoid AscSort( DLinkList L )\n{\n    DLinkList p, q, r;\n    if ( !L ) return;\n    p = L->next; q = p->next; r = q->next;\n    while ( q!=L )\n    {\n        while ( p != L && p->data > q->data )\n            p = p->prior;\n        // 脱链结点p\n        q->prior->next = r;\n        r->prior = q->prior;\n        // 插入节点p\n        q->next = p->next;\n        q->prior = p;\n        p->next->prior = q;\n        p->next = q;\n        // 归位(相对位置)\n        q = r;\n        p = q->prior;\n        r = r->next;\n    }\n}\n```","tags":["顺序表"],"categories":["algorithm"]},{"title":"[algorithm]栈和队列","url":"%2Fposts%2F6027%2F","content":"## 一.栈和队列综合(算法)\n\n### 1.判断单链表(带头结点)的结点值(字符型)是否中心对称\n\n```cpp\nbool IsSymmetry( LinkList& L )\n{\n    char S[MAXSIZE];\n    int top = -1, len = 0, i;\n    LinkList p;\n    p = L->next;\n    while ( p ) { p = p->next; len++; }\n    p = L->next;\n    for (i=0;i<len/2;i++)\n    {\n        S[++top] = p->data;\n        p = p->next;\n    }\n    i--;\n    if ( len % 2 )\n        p = p->next;\n    while ( top != -1 )\n    {\n        if ( p->data != S[top] )\n            return false;\n        top--;\n        p = p->next;\n    }\n    return true;\n}\n```\n\n### 2.共享栈由两个顺序栈S1,S2构成,总大小为100,请设计S1,S2入栈,出栈的算法\n\n```cpp\n#define MAXSIZE 100\nElemType S[MAXSIZE];\nint top[2] = { -1,MAXSIZE };\nbool Push( int i, ElemType x )\n{\n    if ( i < 0 || i>1 || top[1] - top[0] == 1 )\n        return false;\n    if ( i == 0 ) S[++top[0]] = x;\n    else S[--top[1]] = x;\n    return true;\n}\n\nbool Pop( int i, ElemType x )\n{\n    if ( i < 0 || i>1 \n         || ( i == 0 && top[0] == -1 ) \n         || ( i == 1 && top[1] == MAXSIZE ) )\n        return false;\n    if ( i == 0 ) x = S[top[0]--];\n    else x = S[top[1]++];\n    return true;\n}\n```\n\n### 3.如果希望循环队列中的元素都能得到利用,则需设置一个标志域tag,并以tag的值为0或1来区分队头指针front和队尾rear相同时的队列状态是\"空\"还是\"满\",编写与此结构相应的入队和出队算法\n\n```cpp\nElemType Q[MAXSIZE];\nint front = -1, rear = -1;\n// 队空条件:    front==rear&&tag==0\n// 队满条件:    front==rear&&tag==1\n// 进队操作:    rear=(rear+1)%MAXSIZE;\n//            Q[rear]=x;\n//            tag=1;\n// 出队操作:    front=(front+1)%MAXSIZE;\n//            x=Q[front];\n//            tag=0;\n```\n\n#### 1)\"tag\"法循环队列入队算法\n\n```cpp\nbool EnQueue( ElemType x )\n{\n    if ( front == rear && tag == 1 )\n        return false;\n    rear = ( rear + 1 ) % MAXSIZE;\n    Q[rear] = x;\n    tag = 1;\n    return true;\n}\n```\n\n#### 2)\"tag\"法循环队列出队算法\n\n```cpp\nbool DeQueue( ElemType& x )\n{\n    if ( front == rear && tag == 0 )\n        return false;\n    front = ( front + 1 ) % MAXSIZE;\n    x = Q[front];\n    tag = 0;\n    return true;\n}\n```\n\n### 4.Q是一个队列,S是一个空栈,实现将队列中的元素逆置的算法\n\n```cpp\nElemType S[MAXSIZE], Q[MAXSIZE];\nint top = -1, front = -1, rear = -1;\nvoid Inverse(ElemType S[], ElemType Q[])\n{\n    ElemType x;\n    while ( front != rear )\n    {\n        x = Q[++front];\n        S[++top] = x;\n    }\n    while ( top != -1 )\n    {\n        x = S[top--];\n        Q[++rear] = x;\n    }\n}\n```\n\n### 5.利用两个栈S1,S2模拟一个队列\n\n>   已知栈的4个运算如下:\n**    void Push(Stack& S, ElemType x);\n    void Pop(Stack& S, ElemType& x)\n    bool IsEmpty(Stack& S);\n    bool IsOverflow( Stack& S );**\n\n```cpp\nbool EnQueue( Stack& S1, Stack& S2, ElemType x )\n{\n    if ( !IsOverflow( S1 ) )\n    {\n        Push( S1, x );\n        return true;\n    }\n    if ( !IsEmpty( S2 ) )\n        return false;\n    while (!IsEmpty(S1))\n    {\n        Pop( S1, t );\n        Push( S2, t );\n    }\n    Push( S1, x );\n    return true;\n}\n\nbool DeQueue( Stack& S1, Stack& S2, ElemType& x )\n{\n    if (!IsEmpty(S2))\n    {\n        Pop( S2, x ); return true;\n    }\n    if ( IsEmpty( S1 ) )\n        return false;\n    while (!IsEmpty(S1))\n    {\n        Pop( S1, t ); \n        Push( S2, t );\n    }\n    Pop( S2, x );\n    return true;\n}\n\nbool IsEmpty( Stack& S1, Stack& S2 )\n{\n    if ( IsEmpty( S1 ) && IsEmpty( S2 ) )\n        return true;\n    return false;\n}\n```\n\n### 6.括号匹配问题:判别表达式中括号是否匹配(只含有(),[],{})\n\n```cpp\nbool IsBracketMatch( char*str )\n{\n    char S[MAXSIZE];\n    int top = -1;\n    for ( int i = 0; str[i]; i++ )\n    {\n        char c = str[i];\n        switch ( c )\n        {\n        case '(':\n        case '[':\n        case '{':\n            S[++top] = c;\n            break;\n        case ')':\n            c = S[top--];\n            if ( c != '(' )return false;\n            break;\n        case ']':\n            c = S[top--];\n            if ( c != '[' )return false;\n            break;\n        case '}':\n            c = S[top--];\n            if ( c != '{' )return false;\n            break;\n        default:\n            break;\n        }\n    }\n    return top == -1;\n}\n```\n\n### 7.利用栈实现以下递归函数的非递归计算:\n\n$$ \nPn(x)= \n\\begin{cases}\n    1,  & n=0 \\\\\n    2x, & n=1 \\\\\n    2x\\cdot{P_{n-1}}(x)-2(n-1)\\cdot{P_{n-2}}(x) & n>1\n\\end{cases}\n$$\n\n```cpp\ndouble P( int n, double x )\n{\n    struct Stack\n    {\n        int n;        // 层\n        double val;    // 数值结果\n    }S[MAXSIZE];\n    int top = -1, fv1 = 1, fv2 = 2 * x;\n    for ( int i = n; i > 1; i-- )\n        S[++top].n = i;\n    while ( top != -1 )\n    {\n        S[top].val = 2 * x*fv2 - 2 * ( S[top].n - 1 )*fv1;\n        fv1 = fv2;\n        fv2 = S[top--].val;\n    }\n    if ( n == 0 ) return fv1;\n    return fv2;\n}\n```","tags":["队列"],"categories":["algorithm"]},{"title":"[algorithm]树与二叉树","url":"%2Fposts%2F45464%2F","content":"## 一.树与二叉树相关算法\n\n### 1.二叉树按顺序结构存储,求编号为i和j的两个结点的最近公共祖先结点的值\n\n```cpp\nElemType CommonAncestor( SeqTree T, int i, int j )\n{\n    while ( i != j )\n    {\n        if ( i > j ) i /= 2;\n        else j /= 2;\n    }\n    return T[i];\n}\n```\n\n### 2.二叉树前序遍历非递归算法\n\n```cpp\nvoid PreOrder( BiTree T )\n{\n    BiTree S[MAXSIZE], p;\n    int top = -1;\n    p = T;\n    while ( p || top != -1 )\n    {\n        if (p)\n        {\n            visit( p );\n            S[++top] = p; p = p->lchild;\n        }\n        else\n        {\n            p = S[top--]; p = p->rchild;\n        }\n    }\n}\n```\n\n### 3.二叉树中序遍历非递归算法\n\n```cpp\nvoid InOrder( BiTree T )\n{\n    BiTree S[MAXSIZE], p;\n    int top = -1;\n    p = T;\n    while ( p || top != -1 )\n    {\n        if (p )\n        {\n            S[++top] = p; p = p->lchild;\n        }\n        else\n        {\n            p = S[top--]; visit( p ); p = p->rchild;\n        }\n    }\n}\n```\n\n### 4.二叉树后序遍历非递归算法\n\n```cpp\nvoid PostOrder( BiTree T )\n{\n    BiTree Q[MAXSIZE], p, r;\n    int top = -1;\n    p = T; r = NULL;\n    while ( p || top != -1 )\n    {\n        if (p)    // 走到最左边\n        {\n            S[++top] = p; p = p->lchild;\n        }\n        else    // 向右\n        {\n            p = S[top];\n            if (p->rchild&&p->rchild!=r)    // 转向右\n                p = p->rchild;\n            else    // 根\n            {\n                p = S[top--];\n                visit( p );\n                r = p;\n                p = NULL;\n            }\n        }\n    }\n}\n```\n\n### 5.二叉树层次遍历算法\n\n```cpp\nvoid LevelOrder( BiTree T )\n{\n    BiTree Q[MAXSIZE], p;\n    int front = -1, rear = -1;\n    Q[++rear] = T;\n    while ( front != rear )\n    {\n        p = Q[++front];\n        visit( p );\n        if ( p->lchild ) Q[++rear] = p->lchild;\n        if ( p->rchild ) Q[++rear] = p->rchild;\n    }\n}\n```\n\n### 6.二叉树的自下而上,从右到左的层次遍历算法\n\n```cpp\nvoid InvertLevel( BiTree T )\n{\n    BiTree S[MAXSIZE], Q[MAXSIZE], p;\n    int front = -1, rear = -1, top = -1;\n    Q[++rear] = T;\n    while ( front != rear )\n    {\n        p = Q[++front];\n        S[++top] = p;\n        if ( p->lchild ) Q[++rear] = p->lchild;\n        if ( p->rchild ) Q[++rear] = p->rchild;\n    }\n    while ( top!=-1 )\n    {\n        p = S[top--]; visit( p );\n    }\n}\n```\n\n### 7.求二叉树高度(递归)\n\n```cpp\nint BtDepth( BiTree T )\n{\n    if ( T == NULL ) return 0;\n    int ldepth, rdepth;\n    ldepth = BtDepth( T->lchild );\n    rdepth = BtDepth( T->rchild );\n    return ldepth > rdepth ? ldepth + 1 : rdepth + 1;\n}\n```\n\n### 8.求二叉树高度(非递归)\n\n> 法一思路:后序遍历,最大栈长即为树的高度\n\n```cpp\nint BtDepth( BiTree T )\n{\n    BiTree S[MAXSIZE], p, r;\n    int top = -1, depth = 0;\n    while ( p || top != -1 )\n    {\n        if ( p )\n        {\n            S[++top] = p; p = p->lchild;\n        }\n        else\n        {\n            p = S[top];\n            if ( p->rchild&&p->rchild != r )\n                p = p->rchild;\n            else\n            {\n                if (top+1>depth)\n                    depth = top + 1;\n                p = S[top--];\n                r = p;\n                p = NULL;\n            }\n        }\n    }\n    return depth;\n}\n```\n\n> 法二思路:层次遍历,层数即为高度\n\n```cpp\nint BtDepth( BiTree T )\n{\n    BiTree Q[MAXSIZE], p;\n    int front = -1, rear = -1, last = 0, depth = 0;\n    Q[++rear] = T;\n    while ( front != rear )\n    {\n        p = Q[++front];\n        if ( p->lchild )\n            Q[++rear] = p->lchild;\n        if ( p->rchild )\n            Q[++rear] = p->rchild;\n        if ( front == last )\n        {\n            depth++;\n            last = rear;\n        }\n    }\n    return depth;\n}\n```\n\n### 9.先许遍历序列和中序遍历序列分别存放于两个一维数组A[1...n],B[1...n]中,编写算法建立该二叉树的二叉链表\n\n```cpp\nBiTree PreInCreate( ElemType A[], ElemType B[], int l1, int h1, int l2, int h2 )\n{\n    BiTree root = ( BiTree ) malloc( sizeof( BiTNode ) );\n    int i, llen, rlen;\n    root->data = A[l1];\n    for ( i = l2; B[i] != root->data; i++ );\n    llen = i - l2;\n    rlen = h2 - i;\n    if ( llen )\n        root->lchild = PreInCreate( A, B, l1 + 1, l1 + llen, l2, l2 + llen - 1 );\n    else\n        root->rchild = NULL;\n    if ( rlen )\n        root->rchild = PreInCreate( A, B, h1 - rlen + 1, h1, h2 - rlen + 1, h2 );\n    else\n        root->rchild = NULL;\n    return root;\n}\n```\n\n### 10.判断二叉树是否是完全二叉树\n\n```cpp\nbool IsComplete( BiTree T )\n{\n    if ( T == NULL ) return true;\n    BiTree Q[MAXSIZE], p;\n    int front = -1, rear = -1;\n    Q[++rear] = T;\n    while ( front != rear )\n    {\n        p = Q[++front];\n        if (p)\n        {\n            Q[++rear] = p->lchild;\n            Q[++rear] = p->rchild;\n        } \n        else\n        {\n            while ( front != rear )\n            {\n                p = Q[++front];\n                if ( p ) return false;\n            }\n        }\n    }\n    return true;\n}\n```\n\n### 11.计算一棵给定二叉树的所有双分支结点个数\n\n```cpp\nint N2Nodes( BiTree T )\n{\n    if ( T == NULL ) return 0;\n    if ( T->lchild && T->rchild )\n        return N2Nodes( T->lchild ) + N2Nodes( T->rchild ) + 1;\n    return N2Nodes( T->lchild ) + N2Nodes( T->rchild );\n}\n```\n\n### 12.将二叉树中所有结点的左,右子树进行交换\n\n```cpp\nvoid SwapTree( BiTree T )\n{\n    if ( T == NULL ) return;\n    SwapTree( T->lchild );\n    SwapTree( T->rchild );\n    swap( T->lchild, T->rchild );\n}\n```\n\n### 13.求二叉树先序遍历序列中第k(1≤k≤二叉树结点个数)个结点的值\n\n```cpp\nint i = 1;\nElemType PreNodeK( BiTree T, int k )\n{\n    if ( T == NULL ) return '#';\n    if ( i == k ) return T->data;\n    i++;    // 下一个结点\n    ElemType ch = PreNodeK( T->lchild, k );\n    if ( ch != '#' ) return ch;\n    ch = PreNodeK( T->rchild, k );\n    return ch;\n}\n```\n\n### 14.二叉树中,对于每一个元素值为x的结点,删去以它为根的子树,并释放相应的空间\n\n```cpp\nvoid DeleteNode( BiTree T )\n{\n    if ( T == NULL ) return;\n    DeleteNode( T->lchild );\n    DeleteNode( T->rchild );\n    free( T );\n}\n```\n> 法一:递归\n\n```cpp\nvoid DeleteAllXNode( BiTree T, ElemType x )\n{\n    if ( T == NULL ) return;\n    if ( T->data == x )\n    {\n        DeleteNode( T ); return;\n    }\n    DeleteAllXNode( T->lchild, x );\n    DeleteAllXNode( T->rchild, x );\n}\n```\n\n> 法二:非递归\n\n```cpp\nvoid DeleteAllXNode( BiTree T, ElemType x )\n{\n    if ( T == NULL ) return;\n    BiTree Q[MAXSIZE], p;\n    int front = -1, rear = -1;\n    Q[++rear] = T;\n    while ( front != rear )\n    {\n        p = Q[++front];\n        if ( p->data == x ) DeleteNode( p );\n        else\n        {\n            if ( p->lchild ) Q[++rear] = p->lchild;\n            if ( p->rchild ) Q[++rear] = p->rchild;\n        }\n    }\n}\n```\n\n### 15.输出二叉树中值为x的结点(≤1)个的所有祖先\n\n> 法一:递归\n\n```cpp\nbool AllAncestorX( BiTree T, ElemType x )\n{\n    if ( T == NULL ) return false;\n    if ( T->data == x ) return true;\n    bool b1, b2;\n    b1 = AllAncestorX( T->lchild, x );\n    b2 = AllAncestorX( T->rchild, x );\n    if ( b1 || b2 ) visit( T );\n    return b1 || b2;\n}\n```\n\n> 法二:非递归\n> 思路: 后序遍历非递归方式中,保留在栈中所有元素(除栈顶外)必然是栈顶的祖先结点,只要找到x结点,将所有结点出栈即可\n\n```cpp\nvoid AllAncestorX( BiTree T, ElemType x )\n{\n    if ( T == NULL ) return;\n    BiTree S[MAXSIZE], p, r;\n    int top = -1;\n    p = T; r = NULL;\n    while ( p||top!=-1 )\n    {\n        if (p)\n        {\n            S[++top] = p; p = p->lchild;\n        }\n        else\n        {\n            p = S[top];\n            if ( p->rchild&&p->rchild != r )\n                p = p->rchild;\n            else\n            {\n                p = S[top--];\n                if (p->data==x)\n                {\n                    while ( top != -1 )\n                    {\n                        p = S[top--]; visit( p );\n                    }\n                }\n                r = p;\n                p = NULL;\n            }\n        }\n    }\n}\n```\n\n### 16.p,q为二叉树中任意两个结点的指针,编写算法找到p,q的最近公共祖先结点(递归)\n\n```cpp\n// 思路: ①左子树中能找到p(或q),右子树中能找到q(或p),的结点一定为p,q的最近公共结点\n//       ②p,q都在右子树上,则深度低的为公共祖先\n//       ③p,q都在左子树上,则深度低的为公共祖先\n//    三种情况      o  <-root(此时为公共祖先)     o  <-root                             o <-root\n//                / \\                            \\                                   /\n//           p-> o   o  <-q                       o  <-p(此时为公共祖先为right)      o  <-p(此时为公共祖先left)\n//                                                 \\                               /\n//                                                  o  <-q                        o  <-q\nBiTree Ancestor( BiTree root, BiTNode *p, BiTNode *q )\n{\n    if ( !root || !p || !q ) return NULL;\n    if ( p == root || q == root ) return root;\n    BiTree left, right;\n    /* \n     * ①在左子树中,若找到p,q中一个,则返回一个\n     * ②在左子树中,若找到p,q(全),则返回较近的一个(高度较低的)\n     */\n    left = Ancestor( root->lchild, p, q );\n    /*\n     * ①在右子树中,若找到p,q中一个,则返回一个\n     * ②在右子树中,若找到p,q(全),则返回较近的一个(高度较低的)\n     */\n    right = Ancestor( root->rchild, p, q );    \n    if ( left&&right ) return root;\n    return left ? left : right;\n}\n```\n\n### 17.求非空二叉树的宽度\n\n```cpp\nint TreeWidth( BiTree T )\n{\n    BiTree Q[MAXSIZE], p;\n    int front = -1, rear = -1, maxWidth = 0;\n    Q[++rear] = T;\n    while ( front != rear )\n    {\n        int width = rear - front;\n        if ( maxWidth < width )\n            maxWidth = width;\n        while ( width-- )\n        {\n            p = Q[++front];\n            if ( p->lchild ) Q[++rear] = p->lchild;\n            if ( p->rchild ) Q[++rear] = p->rchild;\n        }\n    }\n    return maxWidth;\n}\n```\n\n### 18.一棵满二叉树(所有结点值均不同),已知其先序序列为pre,设计算法求其后序序列post\n\n```cpp\n// 思路: 每次都会确定出后序的一个位置并划分为左右两块,再分别在这左右两块中继续确定其他元素 \n//  先序: x|    |    |\n//  后序:  |    |    |x\nvoid PreToPost( ElemType pre[], int l1, int h1, ElemType post[], int l2, int h2 )\n{\n    if ( h1 < l1 ) return;\n    post[h2] = pre[l1]; // 确定出一个后序位置\n    int half = ( h1 - l1 ) / 2;\n    PreToPost( pre, l1 + 1, l1 + half, post, l2, l2 + half - 1 );\n    PreToPost( pre, h1 - half + 1, h1, post, h2 - half, h2 - 1 );\n}\n```\n\n### 19.将二叉树叶子结点按从左到右连成单链表,表头指针为head,叶结点的右指针域存放单链表指针\n\n```cpp\nBiTree head, pre = NULL;\nBiTree InOrder( BiTree bt )\n{\n    if ( bt == NULL ) return NULL;\n    InOrder( bt->lchild );\n    if ( !bt->lchild && !bt->rchild )\n    {\n        if (!pre)\n        {\n            head = pre = bt;\n        }\n        else\n        {\n            pre->rchild = bt; pre = bt;\n        }\n    }\n    InOrder( bt->rchild );\n    pre->rchild = NULL;\n    return head;\n}\n```\n\n### 20.判断两棵二叉树是否相似.(注:不要求结点值相同,只要树的外形相同即可)\n\n```cpp\nbool Similar( BiTree T1, BiTree T2 )\n{\n    if ( T1 == NULL && T2 == NULL ) \n        return true;\n    else if ( T1 == NULL || T2 == NULL ) \n        return false;\n    else if ( Similar( T1->lchild, T2->lchild ) && Similar( T1->rchild, T2->rchild ) )\n        return true;\n    return false;\n}\n```\n\n### 21.将表达式树转换为等价的中缀表达式(通过括号反映操作符的计算次序)并输出\n\n```cpp\n// 思路: 表达式树的中序序列加上必要的括号即为等价的中缀表达式.除根结点外,遍历到其他结点时在遍历其左子树之前加上左括号,在遍历完右子树后加上右括号\nvoid BiTreeToExp( BiTree T, int deep )\n{\n    if ( T == NULL ) return;\n    else if ( !T->lchild && !T->rchild ) visit( T );\n    else\n    {\n        if ( deep > 1 ) printf( \"(\" );\n        BiTreeToExp( T->lchild, deep + 1 );\n        visit( T );\n        BiTreeToExp( T->rchild, deep + 1 );\n        if ( deep > 1 ) printf( \"(\" );\n    }\n}\n```\n\n### 22.求孩子兄弟表示法存储的森林的叶子节点数\n\n```cpp\ntypedef struct CSNode\n{\n    ElemType data;\n    struct CSNode *firstchild, *nextsibling;\n}CSNode, *CSTree;\n\nint Leaves( CSTree T )\n{\n    if ( T == NULL ) return 0;\n    if ( T->firstchild == NULL )\n        return 1 + Leaves( T->nextsibling );\n    else\n        return Leaves( T->firstchild ) + Leaves( T->nextsibling );\n}\n```\n\n### 23.以孩子兄弟链表为存储结构,求树的高度(深度)(递归)\n\n```cpp\nint Height( CSTree T )\n{\n    if ( T == NULL ) return 0;\n    int hc, hs;\n    hc = Height( T->firstchild ) + 1;\n    hs = Height( T->nextsibling );\n    return hc > hs ? hc : hs;\n}\n```\n\n### 24.二叉排序树的查找(非递归)\n\n```cpp\nBiTree BSTSearch( BiTree T, ElemType key )\n{\n    while ( T && key != T->data )\n    {\n        if ( key < T->data )\n            T = T->lchild;\n        else\n            T = T->rchild;\n    }\n    return T;\n}\n```\n或\n```cpp\nBiTree BSTSearch( BiTree T, ElemType key )\n{\n    while ( T )\n    {\n        if ( T->data == key ) return T;\n        else if ( T->data > key )\n            T = T->lchild;\n        else\n            T = T->rchild;\n    }\n    return T;\n}\n```\n\n### 25.二叉排序树的插入(递归)\n\n```cpp\nbool BSTInsert( BiTree& T, ElemType key )\n{\n    if (!T)\n    {\n        T = ( BiTree ) malloc( sizeof( BiTNode ) );\n        T->data = key;\n        T->lchild = T->rchild = NULL;\n        return true;\n    }\n    else if ( T->data == key ) return false;\n    else if ( T->data > key ) return BSTInsert( T->lchild, key );\n    else return BSTInsert( T->rchild, key );\n}\n```\n\n### 26.计算二叉树的带权路径长度(递归)\n\n```cpp\nint wpl = 0;\nint WPL_PreOrder( BiTree T, int deep )\n{\n    if ( T == NULL ) return 0;\n    if ( !T->lchild && !T->rchild )\n        wpl += deep * T->weight;\n    else\n    {\n        if ( T->lchild ) WPL_PreOrder( T->lchild, deep + 1 );\n        if ( T->rchild ) WPL_PreOrder( T->rchild, deep + 1 );\n    }\n    return wpl;\n}\n```\n\n### 27.计算二叉树的带权路径长度(非递归)\n\n```cpp\n// 思路: 层序遍历的思想\nint wpl = 0;\nint WPL_LevelOrder( BiTree T )\n{\n    if ( T == NULL ) return 0;\n    BiTree Q[MAXSIZE], p;\n    int front = -1, rear = -1, depth = 0, last = 0;\n    Q[++rear] = T;\n    while ( front != rear )\n    {\n        p = Q[++front];\n        if ( !p->lchild && !p->rchild )\n            wpl += depth * p->weight;\n        else\n        {\n            if ( p->lchild ) Q[++rear] = p->lchild;\n            if ( p->rchild ) Q[++rear] = p->rchild;\n        }\n        if ( front == last )\n        {\n            depth++; last = rear;\n        }\n    }\n    return wpl;\n}\n```\n\n### 28.判断二叉树是否为二叉排序树\n\n```cpp\nElemType preVal = MIN;\nbool IsBST( BiTree T )\n{\n    if ( T == NULL ) return true;\n    if ( !IsBST( T->lchild ) ) return false;\n    if ( preVal >= T->data )\n        return false;\n    else\n        preVal = T->data;\n    if ( !IsBST( T->rchild ) ) return false;\n    return true;\n}\n```\n\n### 29.求出指定结点在二叉排序树中的层次\n\n```cpp\nint Level( BiTree T, BiTree p )\n{\n    if ( T == NULL ) return 0;\n    int n = 1;\n    while ( T->data != p->data )\n    {\n        n++;\n        if ( p->data < T->data )\n            T = T->lchild;\n        else\n            T = T->rchild;\n    }\n    return n;\n}\n```\n\n### 30.判断二叉树是否为平衡二叉树\n\n```cpp\nbool IsAVL( BiTree T, int& h )\n{\n    int h1 = 0, h2 = 0;\n    if (T==NULL )\n    {\n        h = 0; return true;\n    }\n    if ( IsAVL( T->lchild, h1 ) && IsAVL( T->rchild, h2 ) )\n    {\n        if ( abs( h1 - h2 ) <= 1 )\n        {\n            h = 1 + ( h1 > h2 ? h1 : h2 );\n            return true;\n        }\n    }\n    return false;\n}\n```\n\n### 31.从大到小输出二叉排序中所有值不小于k的关键字\n\n```cpp\nvoid DesOutput( BiTree T, ElemType k )\n{\n    if ( T == NULL ) return;\n    DesOutput( T->rchild, k );\n    if ( T->data >= k )\n        visit( T );\n    else\n        return;\n    DesOutput( T->lchild, k );\n}\n```\n\n### 32.在二叉排序树上查找第k(1≤k≤n)小的元素,要求平均时间复杂度为O(log2n)二叉排序树上的每个结点中除data,lchild,rchild外,还增加一个count成员,保存以该结点为根的子树上的结点个数\n\n> 法一\n\n```cpp\nBiTree SearchSmallK( BiTree T, int k )\n{\n    if ( k<1 || k>T->count ) return NULL;\n    if ( T->lchild )\n    {\n        if ( k <= T->lchild->count )\n            return SearchSmallK( T->lchild, k );\n        else if ( k == T->lchild->count + 1 )\n            return T;\n        else\n            return SearchSmallK( T->rchild, k - ( T->lchild->count + 1 ) );\n    }\n    else\n    {\n        if ( k == 1 ) return T;\n        else return SearchSmallK( T->rchild, k - 1 );\n    }\n}\n```\n\n> 法二\n\n```cpp\nBiTree SearchSmallK( BiTree T, int k )\n{\n    if ( k<1 || k>T->count ) return NULL;\n    if ( T->lchild )\n    {\n        if ( k <= T->lchild->count )\n            return SearchSmallK( T->lchild, k );\n        else\n            k -= T->lchild->count;\n    }\n    if ( k == 1 ) return T;\n    if ( T->rchild )\n        return SearchSmallK( T->rchild, k - 1 );\n}\n```\n\n### 33.对于含有+,−,∗,/及括号的算术表达式(中缀表达式)写一个算法,将该表达式构造成相应的二叉树表示\n\n```cpp\n// 思想: 最后使用的操作符作为根.即:先+,-后*,/\n// 例如: a+b*(c-d)-e/f构造的表达式树如下:\n//                -\n//               /  \\\n//              +    /\n//             / \\  / \\\n//            a  *  e  f\n//              / \\\n//             b   -\n//                / \\\n//               c   d\n// 通过该表达式树,可以很容易得到:\n// 前缀表达式: -+a*b-cd/ef\n// 中缀表达式: a+b*c-d-e/f\n// 后缀表达式: abcd-*+ef/-\nBiTNode* BuildTree( char* exp, int s, int e )\n{\n    if ( e - s == 1 )\n    {\n        BiTNode* p = ( BiTNode* ) malloc( sizeof( BiTNode ) );\n        p->data = exp[s];\n        p->lchild = p->rchild = NULL;\n        return p;\n    }\n    int c1 = -1, c2 = -1, c = 0, i;\n    for ( i = s; i < e; i++ )\n    {\n        if ( exp[i] == '(' ) c++;\n        else if ( ( exp[i] == '+' || exp[i] == '-' ) && !c )\n            c1 = i;\n        else if ( ( exp[i] == '*' || exp[i] == '/' ) && !c )\n            c2 = i;\n    }\n    if ( c1 < 0 ) c1 = c2;\n    if ( c1 < 0 ) return BuildTree( exp, s + 1, e - 1 );\n    BiTree* p = ( BiTNode* ) malloc( sizeof( BiTNode ) );\n    p->data = exp[c1];\n    p->lchild = BuildTree( exp, s, c1 );\n    p->rchild = BuildTree( exp, c1 + 1, e );\n    return p;\n}\n```","tags":["树"],"categories":["algorithm"]},{"title":"[algorithm]图","url":"%2Fposts%2F3164%2F","content":"## 一.图的算法\n\n### 1. 邻接矩阵和邻接表的表示\n\n#### 1). 邻接矩阵表示的数据结构\n\n```cpp\n#define INFINITY INT_MAX // 无穷大\n#define MAX_VERTEX_NUM 20 // 限制顶点最大数值为20个\n#define MAX_ARC_NUM  MAX_VERTEX_NUM * (MAX_VERTEX_NUM - 1) // 由n个顶点，最多可以确定n(n-2)/2条直线,有向图为2倍\n#define MAX_INFO 20 // 用户输入的弧信息，最多20个字符\n\n/*数组表示法*/\ntypedef int        VRType;\ntypedef char    InfoType;\ntypedef char    VertexType[5];\ntypedef enum    {DG, DN, UDG, UDN} GraphKind; \n\ntypedef struct ArcCell {\n    VRType adj;\n    InfoType *info;\n}ArcCell, AdjMatrix[MAX_VERTEX_NUM][MAX_VERTEX_NUM];\n\ntypedef struct {\n    VertexType    vexs[MAX_VERTEX_NUM];\n    AdjMatrix    arcs;\n    int            vexnum, arcnum;\n}MGraph;\n```\n\n#### 2). 邻接表表示的数据结构\n\n```cpp\n/*邻接表表示法*/\ntypedef struct ArcNode\n{\n    int                adjvex;\n    int                w; // 存储权值，书中的程序没有表示权值的数据成员(书中说用info来存储权值，但是上面的程序又是单独用的adj存权值，为了一致性，info还是用来存储其他信息算了)\n    struct ArcNode    *nextarc;\n    InfoType *info; // 用来存储权值以外的有关弧的信息\n}ArcNode;\n\ntypedef struct VNode\n{\n    VertexType    data;\n    ArcNode        *firstarc;\n}VNode, AdjList[MAX_VERTEX_NUM];\n\ntypedef struct\n{\n    AdjList        vertices;\n    int            vexnum, arcnum;\n    int            kind;\n}ALGraph;\n```\n\n### 2.写出从图的邻接表表示转换成邻接矩阵表示的算法\n\n```cpp\nvoid Convert( ALGraph G, int arcs[][10] )\n{\n    for ( int v = 0; v < G.vexnum; v++ )\n        for ( ArcNode* p = G.vertices[v].firstarc; p; p->nextarc )\n            arcs[v][p->adjvex] = 1;\n}\n```\n\n## 二.图的遍历\n\n>说明: 以下图的算法既可以使用邻接矩阵的方式也可以使用邻接表存储的方式,因此每种算法都可以换成另一种存储形式,只需要把MGraph(邻接矩阵存储)换成ALGraph(邻接表存储)即可\n\n### 1. 寻找邻接点\n\n#### 1). 邻接矩阵下,通用找邻接的函数:\n\n```cpp\n// 第一个邻居\nint FirstNeighbor( MGraph G, int v)\n{ \n    for ( int i = 0; i < G.vexnum; i++ )\n        if ( G.arcs[v][i] == 1 )\n            return i;\n    return -1;\n}\n// 当前的下一个邻居\nint NextNeighbor( MGraph G, int v, int w )\n{\n    for (int i = w+1; i < G.vexnum; i++ )\n        if ( G.arcs[v][i] == 1 )\n            return i;\n    return -1;\n}\n```\n\n#### 2). 邻接表下,通用找邻接的函数:\n\n```cpp\n/*全局变量*/\nbool Visited[MAX_VERTEX_NUM]; // 记录每个顶点是否被访问过\n\n// 找到第一个v相邻的顶点，返回它的下标\nint FirstAdjVex(ALGraph &AL, int v)\n{\n    ArcNode *p = NULL;\n    p = AL.vertices[v].firstarc;\n    if (p == NULL)\n        return -1;\n    else\n        return p->adjvex;\n}\n\n// 找到下一个与v相邻的顶点，返回它的下标\nint NextAdjVex(ALGraph &AL, int v, int w)\n{\n    ArcNode *p = NULL;\n    p = AL.vertices[v].firstarc;\n    while (p->adjvex != w) // 找到下标为w的结点\n        p = p->nextarc;\n    p = p->nextarc; // 指针指向下标为w的结点的后面一个结点\n    if (p == NULL)\n        return -1;\n    else\n        return p->adjvex;\n}\n```\n\n### 2. 遍历方法(BFS+DFS)\n\n#### 1). 广度优先搜索(Breadth-First-Search, BFS)\n\n>法一:采用邻接矩阵\n\n```cpp\nbool visited[MAX_VERTEX_NUM] = { false };\nvoid BFS( MGraph G, int v );\n\nvoid BFSTraverse( MGraph G )\n{\n    int Q[MAXSIZE];\n    int front = -1, rear = -1;\n    for (int v = 0; v < G.vexnum; v++ )\n        if ( !visited[v] )\n            BFS( G, v );\n}\n\nvoid BFS( MGraph G, int v )\n{\n    int Q[MAXSIZE];\n    int front = -1, rear = -1;\n    // BFS顶点三连\n    visit( v );\n    visited[v] = true;\n    Q[++rear] = v;\n    \n    while ( front != rear )\n    {\n        v = Q[++front];\n        for ( int w = FirstNeighbor( G, v ); w >= 0; w = NextNeighbor( G, v, w ) )\n        {\n            if (!visited[w])\n            {\n                // BFS顶点三连\n                visit( w );\n                visited[w] = true;\n                Q[++rear] = w;\n            }\n        }\n    }\n}\n```\n\n>法二:采用邻接表\n\n```cpp\nbool visited[MAX_VERTEX_NUM] = { false };\nvoid BFS( ALGraph G, int v );\n\nvoid BFSTraverse( ALGraph G )\n{\n    int Q[MAXSIZE];\n    int front = -1, rear = -1;\n    for (int v = 0; v < G.vexnum; v++ )\n        if ( !visited[v] )\n            BFS( G, v );\n}\n\nvoid BFS( ALGraph G, int v )\n{\n    int Q[MAXSIZE];\n    int front = -1, rear = -1;\n    // BFS顶点三连\n    visit( v );\n    visited[v] = true;\n    Q[++rear] = v;\n    \n    while ( front != rear )\n    {\n        v = Q[++front];\n        for ( int w = FirstNeighbor( G, v ); w >= 0; w = NextNeighbor( G, v, w ) )\n        {\n            if (!visited[w])\n            {\n                // BFS顶点三连\n                visit( w );\n                visited[w] = true;\n                Q[++rear] = w;\n            }\n        }\n    }\n}\n```\n\n#### 2). 深度优先搜索(Depth-First-Search, DFS)\n\n```cpp\nbool visited[MAX_VERTEX_NUM] = { false };\nvoid DFS( ALGraph &G, int v );\n\nvoid DFSTraverse( ALGraph &G )\n{\n    for ( int v = 0; v < G.vexnum; v++ )\n        if ( !visited[v] )\n            DFS( G, v );\n}\n\nvoid DFS( ALGraph &G, int v )\n{\n    visit( v );\n    visited[v] = true;\n    for ( int w = FirstNeighbor( G, v ); w >= 0; w = NextNeighbor( G, v, w ) )\n        if ( !visited[w] )\n            DFS( G, w );\n}\n```\n\n## 三.综合算法\n\n#### 1. BFS算法求解单源最短路径问题\n\n```cpp\nbool visited[MAXSIZE] = { false };\nunsigned int d[MAXSIZE] = { INFINITE };\nvoid BFS_MIN_Distance( ALGraph G, int u )\n{\n    BiTree Q[MAXSIZE];\n    int front = -1, rear = -1, v, w;\n    // BFS路径三连\n    d[u] = 0;\n    visited[u] = true;\n    Q[++rear] = u;\n    \n    while ( front != rear )\n    {\n        v = Q[++front];\n        for ( w = FirstNeighbor( G, v ); w >= 0; w = NextNeighbor( G, v, w ) )\n        {\n            if (!visited[w])\n            {\n                // BFS路径三连\n                d[w] = d[v] + 1;\n                visited[w] = true;\n                Q[++rear] = w;\n            }\n        }\n    }\n}\n```\n\n#### 2.设计一个算法,判断一个无向图G是否为一棵树\n\n```cpp\nint visited[MAXSIZE] = { 0 };\nvoid DFS( MGraph G, int v, int& Vnum, int& TD );\n\nbool IsTree( MGraph G )\n{\n    int Vnum = 0, TD = 0;    // TD=total degree总度数\n    DFS( G, 0, Vnum, TD );    // 从第一个顶点开始遍历\n    if ( Vnum == G.vexnum&&TD == 2 * ( G.vexnum - 1 ) )\n        return true;\n    return false;\n}\n\nvoid DFS( MGraph G, int v, int& Vnum, int& TD )\n{\n    visited[v] = true; Vnum++;\n    for ( int w = FirstNeighbor( G, v ); w >= 0; w = NextNeighbor( G, v, w ) )\n        if ( !visited[w] )\n            DFS( G, w, Vnum, TD );\n}\n```\n\n#### 3.写出图的深度优先搜索DFS算法的非递归算法\n\n```cpp\nbool visited[MAXSIZE] = { false };\nvoid DFS_NON_RC( MGraph G, int v )\n{\n    int S[MAXSIZE];\n    int top = -1;\n    for ( int i = 0; i < G.vexnum; i++ )\n        visited[i] = false;\n    // 顶点二连\n    visited[v] = true;\n    S[++top] = v;\n\n    while ( top != -1 )\n    {\n        v = S[top--]; \n        visit( v );\n        for ( int w = FirstNeighbor( G, v ); w >= 0; w = NextNeighbor( G, v, w ) )\n            if ( !visited[w] )\n            {\n                // 顶点二连\n                visited[w] = true;\n                S[++top] = w;\n            }\n    }\n}\n```\n\n#### 4.分别采用基于广度优先遍历和深度优先遍历算法判别以邻接表或邻接矩阵存储的有向图中是否存在由顶点v到顶点u的路径(v≠u)\n\n```cpp\n// 采用BFS的方法\nbool visited[MAXSIZE] = { false };\nbool Exist_Path_BFS( MGraph G, int v, int u )\n{\n    int Q[MAXSIZE];\n    int front = -1, rear = -1;\n    visited[v] = true;\n    Q[++rear] = v;\n    while ( front != rear )\n    {\n        v = Q[++front];\n        for ( int w = FirstNeighbor( G, v ); w >= 0; w = NextNeighbor( G, v, w ) )\n        {\n            if (!visited[w])\n            {\n                if ( w == u ) return true;\n                visited[w] = true;\n                Q[++rear] = w;\n            }\n        }\n    }\n    return false;\n}\n```\n\n```cpp\n// 采用DFS的方法\nbool visited[MAXSIZE] = { false };\nbool Exist_Path_DFS( MGraph G, int v, int u )\n{\n    if ( v == u ) return true;\n    visited[v] = true;\n    for ( int w = FirstNeighbor( G, v ); w >= 0; w = NextNeighbor( G, v, w ) )\n    {\n        if ( !visited[w] )\n        {\n            if ( Exist_Path_DFS( G, w, u ) ) return true;\n        }\n    }\n    return false;\n}\n```\n\n#### 5.拓扑排序:判断并输出有向图的拓扑序列\n\n```cpp\nbool Topological( MGraph G, int indegree[] )\n{\n    int S[MAXSIZE];\n    int top = -1, Vnum = 0, v = 0;\n    for ( v = 0; v < G.vexnum; v++ )\n    {\n        if ( indegree[v] == 0 )\n        {\n            visit( v );\n            Vnum++;\n            S[++top] = v;\n        }\n    }\n    while ( top != -1 )\n    {\n        v = S[top--];\n        for ( int w = FirstNeighbor( G, v ); w >= 0; w = NextNeighbor( G, v, w ) )\n        {\n            indegree[w]--;\n            if ( indegree[w] == 0 )\n            {\n                visit( w );\n                Vnum++;\n                S[++top] = w;\n            }\n        }\n    }\n    if ( Vnum == G.vexnum )\n        return true;\n    return false;\n}\n```\n\n#### 6.拓扑排序(DFS):有向无环图的拓扑排序\n\n```cpp\nbool visited[MAXSIZE] = { false };\nint time = 0, finishTime[MAXSIZE] = { 0 };\nvoid DFS( MGraph G, int v );\n\nvoid Topological_DFS( MGraph G )\n{\n    for ( int v = 0; v < G.vexnum; v++ )\n        if ( !visited[v] )\n            DFS( G, v );\n    for ( int t = time - 1; t >= 0; t-- )\n        visit( finishTime[t] );\n}\n\nvoid DFS( MGraph G, int v )\n{\n    visited[v] = true;\n    for ( int w = FirstNeighbor( G, v ); w >= 0; w = NextNeighbor( G, v, w ) )\n        if ( !visited[w] )\n            DFS( G, w );\n    finishTime[time++] = v;\n}\n```","tags":["图"],"categories":["algorithm"]},{"title":"[algorithm]查找","url":"%2Fposts%2F4660%2F","content":"## 一.查找的算法\n\n### 1.折半查找(二分查找)\n\n```cpp\nint binary_search( string s, char ch )\n{\n    int low = 0, high = s.size()-1;\n    while (low <= high)\n    {\n        int mid = ( low + high ) / 2;\n        if ( ch ==  s[mid] )\n            return mid;\n        else if ( ch < s[mid] )\n            high = mid - 1;\n        else\n            low = mid + 1;\n    }\n    return -1;\n}\n```\n时间复杂度: $O(log_2{n})$\n\n### 3.折半查找(二分查找)递归\n```cpp\nint binary_search_rc( string s, char ch, int low, int high )\n{\n    if ( low > high ) \n        return 0;\n    int mid = ( low + high ) / 2;\n    if ( ch == s[mid] ) \n        return mid;\n    else if ( ch < s[mid] )  \n        high = mid - 1;\n    else \n        low = mid + 1;\n    return binary_search_rc( s, ch, low, high );\n}\n```\n时间复杂度: $O(log_2{n})$\n\n---\n## 二.字符串匹配\n\n### 1.简单的模式匹配算法(朴素模式匹配算法)\n\n```cpp\nint naive_search( string S, string T )\n{\n    int i = 0, j = 0, lenS = S.size(), lenT = T.size();\n    while ( i < lenS && j < lenT )\n    {\n        if ( S[i] == T[j] ) { i++; j++; }\n        else { i = i - j + 1; j = 0; }\n    }\n    if ( j >= lenS ) return i - lenT;\n    return 0;\n}\n```\n时间复杂度: $O(m*n)$\n\n### 2.KMP算法\n\n算法需要先求出模式串的next值:\n\n```cpp\nvoid get_next( string T, int next[] )\n{\n    int i = 0, j = -1, lenT = T.size();\n    next[0] = -1;\n    while (i<lenT)\n    {\n        if (j==-1||T[i]==T[j] )\n        {\n            i++; j++; next[i] = j;\n        }\n        else j = next[j];\n    }\n}\n```\n\n也可求出改进后的nextval值:\n\n```cpp\nvoid get_nextval( string T, int nextval[] )\n{\n    int i = 0, j = -1, lenT = T.size();\n    nextval[0] = -1;\n    while ( i < lenT )\n    {\n        if ( j == -1 || T[i] == T[j] )\n        {\n            i++, j++;\n            if ( T[i] == T[j] )\n                nextval[i] = nextval[j];\n            else\n                nextval[i] = j;\n        }\n        else j = nextval[j];\n    }\n}\n```\n\n以下是KMP算法:\n```cpp\nint KMP( string S, string T, int next[], int pos )\n{\n    int i = pos, j = 0, lenS = S.size(), lenT = T.size();\n    while ( i<lenS&&j<lenT )\n    {\n        if ( j == -1 || S[i] == T[j] ) { i++; j++; }\n        else j = next[j];\n    }\n    if ( j >= lenT ) return i - lenT;\n    return 0;\n}\n```\n时间复杂度: $O(m+n)$","tags":["查找"],"categories":["algorithm"]},{"title":"[leetcode]214.最短回文串","url":"%2Fposts%2F44722%2F","content":"{% asset_img Image21.png %}\n### 方法一: KMP算法\n\n时间复杂度: $O(m+n)$\n\n> 解题思路: 实际就是求原串从左到右的最长回文串(必须包含左边所有字符),此处采用**倒置+KMP算法**来缩短匹配时间\n>   1. 将字符串倒置,原串作为模式串pat,倒置串作为主串txt\n>   2. 求出模式串pat的nextval[]值,然后进行字符串匹配,得到的模式串pat最长匹配长度即为模式串pat从第一个字符开始的最大回文串(匹配过程时间复杂度只需要$O(m+n)$)\n\n对于**KMP(Knuth–Morris–Pratt)** 算法,可以借鉴关于[查找][1]部分的介绍\n\n```cpp\nint nextval[40005];\nvoid get_nextval(string pat)\n{\n    int i=0,j=-1,len=pat.size();\n    nextval[0]=-1;\n    for (;i<len&&j<len;)\n    {\n        if (j==-1||pat[i]==pat[j])\n        {\n            i++,j++;\n            if (pat[i]==pat[j])\n                nextval[i]=nextval[j];\n            else\n                nextval[i]=j;\n        }\n        else j=nextval[j];\n    }\n}\nstring shortestPalindrome(string txt) {\n    string pat=txt;\n    reverse(txt.begin(),txt.end()); // 字符串倒置\n    get_nextval(pat); // 计算模式串的nextval值\n    int i=0,j=0,lenTxt=txt.size(),lenPat=pat.size();\n    for(;i<lenTxt&&j<lenPat;)   // 进行模式串匹配,找出最大匹配长度\n    {\n        if (j==-1||txt[i]==pat[j])\n            i++,j++;\n        else\n            j = nextval[j];\n    }\n    return txt.substr(0,lenTxt-j)+pat;\n}\n```\n\n","tags":["KMP"],"categories":["OJ"]},{"title":"[leetcode]TOC汇总","url":"%2Fposts%2F6542%2F","content":"| Date | Times | Title                                                    | Methods(t)                  | T(n)          |\n| :--- | :---- | :------------------------------------------------------- | :-------------------------- | :------------ |\n| 4/22 | 2     | 5.最长回文子串                                           | 中心扩展法                  | $O(n^2)$      |\n|      | 2     |                                                          | Manacher                    | $O(n)$        |\n|      | 1     | 214.最短回文串                                           | 逆置+KMP                    | $O(m+n)$      |\n|      | 1     | 28.  实现strStr()                                        | KMP                         | $O(m+n)$      |\n|      |       | (*)239. 滑动窗口最大值                                  |                             | $$            |\n| 4/23 |       | 53.最大子序列和                                          |                             | $$            |\n|      |       | 152.乘积最大子序列                                       |                             | $$            |\n|      |       | 分数拆分                                                 |                             | $$            |\n|      |       | 378.有序矩阵中第K小的元素                                |                             | $$            |\n| 4/24 |       | 718. 最长重复子数组                                      |                             | $$            |\n|      |       | 1.   最长上升子序列                                      |                             | $$            |\n|      |       | 2. 无重复字符的最长子串                                  |                             | $$            |\n| 4/25 |       | 340.Longest Substring with At Most K Distinct Characters |                             | $$            |\n|      |       | 992.Subarrays with K Different Integers                  |                             | $$            |\n|      |       | 14.最长公共前缀                                          |                             | $$            |\n|      |       | 7.整数反转                                               |                             | $$            |\n| 4/26 |       | 219. 存在重复元素 II                                     |                             | $$            |\n|      |       | 209. 长度最小的子数组                                    |                             | $$            |\n|      |       | 76. 最小覆盖子串                                         |                             | $$            |\n|      |       | 13. 罗马数字转整数                                       |                             | $$            |\n|      |       | 9. 回文数                                                |                             | $$            |\n|      |       | 12. 整数转罗马数字                                       |                             | $$            |\n|      |       | 15. 三数之和                                             |                             | $$            |\n| 4/27 |       | 43. 字符串相乘                                           |                             | $$            |\n|      |       | 67. 二进制求和                                           |                             | $$            |\n|      |       | 66. 加一                                                 |                             | $$            |\n|      |       | 100. 相同的树                                            |                             | $$            |\n|      |       | 94. 二叉树的中序遍历                                     |                             | $$            |\n| 4/28 |       | 5039. 移动石子直到连续                                   |                             | $$            |\n|      |       | (*)\t5042. 逃离大迷宫                                     |                             | $$            |\n| 4/29 |       | 5000. 从始点到终点的所有路径                             |                             | $$            |\n|      |       | 797. 所有可能的路径                                      |                             | $$            |\n|      |       | (*)\t257. 二叉树的所有路径                                |                             | $$            |\n| 4/30 |       | (*)\t5040. 边框着色                                       |                             | $$            |\n|      |       | (*)\t133. 克隆图                                          |                             | $$            |\n|      |       | (*)\t138. 复制带随机指针的链表                            |                             | $$            |\n| 5/1  |       | 144.二叉树的前序遍历                                     |                             | $$            |\n|      |       | 94.二叉树的中序遍历                                      |                             | $$            |\n|      |       | 145.二叉树的后序遍历                                     |                             | $$            |\n| 5/2  |       | (*)\t968.监控二叉树                                       |                             | $$            |\n|      |       | 590. N叉树的后序遍历                                     |                             | $$            |\n|      |       | 429. N叉树的层序遍历                                     |                             | $$            |\n|      |       | 589. N叉树的前序遍历                                     |                             | $$            |\n|      |       | (*)\t106. 从中序与后序遍历序列构造二叉树                  |                             | $$            |\n|      |       | (*)\t105. 从前序与中序遍历序列构造二叉树                  |                             | $$            |\n|      |       | 559. N叉树的最大深度                                     |                             | $$            |\n|      |       | 104. 二叉树的最大深度                                    |                             | $$            |\n|      |       | 111. 二叉树的最小深度                                    |                             | $$            |\n|      |       | 102. 二叉树的层次遍历                                    |                             | $$            |\n|      |       | 107. 二叉树的层次遍历 II                                 |                             | $$            |\n|      |       | 637. 二叉树的层平均值                                    |                             | $$            |\n|      |       | (*)\t114. 二叉树展开为链表                                |                             | $$            |\n| 5/5  |       | 5051. 有效的回旋镖                                       |                             | $$            |\n|      |       | 5050. 从二叉搜索树到更大和树                             |                             | $$            |\n|      |       | 5.最长回文子串(manacher)                                 |                             | $$            |\n| 5/7  |       | 1039. Minimum Score Triangulation of Polygon             | DP                          | $O(n^3)$      |\n| 5/8  |       | 312. Burst Balloons                                      | DP                          | $O(n^3)$      |\n|      |       | 375. Guess Number Higher or Lower II                     | DP                          | $O(n^3)$      |\n| 5/9  |       | 118.杨辉三角                                             | DP                          | $O(n^2)$      |\n|      |       | 119.杨辉三角 II                                          | DP                          | $O(n^2)$      |\n|      |       |                                                          | 公式法                      | $O(n)$        |\n|      |       | 110. 平衡二叉树                                          | 递归                        | $O(n)$        |\n|      |       | 101. 对称二叉树                                          | 递归                        | $O(n)$        |\n|      |       | 103. 二叉树的锯齿形层次遍历                              | 层次遍历+奇数逆序           |               |\n| 5/11 |       | 17. 电话号码的字母组合                                   | 递归+回溯(1)                |               |\n| 5/12 |       | 401.二进制手表                                           | 暴力破解                    |               |\n|      |       | 22. 括号生成                                             | 递归+回溯(1)                |               |\n|      |       | 5055. 困于环中的机器人                                   |                             | $O(n)$        |\n| 5/13 |       | 39. 组合总和                                             | 递归+回溯(1)                |               |\n| 5/14 |       | 6. Z 字形变换                                            | 找规律                      | $O(n)$        |\n|      |       | 8. 字符串转换整数 (atoi)                                 |                             | $O(n)$        |\n|      |       | 190. 颠倒二进制位                                        |                             |               |\n| 5/15 |       | 20. 有效的括号                                           |                             |               |\n|      |       | 121. 买卖股票的最佳时机                                  | DP                          | $O(n)$        |\n|      |       | 122. 买卖股票的最佳时机 II                               | DP                          | $O(n)$        |\n| 5/16 |       | 123. 买卖股票的最佳时机 III                              | DP                          | $O(n)$        |\n|      |       | 188. 买卖股票的最佳时机 IV                               | DP+Greedy                   | $O(n*k)$      |\n|      |       | 376. 摆动序列                                            | 找规律                      | $O(n)$        |\n| 5/17 |       | 11. Container With Most Water                            | 双指针法                    | $O(n)$        |\n|      |       | 19. 删除链表的倒数第N个节点                              | 插入头结点法                | $O(n)$        |\n|      |       | 16. 最接近的三数之和                                     | 双指针法                    | $O(n^2)$      |\n|      |       | 18. 四数之和                                             | 双指针法+去重               |               |\n|      |       | 21. 合并两个有序链表                                     | 头结点+余项链接             | $O(max(m,n))$ |\n|      |       | 23. 合并K个排序链表                                      | 头结点+余项链接             |               |\n| 5/18 |       | 24. 两两交换链表中的节点                                 | 递归                        |               |\n|      |       | 25. k个一组翻转链表                                      | 递归                        |               |\n|      |       | 27. 移除元素                                             | 直接插入法                  | $O(n)$        |\n|      |       | 26. 删除排序数组中的重复项                               | 直接插入法                  | $O(n)$        |\n|      |       | 80. 删除排序数组中的重复项 II                            | 直接插入法                  | $O(n)$        |\n| 5/19 |       | (*) 10. 正则表达式匹配                                   | DP                          | $O(m*n)$      |\n|      |       | 35. 搜索插入位置                                         |                             | $O(n)         |\n|      |       | 34. 在排序数组中查找元素的第一个和最后一个位置           | 二分查找                    | $O(log_2n)$   |\n| 5/20 |       | 62. 不同路径                                             | 排列组合                    | $O(n)$        |\n|      |       |                                                          | DP                          | $O(n*m)$      |\n|      |       | 63. 不同路径II                                           | DP                          | $O(n*m)$      |\n| 5/21 |       | 64. 最小路径和                                           | DP                          | $O(m*n)$      |\n|      |       | 174. 地下城游戏                                          | DP(bottom up)               |               |\n| 5/22 |       | 70.爬楼梯                                                | DP                          | $O(n)$        |\n|      |       |                                                          | Fibonacci                   | $O(1)$        |\n|      |       | RodCutting                                               | DP                          | $O(n^2)$      |\n| 5/24 |       | 204. 计数质数                                            | Eratosthenes                |               |\n|      |       | 279. 完全平方数                                          | DP                          |               |\n| 5/29 |       | 31. 下一个排列                                           |                             | $O(n)$        |\n|      |       | 46. 全排列                                               |                             | $O(n^2)       |\n|      |       | 60. 第k个排列                                            |                             |               |\n| 5/30 |       | 84.柱状图中最大的矩形                                    | 单调栈                      | $O(n)$        |\n|      |       | 85. 最大矩形                                             | 单调栈                      |               |\n| 5/31 |       | 221. 最大正方形                                          | 单调栈                      |               |\n| 6/2  |       | 303. 区域和检索 - 数组不可变                             | DP                          | $O(n)$        |\n|      |       |                                                          | 树状数组                    | $O(logn)$     |\n|      |       | 304. 二维区域和检索 - 矩阵不可变                         | DP                          |               |\n| 6/3  |       | 36. 有效的数独                                           | 约束法:行判断+列判断+块判断 |               |\n|      |       | 37. 解数独                                               | 约束法+回溯                 |               |\n| 6/4  |       | 459. 重复的子字符串                                      | 周期法                      |               |\n|      |       | 686. 重复叠加字符串匹配                                  | KMP                         |               |\n|      |       |                                                          |                             |               |\n|      |       |                                                          |                             |               |\n|      |       |                                                          |                             |               |\n|      |       |                                                          |                             |               |\n|      |       |                                                          |                             |               |","tags":["TOC"],"categories":["OJ"]},{"title":"[algorithm]排序","url":"%2Fposts%2F16867%2F","content":"## 一.排序算法\n\n### 1.插入排序\n\n#### 1) 直接插入排序:(插入类)\n```cpp\nvoid InsertSort( ElemType R[], int n )\n{\n    for ( int i = 2; i <= n; i++ )\n    {\n        if ( R[i].key < R[i - 1].key )\n        {\n            R[0] = R[i];\n            for ( int j = i - 1; j > 0 && ( R[0].key < R[j].key ); j-- )\n                R[j + 1] = R[j];\n            R[j + 1] = R[0];\n        }\n    }\n}\n```\n最好情况(顺序有序):\n\n　　1)比较次数: $\\sum_{i=2}^{n} 1=n-1$\n\n　　2)移动次数: 0\n\n最坏情况(逆序有序):\n\n　　1)比较次数: $\\sum_{i=2}^{n} i=\\frac {(n+2)(n-1)}{2}$\n\n　　2)移动次数: $\\sum_{i=2}^{n} (i+1)=\\frac {(n+4)(n-1)}{2}$\n\n#### 2)折半插入排序:(插入类)\n```cpp\nvoid BiInsertSort( ElemType R[], int n )\n{\n    for ( int i = 2; i <= n; i++ )\n    {\n        R[0] = R[i];\n        int low = 1, high = i - 1;\n        while ( low <= high )\n        {\n            int mid = ( low + high ) / 2;\n            if ( R[0].key < R[m].key ) high = mid - 1;\n            else low = mid + 1;\n        }\n        for ( int j = i - 1; j > high; j-- )\n            R[j + 1] = R[j];\n        R[j + 1] = R[0];\n    }\n}\n```\n#### 3)希尔排序(又称缩小增量排序)(插入类)\n```cpp\n// 当dk=1时,即为直接插入排序\nvoid ShellSort( ElemType R[], int n )\n{\n    for ( int dk = n / 2; dk >= 1; dk /= 2 )\n    {\n        for ( int i = dk + 1; i <= n; i++ )\n        {\n            if ( R[i].key < R[i - dk].key )\n            {\n                R[0] = R[i];\n            for ( j = i - dk; j > 0 && ( R[0].key < R[j].key ); j -= dk )\n                    R[j + dk] = R[j];\n                R[j + dk] = R[0];\n            }\n        }\n    }\n}\n```\n### 2.交换排序\n\n#### 1)起泡排序(冒泡排序)(交换类)\n```cpp\nvoid BubbleSort( ElemType R[], int n )\n{\n    for ( int i = 1; i <= n - 1; i++ )\n    {\n        bool flag = false;\n        for ( int j = n; j > i; j-- )\n        {\n            if (R[j].key < R[j-1].key )\n            {\n                swap( R[j], R[j - 1] );\n                flag = true;\n            }\n        }\n        if ( !flag ) return;\n    }\n}\n```\n#### 2)快速排序:(交换类)\n```cpp\nvoid Partition( ElemType R[], int low, int high );\n\n// 快排\nvoid QuickSort( ElemType R[], int low, int high )\n{\n    if ( low >= high ) return;\n    int pivotpos = Partition( R, low, high );\n    QuickSort( R, low, pivotpos - 1 );\n    QuickSort( R, pivotpos + 1, high );\n}\n\n// 划分\nvoid Partition( ElemType R[], int low, int high )\n{\n    ElemType pivot = R[low];\n    while ( low < high )\n    {\n        while ( low < high && R[high].key >= pivot.key ) high--;\n        R[low] = R[high];\n        while ( low < high && R[low].key <= pivot.key ) low++;\n        R[high] = R[low];\n    }\n    R[low] = pivot;\n    return low;\n}\n```\n### 3.选择排序\n\n#### 1)简单选择排序(选择类)\n```cpp\nvoid SelectSort( ElemType R[], int n )\n{\n    for ( int i = 0; i < n - 1; i++ )\n    {\n        int min = i;\n        for ( int j = i + 1; j < n; j++ )\n        {\n            if ( R[j].key < R[min].key ) min = j;\n        }\n        if ( min != i ) swap( R[i], R[min] );\n    }\n}\n```\n#### 2)堆排序(选择类)\n```cpp\nvoid AdjustDown( ElemType R[], int s, int n );\n\nvoid HeapSort( ElemType R[], int n )\n{\n    for ( int i = n / 2; i > 0; i-- )\n        void AdjustDown( R, i, n );\n    for ( int i = n; i > 1; i-- )\n    {\n        swap( R[i], R[1] );\n        AdjustDown( R, 1, i - 1 );\n    }\n}\n\n// 向下调整\nvoid AdjustDown( ElemType R[], int s, int n )\n{\n    R[0] = R[s];\n    for ( int i = 2 * s; i <= n; i *= 2 )\n    {\n        if ( i < n&&R[i].key < R[i + 1].key ) i++;\n        if (R[0].key  >=R[i].key ) break;\n        else\n        {\n            R[s] = R[i]; s = i;\n        }\n    }\n    R[s] = R[0];\n}\n```\n```cpp\n// 向上调整\nvoid AdjustUp( ElemType R[], int s )\n{\n    R[0] = R[s];\n    int p = s / 2;\n    while ( p >&& R[p].key < R[0].key )\n    {\n        R[s] = R[p];\n        s = p;\n        p /= 2;\n    }\n    R[s] = R[0];\n}\n```\n### 4.归并排序(归并类)\n```cpp\nvoid Merge( ElemType R[], int low, int mid, int high );\n\nvoid MergeSort( ElemType R[], int low, int high )\n{\n    if ( low >= high ) return;\n    int mid = ( low + high ) / 2;\n    MergeSort( R, low, mid );\n    MergeSort( R, mid + 1, high );\n    Merge( R, low, mid, high );\n}\n\nElemType B[MAXSIZE];\nvoid Merge( ElemType R[], int low, int mid, int high )\n{\n    int i,j,k;\n    for ( i = low; i <= high; i++ )\n        B[i] = R[i];\n    i = k = low, j = mid + 1;\n    while ( i <= mid && j <= high )\n    {\n        if ( B[i].key <= B[j].key )\n            R[k++] = B[i++];\n        else\n            R[k++] = B[j++];\n    }\n    while ( i <= mid ) R[k++] = B[i++];\n    while ( j <= high ) R[k++] = B[j++];\n}\n```\n## 二.综合题(算法)\n\n1.设顺序表用数组R[]表示,表中存储在数组下标1~m+n的范围内,前m个元素递增有序,后n个元素递增有序,设计一个算法,使得整个顺序表有序\n```cpp\nvoid InsertSort( ElemType R[], int m, int n )\n{\n    for ( int i = m + 1; i <= m + n; i++ )\n    {\n        if ( R[i].key < R[i - 1].key )\n        {\n            R[0] = R[i];\n            for ( int j = i - 1; j > 0 && ( R[0].key < R[j].key ); j-- )\n                R[j + 1] = R[j];\n            R[j + 1] = R[0];\n        }\n    }\n}\n```\n2.计数排序:对表进行排序并将结果放到另一个新的表中,要求表中所有关键码互不相同\n```cpp\nvoid CountSort( ElemType A[], ElemType B[], int n )\n{\n    for ( int i = 0; i < n; i++ )\n    {\n        int cnt = 0;\n        for ( int j = 0; j < n; j++ )\n            if ( A[i].key > A[j].key )cnt++;\n        B[cnt] = A[i];\n    }\n}\n```\n3.双向冒泡排序\n```cpp\n// 思想:第一趟通过交换把最大的放最后,第二趟通过交换把最小的放最前,反复进行\nvoid BubbleSort( ElemType A[], int n )\n{\n    int low = 0, high = n - 1, i;\n    bool flag = true;\n    while ( low < high && flag )\n    {\n        flag = false;\n        for (i = low; i < high; i++ )\n        {\n            if (A[i]>A[i+1] )\n            {\n                swap( A[i], A[i + 1] ); flag = true;\n            }\n        }\n        high--;\n        for ( i = high; i > low; i-- )\n        {\n            if ( A[i] < A[i - 1] )\n            {\n                swap( A[i], A[i - 1] ); flag = true;\n            }\n        }\n        low++;\n    }\n}\n```\n4.单链表的简单选择排序(假设不带表头结点)\n```cpp\nvoid SelectSort( LinkList& L )\n{\n    LinkList h, p, s, pre, r;\n    h = L;\n    while ( h )\n    {\n        p = s = h; pre = r = NULL;\n        // 找最大结点s\n        while ( p )\n        {\n            if (p->data>s->data )\n            {\n                s = p; r = pre;\n            }\n            pre = p;\n            p = p->next;\n        }\n        // 脱链\n        if ( s == h ) h = h->next;\n        else r->next = s->next;\n        // 头插法\n        s->next = L; L = s;\n    }\n}\n```\n5.顺序表中有n个不同整数(下标1~n),设计算法把所有奇数移动到偶数前面(时,空都最少)\n```cpp\nvoid Move( ElemType A[], int n )\n{\n    int low = 1, high = n;\n    while ( low < high )\n    {\n        while ( low < high&&A[low] % 2 ) low++;\n        while ( low < high && A[high] % 2 == 0 ) high--;\n        if ( low < high )\n        {\n            swap( A[low], A[high] );\n            low++; high--;\n        }\n    }\n}\n```\n6.在顺序表中找出第k小的元素(时空最少)\n```cpp\n// 思想:划分\nint Partition( ElemType R[], int low, int high )\n{\n    int pivot = R[low];\n    while ( low < high )\n    {\n        while ( low < high && R[high].key >= pivot.key ) high--;\n        R[low] = R[high];\n        while ( low < high&& R[low].key <= pivot.key ) low++;\n            R[high] = R[low];\n    }\n    R[low] = pivot;\n    return low;\n}\n\nElemType Kth_elem( ElemType R[], int low, int high, int k )\n{\n    int pivotpos = Partition( R, low, high );\n    if ( pivotpos == k ) return R[pivotpos];\n    else if ( pivotpos > k ) return Kth_elem( R, low, pivotpos - 1, k );\n    else return Kth_elem( R, pivotpos + 1, high, k );\n}\n```\n7.n个正整数构成的集合A,将其划分为两个不相交的子集$A1,A2$,元素个数分别是n1和n2.A1和A2中元素之和分别为S1和S2.设计一个时空高效算法,使|n1-n2|最小且|s1-s1|最大.(下标从1开始)\n```cpp\nint Partition( ElemType R[], int low, int high )\n{\n    int pivot = R[low];\n    while ( low < high )\n    {\n        while ( low < high && R[high].key >= pivot.key ) high--;\n        R[low] = R[high];\n        while ( low < high&& R[low].key <= pivot.key ) low++;\n        R[high] = R[low];\n    }\n    R[low] = pivot;\n    return low;\n}\n\nint SetPartition( ElemType R[], int n, int low, int high )\n{\n    int k = n / 2, s1, s2, i;\n    int pivotpos = Partition( R, low, high );\n    if ( pivotpos == k )\n    {\n        s1 = s2 = 0;\n        for ( i = 1; i <= k; i++ ) s1 += R[i];\n        for ( j = k + 1; j <= n; j++ ) s2 += R[j];\n        return s2 - s1;\n    }\n    else if ( pivotpos > k )\n        return SetPartition( R, n, low, pivotpos - 1 );\n    else return SetPartition( R, n, pivotpos + 1, high );\n}\n```","tags":["排序"],"categories":["algorithm"]},{"title":"[leetcode]133.克隆图","url":"%2Fposts%2F12189%2F","content":"{% asset_img 453425-20190430181854853-2091334093.png %}\n### 方法一:dfs(递归)\n\n```cpp\nmap<Node*,Node*> dict;\nNode* clone(Node* node)\n{\n    if (!node) return node;\n    if (dict.count(node)) return dict[node];\n    dict[node]=new Node(node->val,vector<Node*>{});　　// 这里不能写clone(node),会导致死循环,记住,在new的时候千万不要再递归,递归最低层一定有一个明确结果,所以要把截止条件写清楚\n    for(auto it:node->neighbors)\n        dict[node]->neighbors.push_back(clone(it));\n    return dict[node];\n}\nNode* cloneGraph(Node* node) \n{\n    return clone(node);\n}\n```\n### 方法二:dfs(非递归)\n\n```cpp\nmap<Node*,Node*> dict;\nNode* cloneGraph(Node* node) \n{\n    stack<Node*> S;\n    S.push(node);\n    while (!S.empty())\n    {\n        Node *p = S.top();\n        S.pop();\n        if (!dict.count(p))　　// 从栈中出来的都是没有进行访问过的点\n            dict[p]=new Node(p->val,vector<Node*>{});\n        for (auto it:p->neighbors)\n        {\n            if (!dict.count(it))　　// 判断是否已经访问过该点\n            {\n                dict[it]=new Node(it->val,vector<Node*>{});\n                S.push(it);\n            }\n            dict[p]->neighbors.push_back(dict[it]);　　// 将新点的拷贝放入neighbors中\n        }\n    }\n    return dict[node];\n}\n```","tags":["图"],"categories":["OJ"]},{"title":"[leetcode]138.复制带随机指针的链表","url":"%2Fposts%2F48962%2F","content":"{% asset_img 453425-20190430193735900-662137168.png %}\n### 方法一:递归\n```cpp\nunordered_map<Node*,Node*> dict;\nNode* copyRandomList(Node* head) \n{\n    if (!head) return head;\n    if (dict.count(head)) return dict[head];\n    dict[head]=new Node(head->val, nullptr, nullptr);\n    dict[head]->next=copyRandomList(head->next);\n    dict[head]->random=copyRandomList(head->random);\n    return dict[head];\n}\n```\n### 方法二:非递归\n```cpp\nNode* copyRandomList(Node* head) \n{\n    if (!head) return head;\n    unordered_map<Node*,Node*> m;\n    Node *p=head;\n    while(p)    // make a copy of nodes\n    {\n        m[p]=new Node(p->val,nullptr,nullptr);\n        p=p->next;\n    }\n    p=head;\n    while(p)    // link everyone and fill the random field\n    {\n        m[p]->next=m[p->next];\n        m[p]->random=m[p->random];\n        p=p->next;\n    }\n    return m[head];\n}\n```","tags":["链表"],"categories":["OJ"]},{"title":"[leetcode]144.二叉树的前序遍历","url":"%2Fposts%2F11169%2F","content":"{% asset_img 453425-20190501113746021-447522744.png %}\n前往二叉树的: [**前序**](https://brianyi.github.io/post/11169),[**中序**](https://brianyi.github.io/post/40851),[**后序**](https://brianyi.github.io/post/34771) 遍历算法\n### 方法一:递归\n```cpp\nvector<int> res;\nvector<int> preorderTraversal(TreeNode* root) \n{\n    if (!root) return res;\n    res.push_back(root->val);\n    if (root->left) preorderTraversal(root->left);\n    if (root->right) preorderTraversal(root->right);\n    return res;\n}\n```\n### 方法二:非递归\n```cpp\nvector<int> preorderTraversal(TreeNode* root) \n{\n    vector<int> res;\n    if (!root) return res;\n    stack<TreeNode*> S;\n    TreeNode* p = root;\n    while(p||!S.empty())\n    {\n        if (p)  // 访问左子树\n        {\n            res.push_back(p->val);\n            S.push(p);\n            p=p->left;\n        }\n        else    // 访问右子树\n        {\n            p=S.top();\n            S.pop();\n            p=p->right;\n        }\n    }\n    return res;\n}\n```\n### 方法三:非递归(该方法可用于后序遍历,需要修改几处代码)\n```cpp\nvector<int> res;\nvector<int> preorderTraversal(TreeNode* root) \n{\n    if (!root) return res;\n    stack<TreeNode*> S;\n    S.push(root);\n    while (!S.empty())\n    {\n        root=S.top();\n        S.pop();\n        if (root->right) S.push(root->right);  // 要实现后序遍历,需要以下两行调换\n        if (root->left) S.push(root->left);\n        res.push_back(root->val);   // res.insert(0,root->val)即为后序遍历\n    }\n    return res;\n}\n```\n结论:\n- 方法三这种形式只适合前序和后序遍历,不适合中序遍历,中序遍历较为麻烦\n- 方法二这种形式只适合前序和中序遍历,不适合后序遍历,后序遍历较为麻烦","tags":["递归"],"categories":["OJ"]},{"title":"[leetcode]94.二叉树的中序遍历","url":"%2Fposts%2F40851%2F","content":"{% asset_img 453425-20190501120139941-1568037684.png %}\n前往二叉树的: [**前序**](https://brianyi.github.io/post/11169),[**中序**](https://brianyi.github.io/post/40851),[**后序**](https://brianyi.github.io/post/34771) 遍历算法\n\n### 方法一:递归\n```cpp\nvector<int> res;\nvector<int> inorderTraversal(TreeNode* root) \n{\n    if (!root) return res;\n    if (root->left) inorderTraversal(root->left);      \n    res.push_back(root->val);\n    if (root->right) inorderTraversal(root->right);\n    return res;\n}\n```\n### 方法二:非递归\n```cpp\nvector<int> inorderTraversal(TreeNode* root) \n{\n    vector<int> res;\n    if (!root) return res;\n    stack<TreeNode*> S;\n    TreeNode* p = root;\n    while(p||!S.empty())\n    {\n        if (p)\n        {\n            S.push(p);\n            p=p->left;\n        }\n        else\n        {\n            p=S.top();\n            S.pop();\n            res.push_back(p->val);\n            p=p->right;\n        }\n    }\n    return res;\n}\n```","tags":["递归"],"categories":["OJ"]},{"title":"[leetcode]145.二叉树的后序遍历","url":"%2Fposts%2F34771%2F","content":"{% asset_img 453425-20190501122117295-85727166.png %}\n前往二叉树的: [**前序**](https://brianyi.github.io/post/11169),[**中序**](https://brianyi.github.io/post/40851),[**后序**](https://brianyi.github.io/post/34771) 遍历算法\n### 方法一:递归\n```cpp\nvector<int> res;\nvector<int> postorderTraversal(TreeNode* root) \n{\n    if (!root) return res;\n    if (root->left) postorderTraversal(root->left);\n    if (root->right) postorderTraversal(root->right);\n    res.push_back(root->val);\n    return res;\n}\n```\n### 方法二:非递归\n```cpp\nvector<int> postorderTraversal(TreeNode* root) \n{\n    vector<int> res;\n    if (!root) return res;\n    stack<TreeNode*> S;\n    TreeNode* p=root, *r=nullptr;\n    while (p||!S.empty())\n    {\n        if (p)\n        {\n            S.push(p);\n            p=p->left;\n        }\n        else\n        {\n            p=S.top();\n            if (p->right&&p->right!=r)\n                p=p->right;\n            else\n            {\n                S.pop();\n                res.push_back(p->val);\n                r=p;\n                p=nullptr;\n            }\n        }\n    }\n    return res;\n    }\n```\n### 方法三:非递归\n```cpp\nvector<int> postorderTraversal(TreeNode* root) \n{\n    vector<int> res;\n    if (!root) return res;\n    stack<TreeNode*> S;\n    TreeNode* p=root;\n    S.push(p);\n    while (!S.empty())\n    {\n        p=S.top();\n        S.pop();\n        if (p->left) S.push(p->left);\n        if (p->right) S.push(p->right);\n        res.insert(res.begin(),p->val);\n    }\n    return res;\n}\n```","tags":["递归"],"categories":["OJ"]},{"title":"[leetcode]968.监控二叉树","url":"%2Fposts%2F45343%2F","content":"{% asset_img 453425-20190501230642291-677698823.png %}\n解题思路: 由于叶子节点一定不要安装监视器,这样才能使总监视器数量比较少,因此需要从下往上进行判断当前节点的状态(共:3种状态):\n- 0: 当前节点安装了监视器\n- 1: 当前节点可观,但没有安装监视器\n- 2: 当前节点不可观\n对于空节点,我们认为是可观,但没有安装监视器,因此,叶子节点就为不可观的了,设想一个节点的左右孩子(为空)都可观且没有安装监视器,那该节点必然是不可观即2\n\n有了以上对空节点和叶子节点的处理,我们再来正式分析非终端节点:\n\n- 若一个节点的左孩子或右孩子不可观,那么该节点必然不可观,需要安装监视器,因此返回0状态\n- 若一个节点的左孩子或右孩子都可观且至少有一个安装了监视器,那么该节点必然是可观的,返回1状态\n- 若一个节点的左右孩子都可观且没安装监视器,那么该节点必然是不可观的,返回2状态\n记住,我们以上的分析都是基于从整个二叉树的叶子节点往根部,即从下往上进行,而且要做的就是将不可观的节点变得可观才行(因此要根据左右孩子的节点的状态来判断当前节点状态并做出调整)\n\n这里可能会有疑惑,以上的第一条得出当前节点不可观,然后安装了监视器,而第三条也得出当前节点不可观,但却没有安装监视器,而是直接返回的2状态(当前节点不可观).这是为什么?\n\n因为,对于第一条,因为左右孩子都不可观,为了让左右孩子都可观,则必须给当前节点安装监视器才行,而第三条中,左右孩子都是可观的(没有安装监视器),当前节点的可以直接返回不可观状态,因为后面可以由他的父节点进行摄像头安装,使其变得可观.\n\n### 方法一:递归\n```cpp\n// 0：该节点安装了监视器 1：该节点可观，但没有安装监视器 2：该节点不可观\nint monitor = 0;\nint state(TreeNode* node)\n{\n    if (node == nullptr) return 1;\n    int left  = state(node->left);\n    int right = state(node->right);\n    // 该节点为0的情况\n    if (left == 2 || right == 2)\n    {\n        monitor++;  // 由于左或右节点不可观,则需要给当前节点安装监视器,为0状态\n        return 0;\n    } // 为1的情况\n    else if (left == 0 || right == 0)\n        return 1;   // 当(left!=2&&right!=2)时,才会进行该判断,也就是左右节点一定是可观的,再判断是否有一个安装了监视器,如有安装,则当前节点就不需要安装监视器也可观了,为1状态\n    // 为2的情况\n    else    // 其他:党(left!=2&&right!=2)&&(left!=0&&right!=0),即left==1&&right==1时,左右节点都可观,但没有监视器,当前节点不可观,为2状态\n        return 2;\n}\nint minCameraCover(TreeNode *root)\n{\n    if (root == nullptr) return 0;\n    if (state(root) == 2) monitor++;    // 如果根节点为2的状态,需要加一个监视器\n    return monitor;\n}\n```\n注意:这里的if,else if,else的顺序是不能变的,先判断左右都是不可观的,再就是都可观,左或右至少有一个为监视器,最后才是都可观都无监视器.","tags":["二叉树"],"categories":["OJ"]},{"title":"[leetcode]106.从中序与后序遍历序列构造二叉树","url":"%2Fposts%2F49088%2F","content":"{% asset_img 453425-20190502213938801-1912607084.png %}\n前往 [**中序,后序遍历构造二叉树**](https://brianyi.github.io/posts/49088), [**中序,前序遍历构造二叉树**](https://brianyi.github.io/posts/8827)\n```cpp\nTreeNode* build(vector<int>& inorder, int l1, int r1, vector<int>&postorder, int l2, int r2)\n{\n    if (l1>r1) return nullptr;\n    int x = postorder[r2], i = 0;   // 确定当前根节点\n    for (i = l1; i <= r1 && inorder[i] != x; ++i);  // 在中序遍历序列中找到当前根节点位置(该位置可以划分出左右两个分支)\n    int llen = i - l1;  // 左子树结点数量\n    int rlen = r1 - i;  // 右子树结点数量\n    TreeNode* p = new TreeNode(x);  // 建立根节点\n    p->left = build(inorder, l1, l1 + llen - 1, postorder, l2, l2 + llen - 1);  // 递归建立左子树,-1,-1是把当前根节点位置去掉\n    p->right = build(inorder, r1 - rlen + 1, r1, postorder, r2 - rlen, r2 - 1); // 递归建立右子树,+1,-1是把当前根节点位置去掉\n    return p;\n}\nTreeNode* buildTree(vector<int>& inorder, vector<int>& postorder) {\n    if (inorder.empty()||postorder.empty()) return nullptr;\n    return build(inorder, 0, inorder.size() - 1, postorder, 0, postorder.size() - 1);\n}```","tags":["递归"],"categories":["OJ"]},{"title":"[leetcode]105.从前序与中序遍历序列构造二叉树","url":"%2Fposts%2F8827%2F","content":"{% asset_img 453425-20190502215802448-663626721.png %}\n前往 [**中序,后序遍历构造二叉树**](https://brianyi.github.io/post/49088), [**中序,前序遍历构造二叉树**](https://brianyi.github.io/posts/8827)\n``` cpp\nTreeNode* build(vector<int>& preorder, int l1, int r1, vector<int>& inorder, int l2, int r2)\n{\n    if (l1>r1) return nullptr;\n    int x=preorder[l1], i=0;    // 确定当前根节点\n    for(i=l2;inorder[i]!=x&&i<r2;++i);  // 在中序遍历序列中找到当前根节点位置(该位置可以划分出左右两个分支)\n    int llen=i-l2;  // 左子树结点数量\n    int rlen=r2-i;  // 右子树结点数量\n    TreeNode *p = new TreeNode(x);  // 建立根节点\n    p->left = build(preorder, l1+1, l1+llen, inorder, l2, l2+llen-1);   // 递归建立左子树,+1,-1是把当前根节点位置去掉\n    p->right= build(preorder, r1-rlen+1, r1, inorder, r2-rlen+1, r2);   // 递归建立右子树,+1,+1是把当前根节点位置去掉\n    return p;\n}\nTreeNode* buildTree(vector<int>& preorder, vector<int>& inorder) \n{\n    return build(preorder, 0, preorder.size()-1, inorder, 0, inorder.size()-1);\n}\n```","tags":["递归"],"categories":["OJ"]},{"title":"[leetcode]114.二叉树展开为链表","url":"%2Fposts%2F52504%2F","content":"{% asset_img 453425-20190502231706813-609020347.png %}\n思路:递归,将左子树变成单链表形式,再将右子树变成单链表形式,最后将左子树单链表的末端连接到右子树单链表表头,将根节点的左孩子置空\n### 方法一: 递归\n``` cpp\nvoid flatten(TreeNode* root) \n{\n    if (root==nullptr) return;\n    flatten(root->left);    // 将左子树变成单链表形式\n    flatten(root->right);   // 将右子树变成单链表形式\n    if (root->left) // 将左子树单链表的末端连接到右子树单链表表头\n    {\n        TreeNode* p=root->left;\n        while(p->right) p=p->right;\n        p->right=root->right;\n        root->right=root->left;\n        root->left=nullptr;\n    }\n}\n```","tags":["递归"],"categories":["OJ"]},{"title":"[leetcode]239.滑动窗口最大值","url":"%2Fposts%2F45513%2F","content":"{% asset_img 453425-20190503094513334-1691572073.png %}\n思路:滑动窗口的思想,只要是求连续子序列或者子串问题,都可用滑动窗口的思想\n### 方法一: 滑动窗口\n``` cpp\nvector<int> maxSlidingWindow(vector<int>& nums, int k) \n{\n    vector<int> res;\n    if (nums.size()==0) return res;\n    int i=0;\n    deque<int> dq;　　\n    for (i=0;i<nums.size();++i)\n    {\n        while(!dq.empty()&&nums[i]>nums[dq.back()]) //在尾部添加元素，并保证左边元素都比尾部大\n            dq.pop_back();\n        dq.push_back(i);\n        if (i-k==dq.front())    //在头部移除元素\n            dq.pop_front();\n        if (i>=k-1)\n            res.push_back(nums[dq.front()]);    // 存放每次窗口内的最大值\n    }\n    return res;\n}\n```","tags":["滑动窗口"],"categories":["OJ"]},{"title":"[leetcode]76.最小覆盖子串","url":"%2Fposts%2F39410%2F","content":"{% asset_img 453425-20190503105807149-1455260747.png %}\n思路:滑动窗口思想\n### 方法一:滑动窗口\n``` cpp\nstring minWindow(string s, string t) {\n    // 1.tdict记录T中每个字母与字母个数\n    // 2.维护一个滑动窗口字母的计数表sdict,计数当前窗口内T中字母出现的次数\n    // 3.当窗口内T中字母出现的次数大于等于T中每个字母出现的次数一样,这时第一个最短子串出现,再逐步从左边缩短窗口,直到不满足上述条件,然后再从右边扩大窗口,直到满足条件时,再进行最短子串长度对比,一直更新最短长度子串直到结束\n    if (s.size()<t.size()||s.size()==0) return \"\";\n    unordered_map<char,int> tdict,sdict;\n    int l=0,r=0,k=t.size();\n    for(auto it:t)  // 填充T的字母与字母计数表\n    {\n        if (!tdict.count(it)) tdict[it]=1;\n        else tdict[it]++;\n    }\n    string res=\"\";\n    for(r=0;r<s.size();++r)\n    {\n        if (tdict.count(s[r])) // 有字符,则进行记录\n        {\n            if (!sdict.count(s[r]))\n                sdict[s[r]]=0;\n            sdict[s[r]]++;\n            if (sdict[s[r]]<=tdict[s[r]])\n                k--;\n        }\n        while(k==0) // 满足条件,滑动窗口从左边逐步缩短,直到剔除第一个属于T中的字符为止\n        {\n            if (res.empty()||r-l+1<res.size())  // 最短子串更新\n                res=s.substr(l,r-l+1);\n            if(tdict.count(s[l]))\n            {\n                sdict[s[l]]--;\n                if (sdict[s[l]]<tdict[s[l]])\n                    k++;\n            }\n            l++;\n        }\n    }\n    return res;\n}\n```","tags":["滑动窗口"],"categories":["OJ"]},{"title":"[leetcode]3.无重复字符的最长子串","url":"%2Fposts%2F37922%2F","content":"{% asset_img 453425-20190503112154412-1836563639.png %}\n思路:滑动窗口的思想\n### 方法一:滑动窗口\n``` cpp\nint lengthOfLongestSubstring(string s) \n{\n    /*\n        控制一个滑动窗口,窗口内的字符都是不重复的,通过set可以做到判断字符是否重复\n    */\n    unordered_set<char> set;\n    size_t maxL=0;\n    for(int l=0,r=0;r<s.size();++r)\n    {\n        if (!set.count(s[r]))   // 当前判断的元素不存在于滑动窗口[l,r-1]中\n            set.insert(s[r]);   // 将元素放入滑动窗口(即记录不重复字符)\n        else    // 当前判断的元素已经存在于滑动窗口[l,r-1]中\n        {\n            while(set.count(s[r]))  // 从左缩短窗口,直到剔除当前判断的元素为止\n                set.erase(s[l++]);\n            set.insert(s[r]);   // 将当前判断元素放入到滑动窗口中\n        }\n        maxL=max(maxL,set.size());  // 更新无重复字符的最长子串\n    }\n    return maxL;\n}\n```","tags":["滑动窗口"],"categories":["OJ"]},{"title":"[leetcode]1028.从先序遍历还原二叉树","url":"%2Fposts%2F38760%2F","content":"{% asset_img 453425-20190503131537591-466226739.png %}\n思路:用一个栈来管理树的层次关系,索引代表节点的深度\n### 方法一:\n``` cpp\nTreeNode* recoverFromPreorder(string S) \n{\n    /*\n        由题意知,最上层节点深度为0(数字前面0条横线),而第二层节点前有1条横线,表示深度为1\n        树的前序遍历: 根-左-右\n        因此,\n    */\n    if (S.empty()) return nullptr;\n    vector<TreeNode*> stack;  // 结果栈\n    for(int i=0,depth=0,val=0;i<S.size();)\n    {\n        for(depth=0;i<S.size()&&S[i]=='-';++i)  // 计算节点的深度\n            depth++;\n        for(val=0;i<S.size()&&S[i]!='-';++i)    // 计算数值\n            val=val*10+S[i]-'0';\n        while (stack.size()>depth)    // 若当前栈的长度(树的高度)大于节点的深度,则可以把栈中最后几个节点pop掉(这些节点各已经成为完整的子树,可以pop掉了)\n            stack.pop_back();\n        TreeNode* node=new TreeNode(val);   // 新建节点用于存放当前深度的结点\n        if (!stack.empty()) // 节点间关联\n        {\n            if (!stack.back()->left)      stack.back()->left=node;\n            else if(!stack.back()->right) stack.back()->right=node;\n        }\n        stack.push_back(node);\n    }\n    return stack[0];\n}\n```\n","tags":["树"],"categories":["OJ"]},{"title":"[leetcode]5040.边框着色","url":"%2Fposts%2F3454%2F","content":"{% asset_img 453425-20190430173552247-46622747.png %}\n### 方法一：dfs的非递归形式\n``` cpp\nusing ll=long long;\nconst ll MAXN=50LL;\nunordered_set<ll> vis,mark;\nvector<vector<int>> colorBorder(vector<vector<int>>& G, int r0, int c0, int color) {\n    queue<ll> Q;\n    Q.push(r0*MAXN+c0);\n    int c=G[r0][c0];\n    int dx[]={-1,1,0,0},dy[]={0,0,-1,1};\n    while (!Q.empty())\n    {\n        int x=Q.front()/MAXN;\n        int y=Q.front()%MAXN;\n        Q.pop();\n        vis.insert(x*MAXN+y);\n        if (x==0||x==G.size()-1||y==0||y==G[0].size()-1)    // 边界方块可变色\n            mark.insert(x*MAXN+y);\n        else if (G[x-1][y]!=c||G[x+1][y]!=c||G[x][y-1]!=c||G[x][y+1]!=c)    // 四个方向中,任意一个方块颜色不同,则可变色\n            mark.insert(x*MAXN+y);\n        for (int d=0;d<4;d++)   // 放入连通分量的所有方块\n        {\n            int nx=x+dx[d],ny=y+dy[d];\n            if (0<=nx&&nx<G.size()&&0<=ny&&ny<G[0].size()&&!vis.count(nx*MAXN+ny)&&G[nx][ny]==c)\n                Q.push(nx*MAXN+ny);\n        }\n    }\n    for (auto it:mark)\n        G[it/MAXN][it%MAXN]=color;\n    return G;\n}\n```\n思路:用vis记录访问过的方块,mark标记连通分量中需要修改颜色的方块,并非连通分量中所有的方块都要修改颜色,比如:一个方块如果四周(四个方向邻接的)都是相同颜色,那么只需要修改四周方块的颜色,而自己颜色不变(开始的时候没理解题意,以为只要是连通分量内的方块颜色都需要改变)\n\n### 方法二: dfs递归形式,只不过把上面的非递归改为递归了\n``` cpp\nusing ll=long long;\nconst ll MAXN=50LL;\nunordered_set<ll> vis,mark;\nvoid dfs(vector<vector<int>>& G, int x, int y, int c)\n{\n    int dx[]={-1,1,0,0},dy[]={0,0,-1,1};\n    vis.insert(x*MAXN+y);\n    if (x==0||x==G.size()-1||y==0||y==G[0].size()-1)    // 边界方块可变色\n        mark.insert(x*MAXN+y);\n    else if (G[x-1][y]!=c||G[x+1][y]!=c||G[x][y-1]!=c||G[x][y+1]!=c)    // 四个方向中,任意一个方块颜色不同,则可变色\n        mark.insert(x*MAXN+y);\n    for (int d=0;d<4;d++)   // 放入连通分量的所有方块\n    {\n        int nx=x+dx[d],ny=y+dy[d];\n        if (0<=nx&&nx<G.size()&&0<=ny&&ny<G[0].size()&&!vis.count(nx*MAXN+ny)&&G[nx][ny]==c)\n            dfs(G,nx,ny,c);\n    }\n}\nvector<vector<int>> colorBorder(vector<vector<int>>& G, int r0, int c0, int color) {\n    dfs(G,r0,c0,G[r0][c0]);\n    for (auto it:mark)\n        G[it/MAXN][it%MAXN]=color;\n    return G;\n}\n```\n### 方法三:dfs递归,但通过修改G中的数据,来记录是否访问过,和是否需要修改颜色,国外的一个[大佬](https://leetcode.com/problems/coloring-a-border/discuss/282847/C%2B%2B-with-picture-DFS)写的\nFrom an initial point, perform DFS and flip the cell color to negative to track visited cells.\nAfter DFS is complete for the cell, check if this cell is inside. If so, flip its color back to the positive.\nIn the end, cells with the negative color are on the border. Change their color to the target color.\n{% asset_img image_1556425139.png %}\n``` cpp\nvoid dfs(vector<vector<int>>& g, int r, int c, int cl) {\n    if (r < 0 || c < 0 || r >= g.size() || c >= g[r].size() || g[r][c] != cl) return;    // 剪枝(越界,非着色块)\n    g[r][c] = -cl;    // 着色\n    dfs(g, r - 1, c, cl), dfs(g, r + 1, c, cl), dfs(g, r, c - 1, cl), dfs(g, r, c + 1, cl);\n    if (r > 0 && r < g.size() - 1 && c > 0 && c < g[r].size() - 1 && cl == abs(g[r - 1][c]) &&\n        cl == abs(g[r + 1][c]) && cl == abs(g[r][c - 1]) && cl == abs(g[r][c + 1]))    // 将原四周同色的块,颜色还原\n        g[r][c] = cl;\n}\nvector<vector<int>> colorBorder(vector<vector<int>>& grid, int r0, int c0, int color) {\n    dfs(grid, r0, c0, grid[r0][c0]);\n    for (auto i = 0; i < grid.size(); ++i)    // 根据dfs标记(负数)过的方块进行着色\n        for (auto j = 0; j < grid[i].size(); ++j) grid[i][j] = grid[i][j] < 0 ? color : grid[i][j];\n    return grid;\n}\n```\n结论: 无论是递归还是非递归,先标记(标记vis),再遍历\n","tags":["图"],"categories":["OJ"]},{"title":"[leetcode]5.最长回文子串","url":"%2Fposts%2F21336%2F","content":"{% asset_img 453425-20190505125039549-1852205917.png %}\n### 方法一:中心扩展算法\n\n解题思路:从左到右每一个字符都作为中心轴,然后逐渐往两边扩展,只要发现有不相等的字符,则确定了以该字符为轴的最长回文串,但需要考虑长度为奇数和偶数的不同情况的处理(长度为偶数时轴心为中间两个数的中心,长度为奇数时轴心为中间那个数)\n\n算法时间复杂度: $O(n^{2})$\n```cpp\nstring longestPalindrome(string s) \n{        \n    int idx = 0, maxL = 0;\n    for (int i = 0; i < s.size(); ++i)　　// i为轴的位置,j为回文串半径\n    {\n        for (int j = 0; i - j >= 0 && i + j < s.size(); ++j)    // 奇数\n        {\n            if (s[i - j] != s[i + j])\n                break;\n            if (2 * j + 1 > maxL)\n            {\n                maxL = 2 * j + 1;\n                idx = i - j;\n            }\n        }\n        for (int j = 0; i - j >= 0 && i + j + 1 < s.size(); ++j)    // 偶数\n        {\n            if (s[i-j]!=s[i+j+1])\n                break;\n            if (2 * j + 2 > maxL)\n            {\n                maxL = 2 * j + 2;\n                idx = i - j;\n            }\n        }\n    }\n    return s.substr(idx, maxL);\n}\n```\n---\n### 方法二: manacher(马拉车法)\n解题思路:详见[P3805【模板】manacher算法][1]\n为了使奇数串和偶数串一致性处理,首先进行字符填充,使其成为奇数串,即在每个字符的前后填充字符,例如:\n原串: ABCCBA\n填充后: ~#A#B#C#C#B#A#\n原始串在数组中的位置:\n\n|   A   | B    |   C   |   C   |   B   |   A   |\n| :---: | :--- | :---: | :---: | :---: | :---: |\n|   0   | 1    |   2   |   3   |   4   |   5   |\n\n填充串在数组中的位置\n\n|   ~   |   #   |   A   |   #   |   B   |   #   |   C   |   #   |   C   |   #   |   B   |   #   |   A   |   #   |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|   0   |   1   |   2   |   3   |   4   |   5   |   6   |   7   |   8   |   9   |  10   |  11   |  12   |  13   |\n\n首先说明: \n- 奇数+偶数=奇数, 因此,奇数串填充偶数个#后为奇数串,偶数串填充奇数个#后为奇数串\n- ~字符用来作为边界,用处在于进行两边扩展时做为结束条件\n- 填充串中字符的最大回文半径 - 1 = 原字符串中该字符的回文串长度\n\n关于上述第3条我需要解释一下:\n\n| s_copy |   ~   |   #   |   A   |   #   |   B   |   #   |   C   |   #   |   C   |   #   |   B   |   #   |   A   |   #   |\n| :----: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| index  |   0   |   1   |   2   |   3   |   4   |   5   |   6   |   7   |   8   |   9   |  10   |  11   |  12   |  13   |\n|  pos   |   0   |   0   |   0   |   0   |   1   |   1   |   2   |   2   |   3   |   3   |   4   |   4   |   5   |   5   |\n|   p    |   0   |   1   |   2   |   1   |   2   |   1   |   2   |   7   |   2   |   1   |   2   |   1   |   2   |   1   |\n\n可以看到index=7的位置,#对应的p为7(即最大回文半径),即在C与C之间,表示原字符串中该字符的回文长度为6,那么原字符串ABCCBA的前面3个字符ABC构成的回文串长度为6\n\n>该题思路:\n>1. 字符串填充统一为奇数串\n>2. Manacher法,从左到右遍历每个字符\n>   1. 记录每个字符的最大回文半径\n>   2. 确定已经记录的最大回文串右边界r,和中间轴m\n>   3. 当前字符s[i]是否能关于m找到一个对称点,即要满足:m<=i<=r\n>      1. 能:则得到一个有可能的最大回文半径,并从该半径开始扩展\n>      2. 否:则从新计算最大回文半径\n\n算法时间复杂度为: $O(n)$\n```cpp\nint pos[2005],p[2005];  // pos用于记录填充串与原始字串的位置关系,p用于记录填充串当前字符的最大回文半径\nstring longestPalindrome(string s) \n{        \n    /* 填充字符,统一为奇数串 */\n    string s_new=\"~\";\n    for (int i=0,k=1;i<s.size();++i)\n    {\n        s_new+=\"#\";\n        s_new+=s[i];\n        pos[k++]=i;\n        pos[k++]=i;\n    }\n    s_new+=\"#\";\n    \n    /* manacher */\n    int m=0,r=0,maxL=0,idx=0;\n    for (int i=1;i<s_new.size();++i)\n    {\n        // 获取已知的最大回文半径,p[i]用于记录填充串对应字符的最大回文半径\n        if (i<r)    // m左边的最大回文半径都已经求出来了\n            p[i]=min(p[2*m-i],r-i); // 当m<=i<=r时,i关于m中心轴对称的点为2*m-i,而p[2*m-i]是一定已经知道的\n        else\n            p[i]=1; // 如果i超出了已知的最大回文右边界,则比如不能找到关于m对称的点,只能重新计算最大回文半径\n        // 暴力拓展左右两侧,计算当前的最大回文半径\n        while (s_new[i-p[i]]==s_new[i+p[i]])\n            p[i]++;\n        // 新的回文半径比较大,则更新\n        if (r-i<p[i])\n        {\n            m=i;\n            r=i+p[i];\n        }\n        // 更新回文长度(原始字串的回文长度为新字串回文半径-1)\n        if (p[i]-1>=maxL)\n        {\n            maxL=p[i]-1;\n            idx=pos[i]-maxL/2;  // 更新原始回文字串的起始位置\n        }\n    }\n    return s.substr(idx,maxL);\n}\n```\n\n[1]: https://www.luogu.org/problemnew/solution/P3805","tags":["回文串"],"categories":["OJ"]}]